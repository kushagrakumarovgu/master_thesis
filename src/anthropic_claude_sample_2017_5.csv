review,reasoning,sentiment_score,politeness_score
"The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance of the topic and states that the study provides a good baseline and contributes to the community. However, it's not overwhelmingly positive as the reviewer notes that it 'misses strong novel methods for the task'. The politeness score is 50 (moderately polite) because the language used is professional and respectful. The reviewer offers a balanced view, highlighting both strengths and limitations without using harsh or critical language. The tone is constructive and appreciative of the work's contribution, even while pointing out areas for improvement.",50,50
"This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contributions and its generally better performance on multiple datasets. However, they also point out that it's an incremental work with a lack of innovation, which tempers the positivity. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language without harsh criticism. They offer constructive feedback and suggestions for improvement, such as including a qualitative evaluation, which is a polite way to address limitations. The reviewer balances positive aspects ('highlighted part', 'better performance') with areas for improvement ('weak point', 'lack of innovation') in a respectful manner.",20,50
"Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.","The sentiment score is 80 (positive) because the reviewer expresses clear appreciation for the paper, calling it 'a pleasure to read' and stating that it provides 'clear and interesting conclusions' and 'represents a significant step forward'. The reviewer also praises the paper's presentation and experiments. The score is not 100 because the reviewer notes that the novelty is somewhat reduced due to concurrent work. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offers constructive feedback, and frames criticisms as questions or suggestions rather than direct criticisms. The reviewer also balances any potential negatives with positive comments. The score is not higher because the language, while polite, doesn't go out of its way to be exceptionally courteous.",80,70
"This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.","The sentiment score is 50 (moderately positive) because the reviewer expresses a generally favorable view of the paper, noting that it addresses a known challenge and that the results look compelling. However, they also mention not being an expert in the area and defer to other reviewers, which tempers the positivity somewhat. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive suggestions for improvement, and frames criticisms as helpful recommendations rather than harsh judgments. The reviewer also acknowledges their own potential limitations in expertise, which comes across as humble and polite. The use of phrases like 'I'm happy to support' and 'One suggestion is...' further contribute to the polite tone.",50,80
"I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.","The sentiment score is slightly positive (20) because the reviewer expresses liking the core idea and acknowledges consistent gains across tasks, but also raises several concerns and requests for additional analysis. This mix of positive and critical feedback results in a mildly positive overall sentiment. The politeness score is moderately positive (50) as the reviewer uses polite language like 'I'd like to see' and frames critiques as suggestions rather than demands. They also acknowledge the paper's strengths before diving into areas for improvement. However, the tone remains professional rather than overly deferential, keeping the score from being higher.",20,50

title,decision,year,Authors,rating_score,rating_text,review,confidence_score,confidence_text,academic_age,current_age,total_num_pub,total_num_conference,total_num_informal,total_num_journal
Maximum Entropy Flow Networks,Accept,2017,"['Gabriel Loaiza-Ganem *', 'Yuanjun Gao *', 'John P. Cunningham']","[6, 9, 6]","['Marginally above acceptance threshold', '9', 'Marginally above acceptance threshold']","This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization. The paper is clearly written and is easy to follow. Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers. Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{phi};f], where f is the unormalized likelihood f propto exp sum_k( - lambda_k T - c_k ||T_k||^2 ). This type of derivation is typical in all previous works using NFs for variational inference. A few experiments on more complex data would strengthen the paper*s results. The two experiments provided show good results but both of them are toy problems. Minor point: Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 3, 12]","[7, 6, 18]","[29, 5, 96]","[11, 4, 50]","[17, 1, 31]","[1, 0, 15]"
Metacontrol for Adaptive Imagination-Based Optimization,Accept,2017,"['Jessica B. Hamrick', 'Andrew J. Ballard', 'Razvan Pascanu', 'Oriol Vinyals', 'Nicolas Heess', 'Peter W. Battaglia']","[8, 8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","A well written paper and an interesting construction - I thoroughly enjoyed reading it. I found the formalism a bit hard to follow without specific examples- that is, it wasn*t clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well. In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework. Experimental results were presented clearly and well illustrated the usefulness of the metacontroller. I*m curious to see the results of using more metaexperts.","['3', '3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[7, 2, 8, 11, 9, 13]","[13, 7, 14, 17, 15, 19]","[43, 9, 156, 209, 200, 93]","[25, 2, 63, 101, 81, 38]","[18, 6, 87, 98, 111, 49]","[0, 1, 6, 10, 8, 6]"
Mode Regularized Generative Adversarial Networks,Accept,2017,"['Tong Che', 'Yanran Li', 'Athul Jacob', 'Yoshua Bengio', 'Wenjie Li']","[6, 4, 7, 7]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution. The presented approaches are entirely based on some intuitive arguments. As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point. The paper will benefit from more rigorous theoretical justification of the presented approaches.","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 4, 2, 30, 23]","[12, 10, 7, 36, 28]","[38, 62, 13, 977, 241]","[14, 27, 6, 405, 144]","[18, 23, 7, 456, 29]","[6, 12, 0, 116, 68]"
Mollifying Networks,Accept,2017,"['Caglar Gulcehre', 'Marcin Moczulski', 'Francesco Visin', 'Yoshua Bengio']","[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","The paper shows the relation between stochastically perturbing the parameter of a model at training time, and considering a mollified objective function for optimization. Aside from Eqs. 4-7 where I found hard to understand what the weak gradient g exactly represents, Eq. 8 is intuitive and the subsequent Section 2.3 clearly establishes for a given class of mollifiers the equivalence between minimizing the mollified loss and training under Gaussian parameter noise. The authors then introduce generalized mollifiers to achieve a more sophisticated annealing effect applicable to state-of-the-art neural network architectures (e.g. deep ReLU nets and LSTM recurrent networks). The resulting annealing effect can be counterintuitive: In Section 4, the Binomial (Bernoulli?) parameter grows from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), meaning that the network goes initially through a phase of adding noise. This might effectively have the reverse effect of annealing. Annealing schemes used in practice seem very engineered (e.g. Algorithm 1 that determines how units are activated at a given layer consists of 9 successive steps). Due to the more conceptual nature of the authors contribution (various annealing schemes have been proposed, but the application of the mollifying framework is original), it could have been useful to reserve a portion of the paper to analyze simpler models with more basic (non-generalized) mollifiers. For example, I would have liked to see simple cases, where the perturbation schemes derived from the mollifier framework would be demonstrably more suitable for optimization than a standard heuristically defined perturbation scheme.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 4, 3, 30]","[11, 7, 7, 36]","[74, 14, 20, 977]","[28, 7, 8, 405]","[38, 7, 12, 456]","[8, 0, 0, 116]"
Multi-view Recurrent Neural Acoustic Word Embeddings,Accept,2017,"['Wanjia He', 'Weiran Wang', 'Karen Livescu']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Pros: Interesting training criterion. Cons: Missing proper ASR technique based baselines. Comments: The dataset is quite small. ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP. More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection performance of out-of-vocabulary words. It would be interesting to show scatter plots for embedding vs. orthographic distances.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 8, 18]","[2, 14, 23]","[2, 107, 177]","[1, 54, 103]","[1, 49, 63]","[0, 4, 11]"
Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Accept,2017,"['Eleanor Batty', 'Josh Merel', 'Nora Brackbill', 'Alexander Heitman', 'Alexander Sher', 'Alan Litke', 'E.J. Chichilnisky', 'Liam Paninski']","[8, 7, 4]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Ok but not good enough - rejection']","This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a *movie.* My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the *frame rate* is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 6, 1, 1, 8, 8, 24, 17]","[5, 11, 6, 1, 14, 14, 30, 22]","[5, 49, 3, 1, 9, 11, 23, 108]","[4, 19, 2, 1, 7, 6, 16, 51]","[0, 25, 0, 0, 0, 0, 0, 8]","[1, 5, 1, 0, 2, 5, 7, 49]"
Neural Photo Editing with Introspective Adversarial Networks,Accept,2017,"['Andrew Brock', 'Theodore Lim', 'J.M. Ritchie', 'Nick Weston']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","The paper presents two main contributions: (1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software. (2) A hybridization of GANs and VAEs called Introspective Adversarial Network. The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together. On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models. On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a *list of things to make it work* fashion. I would like to see more empirical results in that direction to help clear up things. Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication. UPDATE: The rating has been updated to a 6 following the authors* reply.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 17, 14, 2]","[8, 22, 18, 3]","[32, 62, 38, 6]","[12, 28, 12, 2]","[19, 9, 4, 4]","[1, 25, 22, 0]"
Neural Program Lattices,Accept,2017,"['Chengtao Li', 'Daniel Tarlow', 'Alexander L. Gaunt', 'Marc Brockschmidt', 'Nate Kushman']","[4, 7, 7]","['Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","First I would like to apologize for the late review. This paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al.. This allows them to train their model with less supervision than Reed & de Freitas. Overall the model is a nice extension of NPI. While it requires less supervision than NPI, it still requires *sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy*. This may limit the scope of this work. The paper claims that their *method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic). Such sequences are relatively easy to gather in many natural settings*. It would be great if the authors clarify what they mean by *relatively easy to gather in many natural settings*. They also claim that *the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems*. However, this paper only addresses two toy problems which are neither *natural settings* nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition). In the introduction, the author states that *Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.* What are the *existing techniques*, they are referring to? This work only addresses the problem of long addition and puzzle solving in a block world. Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and Sukhbaatar et al. (*Mazebase: A sandbox for learning from games*) shows that a memory network can solve puzzles in a blockworld with little supervision. In the conclusion, the author states that *remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.* However for all the experiments, they *include a small number of FULL samples* (FULL == *samples with full program traces*). Unfortunately even if this means that they need less FULL examples, they still need *full program traces*, contradicting their final claim. Moreover, as shown figure 7, their model does not use a *small number of FULL samples* but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128. *All experiments were run with 10 different random seeds*: does the environment change as well between the runs, i.e. are the FULL examples different between the runs? If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft. Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples. Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( *123+45* would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.? Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version. Overall, it is a nice extension of Reed & de Freitas, but I*m a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work). This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before. Missing references: *Learning simple arithmetic procedures*, Cottrell et al. *Neural gpus learn algorithms*, Kaiser & Sutskever *Mazebase: A sandbox for learning from games*, Sukhbaatar et al. *Learning simple algorithms from examples*, Zaremba et al.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 12, 2, 8, 11]","[10, 18, 7, 14, 16]","[36, 83, 26, 75, 33]","[20, 46, 13, 41, 19]","[13, 35, 13, 29, 13]","[3, 2, 0, 5, 1]"
Neuro-Symbolic Program Synthesis,Accept,2017,"['Emilio Parisotto', 'Abdel-rahman Mohamed', 'Rishabh Singh', 'Lihong Li', 'Dengyong Zhou', 'Pushmeet Kohli']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program*s parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 9, 15, 17, 15]","[9, 14, 14, 20, 21, 21]","[32, 112, 133, 170, 62, 322]","[13, 50, 62, 89, 37, 180]","[17, 54, 50, 66, 17, 102]","[2, 8, 21, 15, 8, 40]"
Nonparametric Neural Networks,Accept,2017,"['George Philipp', 'Jaime G. Carbonell']","[7, 7, 5]","['Good paper, accept', 'Good paper, accept', '5']","This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 25]","[7, 31]","[20, 350]","[9, 196]","[11, 79]","[0, 75]"
Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Accept,2017,"['Mengye Ren', 'Renjie Liao', 'Raquel Urtasun', 'Fabian H. Sinz', 'Richard S. Zemel']","[7, 5, 9]","['Good paper, accept', '5', '9']","The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines. Pros: * It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution. * The paper is clearly written. * From an architectural standpoint, the actual comparisons seem well motivated. (For instance, I*m glad they tried DN* and BN* -- if they hadn*t tried those, I would have wanted them too.) Cons: * I*m not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization. (Oh, actually -- looking at the other reviews, everyone else seems to have noticed this too. I*ll not beat a dead horse about this any further.) * I*m nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models. I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks. In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn*t contribute that much to performance but was somewhat expensive computationally. Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 8, 17, 14, 29]","[8, 14, 22, 20, 35]","[65, 100, 375, 35, 243]","[29, 52, 223, 19, 136]","[36, 44, 137, 9, 83]","[0, 4, 15, 7, 24]"
"Offline bilingual word vectors, orthogonal transformations and the inverted softmax",Accept,2017,"['Samuel L. Smith', 'David H. P. Turban', 'Steven Hamblin', 'Nils Y. Hammerla']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted. The inverted Softmax idea is very nice. A few minor issues that ought to be addressed in a published version of this paper: 1) There is no mention of Haghighi et al (2008) *Learning Bilingual Lexicons from Monolingual Corpora.*, which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed. 2) Likewise, Hermann & Blunsom (2013) *Multilingual distributed representations without word alignment.* is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data. 3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages 4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion 5) I don*t have a better suggestion, but is there an alternative to using the term *translation (performance/etc.)* when discussing word alignment across languages? Translation implies something more complex than this in my mind. 6) The Mikolov citation in the abstract is messed up","['5', '5', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[2, 1, 3, 7]","[8, 1, 8, 12]","[30, 2, 5, 57]","[10, 1, 2, 34]","[20, 1, 3, 19]","[0, 0, 0, 4]"
On Detecting Adversarial Perturbations,Accept,2017,"['Jan Hendrik Metzen', 'Tim Genewein', 'Volker Fischer', 'Bastian Bischoff']","[7, 7, 5]","['Good paper, accept', 'Good paper, accept', '5']","This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance. My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant. Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[13, 6, 3, 7]","[19, 12, 8, 13]","[70, 31, 33, 16]","[33, 9, 18, 10]","[25, 18, 15, 2]","[12, 4, 0, 4]"
On the Quantitative Analysis of Decoder-Based Generative Models,Accept,2017,"['Yuhuai Wu', 'Yuri Burda', 'Ruslan Salakhutdinov', 'Roger Grosse']","[7, 7, 6]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","# Review This paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)). The authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model. # Pros Their evaluation framework is public and is definitely a nice contribution to the community. This paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution. # Cons/Questions It is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set? It is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both? 16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2? I would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others? # Minor comments Table 1 is not referenced in the text and lacks description of what the different columns represent. Figure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2). Figure 2(c), I*m guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder? Are the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ? Typo in caption of Figure 3: *(c) GMMN-10* but actually showing GMMN-50 according to the graph title and subcaption.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 4, 15, 11]","[8, 10, 21, 17]","[70, 17, 419, 123]","[28, 8, 207, 60]","[39, 9, 201, 61]","[3, 0, 11, 2]"
Online Bayesian Transfer Learning for Sequential Data Modeling,Accept,2017,"['Priyank Jaini', 'Zhitang Chen', 'Pablo Carbajal', 'Edith Law', 'Laura Middleton', 'Kayla Regan', 'Mike Schaekermann', 'George Trimponias', 'James Tung', 'Pascal Poupart']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes an online inference algorithm by using online Bayesian moment matching for HMM-GMM. The method uses transfer learning by utilizing individual sequence estimators to predict a target sequence based on a weighted combination of individual HMM-GMM. Online Bayesian moment matching has a benefit of updating HMM-GMM parameters frame-by-frame, and fits to this problem. The authors compare the proposed method with the other sequential modeling methods including RNN and EM, and show the effectiveness of the proposed method. The paper is well written overall. Comments: 1) Could you provide the average performance in table? It is difficult to compare the performance only with individual performance. Also, it seems that the EM performance is sometimes good 2) I’m curious how initialization and hyper-parameter settings affect the final performance. If you provide some information about it, that is great. 3) It would be better to provide a figure of describing the transfer-learning-based proposed methods, since this is a unique and a little bit complicated setup.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 7, 1, 18, 1, 1, 1, 5, 8, 18]","[8, 13, 1, 24, 6, 1, 7, 11, 14, 24]","[27, 75, 1, 75, 5, 1, 22, 23, 20, 195]","[12, 34, 1, 43, 5, 1, 10, 11, 13, 124]","[13, 29, 0, 18, 0, 0, 6, 6, 3, 57]","[2, 12, 0, 14, 0, 0, 6, 6, 4, 14]"
Optimal Binary Autoencoding with Pairwise Correlations,Accept,2017,['Akshay Balsubramani'],"[7, 6]","['Good paper, accept', 'Marginally above acceptance threshold']","The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[5],[10],[30],[11],[18],[1]
Paleo: A Performance Model for Deep Neural Networks,Accept,2017,"['Hang Qi', 'Evan R. Sparks', 'Ameet Talwalkar']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. The paper is very clear. The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature. Furthermore, their code is open-source and the live demo is looking good. The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful. It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 10, 10]","[15, 13, 16]","[44, 21, 126]","[19, 9, 54]","[11, 10, 61]","[14, 2, 11]"
Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,Accept,2017,"['Sergey Zagoruyko', 'Nikos Komodakis']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 14]","[9, 19]","[29, 130]","[9, 71]","[15, 28]","[5, 31]"
PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,Accept,2017,"['Tim Salimans', 'Andrej Karpathy', 'Xi Chen', 'Diederik P. Kingma']","[9, 6, 7, 7]","['9', 'Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. On a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren*t we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a *fully-generative* neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['4', '3', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 7, 3, 8]","[12, 7, 8, 14]","[60, 19, 47, 46]","[21, 9, 21, 21]","[36, 6, 24, 24]","[3, 4, 2, 1]"
PixelVAE: A Latent Variable Model for Natural Images,Accept,2017,"['Ishaan Gulrajani', 'Kundan Kumar', 'Faruk Ahmed', 'Adrien Ali Taiga', 'Francesco Visin', 'David Vazquez', 'Aaron Courville']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance. -------------- This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder. I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments. I have the following suggestions/concerns about the paper: 1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper. 2) A similar idea is also presented in a concurrent ICLR submission *Variational Lossy Autoencoder*. It would be interesting to have a discussion included in the paper and compare these works. 3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce. 4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure. I will gladly raise the score if the authors address my concerns.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12, 10, 2, 3, 8, 17]","[9, 18, 14, 6, 7, 14, 23]","[20, 57, 26, 12, 20, 116, 309]","[9, 29, 18, 5, 8, 47, 135]","[11, 17, 6, 7, 12, 52, 160]","[0, 11, 2, 0, 0, 17, 14]"
Pointer Sentinel Mixture Models,Accept,2017,"['Stephen Merity', 'Caiming Xiong', 'James Bradbury', 'Richard Socher']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets. The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application. While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1: “Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.” As far as I can tell, your model also uses the recent hidden state to form a query vector, which is matched by the pointer network to previous words. Can you please clarify what you mean here? In addition, quoting from section 3 which describes the model of Gulcehre et al.: “Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use” This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text. Finally, in the following quote, also from section 3: “The pointer network is not used as a source of information for the switching network as in our model.” It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is. With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well? I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 9, 3, 11]","[9, 15, 6, 17]","[14, 385, 18, 229]","[6, 165, 9, 111]","[8, 210, 9, 111]","[0, 10, 0, 7]"
Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,Accept,2017,"['Jacek M. Bajor', 'Thomas A. Lasko']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","This is a well written, organized, and presented paper that I enjoyed reading. I commend the authors on their attention to the narrative and the explanations. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning. That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture. A few points of criticism: -The numerical results are in my view too brief. Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1. I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances. To be clear I don*t think this is disqualifying or deeply concerning; I simply found it a bit underwhelming. - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance. I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure. - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader. I saw that another reviewer suggested perhaps ICLR is not the right venue for this work. While I certainly see the reviewer*s point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). Overall, a nice paper.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 17]","[1, 23]","[1, 61]","[1, 26]","[0, 14]","[0, 21]"
Program Synthesis for Character Level Language Modeling,Accept,2017,"['Pavol Bielik', 'Veselin Raychev', 'Martin Vechev']","[5, 8]","['5', 'Top 50% of accepted papers, clear accept']","This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 9, 15]","[10, 13, 21]","[26, 28, 204]","[18, 25, 137]","[6, 2, 51]","[2, 1, 16]"
Pruning Convolutional Neural Networks for Resource Efficient Inference,Accept,2017,"['Pavlo Molchanov', 'Stephen Tyree', 'Tero Karras', 'Timo Aila', 'Jan Kautz']","[7, 9, 6]","['Good paper, accept', '9', 'Marginally above acceptance threshold']","Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network*s activation. It is possible that a modified criterion - eqn (7) + lambda feature activation, (where lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don*t think this missing comparison is justified. (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary). (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts. (3) Iterative finetuning. However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 8, 15, 20]","[9, 16, 14, 21, 26]","[68, 50, 55, 69, 397]","[33, 25, 26, 28, 194]","[34, 25, 20, 20, 141]","[1, 0, 9, 21, 62]"
Pruning Filters for Efficient ConvNets,Accept,2017,"['Hao Li', 'Asim Kadav', 'Igor Durdanovic', 'Hanan Samet', 'Hans Peter Graf']","[7, 7, 6, 7]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","The idea of *pruning where it matters* is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too. Extra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (mathbb symbol - two thumbs up!). The experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case? My question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the *min sum of weights* criterion compares to other approaches. How does it compare to other pruning criteria? Is it better than *pruning at random*? Overall, I liked the paper.","['4', '5', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 28, 43, 38]","[7, 16, 28, 49, 44]","[11, 48, 13, 330, 69]","[7, 23, 10, 201, 46]","[4, 21, 1, 10, 10]","[0, 4, 2, 119, 13]"
Quasi-Recurrent Neural Networks,Accept,2017,"['James Bradbury', 'Stephen Merity', 'Caiming Xiong', 'Richard Socher']","[6, 7, 5, 7]","['Marginally above acceptance threshold', 'Good paper, accept', '5', 'Good paper, accept']","This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication. The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. While I feel that the contribution is somewhat incremental, I recommend acceptance.","['4', '4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-1, 22]","[4, 27]","[5, 77]","[5, 38]","[0, 1]","[0, 38]"
Query-Reduction Networks for Question Answering,Accept,2017,"['Minjoon Seo', 'Sewon Min', 'Ali Farhadi', 'Hannaneh Hajishirzi']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The paper proposed a simple and effective model for QA. The paper is easy to read and result is impressive on the synthetic and real dataset. The one question is the paper is called query-reduction, but there is no place to show this reduction explicitly.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 1, 15, 11]","[10, 7, 21, 17]","[69, 51, 241, 238]","[26, 23, 117, 116]","[43, 28, 110, 119]","[0, 0, 14, 3]"
Reasoning with Memory Augmented Neural Networks for Language Comprehension,Accept,2017,"['Tsendsuren Munkhdalai', 'Hong Yu']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments: 1. Actually the model in the paper is not single model, it proposed two models. One consists of *reading*, *writing*, *adaptive computation* and * Answer module 2*, the other one is *reading*, *composing*, *writing*, *gate querying* and *Answer module 1*. Based on the method section and the experiment, it seems the *adaptive computation* model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine. 2. What is the MLP setting in the composing module? 3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers? 4. It needs more ablation study about using different T such as T=1,2.. 5. According to my understanding, for the adaptive computation, it would stop when the P_T <0. So what is the distribution of T in the testing data?","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 19]","[13, 25]","[52, 137]","[24, 69]","[17, 36]","[11, 32]"
Recurrent Batch Normalization,Accept,2017,"['Tim Cooijmans', 'Nicolas Ballas', 'César Laurent', 'Çağlar Gülçehre', 'Aaron Courville']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[20, 2, 15, 21]","[26, 7, 21, 27]","[973, 13, 231, 1196]","[440, 8, 140, 568]","[87, 1, 14, 44]","[446, 4, 77, 584]"
Recurrent Environment Simulators,Accept,2017,"['Silvia Chiappa', 'Sébastien Racaniere', 'Daan Wierstra', 'Shakir Mohamed']","[7, 5, 8]","['Good paper, accept', '5', 'Top 50% of accepted papers, clear accept']","[UPDATE] After going through the response from the author and the revision, I increased my review score for two reasons. 1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it. This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions. It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text). 2. The revised paper contains more comprehensive results than before. The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive. - Summary This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment. - Novelty The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. - Experiment The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration. - Clarity The paper is well-written and easy to follow. - Overall Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community. [Reference] Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[14, 7, 14, 12]","[20, 13, 16, 17]","[38, 40, 70, 66]","[17, 11, 35, 27]","[18, 25, 27, 33]","[3, 4, 8, 6]"
Recurrent Hidden Semi-Markov Model,Accept,2017,"['Hanjun Dai', 'Bo Dai', 'Yan-Ming Zhang', 'Shuang Li', 'Le Song']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. That being said, I think two aspects may need further improvement: (1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification). (2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 1, 2, 15, 13, 14]","[21, 7, 7, 21, 19, 20]","[564, 15, 31, 317, 105, 267]","[260, 5, 14, 147, 60, 107]","[66, 5, 12, 137, 23, 89]","[238, 5, 5, 33, 22, 71]"
Recurrent Mixture Density Network for Spatiotemporal Visual Attention,Accept,2017,"['Loris Bazzani', 'Hugo Larochelle', 'Lorenzo Torresani']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite Kümmerer et. al 2015 as a model which *learns ... indirectly rather than from explicit information of where humans look*, however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 13, 21]","[15, 19, 27]","[42, 162, 154]","[27, 64, 83]","[8, 74, 61]","[7, 24, 10]"
Regularizing CNNs with Locally Constrained Decorrelations,Accept,2017,"['Pau Rodríguez', 'Jordi Gonzàlez', 'Guillem Cucurull', 'Josep M. Gonfaus', 'Xavier Roca']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 18, 2, 8, 21]","[8, 24, 7, 13, 27]","[75, 138, 16, 16, 65]","[23, 71, 5, 5, 34]","[39, 11, 8, 4, 4]","[13, 56, 3, 7, 27]"
Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,Accept,2017,"['Mohammad Babaeizadeh', 'Iuri Frosio', 'Stephen Tyree', 'Jason Clemons', 'Jan Kautz']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors* own implementation of A3C as well as to published reference scores. The paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: https://github.com/muupan/async-rl One or two plots like that would complete this paper very nicely. ----------------------------------- I appreciate the additional experiments included in the revised version of the paper. The learning speed comparison makes the paper more complete and I’m slightly revising my score to reflect that. Having said that, there is still no clear demonstration that the higher throughput of GA3C leads to consistently faster learning. With the exception of Pong, the training curves in Figure 6 seem to significantly underperform the original A3C results or even the open source implementation of A3C on Breakout and Space Invaders (https://github.com/muupan/async-rl).","['5', '5', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[6, 15, 10, 7, 20]","[11, 21, 16, 12, 26]","[25, 56, 50, 22, 397]","[11, 27, 25, 15, 194]","[14, 15, 25, 4, 141]","[0, 14, 0, 3, 62]"
Revisiting Classifier Two-Sample Tests,Accept,2017,"['David Lopez-Paz', 'Maxime Oquab']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","I would like first to apologize for the delay. Summary: A framework for two-samples statistical test using binary classification is proposed. It allows multi-dimensional sample testing and an interpretability that other tests lack. A theoritical analysis is provided and various empirical tests reported. A very interesting approach. I have however two main concerns. The clarity of the presentation is obscured by too much content. It would be more interesting if the presentation could be somewhat self-contained. You could consider making 2 papers out of this paper. Seriously, you cram a lot of experiments in this paper. But the setting of the experiments is not really explained. We are supposed to have read Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All this is okay but reduces your public to a very few. For example, if I am not mistaken, you never explained what SCF is, despite the fact that its performances are reported. As a second point, given also that the number of submissions to this conference are exploding, I would like to challenge you with the following question: Why is this work significant to the representation learning community?","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 4]","[13, 10]","[60, 18]","[28, 8]","[27, 9]","[5, 1]"
SGDR: Stochastic Gradient Descent with Warm Restarts,Accept,2017,"['Ilya Loshchilov', 'Frank Hutter']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks Working on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another. Revision: - the authors substantially improved the contents of the paper, including experiments on another set than Cifar - the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate I have therefore improved my rating","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 16]","[10, 22]","[41, 252]","[23, 103]","[16, 120]","[2, 29]"
SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,Accept,2017,"['Soroush Mehri', 'Kundan Kumar', 'Ishaan Gulrajani', 'Rithesh Kumar', 'Shubham Jain', 'Jose Sotelo', 'Aaron Courville', 'Yoshua Bengio']","[9, 8, 8]","['9', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version. The authors attempted to compare with wavenet model, but they didn*t manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12, 3, 2, 5, 2, 17, 30]","[7, 18, 9, 7, 11, 5, 23, 36]","[12, 57, 20, 10, 101, 11, 309, 977]","[7, 29, 9, 3, 63, 5, 135, 405]","[5, 17, 11, 7, 19, 6, 160, 456]","[0, 11, 0, 0, 19, 0, 14, 116]"
Semi-Supervised Classification with Graph Convolutional Networks,Accept,2017,"['Thomas N. Kipf', 'Max Welling']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 11, 24]","[7, 17, 30]","[11, 68, 236]","[3, 29, 104]","[6, 21, 63]","[2, 18, 69]"
Sigma Delta Quantized Networks,Accept,2017,"[""Peter O'Connor"", 'Max Welling']","[8, 6, 8]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it*s neat to see a practical algorithm driven by what is essentially a spiking network. The benefits of this approach are still more theoretical than practical -- it seems unlikely to be worthwhile to do this on current hardware. I strongly suspect that if deep networks were trained with an appropriate sparse slowness penalty, the reduction in computation would be much larger.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[21, 18]","[23, 24]","[32, 391]","[20, 190]","[5, 167]","[7, 34]"
"Snapshot Ensembles: Train 1, Get M for Free",Accept,2017,"['Gao Huang', 'Yixuan Li', 'Geoff Pleiss', 'Zhuang Liu', 'John E. Hopcroft', 'Kilian Q. Weinberger']","[9, 7, 8]","['9', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima*s well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost). This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4. The publicly available code to ensure reproducibility is also greatly appreciated. I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles. Preliminary rating: This is an interesting work with convincing experiments and clear writing. Minor note: Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[6, 3, 1, 2, 53, 14]","[12, 9, 7, 8, 59, 20]","[207, 55, 49, 40, 184, 199]","[71, 23, 22, 16, 78, 106]","[85, 29, 26, 20, 36, 82]","[51, 3, 1, 4, 70, 11]"
Soft Weight-Sharing for Neural Network Compression,Accept,2017,"['Karen Ullrich', 'Edward Meeds', 'Max Welling']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). This clustering effect can exploited for parameter quantisation and compression of the network parameters. The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process. A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. In particular a compression rate of 64.2 is obtained on the LeNet300-100 model. In section 6.1 the text refers to figure C, I suppose this should be figure 1. Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation. Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016). Comparable results are obtained in terms of compression rate and accuracy. The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model. The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training. This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques. The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets. Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 13, 18]","[10, 19, 24]","[21, 21, 391]","[8, 10, 190]","[12, 9, 167]","[1, 2, 34]"
Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,Accept,2017,"['Arash Ardakani', 'Carlo Condo', 'Warren J. Gross']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart*s paper cited for back-propagation than the Deep Learning book.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 7, 20]","[11, 13, 26]","[32, 98, 329]","[17, 38, 135]","[8, 32, 76]","[7, 28, 118]"
Steerable CNNs,Accept,2017,"['Taco S. Cohen', 'Max Welling']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters. The basic idea of steerability makes huge sense and seems like a very important idea to develop. It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s. This paper approaches it through a formal treatment of group theory. But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image. Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill. I*m not sure what this buys us in the end. it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful. Also the description of the experiments is fairly opaque. I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[-1, -1, 1]","[4, 4, 6]","[8, 6, 21]","[4, 3, 9]","[4, 3, 12]","[0, 0, 0]"
Stick-Breaking Variational Autoencoders,Accept,2017,"['Eric Nalisnick', 'Padhraic Smyth']","[8, 4, 8]","['Top 50% of accepted papers, clear accept', 'Ok but not good enough - rejection', 'Top 50% of accepted papers, clear accept']","This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper. There*s a lot of interest in VAEs these days; many lines of work seek to achieve automatic *black-box* inference in these models. For example, the authors themselves mention parallel work by Blei*s lab (also others) towards this direction. However, there*s a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn*t been popular in ML. Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts. I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they*re investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention. The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer. Overall, I found this to be an interesting paper, it would be a good fit for ICLR.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 30]","[10, 36]","[38, 216]","[19, 129]","[18, 29]","[1, 58]"
Stochastic Neural Networks for Hierarchical Reinforcement Learning,Accept,2017,"['Carlos Florensa', 'Yan Duan', 'Pieter Abbeel']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","Interesting work on hierarchical control, similar to the work of Heess et al. Experiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side. (1) Like other reviewers, I find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in RL). Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. (2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result? (3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it? (4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 10, 16]","[5, 15, 22]","[18, 52, 610]","[8, 28, 291]","[9, 19, 293]","[1, 5, 26]"
Structured Attention Networks,Accept,2017,"['Yoon Kim', 'Carl Denton', 'Luong Hoang', 'Alexander M. Rush']","[8, 8, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network. Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures. The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc. I think it’s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model. In sum, I think this is a solid contribution and the approach will benefit the research in other problems.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[-1, 26, 11, -3, -1, -3]","[4, 32, 17, 2, 4, 2]","[3, 148, 82, 3, 2, 2]","[3, 127, 69, 3, 2, 2]","[0, 4, 2, 0, 0, 0]","[0, 17, 11, 0, 0, 0]"
Support Regularized Sparse Coding and Its Fast Encoder,Accept,2017,"['Yingzhen Yang', 'Jiahui Yu', 'Pushmeet Kohli', 'Jianchao Yang', 'Thomas S. Huang']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 2, 15, 10, 53]","[16, 8, 21, 15, 59]","[60, 114, 322, 154, 1126]","[32, 52, 180, 80, 740]","[23, 40, 102, 39, 101]","[5, 22, 40, 35, 285]"
Temporal Ensembling for Semi-Supervised Learning,Accept,2017,"['Samuli Laine', 'Timo Aila']","[7, 9, 8]","['Good paper, accept', '9', 'Top 50% of accepted papers, clear accept']","This paper presents a model for semi-supervised learning by encouraging feature invariance to stochastic perturbations of the network and/or inputs. Two models are described: One where an invariance term is applied between different instantiations of the model/input a single training step, and a second where invariance is applied to features for the same input point across training steps via a cumulative exponential averaging of the features. These models evaluated using CIFAR-10 and SVHN, finding decent gains of similar amounts in each case. An additional application is also explored at the end, showing some tolerance to corrupted labels as well. The authors also discuss recent work by Sajjadi &al that is very similar in spirit, which I think helps corroborate the findings here. My largest critique is it would have been nice to see applications on larger datasets as well. CIFAR and SVHN are fairly small test cases, though adequate for demonstration of the idea. For cases of unlabelled data especially, it would be good to see tests with on the order of 1M+ data samples, with 1K-10K labeled, as this is a common case when labels are missing. On a similar note, data augmentations are restricted to only translations and (for CIFAR) horizontal flips. While *standard,* as the paper notes, more augmentations would have been interesting to see --- particularly since the model is designed explicitly to take advantage of random sampling. Some more details might also pop up, such as the one the paper mentions about handling horizontal flips in different ways between the two model variants. Rather than restrict the system to a particular set of augmentations, I think it would be interesting to push it further, and see how its performance behaves over a larger array of augmentations and (even fewer) numbers of labels. Overall, this seems like a simple approach that is getting decent results, though I would have liked to see more and larger experiments to get a better sense for its performance characteristics. Smaller comment: the paper mentions *dark knowledge* a couple times in explaining results, e.g. bottom of p.6. This is OK for a motivation, but in analyzing the results I think it may be possible to have something more concrete. For instance, the consistency term encourages feature invariance to the stochastic sampling more strongly than would a classification loss alone.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[14, 15]","[20, 21]","[67, 69]","[30, 28]","[18, 20]","[19, 21]"
The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,Accept,2017,"['Chris J. Maddison', 'Andriy Mnih', 'Yee Whye Teh']","[7, 9, 8]","['Good paper, accept', '9', 'Top 50% of accepted papers, clear accept']","The authors of the paper present a novel distribution for discrete variables called the *concrete distribution*. The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs. I think the paper is well written and sound, definitely of interest for the conference program. As to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets? Similarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not. I believe the paper should be accepted to the conference, however please consider that I*m not an expert in this field. Some minor observations/comments/issues: -Section 2.1: there is a repetition *be be* in the first paragraph. -Section 2.4: I would add a reference for the *multi-sample variational objective* -Section 3.1, just before Section 3.2: *the Gumbel is a crucial 1*. Why 1 and not *one*? -Section 3.3, last paragraph: *Thus, in addition to relaxing the sampling pass of a SCG the log...* I would add a comma after *SCG*. More in general, the second part of the paragraph is very dense and not easy to *absorb*. I don*t think it*s an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader. -Section 5.1, second paragraph: *All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k*. It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}. -Section 5.2: After *this distribution.* and *We will* there is an extra space. -If a compare the last formula in Section 5.3 with Eq. 8, I don*t see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 15, 20]","[10, 20, 26]","[57, 55, 267]","[28, 31, 139]","[27, 22, 107]","[2, 2, 21]"
The Neural Noisy Channel,Accept,2017,"['Lei Yu', 'Phil Blunsom', 'Chris Dyer', 'Edward Grefenstette', 'Tomas Kocisky']","[7, 7, 6]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation. PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results. CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It*s worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this. Other Comments: - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. - It initially seems strange to suggest a noisy-channel model as a way of addressing the *explaining away* problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[18, 8]","[22, 14]","[29, 397]","[17, 154]","[9, 216]","[3, 27]"
Third Person Imitation Learning,Accept,2017,"['Bradly C Stadie', 'Pieter Abbeel', 'Ilya Sutskever']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point. While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions. Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 13, 17, 7]","[7, 19, 23, 13]","[42, 76, 481, 13]","[17, 34, 227, 4]","[11, 14, 57, 2]","[14, 28, 197, 7]"
Tighter bounds lead to improved classifiers,Accept,2017,['Nicolas Le Roux'],"[8, 6, 4]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. Consequently, the optimized upper bound (log-loss) gets looser. As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. I would like to draw the author*s attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve). [1] Training Highly Multiclass Classifiers, Gupta et al. 2014. [2] Direct Loss Minimization for Structured Prediction, McAllester et al. [3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet. Final comment: I believe the material presented in this paper is of interest to a wide audience at ICLR. The problem studied is interesting and the proposed approach is sound. I recommend to accept the paper and increase my score (from 7 to 8).","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[15],[21],[70],[34],[30],[6]
TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,Accept,2017,"['Adji B. Dieng', 'Chong Wang', 'Jianfeng Gao', 'John Paisley']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']",This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.,"['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 14, 18, 11]","[7, 20, 24, 17]","[22, 80, 545, 117]","[7, 41, 263, 49]","[14, 30, 250, 38]","[1, 9, 32, 30]"
Topology and Geometry of Half-Rectified Network Optimization,Accept,2017,"['C. Daniel Freeman', 'Joan Bruna']","[2, 7, 8]","['Strong rejection', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.","['5', '3', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 8]","[7, 14]","[19, 162]","[5, 64]","[14, 88]","[0, 10]"
Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music,Accept,2017,"['Haizi Yu', 'Lav R. Varshney']","[6, 8, 6]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper. Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity. I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper*s current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that. If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission. [Addendum: based on discussions here & revisions, I have revised my score]","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12]","[8, 18]","[15, 294]","[7, 132]","[7, 98]","[1, 64]"
Towards a Neural Statistician,Accept,2017,"['Harrison Edwards', 'Amos Storkey']","[8, 6]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","Sorry for the late review -- I*ve been having technical problems with OpenReview which prevented me from posting. This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to *learn to learn* by acquiring the ability to learn distributions from small numbers of examples. Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read. The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that *someone who thinks up statistics*. The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method. The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples? (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) Will the authors release the code?","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 22]","[8, 28]","[22, 135]","[9, 62]","[12, 54]","[1, 19]"
Towards the Limit of Network Quantization,Accept,2017,"['Yoojin Choi', 'Mostafa El-Khamy', 'Jungwon Lee']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters. The authors assume that the network is already pruned and aim for compressing the non-pruned parameters. The problem of network compression is a well-motivated problem and of interest to the ICLR community. The main drawback of the paper is its novelty. The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks. It should be noted that the proposed method in this paper has not been proposed before. The paper is well-structured and easy to follow. Although it heavily builds on Han 2015, it is still much longer than Han 2015. I believe that there is still some redundancy in the paper. The experiments section starts on Page 12 whereas for Han 2015 the experiments start on page 5. Therefore, I believe much of the introductory text is redundant and can be efficiently cut. Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy. Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1? Some comments: 1) It is not clear whether the procedure depicted in figure 1 is the authors’ contribution or has been in the literature. 2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix. Can the authors please explain how this approximation affects the final compression? Also how much does one lose by making such an approximation? minor typos (These are for the revised version of the paper): 1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned 2) Page 2, one para to the end, last line: assigned for -> assigned to 3) Page 5, line 2, same as above 4) Page 8, Section 5, Line 3: explore -> explored","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 15, 18]","[17, 20, 24]","[29, 122, 213]","[14, 60, 109]","[11, 43, 48]","[4, 19, 56]"
Tracking the World State with Recurrent Entity Networks,Accept,2017,"['Mikael Henaff', 'Jason Weston', 'Arthur Szlam', 'Antoine Bordes', 'Yann LeCun']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']",The paper proposed a multi-memory mechanism that memorizes different information into different components/entities. It could be considered as a mixture model in RNN. This is a very interesting model and result is convincing. A limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.,"['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 19, 9, 13, 30]","[12, 25, 15, 18, 36]","[24, 237, 138, 86, 315]","[11, 126, 54, 48, 162]","[12, 84, 74, 29, 113]","[1, 27, 10, 9, 40]"
Trained Ternary Quantization,Accept,2017,"['Chenzhuo Zhu', 'Song Han', 'Huizi Mao', 'William J. Dally']","[3, 7, 7, 3, 8]","['Clear rejection', 'Good paper, accept', 'Good paper, accept', 'Clear rejection', 'Top 50% of accepted papers, clear accept']","The paper shows a different approach to a ternary quantization of weights. Strengths: 1. The paper shows performance improvements over existing solutions 2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning. Weaknesses: 1. The paper is very incremental. 2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is *what is new in the topic* update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience. 3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation *it is related to mobile, therefore it is cool* sufficient. This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience. Also - the code was not released is my understanding.","['3', '5', '3', '5', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 3, 3, 33]","[8, 9, 8, 39]","[7, 150, 24, 274]","[2, 66, 13, 174]","[3, 67, 10, 23]","[2, 17, 1, 77]"
Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Accept,2017,"['Yuxin Wu', 'Yuandong Tian']","[6, 4, 7]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Good paper, accept']","The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge. The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn*t learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules. If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though. I*m not sure we can realistically draw any conclusions about Figure 6 in the paper*s current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images. --- Added after rebuttal: I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 12]","[14, 18]","[49, 150]","[10, 69]","[16, 73]","[23, 8]"
Training Compressed Fully-Connected Networks with a Density-Diversity Penalty,Accept,2017,"['Shengjie Wang', 'Haoran Cai', 'Jeff Bilmes', 'William Noble']","[9, 6]","['9', 'Marginally above acceptance threshold']","The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights. This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer. Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity. As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient. The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights. The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance. The paper is presented very clearly, presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data. The result tables are a bit confusing unfortunately. minor issues: p1 english mistake: “while networks *that* consist of convolutional layers”. p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse: In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 2, 29, 22]","[13, 7, 35, 28]","[50, 18, 299, 157]","[26, 14, 211, 62]","[13, 0, 49, 6]","[11, 4, 39, 89]"
Training deep neural-networks using a noise adaptation layer,Accept,2017,"['Jacob Goldberger', 'Ehud Ben-Reuven']","[5, 7, 5]","['5', 'Good paper, accept', '5']","This paper looks at how to train if there are significant label noise present. This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label. The second directly integrates out the true label and simply optimizes the p(z|x). Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated. Cons: the results on mnist is all synthetic and it*s hard to tell if this would translate to a win on real datasets. - comments: Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes? It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[21, 16]","[27, 17]","[196, 5]","[116, 4]","[35, 1]","[45, 0]"
Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Accept,2017,"['Zhilin Yang', 'Ruslan Salakhutdinov', 'William W. Cohen']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks. The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared *layers* being dependent of the task of interest. The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks. The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale. Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results. Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 15, 33]","[21, 21, 39]","[82, 419, 345]","[34, 207, 218]","[35, 201, 80]","[13, 11, 47]"
Transfer of View-manifold Learning to Similarity Perception of Novel Objects,Accept,2017,"['Xingyu Lin', 'Hao Wang', 'Zhihao Li', 'Yimeng Zhang', 'Alan Yuille', 'Tai Sing Lee']","[5, 6, 7]","['5', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a model to learn across different views of objects. The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object. The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Furthermore, a comparison against human perception on the *Tenenbaum objects” is shown. Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below). The paper is reasonably written. Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach. More details: The “image purification” paper is very related to this work: [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015. There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval. If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]). It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/). Somewhat related to the proposed method is recent work on multi-view 3D object retrieval: [B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016. There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object). Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view. Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape): [C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016. [D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016. For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean). It has been shown that different layers and feature comparisons perform differently for a given task, e.g., [E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 27, 14, 10, 35, 26]","[7, 33, 20, 16, 41, 31]","[30, 1335, 94, 64, 704, 43]","[11, 627, 49, 33, 337, 20]","[16, 199, 18, 9, 265, 8]","[3, 509, 27, 22, 102, 15]"
Tree-structured decoding with doubly-recurrent neural networks,Accept,2017,"['David Alvarez-Melis', 'Tommi S. Jaakkola']","[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what*s presented in the paper) that need to generate tree structures given an unstructured data. One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture. A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice. I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 25]","[9, 31]","[46, 294]","[17, 149]","[27, 112]","[2, 33]"
Trusting SVM for Piecewise Linear CNNs,Accept,2017,"['Leonard Berrada', 'Andrew Zisserman', 'M. Pawan Kumar']","[5, 4, 6]","['5', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities. The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. Pros: - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow. Cons: - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. - The test accuracy performance on CIFAR-10 reported in the paper doesn*t look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, Other comments: - If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 33, 16]","[8, 39, 22]","[15, 746, 118]","[5, 455, 57]","[9, 188, 43]","[1, 103, 18]"
Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,Accept,2017,"['Hakan Inan', 'Khashayar Khosravi', 'Richard Socher']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let*s rename it to L*) should be the same as that in Eq. 2.1. For example, L* could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L* is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn*t the RHS in Eq 3.5 be sum 	ilde{y_{t,i}}(frac{hat{y}_t}{	ilde{y_{t,i}}} - e_i) ?","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 4, 11]","[9, 9, 17]","[10, 14, 229]","[4, 5, 111]","[5, 6, 111]","[1, 3, 7]"
Understanding Trainable Sparse Coding with Matrix Factorization,Accept,2017,"['Thomas Moreau', 'Joan Bruna']","[6, 8]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below. It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. Minor comments: - E(z_k) in (3) and (4) are not defined. - E_x in (19) is not defined. - Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",['skipped'],['skipped'],"[48, 162]","[19, 64]","[22, 88]","[7, 10]"
Unrolled Generative Adversarial Networks,Accept,2017,"['Luke Metz', 'Ben Poole', 'David Pfau', 'Jascha Sohl-Dickstein']","[7, 7, 9]","['Good paper, accept', 'Good paper, accept', '9']","This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs. The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we*re able to find such a z, then the generator can generate this particular training example. It*s demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor*s proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it*s harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z. The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric. There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: https://openreview.net/pdf?id=HJKkY35le Whether this paper is accepted or not, I encourage the authors to investigate this approach further, since the method is promising and interesting. # Post-rebuttal review The authors have incorporated changed in the paper by adding more experiments. These experiments now demonstrate the claims of the paper better. The paper was already well-written and introduced a novel idea and addressed an important problem. The only thing holding this paper back was unconvincing experiments, which now has been corrected. Thus, I would increase my score by 2 points, and recommend accepting the paper.","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 7, 8, 9]","[8, 13, 14, 15]","[48, 62, 27, 130]","[19, 24, 11, 52]","[29, 37, 14, 74]","[0, 1, 2, 4]"
Unsupervised Cross-Domain Image Generation,Accept,2017,"['Yaniv Taigman', 'Adam Polyak', 'Lior Wolf']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. -It will be more interesting to show results in other domains such as texts and images. -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[9, 3, 18]","[15, 9, 24]","[42, 41, 416]","[19, 16, 213]","[21, 23, 168]","[2, 2, 35]"
Variable Computation in Recurrent Neural Networks,Accept,2017,"['Yacine Jernite', 'Edouard Grave', 'Armand Joulin', 'Tomas Mikolov']","[4, 7, 7]","['Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., *variable computation*) of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline. === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism. Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and _x0008_ar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are. This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated. A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well. === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell. === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments. The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions. One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar. === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion! References Chung et al., *Hierarchical Multiscale Recurrent Neural Networks,* in 2016. Graves et al., *Adaptive Computation Time for Recurrent Neural Networks,* in 2016. Wu et al., *On Multiplicative Integration with Recurrent Neural Networks,* in 2016.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 7, 8, 10]","[11, 13, 14, 15]","[49, 93, 130, 83]","[14, 36, 59, 46]","[33, 52, 67, 33]","[2, 5, 4, 4]"
Variational Lossy Autoencoder,Accept,2017,"['Xi Chen', 'Diederik P. Kingma', 'Tim Salimans', 'Yan Duan', 'Prafulla Dhariwal', 'John Schulman', 'Ilya Sutskever', 'Pieter Abbeel']","[9, 7, 7, 6]","['9', 'Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s. It is only an incremental but important step. Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its *creative* regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over. We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights. Some disagreements with the theoretical suppositions in the paper: i) The VAE-s posterior converges to the prior faster than we would like because the gradients of the *generative* error (the KL divergence of prior and posterior) w.r.t. mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp. gradients of the reconstruction error. Especially when more *hairy* decoders like pixelCNN are used. We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain. Dumbing-down the decoder via variations of dropout or *complexifying* the sampler as in here, or slapping the generative error with a *DeepMind* constant (beta_VAE), are the natural band-aids, but seem to fail in the *creative* regime, for real-life sets like CIFAR10 or more complex ones. Other conceptual solutions are needed, some are discussed in [2]. ii) The claim near the end of section 2.2 that *the extra coding cost a.k.a. variational error will exist and will not be negligible* is a speculation, which, in our empirical experience at least, is incorrect. The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work. In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc. Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations. On the lighter side, guys, please do not recycle ridiculous terms like *optimizationally challenged*, as in section 2.2! The English language has recently acquired *mentally-challenged*, *emotionally-challenged*, etc, and now political correctness has sadly found its way to machines? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['4', '4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 6, 10, 2, 7, 11, 16]","[8, 14, 12, 15, 8, 13, 17, 22]","[47, 46, 60, 52, 22, 66, 99, 610]","[21, 21, 21, 28, 8, 27, 49, 291]","[24, 24, 36, 19, 14, 37, 45, 293]","[2, 1, 3, 5, 0, 2, 5, 26]"
Variational Recurrent Adversarial Deep Domain Adaptation,Accept,2017,"['Sanjay Purushotham', 'Wilka Carvalho', 'Tanachat Nilanon', 'Yan Liu']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings. The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4. Detailed comments: 1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA. What do you think are the two dominant latent factors encoded in figure 1 (c)? 2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this? 3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 1, 2, 16]","[15, 7, 2, 22]","[43, 9, 2, 219]","[25, 4, 2, 132]","[14, 5, 0, 57]","[4, 0, 0, 30]"
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis,Accept,2017,"['Luisa M Zintgraf', 'Taco S Cohen', 'Tameem Adel', 'Max Welling']","[6, 9, 6]","['Marginally above acceptance threshold', '9', 'Marginally above acceptance threshold']","The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008). The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 4, 8, 18]","[8, 10, 12, 24]","[36, 86, 25, 391]","[15, 32, 18, 190]","[18, 49, 7, 167]","[3, 5, 0, 34]"
What does it take to generate natural textures?,Accept,2017,"['Ivan Ustyuzhaninov *', 'Wieland Brendel *', 'Leon Gatys', 'Matthias Bethge']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods. Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 7, 3, 19]","[7, 13, 7, 25]","[15, 63, 19, 142]","[6, 22, 8, 52]","[9, 34, 10, 58]","[0, 7, 1, 32]"
Why Deep Neural Networks for Function Approximation?,Accept,2017,"['Shiyu Liang', 'R. Srikant']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","SUMMARY This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. PROS The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. CONS The main result appears to address only strongly convex univariate functions. SPECIFIC COMMENTS - Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. I still have not read the revision and maybe you have already addressed some of these points there. - The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. - In Lemma 3, there is an i that should be x - In Theorem 4, ``	ilde f** is missing the (x). - Theorem 11, the lower bound always increases with L ? - In Theorem 11, _x0008_f xin [0,1]^d?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 28]","[10, 34]","[20, 378]","[9, 165]","[7, 75]","[4, 138]"
Words or Characters? Fine-grained Gating for Reading Comprehension,Accept,2017,"['Zhilin Yang', 'Bhuwan Dhingra', 'Ye Yuan', 'Junjie Hu', 'William W. Cohen', 'Ruslan Salakhutdinov']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[15, 2, 14, 6, 33, 15]","[21, 8, 20, 12, 39, 21]","[82, 71, 245, 63, 345, 419]","[34, 30, 99, 31, 218, 207]","[35, 39, 50, 30, 80, 201]","[13, 2, 96, 2, 47, 11]"
Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,Accept,2017,"['David Krueger', 'Tegan Maharaj', 'Janos Kramar', 'Mohammad Pezeshki', 'Nicolas Ballas', 'Nan Rosemary Ke', 'Anirudh  Goyal', 'Yoshua Bengio', 'Aaron Courville', 'Christopher Pal']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability. Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs. The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case. Overall, the paper bears great potential. However, I do see some points. 1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available. I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks. 2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis. 3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away. An extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2) Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 2, 8, 5, 6, 7, 4, 30, 17, 21]","[9, 8, 14, 10, 12, 13, 10, 36, 23, 27]","[39, 22, 21, 19, 75, 61, 102, 977, 309, 231]","[11, 6, 10, 8, 39, 22, 40, 405, 135, 98]","[28, 15, 11, 11, 35, 38, 61, 456, 160, 110]","[0, 1, 0, 0, 1, 1, 1, 116, 14, 23]"
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,Accept,2017,"['Irina Higgins', 'Loic Matthey', 'Arka Pal', 'Christopher Burgess', 'Xavier Glorot', 'Matthew Botvinick', 'Shakir Mohamed', 'Alexander Lerchner']","[7, 5, 7, 6]","['Good paper, accept', '5', 'Good paper, accept', 'Marginally above acceptance threshold']","The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question. The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. This of course does not handle complex factors like *type of legs* but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!) By complete luck, the authors may have hit upon something else of fundamental importance: the letter *beta* for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative *energy* and *entropy* and is partially addressed in section 2.9 of reference [1]. The correct definition of *generative temperature* is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the *correct* physical model instead. A question to the authors: creating a new metric (*disentanglement*) as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 10, 2, 1, 8, 17, 12, 15]","[11, 14, 3, 5, 13, 23, 17, 20]","[36, 28, 7, 22, 24, 110, 66, 30]","[11, 12, 3, 9, 14, 40, 27, 11]","[20, 15, 4, 13, 9, 54, 33, 16]","[5, 1, 0, 0, 1, 16, 6, 3]"
A Context-aware Attention Network for Interactive Question Answering,Reject,2017,"['Huayu Li', 'Martin Renqiang Min', 'Yong Ge', 'Asim Kadav']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","1. the QA model is not novel, very similar to the existing model. 2. The IQA model is very confusing. If it needs human interactive in the training process, how could it be practical to ask human to join the training in each iteration? It sounds impractical. If the human interactive questions are predefined, then it is not interactive at all, since it is not based on the current state of model output.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 11, 17, 10]","[16, 17, 23, 16]","[59, 79, 232, 48]","[31, 37, 104, 23]","[13, 32, 15, 21]","[15, 10, 113, 4]"
A Convolutional Encoder Model for Neural Machine Translation,Reject,2017,"['Jonas Gehring', 'Michael Auli', 'David Grangier', 'Yann N. Dauphin']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The system described works comparably to bi-directional LSTM baseline for NMT, and CNN*s are naturally parallelizable. Key ideas include the use of two stacked CNN*s (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN*s for translation has been attempted previously (as described by the authors), but presumably it is the authors* combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN*s, whereas earlier attempts were not. They describe system*s sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN*s). The experimental results are well reported in detail. One or two figures would definitely be required to help clarify the architecture. This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 9, 13, 7]","[11, 15, 18, 13]","[23, 137, 94, 67]","[15, 68, 44, 32]","[7, 67, 41, 32]","[1, 2, 9, 3]"
A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games,Reject,2017,"['Felix Leibfried', 'Nate Kushman', 'Katja Hofmann']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames. Pros: - Paper is well written and easy to follow. - Model is clear to understand. Cons: - The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope. This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 11, 13]","[8, 16, 19]","[23, 33, 146]","[8, 19, 80]","[13, 13, 53]","[2, 1, 13]"
A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,Reject,2017,"['Kazuma Hashimoto', 'Caiming Xiong', 'Yoshimasa Tsuruoka', 'Richard Socher']","[6, 3, 5]","['Marginally above acceptance threshold', 'Clear rejection', '5']","The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks). Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP *complexity*) of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call *successive regularization*. Experiments: - The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable; - The information *transferred* from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing. - The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 9, 17, 11]","[21, 15, 23, 17]","[66, 383, 139, 229]","[31, 165, 81, 111]","[31, 208, 35, 111]","[4, 10, 23, 7]"
A Neural Knowledge Language Model,Reject,2017,"['Sungjin Ahn', 'Heeyoul Choi', 'Tanel Parnamaa', 'Yoshua Bengio']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words. At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this. However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. - I cannot find where f_{symbkey} / f_{voca} / f_{copy} are defined - w^v, w^s are confusing. - e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough. - (h_t, c_t) = f_LSTM(x_{t?1}, h_{t?1}) c_t is not used? - The notion of “fact embeddings” is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end). For the anchor / “topic-itself” facts, do you learn the embedding for the special relations and use the entity embeddings from TransE? On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first. Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention “Obama” for the entity “Barack Obama”?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[25, -1, 15, 10, 14, 8]","[31, 5, 21, 16, 20, 14]","[106, 27, 215, 168, 275, 184]","[55, 6, 129, 82, 162, 73]","[7, 9, 40, 54, 58, 54]","[44, 12, 46, 32, 55, 57]"
A Neural Stochastic Volatility Model,Reject,2017,"['Rui Luo', 'Xiaojun Xu', 'Weinan Zhang', 'Jun Wang']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.’s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful. I have a few comments and reservations with the paper: 1) Although not mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons. 2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. 3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature. 4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4) 5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. 6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed. All in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 7, 10, 15]","[6, 13, 16, 21]","[17, 375, 78, 359]","[8, 185, 30, 163]","[9, 163, 19, 155]","[0, 27, 29, 41]"
A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,Reject,2017,"['Shayne Longpre', 'Sabeek Pradhan', 'Caiming Xiong', 'Richard Socher']","[5, 5, 5]","['5', '5', '5']","The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. Although the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in *LSTM: A Search Space Odyssey*). Besides, those extensions are not really novel.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 9, 11]","[8, 7, 15, 17]","[33, 5, 385, 229]","[11, 2, 165, 111]","[21, 3, 210, 111]","[1, 0, 10, 7]"
Adding Gradient Noise Improves Learning for Very Deep Networks,Reject,2017,"['Arvind Neelakantan', 'Luke Vilnis', 'Quoc V. Le', 'Lukasz Kaiser', 'Karol Kurach', 'Ilya Sutskever', 'James Martens']","[4, 4, 7]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Good paper, accept']","The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines). On these networks, the question of optimization has so far not been studied as extensively as for more standard networks. A study specific to this class of models is therefore welcome. Results consistently show better optimization properties from adding noise in the training procedure. One issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise. A comparison to results obtained in the literature would be desirable. For example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy). I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 5, 13, 11, 13, 6, 8]","[11, 10, 19, 17, 18, 9, 14]","[34, 32, 300, 99, 84, 28, 38]","[15, 16, 143, 49, 44, 12, 19]","[18, 16, 146, 45, 32, 14, 16]","[1, 0, 11, 5, 8, 2, 3]"
Adversarial examples for generative models,Reject,2017,"['Jernej Kos', 'Ian Fischer', 'Dawn Song']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","Comments: *This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied, which often have telltale noise* Is this really true? If it were the case, wouldn*t it imply that training *against* adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)? Pros: -The question of whether adversarial examples exist in generative models, and indeed how the definition of *adversarial example* carries over is an interesting one. -Finding that a certain type of generative model *doesn*t have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result. -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014). Is this because it*s actually harder to find adversarial examples in these types of generative models? Issues: -Paper is significantly over length at 13 pages. -The beginning of the paper should more clearly motivate its purpose. -Paper has *generative models* in the title but as far as I can tell the whole paper is concerned with autoencoder-type models. This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they*re distinct from a paper called *adversarial examples for generative models*. -I think that the introduction contains too much background information - it could be tightened.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[23, 9, 16]","[29, 14, 22]","[111, 41, 126]","[52, 37, 65]","[1, 0, 25]","[58, 4, 36]"
Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks,Reject,2017,"['Farkhondeh Kiaee', 'Christian Gagné', 'and Mahdieh Abbasi']","[7, 7, 6, 5]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold', '5']","The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks. The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos). Pros: 1) Put an old algorithm to good use in a new setting 2) The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention). This contributes to the efficient trainability of the model 3) Seems to dovetail nicely with other results to encourage sparsity--that is, it can be used simultaneously--and is quite generalizable. Cons: 1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity--some sort of comparison involving training time would be great, and is currently lacking. EDIT: Authors addressed this by addition of results in Appendix B. 2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment. Is this a general feature? Is this a statistical fluke? etc. Even if the answer is *it is not obvious, and determining why goes outside the scope of this work*, I would like to know it! EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A. 3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field. I don*t think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper. EDIT: Authors addressed this by followup to question and additional text in the paper. Additional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7. The core of this paper is quite solid, it just needs a little bit more polishing. EDIT: Score has been updated. Note: the authors probably meant *In order to verify* in the first sentence of Appendix A.","['3', '5', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 17, 3]","[6, 23, 8]","[6, 128, 16]","[2, 60, 5]","[1, 44, 9]","[3, 24, 2]"
An Analysis of Deep Neural Network Models for Practical Applications,Reject,2017,"['Alfredo Canziani', 'Adam Paszke', 'Eugenio Culurciello']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important. However, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances. Main concerns: 1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance! 2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks. 3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well. Minor concern: 1) Why did the authors choose to use batch normalization in NiN and AlexNet?","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 2, 18]","[8, 8, 24]","[10, 20, 116]","[5, 4, 80]","[5, 13, 17]","[0, 3, 19]"
An Empirical Analysis of Deep Network Loss Surfaces,Reject,2017,"['Daniel Jiwoong Im', 'Michael Tao', 'Kristin Branson']","[4, 4, 4, 6]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here. Major points: -- *As we saw in the previous section, the minima of deep network loss functions are for the most part decent.* All you said in the previous section was that theory shows that there are no bad minima under *strong assumptions*. There is no practical proof that minima do not vary in quality. -- *This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a *decent minima quickly* is reduced to the task of finding any minima quickly.* First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being *reduced to finding any minima quickly*. -- Figure 1 I don*t like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don*t think you did. Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don*t join up horizontally. Please be more careful. -- Misuse of the transient phase / minimization phase concept In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training? -- Only 1 dataset You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset. -- Many figures are unclear For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some. -- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc. -- Lack of confidence intervals The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented. -- Lack of information regarding learning rate There is big question mark left open regarding how all your results would change if different learning rates were used. You don*t even tell us how you chose the learning rates from the intervals you gave in section 3.4. -- Lack of information regarding the absolute distance of interpolated points In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big *hump* between them, it means that those points are more *brittle* than if they are far apart and there is a big *hump* between them. Minor points: -- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used. -- The font size for the legends in the upper triangle of Table 1 is too small. You can*t just write *best viewed in zoom* in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.","['3', '4', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 2, 13]","[8, 7, 18]","[26, 9, 21]","[8, 2, 8]","[17, 3, 11]","[1, 4, 2]"
An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Reject,2017,"['Ziyuan Lin', 'Jaakko Peltonen']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross-view similarity between neighborhoods of data samples. The motivation comes from information retrieval tasks. Authors position their work as an alternative to CCA-based multiview learning; note, however, that CCA based techniques have very different purpose and are rather broadly applicable than the setting considered here. Main points: - I am not sure what authors mean by time complexity. It would appear that they simply report the computational cost of evaluating the objective in equation (7). Is there a sense of how many iterations of the L-BFGS method? Since that is going to be difficult given the nature of the optimization problem, one would appreciate some sense of how hard or easy it is in practice to optimize the objective in (7) and how that varies with various problem dimensions. Authors argue that scalability is not their first concern, which is understandable, but if they are going to make some remarks about the computational cost, it better be clarified that the reported cost is for some small part of their overall approach rather than “time complexity”. - Since authors position their approach as an alternative to CCA, they should remark about how CCA, even though a nonconvex optimization problem, can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation. The method proposed in the paper does not seem to be tractable, at least not immediately. - The empirical results with synthetic data are a it confusing. First of all the data generation procedure is quite convoluted, I am not sure why we need to process each coordinate separately in different groups, and then permute and combine etc. A simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable. I am also baffled why CCA does not recover the true subspace - arguably it is the level of additive noise that would impact the recoverability - however the proposed method is nearly exact so the noise level is perhaps not so severe. It is also not clear if authors are using regularization with CCA - without regularization CCA can be have in a funny manner. This needs to be clarified.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 17]","[10, 23]","[11, 92]","[6, 63]","[3, 6]","[2, 23]"
Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Reject,2017,"['Chun-Liang Li', 'Siamak Ravanbakhsh', 'Barnabas Poczos']","[5, 5, 6, 5]","['5', '5', 'Marginally above acceptance threshold', '5']","The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It*s interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 8, 16]","[12, 14, 22]","[94, 58, 239]","[40, 26, 120]","[47, 30, 105]","[7, 2, 14]"
Attentive Recurrent Comparators,Reject,2017,"['Pranav Shyam', 'Ambedkar Dukkipati']","[4, 5, 3]","['Ok but not good enough - rejection', '5', 'Clear rejection']","This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input. The paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character? Although it is zooming in, shouldn’t it attend to relevant parts of the character? Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from. While the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.","['5', '2', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[1, 9, 16]","[6, 15, 22]","[12, 125, 121]","[4, 46, 41]","[8, 30, 65]","[0, 49, 15]"
BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Reject,2017,"['Vincent Roger', 'Marius Bartcus', 'Faicel Chamroukhi', 'Hervé Glotin']","[5, 4, 5]","['5', 'Ok but not good enough - rejection', '5']","This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped']
Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context,Reject,2017,"['Shyam Upadhyay', 'Kai-Wei Chang', 'James Zou', 'Matt Taddy', 'Adam Kalai']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs). While this idea in itself isn*t new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data. Broadly this is fine, but unfortunately the paper then falls short in a number of ways. For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion. Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion. This paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods. In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don*t really add anything to the story.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 10, 10, 21, 8]","[9, 16, 12, 27, 14]","[43, 317, 18, 142, 208]","[24, 146, 8, 81, 78]","[18, 157, 6, 42, 109]","[1, 14, 4, 19, 21]"
Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Reject,2017,"['Aryk Anderson', 'Kyle Shaffer', 'Artem Yankov', 'Court Corley', 'Nathan Hodas']","[4, 6, 6]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard “fine-tuning” approach. Pros: -The method is simple and clearly explained. -Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest. -Experiments are performed in multiple domains -- vision and NLP. Cons: -The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the “stiched” network). These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important. -Given these large additional costs, the core of the idea is not sufficiently validated, to me. In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed: (1) Allowing the original network weights to be learned for the target task, as well as the additional module. Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network. (2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task. Outperforming this baseline would verify that having a set of weights which never “sees” the source dataset is useful. -The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks. I’ve never seen networks pre-trained on CIFAR deployed anywhere, and it’s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet. (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.) Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application. =============== Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction. However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the *stitched network* idea which is the real novelty of the paper. Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (*ResNet+VGG > ResNet* would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don*t sufficiently validate the stitched network idea, in my opinion.","['4', '2', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 3, 2, 13, 8]","[1, 9, 3, 17, 12]","[1, 9, 4, 44, 52]","[0, 6, 1, 29, 16]","[1, 3, 3, 9, 35]","[0, 0, 0, 6, 1]"
Binary Paragraph Vectors,Reject,2017,"['Karol Grzegorczyk', 'Marcin Kurdziel']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov*14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding. For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents. Pros: - the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton *09 - the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton *09, but since in the meantime document representations improved (Le & Mikolov*14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM Cons: - the insertion of the sigmoid to produce binary codes (from Lin & al. *15) in the training process is incremental - the explanation is too abstract and difficult to follow for a non-expert (see details below) - a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR*16 Detailed comments: Section 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why *embedding lookup* and *linear projection* cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this). p2: *This way, the length of binary codes is not tied to the dimensionality of word embeddings.* -> why not? section 3: This is the experimental setup of Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups. *similarity of the inferred codes*: say here that codes are compared using Hamming distances. *binary codes perform very well, despite their far lower capacity* -> do you mean smaller size than real vectors? fig 5: these plots could be dropped if space is needed. section 3.1: one could argue that *transferring* from Wikipedia to anything else cannot be called transferring, since Wikipedia*s purpose is to include all topics and lexical domains section 3.2: specify how the 300D real vectors are compared. L2 distance? inner product? fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.","['2', '5', '3']","['The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[4, 15]","[9, 20]","[12, 30]","[5, 18]","[3, 2]","[4, 10]"
Boosted Generative Models,Reject,2017,"['Aditya Grover', 'Stefano Ermon']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc. The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC. The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS. With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed. Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal. [R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf PROS: Novel and intriguing idea Strong theoretical guarantees CONS: Resulting boosted model is un-normalized Discriminator based boosting is expensive, due to sampling via MCMC Weak experimental section","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 9]","[9, 15]","[87, 406]","[42, 199]","[43, 200]","[2, 7]"
Boosted Residual Networks,Reject,2017,"['Alan Mosca', 'George D. Magoulas']","[3, 4, 3]","['Clear rejection', 'Ok but not good enough - rejection', 'Clear rejection']","This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN*s(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training. The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. The empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation. Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary. In addition, it is not clear how sensitive the boosting to the selection of injection point. This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant. Pros -provides some preliminary results for boosting of Res Nets Cons -not sufficiently novel: an incremental approach -empirical analysis is not satisfactory","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 23]","[8, 28]","[12, 152]","[7, 85]","[3, 5]","[2, 62]"

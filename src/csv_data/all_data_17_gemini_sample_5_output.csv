reviews,sentiment_scores,politeness_scores,reasonings
"['The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.', 'The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence. It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol. It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language*s speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language. In section 4, the description of the auxiliary decoder setup might benefit from more detail. There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 1. Ogawa, Tetsuji, et al. *Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation.* Proceedings of ICASSP. 2015. 2. Hermansky, Hynek, et al. *Towards machines that know when they do not know.* Proceedings of ICASSP, 2015. 3. Variani, Ehsan et al. *Multi-stream recognition of noisy speech with performance monitoring.* INTERSPEECH. 2013.', 'The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. My comments appear below: 1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold). 2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases). 3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?) 4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required? 5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.']","[60, 50, 60]","[80, 75, 80]","['The review acknowledges the importance of the topic and describes the study as a ""good baseline"". It also mentions that the study ""contributes to the community."" These are all positive remarks. However, the statement that the study ""misses strong novel methods"" slightly lowers the overall positivity. Therefore, the sentiment is mildly positive. The language used is formal, respectful, and devoid of any harsh or negative wording, indicating a polite tone.', ""The reviewer provides constructive criticism and suggestions for improvement without using harsh language. They find the paper interesting enough to suggest further exploration ('It would be great...', 'It would be interesting...'). The suggestions for improvement and additional references contribute to the overall positive, yet neutral tone."", ""The review is constructive and suggestive. The reviewer provides specific points of improvement and additional analyses that would strengthen the paper. While they point out areas for improvement, the tone is collaborative and aims to enhance the paper's quality. There are no overtly negative statements or criticisms of the authors' work.""]"
"['This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.', 'This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines. The paper is well written overall. A few detailed comments: * page 4, line5: including a some -> including some * What*s the benefit of the preprocessing and attention step? Can you provide the results without it? * Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.', 'The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model. This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I*d strongly recommend acceptance. Detail: - The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections. - If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. *15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy). - Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)? - You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite. - Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.']","[20, 60, 75]","[70, 80, 90]","['The review acknowledges the merits of the paper\'s approach, highlighting the comparison aspect as particularly noteworthy. However, it also points out a lack of innovation, describing the work as ""incremental."" The suggestion for qualitative evaluation further emphasizes the need for more in-depth analysis. Overall, the tone is balanced, presenting both positive and negative aspects without being overly enthusiastic or critical.', 'The review starts with positive statements, highlighting the well-written nature of the paper and the extensive experimental results. While it points out areas of improvement, the language remains constructive and suggestive, rather than critical. The use of phrases like ""Please enhance"" further indicates politeness.', 'The reviewer provides a generally positive assessment, stating that the work is ""timely"", ""straightforward and reasonable"", and yields ""good results."" They recommend acceptance despite acknowledging the work as ""bordering on incremental."" The reviewer also provides constructive and detailed feedback, indicating a willingness to see the paper improved.']"
"['Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.', 'Paper proposes a neural physics engine (NPE). NPE provides a factorization of physical scene into composable object-based representations. NPE predicts a future state of the given object as a function composition of the pairwise interactions between itself and near-by objects. This has a nice physical interpretation of forces being additive. In the paper NPE is investigated in the context of 2D worlds with balls and obstacles. Overall the approach is interesting and has an interesting flavor of combining neural networks with basic properties of physics. Overall, it seems like it may lead to interesting and significant follow up work in the field. The concerns with the paper is mainly with evaluation, which in places appears to be weak (see below). > Significance & Originality: The approach is interesting. While other methods have tried to build models that can deal with physical predictions, the idea of summing over pair-wise terms, to the best of my knowledge, is novel and much more in-line with the underlying principles of mechanics. As such, while relatively simple, it seems to be an important contribution. > Clarity: The paper is generally well written. However, large portion of the early introduction is rather abstract and it is difficult to parse until one gets to 5th paragraph. I would suggest editing the early part of introduction to include more specifics about the approach or even examples ... to make text more tangible. > Experiments Generally there are two issues with experiments in my opinion: (1) the added indirect comparison with Fragkiadaki et al (2015) does not appears to be quantitatively flattering with respect to the proposed approach, and (2) quantitative experiments on the role the size of the mask has on performance should really be added. Authors mention that they observe that mask is helpful, but it is not clear how helpful or how sensitive the overall performance is to this parameter. This experiment should really be added. I do feel that despite few mentioned shortcomings that would make the paper stronger, this is an interesting paper and should be published.', '- summary The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls. - novelty The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions. Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances. - citations This work includes all relevant citations. - clarity The article is well written and easy to understand. - experiments Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different. Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration. - conclusion / recommendation The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS*16. A quantitative and qualitative comparison with Battaglia et al. is lacking. But the authors state that their work was developed independently. Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other *bottom-up* approaches like that of Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component. Such approaches could be potentially more useful of a robot in *common-sense* everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear.']","[75, 75, -20]","[80, 75, 50]","['The review uses phrases like ""This paper was a pleasure to read"" and ""It offers a novel approach... which represents a significant step forward"", which indicate a positive sentiment. The reviewer also provides constructive feedback and suggestions, but these are presented in a neutral and professional manner. There\'s no use of harsh language or personal criticism.', 'The reviewer finds the paper interesting and potentially impactful (""it seems like it may lead to interesting and significant follow up work in the field""). They do have some concerns, mainly around the evaluation, but overall are positive and recommend publication (""this is an interesting paper and should be published""). The language used is constructive and professional throughout.', 'The review acknowledges the novelty of the differentiable NPE concept but points out significant overlap with concurrent work (Battaglia et al.). While praising the clarity and some aspects of the experiments, the reviewer criticizes the lack of comparison with Battaglia et al. and questions the practical advantages and potential applications of the NPE. The language is critical but professional and avoids personal attacks.']"
"['This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.', 'The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer. Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space. Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as “The parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles” can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting. In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.', 'CONTRIBUTIONS The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results. NOVELTY The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style. MISSING CITATION The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al: Li and Wand, *Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks*, ECCV 2016 CLARITY The paper is very well written and easy to follow. SIGNIFICANCE Though simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device. EVALUATION Like many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively. SUMMARY The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference. Pros - Simple modification to feedforward neural style transfer with several improvements over prior work - Strong qualitative results - Well-written - Open-source code has already been released Cons - Slightly incremental - Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic']","[60, 90, 85]","[80, 100, 90]","['The reviewer starts by acknowledging the paper addresses a relevant problem and finds the results compelling. They do express some uncertainty due to lack of expertise in the specific area but are open to supporting the paper. The feedback focuses on improving clarity and presentation, which suggests a positive disposition towards the paper. The language used is constructive and professional throughout.', 'The review is overwhelmingly positive. The reviewer uses terms like ""elegant method"", ""valuable contribution"", and ""considerable impact"" to describe the paper. While it does point out areas of potential improvement, it frames them as opportunities for further exploration rather than criticisms.', ""The review is very positive overall. The reviewer points out the significance of the work, praises the clarity of the writing, and finds the experiments convincing. While some minor drawbacks are mentioned (incrementality, lack of quantitative evaluation), these are framed as common within the field and do not detract from the reviewer's positive view. The language used throughout is polite and professional.""]"
"['I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.', 'This paper proposes a method for representing sentences as a 2d matrix by utilizing a self-attentive mechanism on the hidden states of a bi-directional LSTM encoder. This work differs from prior work mainly in the 2d structure of embedding, which the authors use to produce heat-map visualizations of input sentences and to generate good performance on several downstream tasks. There is a substantial amount of prior work which the authors do not appropriately address, some of which is listed in previous comments. The main novelty of this work is in the 2d structure of embeddings, and as such, I would have liked to see this structure investigated in much more depth. Specifically, a couple important relevant experiments would have been: * How do the performance and visualizations change as the number of attention vectors (r) varies? * For a fixed parameter budget, how important is using multiple attention vectors versus, say, using a larger hidden state or embedding size? I would recommend changing some of the presentation in the penalization term section. Specifically, the statement that *the best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors* runs somewhat counter to the authors* comments about this topic below. In Fig. (2), I did not find the visualizations to provide particularly compelling evidence that the multiple attention vectors were doing much of interest beyond a single attention vector, even with penalization. To me this seems like a necessary component to support the main claims of this paper. Overall, while I found the architecture interesting, I am not convinced that the model*s main innovation -- the 2d structure of the embedding matrix -- is actually doing anything important or meaningful beyond what is being accomplished by similar attentive embedding models already present in the literature. Further experiments demonstrating this effect would be necessary for me to give this paper my full endorsement.', 'This paper introduces a sentence encoding model (for use within larger text understanding models) that can extract a matrix-valued sentence representation by way of within-sentence attention. The new model lends itself to (slightly) more informative visualizations than could be gotten otherwise, and beats reasonable baselines on three datasets. The paper is reasonably clear, I see no major technical issues, and the proposed model is novel and effective. It could plausibly be relevant to sequence modeling tasks beyond NLP. I recommend acceptance. There is one fairly serious writing issue that I*d like to see fixed, though: The abstract, introduction, and related work sections are all heavily skewed towards unsupervised learning. The paper doesn*t appear to be doing unsupervised learning, and the ideas are no more nor less suited to unsupervised learning than any other mainstream ideas in the sentence encoding literature. Details: - You should be clearer about how you expect these embeddings to be used, since that will be of certain interest to anyone attempting to use the results of this work. In particular, how you should convert the matrix representation into a vector for downstream tasks that require one. Some of the content of your reply to my comment could be reasonably added to the paper. - A graphical representation of the structure of the model would be helpful. - The LSTMN (Cheng et al., EMNLP *16) is similar enough to this work that an explicit comparison would be helpful. Again, incorporating your reply to my comment into the paper would be more than adequate. - Jiwei Li et al. (Visualizing and Understanding Neural Models in NLP, NAACL *15) present an alternative way of visualizing the influence of words on sentence encodings without using cross-sentence attention. A brief explicit comparison would be nice here.']","[40, -25, 85]","[60, 0, 90]","['The reviewer starts with positive remarks, acknowledging the good idea and consistent gains. However, they raise several concerns and ask for more experiments and analysis, which suggests that the review is more critical than merely positive. The language used is constructive and professional, suggesting a neutral to polite tone.', 'The reviewer acknowledges the novelty of the 2D embedding structure but expresses concerns about its practical significance. They find the supporting evidence, particularly the visualizations, not compelling enough. The reviewer suggests additional experiments to validate the claims and address the lack of comparison with existing models. The language used is critical but professional and provides concrete suggestions for improvement. Therefore, the sentiment leans towards the negative side, but the politeness remains neutral.', 'The reviewer explicitly recommends acceptance and calls the work novel and effective. They do not raise any major issues and their suggestions for improvement are constructive and typical of peer reviews. Thus, the sentiment is considered positive. The language used is polite and professional, with no signs of harsh criticism or disrespect.']"

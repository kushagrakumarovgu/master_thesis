title,Authors,decision,year,academic_age,current_age,total_num_pub,total_num_conference,total_num_informal,total_num_journal,review,rating_score,rating_text,confidence_score,confidence_text
Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,"['William Falcon', 'Henning Schulzrinne']",Accept,2018,"[2, 33]","[6, 37]","[6, 406]","[2, 200]","[3, 116]","[1, 90]","Update: Based on the discussions and the revisions, I have improved my rating. However I still feel like the novelty is somewhat limited, hence the recommendation.

======================

The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM to determine when a smartphone enters or exits a building, then using the change in barometric pressure from the entrance of the building to indoor location. Overall the methodology is a fairly simple application of existing methods to a problem, and  there remain some methodological issues (see below).

General Comments
- The claim that the bmp280 device is in most smartphones today doesn’t seem to be backed up by the “comScore” reference (a simple ranking of manufacturers).  Please provide the original source for this information.
- Almost all exciting results based on RNNs are achieved with LSTMs, so calling an RNN with LSTM hidden units a new name IOLSTM seems rather strange - this is simply an LSTM.
- There exist models for modelling multiple levels of abstraction, such as the contextual LSTM of [1]. This would be much more satisfying that the two level approach taken here, would likely perform better, would replace the need for the clustering method, and would solve issues such as the user being on the roof.  The only caveat is that it may require an encoding of the building (through a one-hot encoding) to ensure that the relationship between the floor height and barometric pressure is learnt. For unseen buildings a background class could be used, the estimators as used before, or aggregation of the other buildings by turning the whole vector on.
- It’s not clear if a bias of 1 was added to the forget gate of the LSTM or not. This has been shown to improve results [2].
- Overall the whole pipeline feels very ad-hoc, with many hand-tuned parameters.  Notwithstanding the network architecture, here I’m referring to the window for the barometric pressure, the Jaccard distance threshold, the binary mask lengths, and the time window for selecting p0.
- Are there plans to release the data and/or the code for the experiments? Currently the results would be impossible to reproduce.
- The typo of accuracy given by the authors is somewhat worrying, given that the result is repeated several times in the paper.

Typographical Issues
- Page 1:  ”floor-level accuracy” back ticks
- Page  4:   Figure  4.1→Figure  1;  Nawarathne  et  al  Nawarathne  et  al.→Nawarathne et al.
- Page 6:  ”carpet to carpet” back ticks
- Table 2:  What does -4+ mean?
- References.  The references should have capitalisation where appropriate.For example,  Iodetector→IODetector,  wi-fi→Wi-Fi,  apple→Apple, iphone→iPhone, i→I etc.

[1]  Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, and LarryHeck. Contextual LSTM (CLSTM) models for large scale NLP tasks. arXivpreprint arXiv:1602.06291, 2016.
[2]  Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.  An empirical exploration of recurrent network architectures.  InProceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2342–2350,2015","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Identifying Analogies Across Domains,"['Yedid Hoshen', 'Lior Wolf']",Accept,2018,"[6, 19]","[11, 24]","[74, 413]","[32, 213]","[41, 165]","[1, 35]","The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets.

While the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.

Instead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insight.

Also, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean.



","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,"['Tao Shen', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']",Accept,2018,"[2, 10, 8, 8, 29]","[7, 15, 13, 13, 34]","[63, 112, 200, 123, 413]","[27, 55, 89, 59, 229]","[35, 48, 81, 54, 44]","[1, 9, 30, 10, 140]","This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP. The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications. The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed.

The advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.

While I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR. First, there is not much innovation in the model architecture. The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level. I think this more counts as careful engineering of the SAN model rather than a main innovation. Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders. What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?

====

I appreciate the answers the authors added and I change the score to 6.","[6, 6, 9]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling,"['Hao Zhang', 'Bo Chen', 'Dandan Guo', 'Mingyuan Zhou']",Accept,2018,"[4, 13, 8, 11]","[9, 18, 12, 16]","[41, 136, 32, 213]","[15, 46, 12, 99]","[12, 24, 9, 92]","[14, 66, 11, 22]","The authors propose a hybrid Bayesian inference approach for deep topic models that integrates stochastic gradient MCMC for global parameters and Weibull-based multilayer variational autoencoders (VAEs) for local parameters. The decoding arm of the VAE consists of deep latent Dirichlet allocation, and an upward-downward structure for the encoder. Gamma distributions are approximated as Weibull distributions since the Kullback-Leibler divergence is known and samples can be efficiently drawn from a transformation of samples from a uniform distribution. 

The results in Table 1 are concerning for several reasons, i) the proposed approach underperfroms DLDA-Gibbs and DLDA-TLASGR. ii) The authors point to the scalability of the mini-batch-based algorithms, however, although more expensive, DLDA-Gibbs, is not prohibitive given results for Wikipedia are provided. iii) The proposed approach is certainly faster at test time, however, it is not clear to me in which settings such speed (compared to Gibbs) would be needed, provided the unsupervised nature of the task at hand. iv) It is not clear to me why there is no test-time difference between WAI and WHAI, considering that in the latter, global parameters are sampled via stochastic-gradient MCMC. One possible explanation being that during test time, the approach does not use samples from W but rather a summary of them, say posterior means, in which case, it defeats the purpose of sampling from global parameters, which may explain why WAI and WHAI perform about the same in the 3 datasets considered.

- \Phi is in a subset of R_+, in fact, columns of \Phi are in the P_0-dimensional simplex.
- \Phi should have K_1 columns not K.
- The first paragraph in Page 5 is very confusing because h is introduced before explicitly connecting it to k and \lambda. Also, if k = \lambda, why introduce different notations?","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,"['Pouya Samangouei', 'Maya Kabkab', 'Rama Chellappa']",Accept,2018,"[4, 7, 38]","[6, 11, 43]","[16, 12, 910]","[9, 7, 494]","[6, 4, 140]","[1, 1, 276]","This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.  Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).  This projected input is then used to produce the classification probabilities.  The authors test their method on various adversarially constructed inputs (with varying degrees of noise). 

Questions/Comments:

- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.  Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?  If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?  

- Is there anything special about the GAN approach, versus other generative approaches? 

- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?  Is that what is meant by the ""defense network"" (in experiments bullet 2)?

- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? 

Quality: The method appears to be technically correct.

Clarity: This paper clearly written; both method and experiments are presented well. 

Originality: I am not familiar enough with adversarial learning to assess the novelty of this approach. 

Significance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration,"['Alexandre Péré', 'Sébastien Forestier', 'Olivier Sigaud', 'Pierre-Yves Oudeyer']",Accept,2018,"[1, 6, 19, 20]","[3, 11, 24, 25]","[6, 14, 139, 209]","[2, 6, 64, 92]","[3, 4, 44, 72]","[1, 4, 31, 45]","The paper investigates different representation learning methods to create a latent space for intrinsic goal generation in guided exploration algorithms.  The research is in principle very important and interesting.

The introduction discusses a great deal about intrinsic motivations and about goal generating algorithms. This is really great, just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals.
I think the paper (not only the Intro) could be a bit condensed to more concentrate on the actual contribution. 

The contribution is that the quality of the representation and the sampling of goals is important for the exploration performance and that classical methods like ISOMap are better than Autoencoder-type methods. 

Also, it is written in the Conclusions (and in other places): ""[..] we propose a new intrinsically Motivated goal exploration strategy...."". This is not really true.  There is nothing new with the intrinsically motivated selection of goals here, just that they are in another space. Also, there is no intrinsic motivation. I also think the title is misleading.

The paper is in principle interesting. However, I doubt that the experimental evaluations are substantial enough for profound conclusion. 

Several points of critic: 
- the input space was very simple in all experiments, not suitable for distinguishing between the algorithms, for instance, ISOMap typically suffers from noise and higher dimensional manifolds, etc.
- only the ball/arrow was in the input image, not the robotic arm. I understand this because in phase 1 the robot would not move, but this connects to the next point:
- The representation learning is only a preprocessing step requiring a magic first phase.
    -> Representation is not updated during exploration
- The performance of any algorithm (except FI) in the Arm-Arrow task is really bad but without comment. 
- I am skeptical about the VAE  and RFVAE results. The difference between Gaussian sampling and the KDE is a bit alarming, as the KL in the VAE training is supposed to match the p(z) with N(0,1). Given the power of the encoder/decoder it should be possible to properly represent the simple embedded 2D/3D manifold and not just a very small part of it as suggested by Fig 10. 
I have a hard time believing these results. I urge you to check for any potential errors made. If there are not mistakes then this is indeed alarming.

Questions:
- Is it true that the robot always starts from same initial condition?! Context=Emptyset. 
- For ISOMap etc, you also used a 10dim embedding?

Suggestion:
- The main problem seems to be that some algorithms are not representing the whole input space.
- an additional measure that quantifies the difference between true input distribution and reproduced input distribution could tier the algorithms apart and would measure more what seems to be relevant here.  One could for instance measure the KL-divergence between the true input and the sampled (reconstructed) input (using samples and KDE or the like). 
- This could be evaluated on many different inputs (also those with a bit more complicated structure) without actually performing the goal finding.
- BTW: I think Fig 10 is rather illustrative and should be somehow in the main part of the paper
 
On the positive side, the paper provides lots of details in the Appendix.
Also, it uses many different Representation Learning algorithms and uses measures from manifold learning to access their quality.

In the related literature, in particular concerning the intrinsic motivation, I think the following papers are relevant:
J. Schmidhuber, PowerPlay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Front. Psychol., 2013.

and

G. Martius, R. Der, and N. Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013.


Typos and small details:
p3 par2: for PCA you cited Bishop. Not critical, but either cite one the original papers or maybe remove the cite altogether
p4 par-2: has multiple interests...: interests -> purposes?
p4 par-1: Outcome Space to the agent is is ...
Sec 2.2 par1: are rapidly mentioned... -> briefly
Sec 2.3 ...Outcome Space O, we can rewrite the architecture as:
  and then comes the algorithm. This is a bit weird
Sec 3: par1: experimental campaign -> experiments?
p7: Context Space: the object was reset to a random position or always to the same position?
Footnote 14: superior to -> larger than
p8 par2: Exploration Ratio Ratio_expl... probably also want to add (ER) as it is later used
Sec 4: slightly underneath -> slightly below
p9 par1: unfinished sentence: It is worth noting that the....
one sentence later: RP architecture? RPE?
Fig 3: the error of the methods (except FI) are really bad. An MSE of 1 means hardly any performance!
p11 par2: for e.g. with the SAGG..... grammar?

Plots in general: use bigger font sizes.

","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Word translation without parallel data,"['Guillaume Lample', 'Alexis Conneau', ""Marc'Aurelio Ranzato"", 'Ludovic Denoyer', 'Hervé Jégou']",Accept,2018,"[4, 3, 13, 16, 16]","[9, 8, 18, 20, 21]","[51, 56, 106, 147, 178]","[24, 27, 57, 88, 89]","[27, 29, 44, 40, 59]","[0, 0, 5, 19, 30]","An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning.

The paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times.

There are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).

The evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.

In my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.","[8, 9, 3]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Clear rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties,"['Yi Zhou', 'Yingbin Liang']",Accept,2018,"[4, 15]","[8, 20]","[81, 275]","[37, 112]","[36, 99]","[8, 64]","Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks. The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points.

The paper is well organized and well written. Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points. This technique generalizes existing work under full relaxation of assumptions. In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices; In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices. Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work.

The results obtained from the analytical forms of the critical points are interesting, but one problem is that how to obtain the proper solution of equation (3)? In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12). It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm,"['Chelsea Finn', 'Sergey Levine']",Accept,2018,"[6, 10]","[11, 15]","[306, 743]","[128, 326]","[172, 396]","[6, 21]","The paper provides proof that gradient-based meta-learners (e.g. MAML) are ""universal leaning algorithm approximators"".

Pro:
- Generally well-written with a clear (theoretical) goal
- If the K-shot proof is correct*, the paper constitutes a significant contribution to the theoretical understanding of meta-learning.
- Timely and relevant to a large portion of the ICLR community (assuming the proofs are correct)

Con:
- The theoretical and empirical parts seem quite disconnected. The theoretical results are not applied nor demonstrated in the empirical section and only functions as an underlying premise. I wonder if a purely theoretical contribution would be preferable (or with even fewer empirical results).

* It has not yet been possible for me to check all the technical details and proofs.
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[1, 1, 3]","["" The reviewer's evaluation is an educated guess"", "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']"
Maximum a Posteriori Policy Optimisation,"['Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Yuval Tassa', 'Remi Munos', 'Nicolas Heess', 'Martin Riedmiller']",Accept,2018,"[8, 7, 12, 23, 10, 26]","[13, 12, 17, 28, 15, 31]","[69, 76, 51, 266, 200, 168]","[33, 37, 23, 143, 81, 100]","[30, 35, 24, 96, 111, 47]","[6, 4, 4, 27, 8, 21]","The paper presents a new algorithm for inference-based reinforcement learning for deep RL. The algorithm decomposes the policy update in two steps, an E and an M-step. In the E-step, the algorithm estimates a variational distribution q which is subsequentially used for the M-step to obtain a new policy. Two versions of the algorithm are presented, using a parametric or a non-parametric (sample-based) distribution for q. The algorithm is used in combination with the retrace algorithm to estimate the q-function, which is also needed in the policy update.

This is a well written paper presenting an interesting algorithm. The algorithm is similar to other inference-based RL algorithm, but is the first application of inference based RL to deep reinforcement learning. The results look very promising and define a new state of the art or deep reinforcement learning in continuous control, which is a very active topic right now. Hence, I think the paper should be accepted. 


I do have a few comments / corrections / questions about the paper:

- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?

- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    

- I did not get whether the additional ""one-step KL regularisation"" is obtained from the lower bound or just added as additional regularisation? Could you explain?

- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?

- I think the following experiments would be very informative:

   - MPO without trust region in M-step
   
   - MPO without retrace algorithm for getting the Q-value

   - test different epsilons for E and M step


","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 1]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
The Implicit Bias of Gradient Descent on Separable Data,"['Daniel Soudry', 'Elad Hoffer', 'Mor Shpigel Nacson', 'Nathan Srebro']",Accept,2018,"[9, 4, 1, 18]","[14, 8, 6, 23]","[90, 35, 16, 269]","[35, 14, 8, 140]","[42, 19, 7, 115]","[13, 2, 1, 14]","(a) Significance
The main contribution of this paper is to characterize the implicit bias introduced by gradient descent on separable data. The authors show the exact form of this bias (L_2 maximum margin separator), which is independent of the initialization and step size. The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small. The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods, such as coordinate descent. In addition, the proposed analytic framework seems promising since it may be extended to analyze other models, like neural networks.

(b) Originality
This is the first work to give the detailed characterizations of the implicit bias of gradient descent on separable data. The proposed assumptions are reasonable, but it seems to limit to the loss function with exponential tail. I’m curious whether the result in this paper can be applied to other loss functions, such as hinge loss.

(c) Clarity & Quality 
The presentation of this paper is OK. However, there are some places can be improved in this paper. For example, in Lemma 1, results (3) and (4) can be combined together. It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1. 

Minor comments: 
1. In Lemma 1 (4), w^T(t)->w(t)^T
2. In the proof of Lemma 1, it’s better to use vector 0 for the gradient L(w)
3. In Theorem 4, the authors should specify eta
4. In appendix A, page 11, beta is double used
5. In appendix D, equation (D.5) has an extra period
","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Online Learning Rate Adaptation with Hypergradient Descent,"['Atilim Gunes Baydin', 'Robert Cornish', 'David Martinez Rubio', 'Mark Schmidt', 'Frank Wood']",Accept,2018,"[11, 2, 2, 14, 23]","[15, 6, 7, 19, 28]","[67, 10, 12, 117, 153]","[16, 5, 5, 55, 71]","[48, 5, 7, 51, 71]","[3, 0, 0, 11, 11]","SUMMARY:

The authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.


GENERAL IMPRESSION:

One central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value.
Despite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.


CRITICISM:

The experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \alpha_0 and \beta.

The extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.


MINOR POINTS:

page 4, bottom: use \citep for Duchi et al. (2011).

None of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve.

In figure 2, top row, please display the learning rate on a log scale.

page 8, line 7 in section 4.3: ""the the"" (unintended repetition)

End of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Syntax-Directed Variational Autoencoder for Structured Data,"['Hanjun Dai', 'Yingtao Tian', 'Bo Dai', 'Steven Skiena', 'Le Song']",Accept,2018,"[5, 3, 16, 34, 15]","[10, 8, 21, 38, 20]","[91, 41, 315, 233, 343]","[47, 19, 147, 122, 185]","[42, 22, 135, 44, 122]","[2, 0, 33, 67, 36]","NOTE: 

Would the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset? Fair comparison of the data is a serious concern. I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author's response.

*Overview*

This paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose “stochastic lazy attributes”, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid. Using Bayesian Optimization, search over this space can yield decodings with targeted properties.

Many of the ideas presented are novel. The results presented are state-of-the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem. This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning.

*Remarks on methodology*

By initializing a decoding by “guessing” a value, the decoder will focus on high-probability starting regions of the space of possible structures. It is not clear to me immediately how this will affect the output distribution. Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space. Does this sacrifice exploration for exploitation in some quantifiable way? Some exploration of this issue or commentary would be valuable. 

*Nitpicks*

I found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background. In my opinion, this section could benefit from a little more expansion and conceptual definition.

The first 3 sections of the paper are very clearly written, but the remainder has many typos and grammatical errors (often word omission). The draft could use a few more passes before publication.
","[7, 5, 3]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 1, 2]","[' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Learning to Count Objects in Natural Images for Visual Question Answering,"['Yan Zhang', 'Jonathon Hare', 'Adam Prügel-Bennett']",Accept,2018,"[1, 15, 26]","[3, 20, 31]","[8, 101, 107]","[4, 57, 48]","[4, 27, 21]","[0, 17, 38]","Summary
 - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism. The authors propose a differentiable counting component, which explicitly counts the number of objects. Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals. In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset.

Strengths
 - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly. 
 - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model.
 - The proposed model can be easily applied to any VQA model using soft attention. 
 - The paper is well written and the contribution is clear.

Weaknesses
 - Although the proposed model is helpful to model counting information in VQA, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. 
 - Qualitative examples of intermediate values in counting component--adjacency matrix (A), distance matrix (D) and count matrix (C)--need to be presented to show the contribution of each part, especially in the real examples that are not compatible with the strong assumptions in modeling counting component.

Comments
 - It is not clear if the value of count ""c"" is same with the final answer in counting questions. 

","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Smooth Loss Functions for Deep Top-k Classification,"['Leonard Berrada', 'Andrew Zisserman', 'M. Pawan Kumar']",Accept,2018,"[3, 34, 17]","[8, 39, 22]","[15, 745, 118]","[5, 455, 57]","[9, 187, 43]","[1, 103, 18]","This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks. The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the ""temperature parameter"". The paper is well organized and clearly written. The idea deserves a publication.

On the other hand, there might be better and more direct solutions to reduce the combinatorial complexity. When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}.","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,"['Pietro Morerio', 'Jacopo Cavazza', 'Vittorio Murino']",Accept,2018,"[7, 3, 29]","[12, 7, 34]","[85, 41, 443]","[41, 18, 286]","[33, 20, 59]","[11, 3, 98]","Summary:
This paper proposes minimal-entropy correlation alignment, an unsupervised domain adaptation algorithm which links together two prior class of methods: entropy minimization and correlation alignment. Interesting new idea. Make a simple change in the distance function and now can perform adaptation which aligns with minimal entropy on target domain and thus can allow for removal of hyperparameter (or automatic validation of correct one).

Strengths
-  The paper is clearly written and effectively makes a simple claim that geodesic distance minimization is better aligned to final performance than euclidean distance minimization between source and target. 
- Figures 1 and 2 (right side) are particularly useful for fast understanding of the concept and main result.


Questions/Concerns:
- Can entropy minimization on target be used with other methods for DA param tuning? Does it require that the model was trained to minimize the geodesic correlation distance between source and target?
- It would be helpful to have a longer discussion on the connection with Geodesic flow kernel [1] and other unsupervised manifold based alignment methods [2]. Is this proposed approach an extension of this prior work to the case of non-fixed representations in the same way that Deep CORAL generalized CORAL?
- Why does performance suffer compared to TRIPLE on the SYN->SVHN task? Is there some benefit to the TRIPLE method which may be combined with the MECA approach?

					
[1] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, 2012.
					
[2] Raghuraman Gopalan and Ruonan Li. Domain adaptation for object recognition: An unsupervised approach. In ICCV, 2011. 
","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
PixelNN: Example-based Image Synthesis,"['Aayush Bansal', 'Yaser Sheikh', 'Deva Ramanan']",Accept,2018,"[6, 15, 19]","[11, 20, 24]","[31, 148, 255]","[12, 76, 134]","[18, 37, 102]","[1, 35, 19]","This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets. 

Pros:
- Simple approach based on nearest-neighbors, likely easier to train compared to GANs.
- Scales to high-resolution images.

Cons:
- Requires a potentially costly search procedure to generate images.
- Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task.","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Trust-PCL: An Off-Policy Trust Region Method for Continuous Control,"['Ofir Nachum', 'Mohammad Norouzi', 'Kelvin Xu', 'Dale Schuurmans']",Accept,2018,"[3, 10, 4, 30]","[8, 15, 8, 35]","[104, 136, 29, 286]","[42, 59, 13, 182]","[61, 72, 15, 82]","[1, 5, 1, 22]","This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient.

The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing.

In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. 

What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added.

Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Stochastic Activation Pruning for Robust Adversarial Defense,"['Guneet S. Dhillon', 'Kamyar Azizzadenesheli', 'Zachary C. Lipton', 'Jeremy D. Bernstein', 'Jean Kossaifi', 'Aran Khanna', 'Animashree Anandkumar']",Accept,2018,"[1, 3, 5, 2, 5, 2, 13]","[4, 8, 10, 7, 10, 4, 18]","[7, 90, 203, 23, 56, 8, 419]","[3, 26, 77, 10, 19, 3, 154]","[4, 56, 117, 13, 25, 4, 223]","[0, 8, 9, 0, 12, 1, 42]","The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations. Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.

On one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks. On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness. For example, how does this compare to random perturbation (say, zero-mean) of the weights? This adds stochasticity as well so why and why not this work? The authors do not give any insight in this regard.

Overall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner. The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).

","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Sparse Latent Representations with the Deep Copula Information Bottleneck,"['Aleksander Wieczorek*', 'Mario Wieser*', 'Damian Murezzan', 'Volker Roth']",Accept,2018,"[8, 1, 4, 20]","[11, 5, 4, 25]","[17, 19, 4, 97]","[7, 8, 3, 52]","[6, 9, 1, 23]","[4, 2, 0, 22]","The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version. The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'. However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations?

Quality: Ok. The claims appear to be sufficiently verified in the experiments. However, it would have been great to have an experiment that actually makes use of the learned features to make predictions. I struggle a little to see the relevance of the proposed method without a good motivating example.

Clarity: Below average. Section 3 is a little hard to understand. Is q(t|x) in Fig 1 a typo? How about t_j in equation (5)? There is a reference that appeared twice in the bibliography (1st and 2nd).

Originality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model.","[6, 6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[1, 3, 4, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Meta-Learning for Semi-Supervised Few-Shot Classification,"['Mengye Ren', 'Eleni Triantafillou', 'Sachin Ravi', 'Jake Snell', 'Kevin Swersky', 'Joshua B. Tenenbaum', 'Hugo Larochelle', 'Richard S. Zemel']",Accept,2018,"[4, 3, 4, 4, 9, 25, 14, 30]","[8, 8, 8, 9, 14, 30, 19, 35]","[65, 14, 9, 16, 75, 610, 162, 243]","[29, 7, 4, 8, 35, 353, 64, 136]","[36, 7, 5, 8, 38, 226, 74, 83]","[0, 0, 0, 0, 2, 31, 24, 24]","In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes.  The studied problem is interesting, and the paper is well-written. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.  While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning).On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers. Overall, I would like to vote for a weakly acceptance regarding this paper.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep contextualized word representations,"['Matthew E Peters', 'Mark Neumann', 'Mohit Iyyer', 'Matt Gardner', 'Christopher Clark', 'Kenton Lee', 'Luke Zettlemoyer']",Accept,2018,"[-2, 7]","[1, 12]","[1, 23]","[1, 9]","[0, 2]","[0, 12]","This paper proposes a method to learn contextualized word representations (ELMO) by pretraining a multilayer bidirectional LSTM language model and using representations from all levels of the LSTM in the input or output layer of a supervised task of interest. 
Experiments on various datasets (SNLI, SQuAD, SRL, Coref, NER, SST) show that the proposed method improve over baseline models.
Ablation analysis demonstrate that using all layers of ELMO is always better than just using only the final layer, and that representations learned by ELMO capture basic notions of word senses and part of speeches.

The paper is well written and I think learning contextualized word representations is an important topic.
However, one thing that I am not sure about from experiments in the paper is whether the improvements come from an increase in model capacity and (unlabeled) data used to train the model, or whether there are more interesting things going on.
- What makes the proposed approach different than just a deeper architecture for each of the considered tasks, where some parts of the network are trained using unlabeled data?
- Is the pretraining with unlabeled data necessary, or can we just have this deep architecture and train everything with the available supervised data?
- An ELMO enhanced model has more parameters than the baseline model for each task. What is the performance of the (non deep) baseline method with comparable number of parameters (bigger hidden size)?

More generally, it is not surprising that given sufficient training data, a deeper model (e.g., ELMO enhanced models) with multiple connections across layers will perform better than shallower models with fewer parameters.
I would like to see more analysis and/or explanations on why the proposed method contributes more beyond this.","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Stochastic Variational Video Prediction,"['Mohammad Babaeizadeh', 'Chelsea Finn', 'Dumitru Erhan', 'Roy H. Campbell', 'Sergey Levine']",Accept,2018,"[7, 41, 10]","[11, 45, 15]","[27, 376, 123]","[19, 247, 57]","[7, 16, 51]","[1, 113, 15]","The submission presents a method or video prediction from single (or multiple) frames, which is capable of producing stochastic predictions by means of training a variational encoder-decoder model. Stochastic video prediction is a (still) somewhat under-researched direction, due to its inherent difficulty.

The method can take on several variants: time-invariant [latent variable] vs. time-variant, or action-conditioned vs unconditioned. The generative part of the method is mostly borrowed from Finn et al. (2016). Figure 1 clearly motivates the problem. The method itself is fairly clearly described in Section 3; in particular, it is clear why conditioning on all frames during training is helpful. As a small remark, however, it remains unclear what the action vector a_t is comprised of, also in the experiments.

The experimental results are good-looking, especially when looking at the provided web site images. 
The main goal of the quantitative comparison results (Section 5.2) is to determine whether the true future is among the generated futures. While this is important, a question that remains un-discussed is whether all generated stochastic samples are from realistic futures. The employed metrics (best PSNR/SSIM among multiple samples) can only capture the former, and are also pixel-based, not perceptual.

The quantitative comparisons are mostly convincing, but Figure 6 needs some further clarification. It is mentioned in the text that ""time-varying latent sampling is more stable beyond the time horizon used during training"". While true for Figure 6b), this statement is contradicted by both Figure 6a) and 6c), and Figure 6d) seems to be missing the time-invariant version completely (or it overlaps exactly, which would also need explanation). As such, I'm not completely clear on whether the time variant or invariant version is the stronger performer.

The qualitative comparisons (Section 5.3) are difficult to assess in the printed material, or even on-screen. The animated images on the web site provide a much better impression of the true capabilities, and I find them convincing.

The experiments only compare to Reed et al. (2017)/Kalchbrenner et al. (2017), with Finn at al. (2016) as a non-stochastic baseline, but no comparisons to, e.g., Vondrick et al. (2016) are given. Stochastic prediction with generative adversarial networks is a bit dismissed in Section 2 with a mention of the mode-collapse problem.

Overall the submission makes a significant enough contribution by demonstrating a (mostly) working stochastic prediction method on real data.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
An efficient framework for learning sentence representations,"['Lajanugen Logeswaran', 'Honglak Lee']",Accept,2018,"[5, 14]","[10, 19]","[22, 273]","[8, 145]","[14, 123]","[0, 5]","[REVISION]

Thank you for your clarification. I appreciate the effort and think it has improved the paper. I have updated my score accordingly

====== 

This paper proposes a new objective for learning SkipThought-style sentence representations from corpora of ordered sentences. The algorithm is much faster than SkipThoughts as it swaps the word-level decoder for a contrastive classification loss. 

Comments:

Since one of the key advantages of this method is the speed, I was surprised there was not a more formal comparison of the speed of training different models. For instance, it would be more convincing if two otherwise identical encoders were trained on the same machine on the books corpus with the proposed objective and the skipthoughts decoding objective, and the representations compared after X hours of training. The reported 2 weeks required to train Skipthoughts comes from the paper, but things might be faster now with more up-to-date deep learning libraries etc. If this was what was in fact done, then it's probably just a case of presenting the comparison in a more formal way. I would also lose the sentence ""we are able to train many models in the time it takes to train most unsupervised"" (see next point for reasons why this is questionable).

It would have been interesting to apply this method with BOW encoders, which should be even faster than RNN-based encoders reported in this paper. The faster BOW models tend to give better performance on cosine-similarity evaluations ( quantifying the nearest-neighbour analysis that the authors use in this paper). Indeed, it would be interesting (although of course not definitive) to see comparison of the proposed algorithm (with BOW and RNN encoders) on cosine sentence similarity evaluations. 

The proposed novelty is simple and intuitive, which I think is a strength of the method. However, a simple idea makes overlap with other proposed approaches more likely, and I'd like the author to check through the public comments to ensure that all previous related ideas are noted in this paper. 

I think the authors could do more to emphasise what the point is of trying to learn sentence embeddings. An idea of the eventual applications of these embeddings would make it easier to determine, for instance, whether the supervised ensembling method applied here would be applicable in practice. Moreover, many papers have emphasised the limitations of the evaluations used in this paper (although they are still commonly used) so it would be good to acknowledge that it's hard to draw too many conclusions from such numbers. That said, the numbers are comparable Skipthoughts, so it's clear that this method learns representations of comparable quality. 

The justification for the proposed algorithm is clear in terms of efficiency, but I don't think it's immediately clear from a semantic / linguistic point of view. The statement ""The meaning of a sentence is the property that creates bonds...."" seems to have been cooked up to justify the algorithm, not vice versa. I would cut all of that speculation out and focus on empirically verifiable advantages. 

The section of image embeddings comes completely out of the blue and is very hard to interpret. I'm still not sure I understand this evaluation (short of looking up the Kiros et al. paper), or how the proposed model is applied to a multi-modal task.

There is much scope to add more structured analysis of the type hinted by the nearest neighbours section. Cherry picked lists don't tell the reader much, but statistics or more general linguistic trends can be found in these neighbours and aggregated, that could be very interesting. 

","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the importance of single directions for generalization,"['Ari S. Morcos', 'David G.T. Barrett', 'Neil C. Rabinowitz', 'Matthew Botvinick']",Accept,2018,"[1, 7, 4, 18]","[6, 12, 8, 23]","[66, 32, 29, 110]","[22, 15, 11, 40]","[43, 17, 17, 54]","[1, 0, 1, 16]","This is an ""analyze why"" style of paper:  the authors attempt to explain the relationship between some network property (in this case, ""reliance on single directions""), and a desired performance metric (in this case, generalization ability).   The authors quantify a variety of related ways to measure ""reliance on single directions"" and show that the more reliant on a single directions a given network is, the less well it generalizes.   

Clarity:  The paper is fairly clearly written.  Sometimes key details are in the footnotes (e.g. see footnote 3) -- not sure why -- but on the  whole, I think the followed the paper reasonably well.  

Quality: The work makes a good-faith attempt to be fairly systematic -- e.g evaluating several different types of network structures, with reasonable numbers of random initializations, and also illustrates the main point in several different comparatively independent-seeming ways.  I feel fairly confident that the results are basically right within the somewhat limited domain that the authors explore. 

Originality: This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks. I don't know that the concept of ""reliance on a single direction"" seems especially novel to me, but on the other hand, I can't think of another paper that precisely investigates this notion the way it is done here.   

Significance: The work touches on some important issues.  I think the demonstration that the existence of strongly class-selective neurons is not a good correlate for generalization is interesting.   This point illustrates something that has made me a bit uncomfortable with the trend toward ""interpretable machine learning"" that has been arising recently:  in many of those results, it is shown that some fraction of the units at various levels of a trained deepnet have optimal driving stimuli that seem somewhat interpretable, with the implication that the existence of such units is an important correlate of network performance.  There has even been some claims that better-performing networks have more ""single-direction"" interpretable units [1].  The fact that the current results seem directly in contradiction to that line of work is interesting, and the connections to batch normalization and dropout are for the same reason interesting.  However, I wish the authors had grappled more directly with the apparent contradiction with (e.g.) [1].   There is probably a kind of tradeoff here.   The closer the training dataset is to what is being tested for ""generalization"", the more likely that having single-direction units is useful; and vice-versa.   I guess the big question is: what types of generalization are actually demanded / desired in real deployed machine learning systems (or in the brain)?  How does those cases compare with the toy examples analyzed here?   The paper doesn't go far enough in really addressing these questions, but it is sort of beginning to make an effort. 

However, for me the main failing of the paper is that it's fairly descriptive without being that prescriptive. Does using their metric of reliance on a single direction, as a regularizer in and of itself, add anything above any beyond existing regularizers (e.g. batch normalization or dropout)?  It doesn't seem like they tried. This seems to me the key question to understanding the significance of their results.   Is ""reliance on single direction"" actually a good regularizer as such, especially for ""real"" problems like (e.g.) training a deep Convnet on (e.g.) ImageNet or some other challenging dataset?  Would penalizing for this quantity improve the generalization of a network trained on ImageNet to other visual datasets (e.g. MS-COCO)?  If so, this would be a very significant result and would make me really care about their idea of ""reliance on a singe direction"".  If such results do not hold, it seems to me like one more theoretical possibility that would bite the dust when tested at scale.  

[1] http://netdissect.csail.mit.edu/final-network-dissection.pdf","[5, 7, 9]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 15% of accepted papers, strong accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,"['Forough Arabshahi', 'Sameer Singh', 'Animashree Anandkumar']",Accept,2018,"[6, 13, 13]","[11, 18, 18]","[18, 198, 419]","[8, 110, 154]","[10, 81, 223]","[0, 7, 42]","SUMMARY 

The model evaluates symbolic algebraic/trigonometric equalities for validity, with an output unit for validity level at the root of a tree of LSTM nodes feeding up to the root; the structure of the tree matches the parse tree of the input equation and the type of LSTM cell at each node matches the symbol at that node in the equation: there is a different cell type for each symbol. It is these cell types that are learned. The training data includes labeled true and false algebraic/trigonometric identities (stated over symbols for variables) as well as function-evaluation equalities such as ""tan(0.28) = 0.29"" and decimal-expansion equations like ""0.29 = 2*10^(-1) + 9*10^(-2)"".  I believe continuous values like ""0.29"" in the preceding expressions are encoded as the literal value of a single unit (feeding into an embedding unit of type W_{num}), whereas the symbols proper (including digit numerals) are encoded as 1-hot vectors (feeding into an embedding unit of type W_{symb}).
Performance is at least 97% when testing on unseen expressions of the same depth (up to 4) as the training data. Performance when trained on 3 levels (among 1 - 4) and testing on generalization to the held-out level is at least 96% when level 2 is held out, at least 92% when level 4 is withheld. Performance degrades (even on symbolic identities) when the function-evaluation equalities are omitted, and degrades when LSTM cells are replaced by plain RNN cells. The largest degradation is when the tree structure is replaced (presumably) by a sequence structure.
Performance was also tested on a fill-in-the-blank test, where a symbol from a correct equation was removed and all possible replacements for that symbol with expressions of depth up to 2 were tested, then ranked by the resulting validity score from the model. From the graph it looks like an accuracy of about 95% was achieved for the 1-best substituted expression (accuracy was about 32% for a sequential LSTM).

WEAKNESSES

* The title is misleading; ""blackbox function evaluation"" does not suggest what is intended, which is training on function-evaluation equations. The actual work is more interesting than what the title suggests.
* The biggest performance boost (roughly 15%) arises from use of the tree structure, which is given by an oracle (implemented in a symbolic expression parser, presumably): the network does not design its own example-dependent structure.
* What does the sympy baseline mean in Table 2? We are only told that sympy is a ""symbolic solver"". Yet the sympy performance scores are in the 70-80% range. If the solver’s performance is that weak, why is it used during generation of training data to determine the validity of possible equations?
* Given that this is a conference on ""learning representations"" it would have been nice to see at least a *little* examination of the learned representations. It would be easy to do some interesting tests. How well does the vector embedding for ""2*10^(-1) + 9*10^(-2)"" match the vector for the real value 0.29? W_{num} embeds a continuum of real values in R^d: what is this 1-dimensional embedding manifold like? How do the embeddings of different integers provided by W_{sym} relate to one another? My rating would have been higher had there been some analysis of the learned representations.
* We are told only that the ""hidden dimension … varies""; it would be nice if the text or results tables gave at least some idea of what magnitude of embedding dimension we’re talking about.

STRENGTHS

The weaknesses above notwithstanding, this is a very interesting piece of work with impressive results. 
* The number of functions learned, 28, is a quantum jump from previous studies using 8 or fewer functions.
* It is good to see the power of training the same system to learn the semantics of functions from the equations they satisfy AND from the values they produce. 
* The inclusion of decimal-expansion equations for relating numeral embeddings to number embeddings is clever. 
* The general method used for randomly generating a non-negligible proportion of true equations is useful.
* The evaluation of the model is thorough and clear.
* In fact the exposition in the paper as a whole is very clear.","[8, 5, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Complex Networks,"['Chiheb Trabelsi', 'Olexa Bilaniuk', 'Ying Zhang', 'Dmitriy Serdyuk', 'Sandeep Subramanian', 'Joao Felipe Santos', 'Soroush Mehri', 'Negar Rostamzadeh', 'Yoshua Bengio', 'Christopher J Pal']",Accept,2018,"[-2, 4, 6, 20]","[3, 9, 11, 25]","[4, 40, 16, 232]","[1, 17, 6, 130]","[3, 21, 8, 80]","[0, 2, 2, 22]","The paper presents an extensive framework for complex-valued neural networks. Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more. 

The contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks. Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc. In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network. 

Empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks. Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models. 
","[8, 4, 7]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Compressing Word Embeddings via Deep Compositional Code Learning,"['Raphael Shu', 'Hideki Nakayama']",Accept,2018,"[3, 16]","[8, 21]","[26, 133]","[10, 82]","[15, 40]","[1, 11]","The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss. 

This paper is well-written and easy to follow.  The motivation is clear and the idea is simple and effective.

It would be better to provide deeper analysis in Subsection 6.1. The current analysis is too simple. It may be interesting to explain the meanings of individual components. Does each component is related to a certain topic? Is it meaningful to perform ADD or SUBSTRACT on the leaned code? 

It may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.
","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,"['Dongsoo Lee', 'Daehyun Ahn', 'Taesu Kim', 'Pierce I. Chuang', 'Jae-Joon Kim']",Accept,2018,"[8, 1, 16, 10, 16]","[13, 6, 21, 15, 21]","[47, 12, 51, 39, 82]","[25, 10, 27, 23, 49]","[17, 0, 12, 10, 8]","[5, 2, 12, 6, 25]","It seems like the authors have carefully thought about this problem, and have come up with some elegant solutions, but I am not sold on whether it's an appropriate match for this conference, mainly because it's not clear how many machine learning people will be interested in this approach.

There was a time about 2 or 3 years ago when sparse-matrix approaches seemed to have a lot of promise, but I get the impression that a lot of people have moved on.  The issue is that it's hard to construct a scenario where it makes sense from a speed or memory standpoint to do this.  The authors seem to have found a way to substantially compress the indexes, but it's not clear to me that this really ends up solving any practical problem.  Towards the end of the paper I see mention of a 38.1% reduction in matrix size.  That is way too little to make sense in any practical application, especially when you consider the overhead of decompression.   It seems to me that you could easily get a factor of 4 to 8 of compression just by finding a suitable way to encode the floating-point numbers in many fewer bits (since the weight parameters are quite Gaussian-distributed and don't need to be that accurate).
","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Interpretable Counting for Visual Question Answering,"['Alexander Trott', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[2, 10, 12]","[7, 15, 17]","[19, 383, 229]","[6, 165, 111]","[12, 208, 111]","[1, 10, 7]","This paper proposed a new approach for counting in VQA called Interpretable Counting in Visual Question Answering.  The authors create a new dataset (HowMany-QA) by processing the VQA 2.0 and Visual Genome dataset. In the paper, the authors use object detection framework (R-FCN) to extract bounding boxes information as well as visual features and propose three different strategies for counting. 1: SoftCount; 2: UpDown; 3: IRLC.  The authors show results on HowMany-QA dataset for the proposed methods, and the proposed IRLC method achieves the best performance among all the baselines.   

[Strenghts]

This paper first introduced a cleaned visual counting dataset by processing existing VQA 2.0 and Visual Genome dataset, which can filter out partial non-counting questions. The proposed split is a good testbed for counting in VQA. 

The authors proposed 3 different methods for counting, which both use object detection feature trained on visual genome dataset.  The object detector is trained with multiple objectives including object detection, relation detection, attribute classification and caption grounding to produce rich object representation. The author first proposed 2 baselines: SoftCount uses a Huber loss, UpDown uses a cross entropy loss. And further proposed interpretable RL counter which enumerates the object as a sequential decision process. The proposed IRLC more intuitive and outperform the previous VQA method  (UpDown) on both accuracy and RMSE. 

[Weaknesses]

This paper proposed an interesting and intuitive counting model for VQA. However, there are several weaknesses existed:

1: The object detector is pre-trained with multiple objectives. However, there is no ablation study to show the differences. Since the model only uses the object and relationship feature as input, the authors could show results on counting with different  objects detector. For example, object detector trained using object + relation v.s. object + relation + attribute v.s. object + relation + attribute + caption. 

2: Figure 9 shows an impressive result of the proposed method. Given the detection result, there are a lot of repetitive candidates detection bounding boxes. Without any strong supervision, IRLC could select the correct bounding boxes associated with the different objects. This is interesting, however, the authors didn't show any quantitative results on this. One experiment could verify the performance on IRLC is to compute the IOU between the GT COCO bounding box annotation on a small validation set. The validation set could be obtained by comparing the number of the bounding box and VQA answer with respect to similar COCO categories and VQA entities. 

3: The proposed IRLC is not significantly outperform baseline method (SoftCount) with respect to RMSE (0.1). However, it would be interesting to see how the counting performance can change the result of object detection. As Chattopadhyay's CVPR2017 paper Sec 5.3 on the same subset as in point 2. 

[Summary]

This paper proposed an interesting and interpretable model for counting in VQA. It formulated the counting as a sequential decision process that enumerated the subset of target objects. The authors introduce several new techniques in the IRLC counter. However, there is a lack of ablation study on the proposed model. Taking all these into account, I suggest accepting this paper if the authors could provide more ablation study on the proposed methods. 
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"['Chris Donahue', 'Zachary C. Lipton', 'Akshay Balsubramani', 'Julian McAuley']",Accept,2018,"[22, 5, 6, 13]","[27, 10, 10, 18]","[38, 203, 30, 294]","[19, 77, 11, 150]","[19, 117, 18, 133]","[0, 9, 1, 11]","Summary:

This paper investigated the problem of controlled image generation. Assuming images can be disentangled by identity-related factors and style factors, this paper proposed an algorithm that produces a pair of images with the same identity. Compared to standard GAN framework, this algorithm first generated two latent variables for the pair images. The two latent variables are partially shared reflecting the shared identity information. The generator then transformed the latent variables into high-resolution images with a deconvolution decoder networks. The discriminator was used to distinguish paired images from database or paired images sampled by the algorithm. Experiments were conducted using DCGAN and BEGAN on portrait images and shoe product images. Qualitative results demonstrated that the learned style representations capture viewpoint, illumination and background color while the identity was well preserved by the identity-related representations.


== Novelty & Significance ==
Paired image generation is an interesting topic but this has been explored to some extent. Compared to existing coupled generation pipeline such as CoGAN, I can see the proposed formulation is more application-driven.

== Technical Quality ==
In Figure 3, the portrait images in the second row and fourth row look quite similar. I wonder if the trained model works with only limited variability (in terms of identity).
In Figure 4, the viewpoint is quite limited (only 4 viewpoints are provided).

I am not very convinced whether SD-GAN is a generic algorithm for controlled image generation. Based on the current results, I suspect it only works in fairly constrained settings. 
It would be good to know if it actually works in more challenging datasets such as SUN bedroom, CUB and Oxford Flowers. 

“the AC-DCGAN model cannot imagine new identities”
I feel the author of this paper made an unfair argument when comparing AC-DCGAN with the proposed method. First, during training, the proposed SD-GAN needs to access the identity information and there is only limited identity in the dataset. Based on the presentation, it is not very clear how does the model generate novel identities (in contrast to simply interpolating existing identities). For example, is it possible to generate novel viewpoints in Figure 4?

Missing references on conditional image generation and coupled image generation:
-- Generative Adversarial Text-to-Image Synthesis. Reed et al., In ICML 2016.
-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016.
-- Domain Separation Networks. Bousmalis et al., In NIPS 2016.
-- Unsupervised Image-to-Image Translation Networks. Liu et al., In NIPS 2017.

Overall, I rate this paper slightly above borderline. It showed some good visualization results on controlled image generation. But the comparison to AC-GAN is not very fair, since the identity pairs are fully supervised for the proposed method. As far as I can see, there are no clear-cut improvements quantitatively. Also, there is no comparison with CoGAN, which I believe is the most relevant work for coupled image generation. 
","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Adaptive Quantization of Neural Networks,"['Soroosh Khoram', 'Jing Li']",Accept,2018,"[5, 1, 1, 4, 1, 33, 17]","[9, 6, 6, 7, 5, 38, 22]","[17, 8, 17, 20, 8, 54, 105]","[9, 6, 5, 11, 5, 33, 60]","[7, 2, 12, 5, 2, 5, 20]","[1, 0, 0, 4, 1, 16, 25]","The authors present an interesting idea to reduce the size of neural networks via adaptive compression, allowing the network to use high precision where it is crucial and low precision in other parts. The problem and the proposed solution is well motivated. However, there are some elements of the manuscript that are hard to follow and need further clarification/information. These need to definitely be addressed before this paper can be accepted.

Specific comments/questions:
- Page 1: Towards the bottom, in the 3rd to last line, reference is missing.
- Page 1: It is a little hard to follow the motivation against existing methods.
- Page 2: DenseNets and DeepCompression need citations
- Lemma 2.1 seems interesting - is this original work? This needs to be clarified.
- Lemma 2.2: Reference to Equation 17 (which has not been presented in the manuscript at this point) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17.
- Alg 2: Should it be $\Delta$ or $\Delta_{k+1}$? Because in one if branch, we use $\Delta$, in the other, we use the subscripted one.
- Derivation in section 2.3 has some typographical errors.
- What is $d$ in Equation 20 (with cases)? Without this information, it is unclear how the singular points are handled.
- Page 6, first paragraph of Section 3: The evaluation is a little confusing - when is the compression being applied during the training process, and how is the training continued post-compression? What does each compression 'pass' constitute of?
- Figure 1b: what is the 'iteration' on the horizontal axis, is it the number of iterations of Alg3 or Alg2? Hoping it is Alg3 but needs to be clarified in the text.
- Section 3: What about compression results for CIFAR and SVNH? ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers,"['Jianbo Ye', 'Xin Lu', 'Zhe Lin', 'James Z. Wang']",Accept,2018,"[10, 7, 14, 22]","[13, 9, 19, 27]","[40, 38, 255, 169]","[17, 22, 141, 79]","[14, 13, 86, 26]","[9, 3, 28, 64]","This paper is well written and it was easy to follow. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers. This is achieved by forcing the output of some channels being constant during training. This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter. 

The authors evaluate the performance of the proposed approach on different classification and segmentation tasks. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.

Some of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc. Could the authors explain their choices? How sensible is the algorithm to these hyperparameters?
It would be nice to see empirically how much of computation the proposed approach takes during training. How much longer does it takes to train the model with the ISTA based constraint?

Overall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above.
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,"['Sainbayar Sukhbaatar', 'Zeming Lin', 'Ilya Kostrikov', 'Gabriel Synnaeve', 'Arthur Szlam', 'Rob Fergus']",Accept,2018,"[8, 3, 5, 9, 10, 16]","[13, 7, 10, 14, 15, 21]","[43, 22, 40, 132, 138, 131]","[17, 11, 17, 59, 54, 72]","[26, 9, 23, 67, 74, 49]","[0, 2, 0, 6, 10, 10]","This paper proposes an interesting model of self-play where one agent learns to propose tasks that are easy for her but difficult for an opponent. This creates a moving target of self-play objectives and learning curriculum.

The idea is certainly elegant and clearly described. 
I don't really feel qualified to comment on the novelty, since this paper is somewhat out of my area of expertise, but I did notice that the authors' own description of Baranes and Oudeyer (2013) seems very close to the proposal in this paper. Given the existence of similar forms of self-play the key issue with paper I see is that there is no strong self-play baseline in the experimental evaluation. It is hard to tell whether this neat idea is really an improvement.

Is progress guaranteed? Is it not possible for Alice to imemdiately find an easy task for her where Bob times out, gets no reward signal, and therefore is unable to learn anything? Then repeating that task will loop forever without progress. This suggests that the adversarial setting is quite brittle.

I also find that the paper is a little light on the technical side.","[5, 8, 8]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Large scale distributed neural network training through online distillation,"['Rohan Anil', 'Gabriel Pereyra', 'Alexandre Passos', 'Robert Ormandi', 'George E. Dahl', 'Geoffrey E. Hinton']",Accept,2018,"[3, 4, 11, 12, 9, 43]","[8, 8, 16, 15, 13, 48]","[34, 7, 23, 23, 47, 282]","[11, 3, 10, 17, 17, 160]","[22, 4, 12, 3, 25, 52]","[1, 0, 1, 3, 5, 70]","This paper provides a very original & promising method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent. As authors point out, scaling distributed stochastic gradient descent to more workers typically requires larger batch sizes in order to fully utilize computational resource, and increasing the batch size has a diminishing return. This is clearly a very important problem, as it is a major blocker for current machine learning models to scale beyond the size of models and datasets we currently use. Authors propose to use distillation as a mechanism of communication between workers, which is attractive because prediction scores are more compact than model parameters, model-agnostic, and can be considered to be more robust to out-of-sync differences. This is a simple and sensible idea, and empirical experiments convincingly demonstrate the advantage of the method in large scale distributed training.

I would encourage authors to experiment in broader settings, in order to demonstrate that the general applicability of the proposed method, and also to help readers better understand its limitations. Authors only provide a single positive data point; that co-distillation was useful in scaling up from 128 GPUs to 258 GPUs, for the particular language modeling problem (commoncrawl) which others have not previously studied. In order for other researchers who work on different problems and different system infrastructure to judge whether this method will be useful for them, however, they need to understand better when codistillation succeeds and when it fails. It will be more useful to provide experiments with smaller and (if possible) larger number of GPUs (16, 32, 64, and 512?, 1024?), so that we can more clearly understand how useful this method is under the regime mini-batch stochastic gradient continues to scale. Also, more diversity of models would also help understanding robustness of this method to the model. Why not consider ImageNet? Goyal et al reports that it took an hour for them to train ResNet on ImageNet with 256 GPUs, and authors may demonstrate it can be trained faster.

Furthermore, authors briefly mention that staleness of parameters up to tens of thousands of updates did not have any adverse effect, but it would good to know how the learning curve behaves as a function of this delay. Knowing how much delay we can tolerate will motivate us to design different methods of communication between teacher and student models.","[8, 4, 6]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Universal Agent for Disentangling Environments and Tasks,"['Jiayuan Mao', 'Honghua Dong', 'Joseph J. Lim']",Accept,2018,"[2, 1, 12]","[7, 3, 17]","[58, 5, 77]","[27, 3, 45]","[29, 2, 29]","[2, 0, 3]","In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules. Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs.

Pros
- “Monolithic” policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable.
- An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards.
- Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further.

Cons
- In general, learning a Path function could very well turn out to be no simpler than learning a good policy for the task at hand. I have 2 main concerns:
The data required for learning a good Path function may include similar states to those visited by some optimal policy. However, there is no such guarantee for random walks; indeed, for most Atari games which have several levels, random policies don’t reach beyond the first level, so I don’t see how a Path function would be informative beyond the ‘portions’ of the state space which were visited by policies used to collect data.
Hence, several policies which are better than random are likely to be required for sampling this data, in general. In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand? How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs? I don’t see this as a given in the current formulation.
- Although the method is atypical compared to standard HRL approaches, the same pitfalls may apply, especially that of ‘option collapse’: given a fixed Path function, the Goal function need only figure out which goal state outputs almost always lead to the same output action in the original action space, irrespective of the current state input phi(s), and hence bypass the Path function altogether; then, the role of phi(s) could be taken by tau(s), and we would end up with the original RL problem but in an arguably noisier (and continuous) action space. I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed. Same goes for using a running average of phi(s) and the correct tau(s) in final policies.
- The ability to use state restoration for Path function learning is actually introducing a strong extra assumption compared to standard A3C, which does not technically require it. For cheap emulators and fully deterministic games (Atari) this assumption holds, but in general restoring expensive, stochastic environments to some state is hard (e.g. robot arms playing ping-pong, ball at given x, y, z above the table, with given velocity vector).
- If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds. Given the variance in deep RL training curves, it is hard to make definitive claims from single runs. If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative.
- How much data was actually used to learn the Path function in each case? If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data.


References
[1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside
convolutional networks: Visualising image classification
models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension,"['Hsin-Yuan Huang', 'Chenguang Zhu', 'Yelong Shen', 'Weizhu Chen']",Accept,2018,"[24, 11, 8, 12]","[29, 16, 13, 17]","[37, 47, 97, 168]","[8, 24, 44, 77]","[27, 23, 51, 89]","[2, 0, 2, 2]","The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive. That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level. The network proposed here, FusionHet, fixes problem. Importantly, the model achieves state-of-the-art performance of the SQuAD dataset.

The paper is very well-written and easy to follow. I found the architecture very intuitively laid out, even though this is not my area of expertise. Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work! What most impressed me, however, was the literature review. Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work. Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2. 

All in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Global Optimality Conditions for Deep Neural Networks,"['Chulhee Yun', 'Suvrit Sra', 'Ali Jadbabaie']",Accept,2018,"[6, 16, 19]","[11, 21, 24]","[33, 191, 366]","[18, 98, 187]","[15, 74, 114]","[0, 19, 65]","Summary:
The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks. In particular:
- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion. This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are.
- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach.

Quality:
The quality is very good. The paper is technically correct and nontrivial. All proofs are provided and easy to follow.

Clarity:
The paper is very clear. Related work is clearly cited, and the novelty of the paper well explained. The technical proofs of the paper are in appendices, making the main text very smooth.

Originality:
The originality is weak. It extends a series of recent papers correctly cited. There is some originality in the proof which differs from recent related papers.

Significance:
The result is not completely surprising, but it is significant given the lack of theory and understanding of deep learning. Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.","[7, 5, 8]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Guide Actor-Critic for Continuous Control,"['Voot Tangkaratt', 'Abbas Abdolmaleki', 'Masashi Sugiyama']",Accept,2018,"[6, 8, 20]","[10, 13, 25]","[38, 69, 631]","[11, 33, 253]","[16, 30, 202]","[11, 6, 176]","The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor. This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form.

The paper is mostly clear and well-presented, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3); and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.

The presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive. It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others. More trials are also needed to alleviate any suspicion of lucky trials.

There are some other issues with the presentation of the method, but these don't affect the merit of the method:

1. Returns are defined from an initial distribution that is stationary for the policy. While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset.

2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories.

3. In the first paragraph of Section 3.3: ""[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation."" What else does it depend on? Then: ""[...] the action a_0 should be in a local vicinity of a.""; and ""[...] the action a_0 should be similar to actions sampled from pi_theta(a|s)."" What do you mean ""should""? In order for the Taylor approximation to be good?

4. The line before (19) is confusing, since (19) is exact and not an approximation. For the approximation (20), it isn't clear if this is a good approximation. Why/when is the 2nd term in (19) small?

5. The parametrization nu of \hat{Q} is never specified in Section 3.6. This is important in order to evaluate the complexities involved in computing its Hessian.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Kronecker-factored Curvature Approximations for Recurrent Neural Networks,"['James Martens', 'Jimmy Ba', 'Matt Johnson']",Accept,2018,"[9, 8, 5]","[14, 13, 8]","[38, 96, 5]","[19, 46, 4]","[16, 49, 0]","[3, 1, 1]","This paper extends the Kronecker-factor Approximate Curvature (K-FAC) optimization method to the setting of recurrent neural networks. The K-FAC method is an approximate 2nd-order optimization method that builds a block diagonal approximation of the Fisher information matrix, where the block diagonal elements are Kronecker products of smaller matrices. 

In order to approximate the Fisher information matrix for RNNs, the authors assume that the derivative of the loss function with respect to each weight matrix at each time step is independent of the length of the sequence, that these derivatives are temporally homogeneous, that the input and derivatives of the output are independent across every point in time, and that either the one-step cross-covariance of these derivatives is symmetric or that the training sequences are effectively infinite in length. Based on these assumptions, the authors show that the Fisher information can be reduced into a form in which the derivatives of the weight matrices can be approximated by a linear Gaussian graphical model and in which the approximate 2nd order method can be efficiently carried out. The authors compare their method to SGD on two language modeling tasks and against Adam for learning differentiable neural computers.

The paper is relatively clear, and the authors do a reasonable job of introducing related work of the original K-FAC algorithm as well as its extension to CNNs before systematically deriving their method for RNNs. The problem of extending the K-FAC algorithm is natural, and the steps taken in this paper seem natural yet also original and non-trivial.  

The main issue that I have with this paper is the lack of theoretical justification or even intuition for the many approximations carried out in the course of approximating the Fisher information matrix. In many instances, it seemed like these approximations were made purely for convenience and tractability without much regard for (even approximate) correctness. This quality of this paper would be greatly  strengthened if it had some bounds on approximation error or even empirical results testing the validity of the assumptions in the paper. Moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work.  

Specific comments and questions
Page 2, ""r is is"". Typo.
Page 2, ""DV"". I found the introduction of V without any explanation to be confusing.
Page 2, ""P_{y|x}(\theta)"". The relation between P_{y|x}(\theta) and f(x,\theta) is never explained.
Page 3, ""common practice of computing the natural gradient as (F + \lambda I) \nabla h instead of F^{-1} \nabla h"". I don't see how the former can serve as a replacement for the latter.
Page 3, ""approximate g and a as statistically independent"". Even though K-FAC already exists, it would be good to explain why this assumption is reasonable, since similar assumptions are made for the work presented in this paper.
Page 4, ""This new approximation, called ""KFC"", is derived by assuming...."". Same as previous comment. It would be good to briefly discuss why these assumptions are reasonable.
Page 5, Independence of T and w_t's, temporal homogeneity of w_t's,, and independence between a_t's and g_t's. I can see why these are convenient assumptions, but why are they reasonable? Moreover, why is it further natural to assume that A and G are temporally homogeneous as well?
Page 7, ""But insofar as the w_t's ... encode the relevant information contained in these external variables, they should be approximately Markovian"". I am not sure what this means.
Page 7, ""The linear-Gaussian assumption meanwhile is a more severe one to make, but it seems necessary for there to be any hope that the required expectations remain tractable"". I am not sure that this is a good enough justification for such an idea, unless there are compelling approximation error bounds. 
Page 8, Option 1. In what situations is it reasonable to assume that V_1 is symmetric? 
Pages 8-9, Option 2. What is a good finite sample size in which the assumption that the training sequences are infinitely long is reasonable in practice? Can the error |\kappa(x) - \zeta_T(x)| be translated into a statement on the approximation error?
Page 9, ""V_1 = V_{1,0} = ..."". Typos (that appear to have been caught by the authors already).
Page 9, ""The 2nd-order statistics ... are accumulated through an exponential moving average during training"". How sensitive is the performance of this method to the decay rate of the exponential moving average? 
Page 10, ""The additional computations required to get the approximate Fisher inverse from these statistics ... are performed asynchronously on the CPU's"". I find it a bit unfair to compare SGD to K-FAC in terms of wall clock time without also using the extra CPU's for SGD as well (e.g. via Hogwild or synchronous parallel SGD).
Page 10, ""The hyperparameters of our approach..."". What is the sensitivity of the experimental results to these hyperparameters? Moreover, how sensitive are the results to initialization?
Page 10, ""we found that each parameter update of our method required about 80% more wall-clock time than an SGD update"". How much of this is attributed to the fact that the statistics are computed asynchronously?
Pages 10-12, Experiments. There are no error bars in any of the plots, so it is impossible to ascertain the statistical significance of any of these results. 
Page 11: Figure 2. Where is the Adam batchsize 50 line in the left plot? Why did the Adam batchsize 200 line disappear halfway through the right plot?
  


  ","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning a neural response metric for retinal prosthesis,"['Nishal P Shah', 'Sasidhar Madugula', 'EJ Chichilnisky', 'Yoram Singer', 'Jonathon Shlens']",Accept,2018,"[1, 6, 25, 27]","[6, 11, 30, 31]","[7, 7, 23, 160]","[7, 6, 16, 99]","[0, 0, 0, 21]","[0, 1, 7, 40]","In their paper, the authors propose to learn a metric between neural responses by either optimizing a quadratic form or a deep neural network. The pseudometric is optimized by positing that the distance between two neural responses to two repeats of the same stimulus should be smaller than the distance between responses to different stimuli. They do so with the application of improving neural prosthesis in mind. 

First of all, I am doubtful about this application: I don't think the task of neural prosthesis can ever be to produce idential output pattern to the same stimuli. Nevertheless, a good metric for neural responses that goes beyond e.g. hamming distance or squared error between spike density function would be clearly useful for understanding neural representations.

Second, I find the framework proposed by the authors interesting, but not clearly motivated from a neurobiological perspective, as the similarity between stimuli does not appear to play a role in the optimized loss function. For two similar stimuli, natural responses of neural population can be more similar than the responses to two repetitions of the same stimulus.

Third, the results presented by the authors are not convincing throughout. For example, 4B suggests that indeed the Hamming distance achieves lower error than the learned representation.

Nevertheless, it is an interesting approach that is worthwhile pursuing further. ","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Certified Defenses against Adversarial Examples ,"['Aditi Raghunathan', 'Jacob Steinhardt', 'Percy Liang']",Accept,2018,"[4, 10, 14]","[9, 15, 19]","[52, 115, 346]","[22, 45, 166]","[30, 62, 170]","[0, 8, 10]","The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data.

First of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.

In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.
","[8, 5, 8]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Activation Maximization Generative Adversarial Nets,"['Zhiming Zhou', 'Han Cai', 'Shu Rong', 'Yuxuan Song', 'Kan Ren', 'Weinan Zhang', 'Jun Wang', 'Yong Yu']",Accept,2018,"[12, 13, 8, 2, 10, 8, 16, 18]","[17, 18, 8, 7, 15, 13, 21, 23]","[34, 92, 10, 25, 68, 375, 359, 503]","[16, 35, 7, 11, 27, 185, 163, 340]","[15, 29, 3, 12, 25, 163, 155, 106]","[3, 28, 0, 2, 16, 27, 41, 57]","+ Pros:
- The paper properly compares and discusses the connection between AM-GAN and class conditional GANs in the literature (AC-GAN, LabelGAN)
- The experiments are thorough
- Relation to activation maximization in neural visualization is also properly mentioned
- The authors publish code and honestly share that they could not reproduce AC-GAN's results and thus using to its best variant AC-GAN* that they come up with. I find this an important practice worth encouraging!
- The analysis of Inception score is sound.
+ Cons:
- A few presentation/clarity issues as below
- This paper leaves me wonder why AM-GAN rather than simply characterizing D as a 2K-way classifier (1K real vs 1K fake).

+ Clarity: 
The paper is generally well-written. However, there are a few places that can be improved:
- In 2.2, the authors mentioned ""In fact, the above formulation is a modified version of the original AC-GAN.."", which puts readers confusion whether they were previously just discussed AC-GAN or AC-GAN* (because the previous paragraph says ""AC-GAN are defined as.."".
- Fig. 2: it's not clear what the authors trying to say if looking at only figures and caption. I'd suggest describe more in the caption and follow the concept figure in Odena et al. 2016.
- A few typos here and there e.g. ""[a]n diversity measurement""

+ Originality: AM-GAN is an incremental work by applying AM to GAN. However, I have no problems with this.
+ Significance: 
- Authors show that in quantitative measures, AM-GAN is better than existing GANs on CIFAR-10 / TinyImageNet. Although I don't find much a real difference by visually comparing of samples of AM-GAN to AC-GAN*.

Overall, this is a good paper with thorough experiments supporting their findings regarding AM-GAN and Inception score!","[7, 8, 5]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering,"['Shuohang Wang', 'Mo Yu', 'Jing Jiang', 'Wei Zhang', 'Xiaoxiao Guo', 'Shiyu Chang', 'Zhiguo Wang', 'Tim Klinger', 'Gerald Tesauro', 'Murray Campbell']",Accept,2018,"[4, 10, 14, 12, 6, 8, 12, 15, 32, 38]","[9, 15, 19, 15, 11, 13, 17, 20, 37, 42]","[94, 187, 145, 33, 86, 212, 202, 38, 118, 78]","[43, 86, 105, 15, 39, 106, 73, 22, 67, 36]","[50, 93, 29, 14, 38, 95, 57, 15, 28, 23]","[1, 8, 11, 4, 9, 11, 72, 1, 23, 19]","Traditional open-domain QA systems typically have two steps: passage retrieval and aggregating answers extracted from the retrieved passages.  This paper essentially follows the same paradigm, but leverages the state-of-the-art reading comprehension models for answer extraction, and develops the neural network models for the aggregating component.  Although the idea seems incremental, the experimental results do seem solid.  The paper is generally easy to follow, but in several places the presentation can be further improved.

Detailed comments/questions:
  1. In Sec. 2.2, the justification for adding H^{aq} and \bar{H}^{aq} is to downweigh the impact of stop word matching.  I feel this is a somewhat indirect and less effective design, if avoiding stop words is really the reason.  A standard preprocessing step may be better.
  2. In Sec. 2.3, it seems that the final score is just the sum of three individual normalized scores. It's not truly a ""weighted"" combination, where the weights are typically assumed to be tuned.
  3. Figure 3: Connecting the dots in the two subfigures on the right does not make sense.  Bar charts should be used instead.
  4. The end of Sec. 4.2: I feel it's a bad example, as the passage does not really support the answer. The fact that ""Sesame Street"" got picked is probably just because it's more famous.
  5. It'd be interesting to see how traditional IR answer aggregation methods perform, such as simple classifiers or heuristics by word matching (or weighted by TFIDF) and counting.  This will demonstrates the true advantages of leveraging modern NN models.

Pros:
  1. Updating a traditional open-domain QA approach with neural models
  2. Experiments demonstrate solid positive results

Cons:
  1. The idea seems incremental
  2. Presentation could be improved
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Few-Shot Learning with Graph Neural Networks,"['Victor Garcia Satorras', 'Joan Bruna Estrach']",Accept,2018,"[2, 9]","[7, 14]","[18, 162]","[7, 64]","[11, 88]","[0, 10]","This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors. The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al. This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks. The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art.

There are a few typos and the presentation of the paper could be improved and polished more. I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot. I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. ","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Action-dependent Control Variates for Policy Optimization via Stein Identity,"['Hao Liu*', 'Yihao Feng*', 'Yi Mao', 'Dengyong Zhou', 'Jian Peng', 'Qiang Liu']",Accept,2018,"[16, 2, 17, 18, 10, 9]","[21, 7, 22, 21, 15, 14]","[564, 29, 76, 62, 166, 183]","[260, 11, 34, 37, 84, 88]","[66, 17, 21, 17, 62, 86]","[238, 1, 21, 8, 20, 9]","The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein’s identity and control functionals. The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance. 

A criticism of the paper is that it does not require Stein’s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate. The derivation through Stein’s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick.

The empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning. However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:

-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action. A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.

-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice. Similar comparison should be done with off-policy fitting in Q-Prop.

I wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations. 

The paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop. As discussed above, it is encouraged to elaborate other potential causes that led to performance differences. The experimental results are presented well for a range of Mujoco tasks. 

Pros:

-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time

-Good empirical evaluation

Cons:

-The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein’s identity etc. and does not inherit novel insights due to this derivation.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Decision Boundary Analysis of Adversarial Examples,"['Warren He', 'Bo Li', 'Dawn Song']",Accept,2018,"[7, 10, 20]","[11, 15, 25]","[27, 300, 440]","[16, 124, 254]","[9, 148, 154]","[2, 28, 32]","The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is.

My comments are the following:

1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks.

2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method. In section 3.2 they explain that ""Our attack uses existing optimization attack techniques to..."", but one should be able to understand the method without reading further references. Specially a formal representation of the method should be included.

3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (""it succeeds on 28% of the samples on MNIST;73% on CIFAR-10""). What is the meaning of success rate in here? Is it the % of times that the classifier is confused?

4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST). So I actually cannot tell which method is better, at least in the MNIST experiment. One could do a method that completely distort the image and therefore will be classified with as a class. But adversarial images should be visually undistinguishable from original images. Generated CIFAR images seem similar than the originals, although CIFAR images are very low resolution, so judging this is hard.

4- As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR-10 benign samples.

As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. I would like to see more formal definitions of the methods presented. Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted. It would be needed to have a way of visually evaluate the similarity between original images and generated images.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Memory-based Parameter Adaptation,"['Pablo Sprechmann', 'Siddhant M. Jayakumar', 'Jack W. Rae', 'Alexander Pritzel', 'Adria Puigdomenech Badia', 'Benigno Uria', 'Oriol Vinyals', 'Demis Hassabis', 'Razvan Pascanu', 'Charles Blundell']",Accept,2018,"[12, 1, 3, 3, 3, 7, 12, 10, 9, 9]","[17, 4, 7, 7, 7, 7, 17, 15, 14, 14]","[56, 24, 36, 27, 25, 19, 209, 54, 156, 88]","[31, 10, 17, 11, 11, 8, 101, 10, 63, 35]","[20, 14, 19, 14, 13, 9, 98, 26, 87, 48]","[5, 0, 0, 2, 1, 2, 10, 18, 6, 5]","Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA.

The paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.)

Here are some comments:
1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query?
2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model.
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
FearNet: Brain-Inspired Model for Incremental Learning,"['Ronald Kemker', 'Christopher Kanan']",Accept,2018,"[2, 9]","[7, 14]","[15, 109]","[3, 46]","[9, 52]","[3, 11]","Quality: The paper presents a novel solution to an incremental classification problem based on a dual memory system. The proposed solution is inspired by the memory storage mechanism in brain.

Clarity: The problem has been clearly described and the proposed solution is described in detail. The results of numerical experiments and the real data analysis are satisfactory and clearly shows the superior performance of the method compared to the existing ones.

Originality: The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain. The memory consolidation is inspired by the mechanisms that occur during sleep. The numerical experiments showing the FearNet performance with sleep frequency also validate the comparison with the brain memory system.

Significance: The work discusses a significant problem of incremental classification. Many of the shelf deep neural net methods require storage of previous training samples too and that slows up the application to larger dataset. Further the traditional deep neural net also suffers from the catastrophic forgetting. Hence, the proposed work provides a novel and scalable solution to the existing problem.

pros: (a) a scalable solution to the incremental classification problem using a brain inspired dual memory system
          (b) mitigates the catastrophic forgetting problem using a memory consolidation by pseudorehearsal.
          (c) introduction of a subsystem that allows which memory system to use for the classification

cons: (a)  How FearNet would perform if imbalanced classes are seen in more than one study sessions?
          (b) Storage of class statistics during pseudo rehearsal could be computationally expensive. How to cope with that?
          (c) How FearNet would handle if there are multiple data sources?","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
 Neural Map: Structured Memory for Deep Reinforcement Learning,"['Emilio Parisotto', 'Ruslan Salakhutdinov']",Accept,2018,"[4, 16]","[9, 21]","[32, 419]","[13, 207]","[17, 201]","[2, 11]","This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity. The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory. Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector. The position {x,y} is a binned representation of discrete or continuous coordinates. The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves.

The experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment. The most important aspect is the capability to build a feature map of previously unseen environments.

This paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited. It has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same. I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there). What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken? I quite like the formulation of the NIPS ratings: ""if this paper does not get accepted, I am considering boycotting the conference"".

* The noise model experiment in Appendix D is commendable, but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation. While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc... Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix).","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,"['Jesse Engel', 'Matthew Hoffman', 'Adam Roberts']",Accept,2018,"[4, 13, 12]","[9, 17, 17]","[61, 85, 65]","[30, 49, 24]","[31, 28, 34]","[0, 8, 7]","# Paper overview:
This paper presents an analysis of a basket of approaches which together enable one to sample conditionally from a class of 
generative models which have been trained to match a joint distribution. Latent space constraints (framed as critics) are learned which confine the generating distribution to lie in a conditional subspace, which when combined with what is termed a 'realism' constraint enables the generation of realistic conditional images from a more-or-less standard VAE trained to match the joint data-distribution.

'Identity preserving' transformations are then introduced within the latent space, which allow the retrospective minimal modification of sample points such that they lie in the conditional set of interest (or not).  Finally, a brief foray into unsupervised techniques for learning these conditional constraints is made, a straightforward extension which I think clouds rather than enlightens the overall exposition.

# Paper discussion:
I think this is a nicely written paper, which gives a good explanation of the problem and their proposed innovations, however I am curious to see that the more recent ""Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space"" by Nguyen et al. was not cited.  This is an empirically very successful approach for conditional generation at 'test-time'. 

Other minor criticisms include:
* I find the 'realism' constraint a bit weak, but perhaps it is simply a naming issue.  Did you experiment with alternative approaches for encouraging marginal probability mass?

* The regularisation term L_dist, why this and not log(1 + exp(z' - z)) (or many arbitrary others)? 

* The claim of identity preservation is (to me) a strong one: it would truly be hard to minimise the trajectory distance wrt. the actual 'identity' of the subject.

* For Figure 6 I would prefer a different colourscheme: the red does not show up well on screen.

* ""Furthermore, CGANs and CVAEs suffer from the same problems of mode-collapse and blurriness as their unconditional cousins"" -> this is debateable, there are many papers which employ various methods to (attempt to) alleviate this issue.


# Conclusion:
I think this is a nice piece of work, if the authors can confirm why ""Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space"" is not placed relative to this work in the paper, I would be happy to see it published.  If stuck for space, I would personally recommend moving the one-shot generation section to the appendix as I do not think it adds a huge amount to the overall exposition.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Matrix capsules with EM routing,"['Geoffrey E Hinton', 'Sara Sabour', 'Nicholas Frosst']",Accept,2018,[6],[9],[9],[5],[1],[3],"The paper describes another instantiation of ""capsules"" which attempt to learn part-whole relationships and the geometric pose transformations between them.  Results are presented on the smallNORB test set obtaining impressive performance.

Although I like very much this overall approach, this particular paper is so opaquely written that it is difficult to understand exactly what was done and how the network works.  It sounds like the main innovation here is using a 4x4 matrix for the pose parameters, and an iterative EM algorithm to find the correspondence between capsules (routing by agreement).  But what exactly the pose matrix represents, and how they get transformed from one layer to the next, is left almost entirely to the reader's imagination.  In addition, how EM factors in, what the probabilities P_ih represent, etc. is not clear.  I think the authors could do a much better job explaining this model, the rationale behind it, and how it works.

Perhaps the most interesting and compelling result is Figure 2, which shows how ambiguity in object class assignment is resolved with each iteration.  This is very intriguing, but it would be great to understand what is going on and how this is happening.

Although the results are impressive, if one can't understand how this was achieved it is hard to know what to make of it.

","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
A Hierarchical Model for Device Placement,"['Azalia Mirhoseini', 'Anna Goldie', 'Hieu Pham', 'Benoit Steiner', 'Quoc V. Le', 'Jeff Dean']",Accept,2018,"[9, 10, 7, 3, 14, 27]","[14, 15, 12, 8, 19, 31]","[68, 24, 58, 26, 299, 105]","[38, 10, 23, 10, 143, 44]","[22, 13, 27, 15, 145, 32]","[8, 1, 8, 1, 11, 29]","This paper proposes a device placement algorithm to place operations of tensorflow on devices. 

Pros:

1. It is a novel approach which trains the placement end to end.
2. The experiments are solid to demonstrate this method works very well.
3. The writing is easy to follow.
4. This would be a very useful tool for the community if open sourced.

Cons:

1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models. The latter would be more exciting. The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph? However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case.
2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.
3. It is not clear how the adjacency information is used.
","[8, 5, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,"['Asit Mishra', 'Debbie Marr']",Accept,2018,"[11, 4]","[14, 7]","[48, 25]","[37, 17]","[8, 6]","[3, 2]","The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network. Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly. The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.

I found the paper interesting but the contribution seems quite limited.

Pros:
1. The paper is well written and easy to read.
2. The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally.

Cons:
1. The name Apprentice seems a bit confusing with apprenticeship learning.
2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights).
3. It is unclear how the proposed method outperforms other methods based on fine-tuning. It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Residual Connections Encourage Iterative Inference,"['Stanisław Jastrzebski', 'Devansh Arpit', 'Nicolas Ballas', 'Vikas Verma', 'Tong Che', 'Yoshua Bengio']",Accept,2018,"[6, 8, 7, 27, 7, 31]","[10, 13, 12, 31, 12, 36]","[53, 45, 75, 34, 38, 975]","[16, 20, 39, 16, 14, 405]","[29, 25, 35, 15, 18, 454]","[8, 0, 1, 3, 6, 116]","
This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its “nonlinear part” to push its values in the negative direction of the loss gradient.  The authors demonstrate this using a Taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference.  Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way.  

Some weaknesses of the paper on the other hand — some of the parts of the paper (e.g. on weight sharing) are only somewhat related to the main topic of the paper. In fact, the authors moved the connection to SGD to the appendix, which I thought would be *more* related.   Additionally, parts of the paper are not as clearly written as they could be and lack rigor.  This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly.  The explanation following is also handwavey despite claims to being formal.   

Some other lower level thoughts:
* Regarding weight sharing for residual layers, I don’t understand why we can draw the conclusion that the initial gradient explosion is responsible for the lower generalization capability of the model with shared weights.  Are there other papers in literature that have shown this connection?
* The name “cosine loss” suggests that this function is actually being minimized by a training procedure, but it is just a value that is being plotted… perhaps just call it the cosine?
* I recommend that the authors also check out Figurnov et al CVPR 2017 (""Spatially Adaptive Computation Time for Residual Networks"") which proposes an “adaptive” version of ResNet based on the intuition of adaptive inference.
* The plots in the later parts of the paper are quite small and hard to read.  They are also spaced together too tightly (horizontally), making it difficult to immediately see what each plot is supposed to represent via the y-axis label.
* Finally, the citations need to be fixed (use \citep{} instead of \cite{})

","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Twin Networks: Matching the Future for Sequence Generation,"['Dmitriy Serdyuk', 'Nan Rosemary Ke', 'Alessandro Sordoni', 'Adam Trischler', 'Chris Pal', 'Yoshua Bengio']",Accept,2018,"[4, 8, 10, 3, 22, 31]","[9, 13, 15, 8, 27, 36]","[28, 61, 95, 82, 230, 975]","[10, 22, 52, 39, 98, 405]","[17, 38, 40, 43, 109, 454]","[1, 1, 3, 0, 23, 116]","
1) Summary
This paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably. The authors propose to train a forward and backward RNN in parallel. The forward RNN predicts forward in time and the backward RNN predicts backwards in time. While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step. In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition.


2) Pros:
+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.
+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.
+ Improved performance in speech recognition task.
+ The idea is clearly explained and well motivated.


3) Cons:
Image captioning experiment:
In the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. This experiment shows improvement over such baselines, however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016. It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done.


Unconditioned generation experiments:
In these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help. Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation. The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case. For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it’s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why.


Additional comments:
The method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN).
It would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).

Experiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).


4) Conclusion
The paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse. Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past. The proposed method presents a possible way of better modeling the future, however, some the results do not clearly back up the claim yet. The given score will improve if the authors are able to address the stated issues.


POST REBUTTAL RESPONSE:
The authors have addressed the comments on the MNIST experiments and show better results, however, as far as I can see, they did not address my concern about the comparisons on the image captioning experiment. In the image captioning experiment the authors choose two networks (Show & Tell and Soft attention) that they improve using the proposed method that end up performing similar to the second best baseline (Yao et al. 2016) based on Table 3 and their response. I requested for the authors to use their method on the best performing baselines (i.e. Yao et al. 2016 or Liu et al. 2017) or explain why this cannot be done (maybe my request was not clearly stated). Applying the proposed method on the strong baselines would highlight the author's claims more strongly than just applying on the average performing chosen baselines. This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines. Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point. However, I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result. My rating changes to marginally above threshold for acceptance.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Towards Image Understanding from Deep Compression Without Decoding,"['Robert Torfason', 'Fabian Mentzer', 'Eirikur Agustsson', 'Michael Tschannen', 'Radu Timofte', 'Luc Van Gool']",Accept,2018,"[3, 3, 5, 7, 10, 35]","[3, 8, 10, 12, 15, 40]","[4, 29, 56, 71, 468, 1396]","[3, 12, 29, 31, 250, 801]","[1, 16, 25, 38, 194, 420]","[0, 1, 2, 2, 24, 175]","Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation?

The fact that it's an obvious question does not mean that it's a question that's worthless. In fact, I am glad someone asked this question and tried to answer it. 

Pros:
- Clear presentation, easy to follow.
- Very interesting, but obvious, question is explored. 
- The paper is very clear, and uses building blocks which have been analyzed before, which leaves the authors free to explore their interactions rather than each individual building block's property.
- Results are shown on two tasks (classification / segmentation) rather than just one (the obvious one would have been to only discuss results on classification), and relatively intuitive results are shown (i.e., more bits = better performance). What is perhaps not obvious is how much impact does doubling the bandwidth have (i.e., initially it means more, then later on it plateaus, but much earlier than expected).
- Joint training of compression + other tasks. As far as I know this is the first paper to talk about this particular scenario.
- I like the fact that classical codecs were not completely discarded (there's a comparison with JPEG 2K).
- The discussion section is of particular interest, discussing openly the pros/cons of the method (I wish more papers would be as straightforward as this one).

Cons:
- I would have liked to have a discussion on the effect of the encoder network. Only one architecture/variant was used.
- For PSNR, SSIM and MS-SSIM I would like a bit more clarity whether these were done channel-wise, or on the grayscale channel.
- While runtime is given as pro, it would be nice for those not familiar with the methods to provide some runtime numbers (i.e., breakdown how much time does it take to encode and how much time does it take to classify or segment, but in seconds, not flops). For example, Figure 6 could be augmented with actual runtime in seconds.
- I wish the authors did a ctrl+F for ""??"" and fixed all the occurrences.
- One of the things that would be cool to add later on but I wished to have beeyn covered is whether it's possible to learn not only to compress, but also downscale. In particular, the input to ResNet et al for classification is fixed sized, so the question is -- would it be possible to produced a compact representation to be used for classification given arbitrary image resolutions, and if yes, would it have any benefit?

General comments:
- The classification bits are all open source, which is very good. However, there are very few neural net compression methods which are open sourced. Would you be inclined to open source the code for your implementation? It would be a great service to the community if yes (and I realize that it could already be open sourced -- feel free to not answer if it may lead to break anonymity, but please take this into consideration).
","[9, 6, 6]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Mixed Precision Training,"['Paulius Micikevicius', 'Sharan Narang', 'Jonah Alben', 'Gregory Diamos', 'Erich Elsen', 'David Garcia', 'Boris Ginsburg', 'Michael Houston', 'Oleksii Kuchaiev', 'Ganesh Venkatesh', 'Hao Wu']",Accept,2018,"[-1, 19]","[3, 24]","[4, 145]","[4, 72]","[0, 3]","[0, 70]","The paper provides methods for training deep networks using half-precision floating point numbers without losing model accuracy or changing the model hyper-parameters. The main ideas are to use a master copy of weights when updating the weights, scaling the loss before back-prop and using full precision variables to store products. Experiments are performed on a large number of state-of-art deep networks, tasks and datasets which show that the proposed mixed precision training does provide the same accuracy at half the memory.

Positives
- The experimental evaluation is fairly exhaustive on a large number of deep networks, tasks and datasets and the proposed training preserves the accuracy of all the tested networks at half the memory cost.

Negatives
- The overall technical contribution is fairly small and are ideas that are regularly implemented when optimizing systems.
- The overall advantage is only a 2x reduction in memory which can be gained by using smaller batches at the cost of extra compute. ","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions,"['Sjoerd van Steenkiste', 'Michael Chang', 'Klaus Greff', 'Jürgen Schmidhuber']",Accept,2018,"[3, 15, 7, 29]","[8, 20, 12, 34]","[34, 18, 45, 440]","[13, 10, 16, 232]","[18, 4, 26, 132]","[3, 4, 3, 76]","Summary
---
This work applies a representaion learning technique that segments entities to learn simple 2d intuitive physics without per-entity supervision. It adds a relational mechanism to Neural Expectation Maximization and shows that this mechanism provides a better simulation of bouncing balls in a synthetic environment.

Neural Expectation Maximization (NEM) decomposes an image into K latent variables (vectors of reals) theta_k. A decoder network reconstructs K images from each of these latent variables and these K images are combined into a single reconstruction using pixel-wise mixture components that place more weight on pixels that match the ground truth. An encoder network f_enc() then updates the latent variables to better explain the reconstructions they produced.
The neural nets are learned so that the latent variables reconstruct the image well when used by the mixture model and match a prior otherwise. Previously NEM has been shown to learn variables which represent individual objects (simple shapes) in a compositional manner, using one variable per object.

Other recent neural models can learn to simulate simple 2d physics environments (balls bouncing around in a 2d plane). That work supervises the representation for each entity (ball) explicitly using states (e.g. position and velocity of balls) which are known from the physics simulator used to generate the training data. The key feature of these models is the use of a pairwise embedding of an object and its neighbors (message passing) to predict the object's next state in the simulation.

This paper paper combines the two methods to create Relational Neural Expectation Maximization (R-NEM), allowing direct interaction at inference time between the latent variables that encode a scene. The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1. R-NEM adds a relational module which computes an embedding used as a third input to the recurrent encoder. Like previous relational models, this one uses a pairwise embedding of the object being updated (object k) and its neighbors. Unlike previous neural physics models, R-NEM uses a soft attention mechanism to determine which objects are neighbors and which are not. Also unlike previous neural models, this method does not require per-object supervision.

Experiments show that R-NEM learns compositional representations that support intuitive physics more effectively than ablative baselines. These experiments
show:
1) R-NEM reconstructs images more accurately than baselines (RNN/LSTM) and NEM (without object interaction).
2) R-NEM is trained with 4 objects per image. It does a bit worse at reconstructing images with 6-8 objects per image, but still performs better than baselines.
3) A version of R-NEM without neighborhood attention in the relation module matches the performance of R-NEM using 4 objects and performs worse than R-NEM at 6-8 objects.
4) R-NEM learns representations which factorize into one latent variable per object as measured by the Adjusted Rand Index, which compares NEM's pixel clustering to a ground truth clustering with one cluster per object.
5) Qualitative and quantitative results show that R-NEM can simulate 2d ball physics for many time steps more effectively than an RNN and while only suffering gradual divergence from the ground truth simulation.

Qualitative results show that the attentional mechanism attends to objects which are close to the context object together, acting like the heuristic neighborhood mechanism from previous work.

Follow up experiments extend the basic setup significantly. One experiment shows that R-NEM demonstrates object permanence by correctly tracking a collision when one of the objects is completely occluded. Another experiment applies the method to the Space Invaders Atari game, showing that it treats columns of aliens as entities. This representation aligns with the game's goal.


Strengths
---

The paper presents a clear, convincing, and well illustrated story.

Weaknesses
---

* RNN-EM BCE results are missing from the simulation plot (right of figure 4).

Minor comments/concerns:

* 2nd paragraph in section 4: Are parameters shared between these 3 MLPs (enc,emb,eff)? I guess not, but this is ambiguous.

* When R-NEM is tested against 6-8 balls is K set to the number of balls plus 1? How does performance vary with the number of objects?

* Previous methods report performance across simulations of a variety of physical phenomena (e.g., see ""Visual Interaction Networks""). It seems that supervision isn't needed for bouncing ball physics, but I wonder if this is the case for other kinds of phenomena (e.g., springs in the VIN paper). Can this method eliminate the need for per-entity supervision in this domain?

* A follow up to the previous comment: Could a supervised baseline that uses per-entity state supervision and neural message passsing (like the NPE from Chang et. al.) be included?

* It's a bit hard to qualitatively judge the quality of the simulations without videos to look at. Could videos of simulations be uploaded (e.g., via anonymous google drive folder as in ""Visual Interaction Networks"")?

* This uses a neural message passing mechanism like those of Chang et. al. and Battaglia et. al. It would be nice to see a citation to neural message passing outside of the physics simulation domain (e.g. to ""Neural Message Passing for Quantum Chemistry"" by Gilmer et. al. in ICML17).

* Some work uses neighborhood attention coefficients for neural message passing. It would be nice to see a citation included.
    * See ""Neighborhood Attention"" in ""One-Shot Imitation Learning"" by Duan et. al. in NIPS17
    * Also see ""Programmable Agents"" by Denil et. al.


Final Evaluation
---

This paper clearly advances the body of work on neural intuitive physics by incorporating NEM entity representation to allow for less supervision. Alternatively, it adds a message passing mechanism to the NEM entity representation technique. These are moderately novel contributions and there are only minor weaknesses, so this is a clear accept.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Intrinsic Sparse Structures within Long Short-Term Memory,"['Wei Wen', 'Yuxiong He', 'Samyam Rajbhandari', 'Minjia Zhang', 'Wenhan Wang', 'Fang Liu', 'Bin Hu', 'Yiran Chen', 'Hai Li']",Accept,2018,"[27, 15, 7, 9, 7, 29, 25, 17, 16]","[32, 20, 12, 14, 12, 34, 30, 22, 21]","[141, 128, 35, 68, 32, 486, 300, 540, 414]","[63, 76, 21, 42, 17, 262, 106, 311, 261]","[28, 35, 13, 24, 11, 26, 29, 94, 76]","[50, 17, 1, 2, 4, 198, 165, 135, 77]","The paper spends lots of (repeated)  texts on motivating and explaining ISS. But the algorithm is simple, using group lasso to find components that are can retained to preserve the performance.  Thus the novelty is limited.

The experiments results are good.

Sec 3.1 should be made more concise. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Empirical Risk Landscape Analysis for Understanding Deep Neural Networks,"['Pan Zhou', 'Jiashi Feng']",Accept,2018,"[12, 9]","[17, 14]","[406, 542]","[134, 214]","[119, 235]","[153, 93]","Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape.  The analysis answers:

1) When empirical gradients are close to true gradients
2) When empirical isolated saddle points are close to true isolated saddle points
3) When the empirical risk is close to the true risk.

The answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities.  I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct. 

Another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.  Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.   An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.  However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one. 

Overall, I believe this paper is a nice contribution to the deep learning theory literature. However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem.   Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters.  I suspect their upper bounds will be loose, but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds.  Even if they are loose - identifying the degree of looseness could inspire interesting future work. 
","[7, 7, 3]","[' Good paper, accept', ' Good paper, accept', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
A DIRT-T Approach to Unsupervised Domain Adaptation,"['Rui Shu', 'Hung Bui', 'Hirokazu Narui', 'Stefano Ermon']",Accept,2018,"[5, 3, 2, 10]","[9, 5, 4, 15]","[53, 13, 4, 406]","[22, 6, 1, 199]","[23, 7, 2, 200]","[8, 0, 1, 7]","The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption. The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. 

I found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?

The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Parameter Space Noise for Exploration,"['Matthias Plappert', 'Rein Houthooft', 'Prafulla Dhariwal', 'Szymon Sidor', 'Richard Y. Chen', 'Xi Chen', 'Tamim Asfour', 'Pieter Abbeel', 'Marcin Andrychowicz']",Accept,2018,"[3, 5, 3, 4, 13, 4, 20, 17, 7]","[6, 5, 8, 8, 13, 8, 25, 22, 11]","[17, 22, 22, 14, 9, 47, 328, 608, 48]","[2, 10, 8, 4, 3, 21, 219, 291, 21]","[11, 8, 14, 9, 6, 24, 39, 291, 25]","[4, 4, 0, 1, 0, 2, 70, 26, 2]","This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.

However, the results are weak. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well. E-greedy approaches will always struggle to choose the same random action repeatedly. Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns. Similarly for the continuous control with sparse rewards environments – if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning. However, for complex domains with sparse reward (e.g. Montezuma’s Revenge) parameter space noise is just not going to get you very far.

Overall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don’t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn’t really gain you much more than policies that are biased towards particular actions. Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise.

A non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Compositional Attention Networks for Machine Reasoning,"['Drew A. Hudson', 'Christopher D. Manning']",Accept,2018,"[1, 26]","[5, 31]","[13, 408]","[5, 284]","[8, 104]","[0, 20]","Summary: 
The paper presents a new model called Compositional Attention Networks (CAN) for visual reasoning. The complete model consists of an input unit, a sequence of the proposed Memory, Attention and Composition (MAC) cell, and an output unit. Experiments on CLEVR dataset shows that the proposed model outperforms previous models.

Strengths: 
— The idea of building a compositional model for visual reasoning and visual question answering makes a lot of sense, and, I think, is the correct direction to go forward in these fields.
— The proposed model outperforms existing models pushing the state-of-the-art.
— The proposed model is computationally cheaper and generalizes well with less training data as compared to existing models.
— The proposed model has been described in detail in the paper.

Weaknesses: 
— Given that the performance of state-on-art on CLEVR dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models.
— It is not clear which part of the proposed model leads to how much improvement in performance. Ablations studies are needed to justify the motivations for each of the components of the proposed model.
— Analysis of qualitative results (including attention maps, gate values, etc.) is needed to justify if the model is actually doing what the authors think it should do. For example, the authors mention an example on page 6 at the end of Section 3.2.2, but do not justify if this is actually what the model is doing.
— Why is it necessary to use both question and memory information to answer the question even when the question was already used to compute the memory information? I would think that including the question information helps in learning the language priors in the dataset. Have the authors looked at some qualitative examples where the model which only uses memory information gives an incorrect answer but adding the question information results in a correct answer?
— Details such as using Glove word embeddings are important and can affect the performance of models significantly. Therefore, they should be clearly mentioned in the main paper while comparing with other models which do not use them.
— The comparisons of number of epochs required for training and the training time need fixed batch sizes and CPU/GPU configurations. Is that true? These should be reported in this section.
— The authors claim that their model is robust to linguistic variations and diverse vocabulary, by which I am guessing they are referring to experiments on CLEVR-Humans dataset. What is there in the architecture of the proposed model which provides this ability? If it is the Glove vectors, it should be clearly mentioned since any other model using Glove vectors should have this ability.
— On page 6, second paragraph, the authors mention that there are cases which necessitate the model to ignore current memories. Can the authors show some qualitative examples for such cases?
— In the intro, the authors claim that their proposed cell encourages transparency. But, the design of their cell doesn’t seem to do so, nor it is justified in the paper.

Overall: The performance reported in the paper is impressive and outperforms previous state-of-the-art, but without proper statistical significance analysis of performance, ablation studies, analysis of various attention maps, memory gates, etc. and qualitative results, I am not sure if this work would be directly useful for the research community.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generalizing Across Domains via Cross-Gradient Training,"['Shiv Shankar*', 'Vihari Piratla*', 'Soumen Chakrabarti', 'Siddhartha Chaudhuri', 'Preethi Jyothi', 'Sunita Sarawagi']",Accept,2018,"[2, 4, 26, 14, 10, 25]","[7, 9, 31, 19, 15, 30]","[31, 23, 193, 77, 99, 154]","[14, 9, 116, 27, 62, 89]","[17, 14, 45, 33, 33, 33]","[0, 0, 32, 17, 4, 32]","The method is posed in the Bayesian setting, the main idea being to achieve the data augmentation through domain-guided perturbations of input instances. Different from traditional adaptation methods, where the adaptation step is applied explicitly, in this paper the authors exploit labeled instances from several domains to collectively train a system that can handle new domains without the adaptation step. While this is another way of looking at domain adaptation, it may be misleading to say 'without' adaptation step. By the gradient perturbations on multi-domain training data, the learning of the adaptation step is effectively done. This should be clarified in the paper. The notion of using 'scarce' training domains to cover possible choices for the target domain is interesting and novel. The experimental validation should also include a deeper analysis of this factor: how the proposed adaptation performance is affected by the scarcity of the training multi-domain data. While this is partially shown in Table 8, it seems that by adding more domains the performance is compromised (compared to the baseline) (?).  It would be useful to see how the model ranks the multiple domains in terms of their relatedness to the target domain. Figs 6-7 are unclear and difficult to read. The captions should provide more information about the main point of these figures. ","[7, 7, 8, 7]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Hierarchical Subtask Discovery with Non-Negative Matrix Factorization,"['Adam C. Earle', 'Andrew M. Saxe', 'Benjamin Rosman']",Accept,2018,"[3, 13, 13]","[4, 18, 18]","[7, 43, 74]","[3, 20, 40]","[3, 19, 25]","[1, 4, 9]","This paper proposes a formulation for discovering subtasks in Linearly-solvable MDPs. The idea is to decompose the optimal value function into a fixed set of sub value functions (each corresponding to a subtask) in a way that they best approximate (e.g. in a KL-divergence sense) the original value.

Automatically discovering hierarchies in planning/RL problems is an important problem that may provide important benefits especially in multi-task environments. In that sense, this paper makes a reasonable contribution to that goal for multitask LMDPs. The simulations also show that the discovered hierarchy can be interpreted. Although the contribution is a methodological one, from an empirical standpoint, it may be interesting to provide further evidence of the benefits of the proposed approach. Overall, it would also be useful to provide a short paragraph about similarities to the literature on discovering hierarchies in MDPs.  

A few other comments and questions: 

- This may be a fairly naive question but given your text I'm under the impression that the goal in LMDPs is to find z(s) for all states (and Z in the multitask formulation). Then, your formulation for discovery subtasks seems to assume that Z is given. Does that mean that the LMDPs must first be solved and only then can subtasks be discovered? (The first sentence in the introduction seems to imply that there's hope of faster learning by doing hierarchical decomposition).

- You motivate your approach (Section 3) using a max-variance criterion (as in PCA), yet your formulation actually uses the KL-divergence. Are these equivalent objectives in this case?


Other (minor) comments: 

- In Section it would be good to define V(s) as well as 'i' in q_i (it's easy to mistake it for an index). ","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 3, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Variational image compression with a scale hyperprior,"['Johannes Ballé', 'David Minnen', 'Saurabh Singh', 'Sung Jin Hwang', 'Nick Johnston']",Accept,2018,"[12, 18, 13, 3, 4]","[17, 22, 17, 7, 8]","[53, 42, 68, 20, 33]","[28, 26, 35, 9, 14]","[21, 13, 23, 10, 16]","[4, 3, 10, 1, 3]","The paper is a step forward for image deep compression, at least when departing from the (Balle et al., 2017) scheme.
The proposed hyperpriors are especially useful for medium to high bpp and optimized for L2/ PSNR evaluation.

I find the description of the maths too laconic and hard to follow. For example, what’s the U(.|.) operator in (5)?

What’s the motivation of using GDN as non linearity instead of e.g. ReLU?

I am not getting the need of MSSSIM (dB).  How exactly was it defined/computed?

Importance of training data? The proposed models are trained on 1million images while others like (Theis et al, 2017) and [Ref1,Ref2] use smaller datasets for training.

I am missing a discussion about Runtime / complexity vs. other approaches?

Why MSSSIM is a relevant measure? The Fig. 6 seem to show better visual results for L2 loss (PSNR) than when optimized for MSSSIM, at least in my opinion.

What's the reason to use 4:4:4 for BPG and 4:2:0 for JPEG?

What is the relation between hyperprior and importance maps / content-weights [Ref1] ?

What about reproducibility of the results? Will be the codes/models made publicly available?

Relevant literature:
[Ref1] Learning Convolutional Networks for Content-weighted Image Compression (https://arxiv.org/abs/1703.10553)
[Ref2] Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks  (https://arxiv.org/abs/1704.00648)
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Active Learning for Named Entity Recognition,"['Yanyao Shen', 'Hyokun Yun', 'Zachary C. Lipton', 'Yakov Kronrod', 'Animashree Anandkumar']",Accept,2018,"[-2, 0, 5]","[3, 5, 10]","[2, 10, 57]","[1, 4, 37]","[1, 6, 14]","[0, 0, 6]","This paper studies the application of different existing active learning strategies for the deep models for NER.

Pros:
* Active learning may be used for improving the performance of deep models for NER in practice
* All the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for NER

Cons:
* The novelty of this paper is marginal. The proposed approaches turn out to be a combination of existing active learning strategies for selecting data to query with the existing deep model for NER. 
* No conclusion can be drawn by comparing with the 4 different strategies.

======= After rebuttal  ================

Thank you for the clarification and revision on this paper. It looks better now.

I understand that the purpose of this paper is to give actionable insights to the practice of deep learning. However, since AL itself is a meta learning framework and neural net as the base learner has been shown to be effective for AL, the novelty and contribution of a general discussion of applying AL for deep neural nets is marginal.  What I really expected is a tightly-coupled active learning strategy that is specially designed for the particular deep neural network structure used for NER. Apparently, however, none of the strategies used in this work is designed for this purpose (e.g., the query strategy or model update strategy should at least reflex some properties of deep learning or NER). Thus, it is still below my expectation. 

Anyway, since the authors had attempted to improved this paper, and the results may provide some information to practice, I would like to slightly raise my rating to give this attempt a chance.

","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","['Sanjeev Arora', 'Mikhail Khodak', 'Nikunj Saunshi', 'Kiran Vodrahalli']",Accept,2018,"[29, 2, 2, 3]","[34, 7, 7, 8]","[217, 41, 25, 19]","[103, 16, 12, 8]","[76, 24, 13, 8]","[38, 1, 0, 3]","The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition.  Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.

I didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,"['William Fedus*', 'Mihaela Rosca*', 'Balaji Lakshminarayanan', 'Andrew M. Dai', 'Shakir Mohamed', 'Ian Goodfellow']",Accept,2018,"[2, 3, 10, 8, 13, 10]","[7, 8, 15, 13, 17, 12]","[38, 23, 90, 74, 66, 107]","[12, 8, 28, 28, 27, 48]","[24, 13, 56, 42, 33, 55]","[2, 2, 6, 4, 6, 4]","Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments. 

Clarity: The paper is well-written and clear. The authors could be more concise when reporting results. I would suggest keeping the main results in the main body and move extended results to an appendix.

Originality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs. More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails. They also show experimentally that penalized gradients stabilize the learning process.

Significance: The problems the authors consider is worth exploring further. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally. However, publishing this  work is in my opinion premature for the following reasons:

- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap;
- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory;
- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.

","[4, 7, 8]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Multi-Mention Learning for Reading Comprehension with Neural Cascades,"['Swabha Swayamdipta', 'Ankur P. Parikh', 'Tom Kwiatkowski']",Accept,2018,"[7, 9, 9]","[12, 14, 14]","[57, 62, 39]","[27, 27, 17]","[30, 31, 17]","[0, 4, 5]","This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long).

Overall, I think it is a nice demonstration that non-recurrent models can work so well, but I also don’t find the results strikingly surprising. It is also a bit hard to get the main takeaway messages. It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547). But all the other components seem to have been demonstrated previously in other papers. 

An important feature of this model is it is easier to parallelize and speed up the training/testing processes. However, I don’t see any demonstration of this in the experiments section.

Also, I am a bit disappointed by how “cascades” are actually implemented. I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on). The proposed model just simply sums up all the occurrences of candidate answers throughout the full document. 3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity. 

I am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. 
","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,"['Víctor Campos', 'Brendan Jou', 'Xavier Giró-i-Nieto', 'Jordi Torres', 'Shih-Fu Chang']",Accept,2018,"[4, 9, 16, 30, 28]","[8, 13, 21, 35, 33]","[20, 35, 168, 167, 601]","[8, 20, 85, 115, 370]","[11, 12, 67, 22, 116]","[1, 3, 16, 30, 115]","UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]).

The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.

Pros:
- Task of reducing computation by skipping inputs is interesting
- Model is novel and interesting
- Experiments on multiple tasks and datasets confirm the efficacy of the method
- Skipping behavior can be controlled via an auxiliary loss term
- Paper is clearly written

Cons:
- Missing comparison to prior work on sequential MNIST
- Low performance on Charades dataset, no comparison to prior work
- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification

The task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.

However, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work.

On permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task.

For Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on “fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge”, and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don’t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.

In a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks.

Though experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results.

On the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author’s own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.


References

[1] Le et al, “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”, arXiv 2015
[2] Arjovsky et al, “Unitary Evolution Recurrent Neural Networks”, ICML 2016
[3] Cooijmans et al, “Recurrent Batch Normalization”, ICLR 2017
[4] Zhang et al, “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS 2016
[5] Sigurdsson et al, “Hollywood in homes: Crowdsourcing data collection for activity understanding”, ECCV 2016
[6] Sigurdsson et al, “Asynchronous temporal fields for action recognition”, CVPR 2017","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Dynamic Neural Program Embeddings for Program Repair,"['Ke Wang', 'Rishabh Singh', 'Zhendong Su']",Accept,2018,"[10, 21]","[14, 26]","[133, 178]","[62, 131]","[50, 23]","[21, 24]","Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. The application is to predict errors made by students on programming tasks. This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).

---

Quality: The experiments compare the three proposed neural network architectures with two syntax-based architectures. It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings.
Clarity: The paper is clearly written.
Originality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know).
Significance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.

---

Some questions/comments:
- Do we need to add the print statements for any new programs that the students submit? What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated?

---

References 

Cai, J., Shin, R., & Song, D. (2017). Making Neural Programming Architectures Generalize via Recursion. In International Conference on Learning Representations (ICLR).","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Can recurrent neural networks warp time?,"['Corentin Tallec', 'Yann Ollivier']",Accept,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","Summary:
This paper shows that incorporating invariance to time transformations in recurrent networks naturally results in a gating mechanism used by LSTMs and their variants. This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known. Experiments demonstrate that the proposed initialization speeds up learning on synthetic tasks, although benefits for next-step prediction tasks are limited.

Quality and significance:
The core insight of the paper is the link between recurrent network design and its effect on how the network reacts to time transformations. This insight is simple, elegant and valuable in my opinion. 

It is becoming increasingly apparent recently that the benefits of the gating and cell mechanisms introduced by the LSTM, now also used in feedforward networks, go beyond avoiding vanishing gradients. The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases. Understanding the link between model architecture and behavior is very useful for the field in general, and this paper contributes to this knowledge. In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8. The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive. 

Clarity:
The paper is well-written in general and easy to understand. A minor complaint is that there are an unnecessarily large number of paragraph breaks, especially on pages 3 and 4, which make reading slightly jarring.","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Consequentialist conditional cooperation in social dilemmas with imperfect information,"['Alexander Peysakhovich', 'Adam Lerer']",Accept,2018,"[5, 10]","[9, 14]","[49, 55]","[21, 26]","[23, 27]","[5, 2]","This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information. The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. 

From a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information. It is basically a straightforward generalization of the idea of punishing, which is common in ""folk theorems"" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be ""the natural"" solution but in general it is far from clear why all players would want to maximize the total payoff. 

The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.

In contrast, the paper ""Coco-Q: Learning in Stochastic Games with Side Payments"" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.

It should also be noted that I was asked to review another ICLR submission entitled ""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING"" which amazingly introduced the same ""Pong Player’s Dilemma"" game as in this paper. 

Notice the following suspiciously similar paragraphs from the two papers:

From ""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING"":
We also look at an environment where strategies must be learned from raw pixels. We use the method
of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a
point they receive a reward of 1 and the other player receives −2. We refer to this game as the Pong
Player’s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully
cooperative agent can be exploited by a defector.

From ""CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION"":
To demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong 
which makes the game into a social dilemma. In what we call the Pong Player’s Dilemma (PPD) when an agent 
scores they gain a reward of 1 but the partner receives a reward of −2. Thus, in the PPD the only (jointly) winning
move is not to play, but selfish agents are again tempted to defect and try to score points even though
this decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this
game.","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generating Natural Adversarial Examples,"['Zhengli Zhao', 'Dheeru Dua', 'Sameer Singh']",Accept,2018,"[-3, 7, 11]","[1, 11, 16]","[1, 38, 72]","[1, 27, 52]","[0, 1, 0]","[0, 10, 20]","
Summary:
 A method for creation of semantical adversary examples in suggested. The ‘semantic’ property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter). The generator is trained with a WGAN optimization. Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space. The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences. It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.
Page 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated. This should be explicitly said and the parameter value should be stated.
Page 4: 
-	the implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers. This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described.
-	the visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier. In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier). In the churce-vs. tower case, a  relatively weak MLP classifier was used. It would be more instructive to see the results for a better, convolutional classifier.
Page 5:
-	the description of the various networks used for text generation is insufficient for understanding:
o	The AREA is described in two sentences. It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task
o	 ‘inverter’ here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces.
o	 It is not clear what the ‘four-layers strided CNN’ is: its structure, its role in the system. How is it optimized?
o	In general: a block diagram showing the relation between all the system’s components may be useful, plus the details about the structure and optimization of the various modules. It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough. Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.
o	SNLI data should be described: content, size, the task it is used for


Pro:
-	A novel idea of producing natural adversary examples with a GAN
-	The generated examples are in some cases useful for interpretation and network understanding 
-	The method enables creation of adversarial examples for block box classifiers
Cons
-	The idea implementation is basic. Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested
-	The generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier). Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. 

","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Large Scale Optimal Transport and Mapping Estimation,"['Vivien Seguy', 'Bharath Bhushan Damodaran', 'Remi Flamary', 'Nicolas Courty', 'Antoine Rolet', 'Mathieu Blondel']",Accept,2018,"[4, 5, 9, 19, 3, 9]","[8, 10, 14, 24, 7, 14]","[12, 32, 112, 129, 8, 65]","[3, 13, 42, 56, 3, 30]","[4, 13, 50, 41, 2, 28]","[5, 6, 20, 32, 3, 7]","The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems.

I like the weak convergence results, but this is just weak convergence. It appears to be an overstatement to claim that the approach ""nearly-optimally"" transports one distribution to the other (Cf e.g. Conclusion). There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2. Also, near-optimality would refer to some parameters being chosen in the best possible way. I do not see that from the paper. However, the weak convergence results are good.

A better result, hinting on how ""optimal"" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist). This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum. 

I also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? After all, the L2 one is just an approximation of the entropic one.

Typoes:

1- Kanthorovich -> Kantorovich (Intro)
2- Cal C <-> C (eq. 4)","[6, 8, 7, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Stabilizing Adversarial Nets with Prediction Methods,"['Abhay Yadav', 'Sohil Shah', 'Zheng Xu', 'David Jacobs', 'Tom Goldstein']",Accept,2018,"[9, 9, 7, 28, 15]","[12, 13, 12, 33, 20]","[13, 15, 64, 186, 296]","[8, 7, 27, 97, 113]","[5, 7, 34, 46, 166]","[0, 1, 3, 43, 17]","NOTE:
I'm very willing to change my recommendation if I turn out to be wrong 
about the issues I'm addressing and if certain parts of the experiments are fixed.

Having said that, I do (think I) have some serious issues: 
both with the experimental evaluation and with the theoretical results.
I'm pretty sure about the experimental evaluation and less sure about the theoretical results.


THEORETICAL CLAIMS:

These are the complaints I'm not as sure about:

Theorem 1 assumes that L is convex/concave.
This is not generally true for GANs.
That's fine and it doesn't necessarily make the statement useless, but:

If we are willing to assume that L is convex/concave, 
then there already exist other algorithms that will provably converge
to a saddle point (I think). [1] contains an explanation of this.
Given that there are other algorithms with the same theoretical guarantees,
and that those algorithms don't magically make GANs work better, 
I am much less convinced about the value of your theorem.

In [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, 
even when L is not convex/concave. 
This seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this?

Finally, I'm not totally sure you can show that simultaneous gradient descent won't converge 
as well under the assumptions you made.
If you actually can't show that, then the therom *is* useless, 
but it's also the thing I've said that I'm the least sure about.


EXPERIMENTAL EVALUATION:

Regarding the claims of being able to train with a higher learning rate:
I would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness')
a high goodness was achieved faster because a higher learning rate was used.
Your experiments don't support this claim presently, because you evaluate all the models at the same step.
In fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate.
This calls into question the usefulness of training with a higher learning rate.
The performance is not a huge amount worse though (based on my understanding of Inception Scores),
so if it turns out that you could get that performance
in 1/10th the time then that wouldn't be so bad.

Regarding the experiment with Stacked GANs, the scores you report are lower than what they report [2].
Their reported mean score for joint training is 8.59.
Are the baseline scores you report from an independent reproduction?
Also, the model they have trained uses label information. 
Does your model use label information?
Given that your reported improvements are small, it would be nice to know what the proposed mechanism is by 
which the score is improved. 
With a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs
had 'stability issues', so it doesn't seem like 'more stable training' can be the answer.

Finally, papers making claims about fixing GAN stability should support those claims by solving problems
with GANs that people previously had a hard time solving (due to instability).
I don't believe this is true of CIFAR10 (especially if you're using the class information).
See [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator.

I didn't pay as much attention to the non-GAN experiments because
a) I don't have as much context for evaluating them, because they are a bit non-standard.
b) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed.


[0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable)

[1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs)

[2] https://arxiv.org/abs/1612.04357 (Stacked GAN)

[3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)

EDIT: 
As discussed below, I have slightly raised my score. 
I would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly).
I have also slightly raised my confidence.
This is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).","[4, 9, 7]","[' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
SEARNN: Training RNNs with global-local losses,"['Rémi Leblond', 'Jean-Baptiste Alayrac', 'Anton Osokin', 'Simon Lacoste-Julien']",Accept,2018,"[3, 4, 10, 14]","[7, 9, 15, 19]","[16, 60, 47, 140]","[5, 25, 26, 59]","[9, 33, 19, 73]","[2, 2, 2, 8]","This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN. Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model.

The underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique.

I am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset. As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset. For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component.

This reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains.

Overall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights.

(extremely minor typo: ""One popular possibility from L2S is go the full reduction route down to binary classification"")","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings,"['Kangwook Lee', 'Hoon Kim', 'Changho Suh']",Accept,2018,"[8, 21, 17]","[13, 26, 22]","[95, 131, 140]","[45, 79, 71]","[37, 1, 41]","[13, 51, 28]","General comment:

This paper proposes a GAN-based method which learns bidirectional mappings between the real-data and the simulated data. The proposed methods builds upon the CycleGAN and the Simulated+Unsupervised (S+U) learning frameworks. The authors show that the proposed method is able to fully leverage the flexibility of simulators by presenting an improved performance on the gaze estimation task.

Detailed comments:

1. The proposed method seems to be a direct combination of the CycleGAN and the S+U learning. Firstly, the CycleGAN propose a to learn a bidirectional GAN model between for image translation. Here the author apply it by ""translating"" the the simulated data to real-data. Moreover, the mapping from simulated data to the real-data is learned, the S+U learning framework propose to train a model on the simulated data.

Hence, this paper seems to directly apply S+U learning to CycleGAN. The properties of the proposed method comes immediately from CycleGAN and S+U learning. Without deeper insights of the proposed method, the novelty of this paper is not sufficient.

2. When discussing CycleGAN, the authors claim that CycleGAN is not good at preserving the labels. However, it is not clear what the meaning of preserving labels is. It would be nice if the authors clearly define this notion and rigorously discuss why CycleGAN is insufficient to reach such a goal and why combining with S+U learning would help.

3. This work seems closely related to S+U learning. It would be nice if the authors also summarize S+U learning in Section 2, in the similar way they summarize CycleGAN in Section 2.2.

4. In Section 2.2, the authors claim that the Cycle-consistency loss in CycleGAN is not sufficient for label preservation. To improve, they propose to use the feature consistency loss. However, the final loss function also contains this cycle-consistency loss. Moreover, in the experiments, the authors indeed use the cycle-consistency loss by setting \lambda_{cyc} = 10. But the feature consistency loss may not be used by setting \lambda_{feature} = 0 or 0.5. From table Two, it appears that whether using the feature-consistency loss does not have significant effect on the performance.

It would be nice to conduct more experiments to show the effect of adding the feature-consistent loss. Say, setting \lambda_{cyc} = 0 and try different values of \lambda_{feature}. Otherwise it is unclear whether the feature-consistent loss is necessary.
","[3, 6, 6]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering,"['Elliot Meyerson', 'Risto Miikkulainen']",Accept,2018,"[5, 31]","[10, 36]","[33, 329]","[17, 204]","[15, 50]","[1, 75]","Summary: This paper proposes a different approach to deep multi-task learning using “soft ordering.”  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across. Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks. The idea of “soft ordering” enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular. 

The methods reviewed prior work which the authors refer to as “parallel order”, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn’t be the case. They authors then extend the approach to “permuted order” and finally present their proposed “soft ordering” approach. The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance. 

The “soft ordering” approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task. The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to “parallel ordering” and “permuted ordering” and show the performance gain.

Positives: 
- The paper is clearly written and easy to follow
- The idea is novel and impactful if its evaluated properly and consistently 
- The authors did a great job summarizing prior work and motivating their approach

Negatives: 
- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation). CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection.  
(TODO) I would recommend that the authors test their approach on such setting.
- Figure 6 is a bit confusing, the authors do not explain why the “Permuted Order” performs worse than “Parallel Order”. Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task. 
 (TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader.
- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)? How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL. 
(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare.

Notation and Typos:
- Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify.
1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order.
 2- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense.
- Figure 4 is very small and not easy to read the text. Does single task mean average performance over the tasks? 
- In eq.(3) Choosing \sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol.
 Conclusion: I would suggest that the authors address the concerns mentioned above. Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Towards better understanding of gradient-based attribution methods for Deep Neural Networks,"['Marco Ancona', 'Enea Ceolini', 'Cengiz Öztireli', 'Markus Gross']",Accept,2018,"[2, 3, 11, 30]","[5, 8, 16, 35]","[6, 25, 60, 471]","[2, 16, 20, 228]","[3, 5, 19, 25]","[1, 4, 21, 218]","This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:
- \epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training.
- This gives a more unified way of understanding, and implementing the methods.
- The paper points out situations when the methods are equivalent
- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity
- The paper proposes a new objective function to measure joint sensitivity

Overall, I believe this paper to be a useful contribution to the literature. It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods. Especially the latter will be appreciated.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Unsupervised Machine Translation Using Monolingual Corpora Only,"['Guillaume Lample', 'Alexis Conneau', 'Ludovic Denoyer', ""Marc'Aurelio Ranzato""]",Accept,2018,"[4, 3, 16, 13]","[9, 8, 20, 18]","[51, 56, 147, 106]","[24, 27, 88, 57]","[27, 29, 40, 44]","[0, 0, 19, 5]","This paper introduces an architecture for training a MT model without any parallel material, and tests it on benchmark datasets (WMT and captions) for two language pairs. Although the resulting performance is only about half that of a more traditional model, the fact that this is possible at all is remarkable.

The method relies on fairly standard components which will be familiar to most readers: a denoising auto-encoder and an adversarial discriminator. Not much detail is given on the actual models used, for which the authors mainly refer to prior work. This is disappointing: the article would be more self-contained by providing even a high-level description of the models, such as provided (much too late) for the discriminator architecture.

Misc comments:

""domain"" seems to be used interchangeably with ""language"". This is unfortunate as ""domain"" has another, specific meaning in NLP in general and SMT in partiular. Is this intentional (if so what is the intention?) or is this just a carry-over from other work in cross-domain learning?

Section 2.3: How do you sample permutations for the noise model, with the constraint on reordering range, in the general case of sentences of arbitrary lengths?

Section 2.5: ""the previously introduced loss [...] mitigates this concern"" -- How? Is there a reference backing this?

Figure 3: In the caption, what is meant by ""(t) = 1""? Are these epochs only for the first iteration (from M(1) to M(2))?

Section 4.1: Care is taken to avoid sampling corresponding src and tgt sentences. However, was the parallel corpus checked for duplicates or near duplicates? If not, ""aligned"" segments may still be present. (Although it is clear that this information is not used in the algorithm)

This yields a natural question: Although the two monolingual sets extracted from the parallel data are not aligned, they are still very close. It would be interesting to check how the method behaves on really comparable corpora where its advantage would be much clearer.

Section 4.2 and Table 1: Is the supervised learning approach trained on the full parallel corpus? On a parallel corpus of similar size?

Section 4.3: What are the quoted accuracies (84.48% and 77.29%) measured on?

Section 4.5: Experimental results show a regular inprovement from iteration 1 to 2, and 2 to 3. Why not keep improving performance? Is the issue training time?

References: (He, 2016a/b) are duplicates

Response read -- thanks.","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
NerveNet: Learning Structured Policy with Graph Neural Networks,"['Tingwu Wang', 'Renjie Liao', 'Jimmy Ba', 'Sanja Fidler']",Accept,2018,"[3, 9, 8, 16]","[8, 14, 13, 21]","[16, 99, 96, 298]","[7, 52, 46, 155]","[9, 43, 49, 133]","[0, 4, 1, 10]","This paper proposes NerveNet to represent and learn structured policy for continuous control tasks. Instead of using the widely adopted fully connected MLP, this paper uses Graph Neural Networks to learn a structured controller for various MuJoco environments. It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine-tuning of transfer.

The idea to build structured policy is novel for continuous control tasks. It is an exciting direction since there are inherent structures that should be exploited in many control tasks, especially for locomotion. This paper explores this less-studied area and demonstrates promising results.

The presentation is mostly clear. Here are some questions and a list of minor suggestions:
1) In the Output Model section, I am not sure how the controller is shared. It first says that ""Nodes with the same node type should share the instance of MLP"", which means all the ""joint"" nodes should share the same controller. But later it says ""Two LeftHip should have a shared controller."" What about RightHip? or Ankle? They all belongs to the same node type ""joint"". Am I missing something here? It seems that in this paper, weights sharing is an essential part of the structured policy, it would be great if it can be described in more details.

2) In States Update of Propagation Model Section, it is not clear how the aggregated message is used in eq. (4).

3) Typo in Caption of Table 1: CentipedeFour not CentipedeSix.

4) If we just use MLP but share weights among joints (e.g. the weights from observation to action of all the LeftHips are constrained to be same), how would it compare to the method proposed in this paper?

In summary, I think that it is worthwhile to develop structured representation of policies for control tasks. It is analogue to use CNN that share weights between kernels for computer vision tasks. I believe that this paper could inspire many follow-up work. For this reason, I would recommend accepting this paper.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"['Benjamin Eysenbach', 'Shixiang Gu', 'Julian Ibarz', 'Sergey Levine']",Accept,2018,"[3, 7, 8, 10]","[8, 12, 13, 15]","[64, 89, 45, 743]","[27, 38, 17, 326]","[36, 49, 24, 396]","[1, 2, 4, 21]","This paper proposes the idea of having an agent learning a policy that resets the agent's state to one of the states drawn from the distribution of starting states. The agent learns such policy while also learning how to solve the actual task. This approach generates more autonomous agents that require fewer human interventions in the learning process. This is a very elegant and general idea, where the value function learned in the reset task also encodes some measure of safety in the environment.

All that being said, I gave this paper a score of 6 because two aspects that seem fundamental to me are not clear in the paper. If clarified, I'd happily increase my score.

1) *Defining state visitation/equality in the function approximation setting:* The main idea behind the proposed algorithm is to ensure that ""when the reset policy is executed from any state, the distribution over final states matches the initial state distribution p_0"". This is formally described, for example, in line 13 of Algorithm 1.
The authors ""define a set of safe states S_{reset} \subseteq S, and say that we are in an irreversible state if the set of states visited by the reset policy over the past N episodes is disjoint from S_{reset}."" However, it is not clear to me how one can uniquely identify a state in the function approximation case. Obviously, it is straightforward to apply such definition in the tabular case, where counting state visitation is easy. However, how do we count state visitation in continuous domains? Did the authors manually define the range of each joint/torque/angle that characterizes the start state? In a control task from pixels, for example, would the exact configuration of pixels seen at the beginning be the start state? Defining state visitation in the function approximation setting is not trivial and it seems to me the authors just glossed over it, despite being essential to your work.

2) *Experimental design for Figure 5*: This setup is not clear to me at all and in fact, my first reaction is to say it is wrong. An episodic task is generally defined as: the agent starts in a state drawn from the distribution of starting states and at the moment it reaches the goal state, the task is reset and the agent starts again. It doesn't seem to be what the authors did, is that right? The sentence: ""our method learns to solve this task by automatically resetting the environment after each episode, so the forward policy can practice catching the ball when initialized below the cup"" is confusion. When is the task reset to the ""status quo"" approach? Also, let's say an agent takes 50 time steps to reach the goal and then it decides to do a soft-reset. Are the time steps it is spending on its soft-reset being taken into account when generating the reported results?


Some other minor points are:

- The authors should standardize their use of citations in the paper. Sometimes there are way too many parentheses in a reference. For example: ""manual resets are necessary when the robot or environment breaks (e.g. Gandhi et al. (2017))"", or ""Our methods can also be used directly with any other Q-learning methods ((Watkins & Dayan, 1992; Mnih et al., 2013; Gu et al., 2017; Amos et al., 2016; Metz et al., 2017))""

- There is a whole line of work in safe RL that is not acknowledged in the related work section. Representative papers are:
    [1] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High-Confidence Off-Policy Evaluation. AAAI 2015: 3000-3006
    [2] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High Confidence Policy Improvement. ICML 2015: 2380-2388

- In the Preliminaries Section the next state is said to be drawn from s_{t+1} ~ P(s'| s, a). However, this hides the fact the next state is dependent on the environment dynamics and on the policy being followed. I think it would be clearer if written: s_{t+1} ~ P(s'| s, \pi(a|s)).

- It seems to me that, in Algorithm 1, the name 'Act' is misleading. Shouldn't it be 'ChooseAction' or 'EpsilonGreedy'? If I understand correctly, the function 'Act' just returns the action to be executed, while the function 'Step' is the one that actually executes the action.

- It is absolutely essential to depict the confidence intervals in the plots in Figure 3. Ideally we should have confidence intervals in all the plots in the paper.","[7, 6, 5, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neural Language Modeling by Jointly Learning Syntax and Lexicon,"['Yikang Shen', 'Zhouhan Lin', 'Chin-wei Huang', 'Aaron Courville']",Accept,2018,"[5, 7, 10, 18]","[10, 12, 15, 23]","[49, 59, 39, 309]","[23, 24, 14, 135]","[25, 31, 19, 160]","[1, 4, 6, 14]","** UPDATE ** upgraded my score to 7 based on the new version of the paper.

The main contribution of this paper is to introduce a new recurrent neural network for language modeling, which incorporates a tree structure More precisely, the model learns constituency trees (without any supervision), to capture syntactic information. This information is then used to define skip connections in the language model, to capture longer dependencies between words. The update of the hidden state does not depend only on the previous hidden state, but also on the hidden states corresponding to the following words: all the previous words belonging to the smallest subtree containing the current word, such that the current word is not the left-most one. The authors propose to parametrize trees using ""syntactic distances"" between adjacent words (a scalar value for each pair of adjacent words w_t, w_{t+1}). Given these distances, it is possible to obtain the constituents and the corresponding gating activations for the skip connections. These different operations can be relaxed to differentiable operations, so that stochastic gradient descent can be used to learn the parameters. The model is evaluated on three language modeling benchmarks: character level PTB, word level PTB and word level text8. The induced constituency trees are also evaluated, for sentence of length 10 or less (which is the standard setting for unsupervised parsing).

Overall, I really like the main idea of the paper. The use of ""syntactic distances"" to parametrize the trees is clever, as they can easily be computed using only partial information up to time t. From these distances, it is also relatively straightforward to obtain which constituents (or subtrees) a word belongs to (and thus, the corresponding gating activations). Moreover, the operations can easily be relaxed to obtain a differentiable model, which can easily be trained using stochastic gradient descent.

The results reported on the language modeling experiments are strong. One minor comment here is that it would be nice to have an ablation analysis, as it is possible to obtain similarly strong results with simpler models (such as plain LSTM).

My main concern regarding the paper is that it is a bit hard to understand. In particular in section 4, the authors alternates between discrete and relaxed values: end of section 4.1, it is implied that alpha are in [0, 1], but in equation 6, alpha are in {0, 1}, then relaxed in equation 9 to [0, 1] again. I am also wondering whether it would make more sense to start by introducing the syntactic distances, then the alphas and finally the gates? I also found the section 5 to be quite confusing. While I get the	general idea, I am not sure what is the relation between hidden states h and m (section 5.1). Is there a mixup between h defined in equation 10 and h from section 5.1? I am aware that it is not straightforward to describe the proposed method, but believe it would be a much stronger paper if written more clearly.

To conclude, I really like the method proposed in this paper, and believe that the experimental results are quite strong.
My main concern	regarding the paper is its clarity: I will gladly increase my score if the authors can improve the writing.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Adaptive Dropout with Rademacher Complexity Regularization,"['Ke Zhai', 'Huan Wang']",Accept,2018,"[8, 19]","[8, 24]","[10, 345]","[8, 147]","[1, 49]","[1, 149]","This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory.

Major comments:
(1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix.
(2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of \sum\sigma_if^L(x_i,w) in (5).
(3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there.
(4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency?

Minor comments:
(1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5}
(2) Abstract: there should be a space before ""Experiments"".
(3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1

Summary:
The mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis.

----------------------------
After Rebuttal:
Thank you for revising the paper. I think there are still some possible problems. 
Let us consider eq (12) in the appendix on the contraction property of Rademacher complexity (RC).
(1) Since you consider a variant of RC with absolute value inside the supermum, to my best knowledge, the contraction property (12) should involve an additional factor of 2, see, e.g., Theorem 12 of ""Rademacher and Gaussian Complexities: Risk Bounds and Structural Results"" by Bartlett and Mendelson. Since you need to apply this contraction property L times, there should be a factor of 2^L in the error bound. This make the bound not appealing for neural networks with a moderate L.
(2) Second, the function g involves an expectation w.r.t. r before the activation function. I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Implicit Causal Models for Genome-wide Association Studies,"['Dustin Tran', 'David M. Blei']",Accept,2018,"[4, 18]","[9, 23]","[76, 259]","[31, 140]","[40, 88]","[5, 31]","This paper tackles two problems common in genome-wide association studies: confounding (i.e. structured noise) due to population structure and the potential presence of non-linear interactions between different parts of the genome. To solve the first problem this paper effectively suggests learning the latent confounders jointly with the rest of the model. For the second problem, this paper proposes “implicit causal models’, that is, models that  leverage neural architectures with an implicit density. 

The main contribution of this paper is to create a bridge between the statistical genetics community and the ML community. The method is technically sound and does indeed generalize techniques currently used in statistical genetics. The main concerns with this paper is that 1) the claim that it can detect epistatic interactions is not really supported. Yes, in principle the neural model used to model y could detect them, but no experiments are shown to really tease this case apart 2) validating GWAS results is really hard, because no causal information is usually available. The authors did a great job on the simulation framework, but table 1 falls short in terms of evaluation metric: to properly assess the performance of the method on simulated data, it would be good to have evidence that the type 1 error is calibrated (e.g. by means of qq plots vs null distribution) for all methods. At the very least, a ROC curve could be used to show the quality of the ranking of the causal SNPs for each method, irrespective of p-value cutoff.

Quality: see above. The technical parts of this paper are definitely high-quality, the experimental side could be improved.
Clarity: if the target audience of this paper is the probabilistic ML community, it’s very clear. If the statistical genetics community is expected to read this, section 3.1 could result too difficult to parse. As an aside: ICLR might be the right venue for this paper given the high ML content, but perhaps a bioinformatics journal would be a better fit, depending on intended audience.
","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Training GANs with Optimism,"['Constantinos Daskalakis', 'Andrew Ilyas', 'Vasilis Syrgkanis', 'Haoyang Zeng']",Accept,2018,"[15, 5, 10, 3]","[20, 10, 15, 6]","[243, 48, 158, 9]","[115, 21, 70, 2]","[103, 27, 78, 1]","[25, 0, 10, 6]","This paper proposes the use of optimistic mirror descent to train Wasserstein Generative Adversarial Networks (WGANS). The authors remark that the current training of GANs, which amounts to solving a zero-sum game between a generator and discriminator, is often unstable, and they argue that one source of instability is due to limit cycles, which can occur for FTRL-based algorithms even in convex-concave zero-sum games. Motivated by recent results that use Optimistic Mirror Descent  (OMD) to achieve faster convergence rates (than standard gradient descent) in convex-concave zero-sum games and normal form games, they suggest using these techniques for WGAN training as well. The authors prove that, using OMD, the last iterate converges to an equilibrium and use this as motivation that OMD methods should be more stable for WGAN training. They then compare OMD against GD on both toy simulations and a DNA sequence task before finally introducing an adaptive generalization of OMD, Optimistic Adam, that they test on CIFAR10. 

This paper is relatively well-written and clear, and the authors do a good job of introducing the problem of GAN training instability as well as the OMD algorithm, in particular highlighting its differences with standard gradient descent as well as discussing existing work that has applied it to zero-sum games. Given the recent work on OMD for zero-sum and normal form games, it is natural to study its effectiveness in training GANs.The issue of last iterate versus average iterate for non convex-concave problems is also presented well.  

The theoretical result on last-iterate convergence of OMD for bilinear games is interesting, but somewhat wanting as it does not provide an explicit convergence rate as in Rakhlin and Sridharan, 2013. Moreover, the result is only at best a motivation for using OMD in WGAN training since the WGAN optimization problem is not a bilinear game. 

The experimental results seem to indicate that OMD is at least roughly competitive with GD-based methods, although they seem less compelling than the prior discussion in the paper would suggest. In particular, they are matched by SGD with momentum when evaluated by last epoch performance (albeit while being less sensitive to learning rates). OMD does seem to outperform SGD-based methods when using the lowest discriminator loss, but there doesn't seem to be even an attempt at explaining this in the paper. 

I found it a bit odd that Adam was not used as a point of comparison in Section 5, that optimistic Adam was only introduced and tested for CIFAR but not for the DNA sequence problem, and that the discriminator was trained for 5 iterations in Section 5 but only once in Section 6, despite the fact that the reasoning provided in Section 6 seems like it would have also applied for Section 5. This gives the impression that the experimental results might have been at least slightly ""gamed"". 

For the reasons above, I give the paper high marks on clarity, and slightly above average marks on originality, significance, and quality.

Specific comments:
Page 1, ""no-regret dynamics in zero-sum games can very often lead to limit cycles"": I don't think limit cycles are actually ever formally defined in the entire paper.  
Page 3, ""standard results in game theory and no-regret learning"": These results should be either proven or cited.
Page 3: Don't the parameter spaces need to be bounded for these convergence results to hold? 
Page 4, ""it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm"": For completeness, this should probably either be (quickly) proven or a reference should be provided.
Page 5, ""the unique equilibrium of the above game is...for the discriminator to choose w=0"": Why is w=0 necessary here?
Page 6, ""We remark that the set of equilibrium solutions of this minimax problem are pairs (x,y) such that x is in the null space of A^T and y is in the null space of A"": Why is this true? This should either be proven or cited.
Page 6, Initialization and Theorem 1: It would be good to discuss the necessity of this particular choice of initialization for the theoretical result. In the Initialization section, it appears simply to be out of convenience.
Page 6, Theorem 1: It should be explicitly stated that this result doesn't provide a convergence rate, in contrast to the existing OMD results cited in the paper.   
Page 7, ""we considered momentum, Nesterov momentum and AdaGrad"": Why isn't Adam used in this section if it is used in  later experiments?
Page 7-8, ""When evaluated by....the lowest discriminator loss on the validation set, WGAN trained with Stochastic OMD (SOMD) achieved significantly lower KL divergence than the competing SGD variants."": Can you explain why SOMD outperforms the other methods when using the lowest discriminator loss on the validation set? None of the theoretical arguments presented earlier in the paper seem to even hint at this. The only result that one might expect from the earlier discussion and results is that SOMD would outperform the other methods when evaluating by the last epoch. However, this doesn't even really hold, since there exist learning rates in which SGD with momentum matches the performance of SOMD.
Page 8, ""Evaluated by the last epoch, SOMD is much less sensitive to the choice of learning rate than the SGD variants"": Learning rate sensitivity doesn't seem to be touched upon in the earlier discussion. Can these results be explained by theory?
Page 8, ""we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training"": These results don't mean much without error bars.
Page 8, ""we only trained the discriminator once after one iteration of generator training. The latter is inline with the intuition behind the use of optimism...."": Why didn't this logic apply to the previous section on DNA sequences, where the discriminator was trained multiple times?


After reading the response of the authors (in particular their clarification of some technical results and the extra experiments they carried out during the rebuttal period), I have decided to upgrade my rating of the paper from a 6 to a 7. Just as a note, Figure 3b is now very difficult to read. 

","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Latent Space Oddity: on the Curvature of Deep Generative Models,"['Georgios Arvanitidis', 'Lars Kai Hansen', 'Søren Hauberg']",Accept,2018,"[7, 29, 11]","[11, 34, 16]","[21, 227, 100]","[11, 127, 48]","[10, 31, 40]","[0, 69, 12]","The paper makes an important observation: the generating function of a generative model (deep or not) induces a (stochastic) Riemannian metric tensor on the latent space. This metric might be the correct way to measure distances in the latent space, as opposed to the Euclidean distance.

While this seems obvious, I had actually always thought of the latent space as ""unfolding"" the data manifold as it exists in the output space. The authors propose a different view which is intriguing; however, they do not, to the best of my understand, give a definitive theoretical reason why the induced Riemannian metric is the correct choice over the Euclidean metric.

The paper correctly identifies an important problem with the way most deep generative models evaluate variance. However the solution proposed seems ad-hoc and not particularly related to the other parts of the paper. While the proposed variance estimation (using RBF networks) might work in some cases, I would love to see (perhaps in future work) a much more rigorous treatment of the subject.

Pros:
1. Interesting observation and mathematical development of a Riemannian metric on the latent space.

2. Good observation about the different roles of the mean and the variance in determining the geodesics: they tend to avoid areas of high variance.

3. Intriguing experiments and a good effort at visualizing and explaining them. I especially appreciate the interpolation and random walk experiments. These are hard to evaluate objectively, but the results to hint at the phenomena the authors describe when comparing Euclidean to Riemannian metrics in the latent space.

Cons:
1. The part of the paper proposing new variance estimators is ad-hoc and is not experimented with rigorously, comparing it to other methods in terms of calibration for example. 

Specific comments:
1. To the best of my understanding eq. (2) does not imply that the natural distance in Z is locally adaptive. I think of eq (2) as *defining* a type of distance on Z, that may or may not be natural. One could equally argue that the Euclidean distance on z is natural, and that this distance is then pushed forward by f to some induced distance over X. 

2. In the definition of paths \gamma, shouldn't they be parametrized by arc-length (also known as unit-speed)? How should we think of the curve \gamma(t^2) for example?

3. In Theorem 2, is the term ""input dimension"" appropriate? Perhaps ""data dimension"" is better?

4. I did not fully understand the role of the LAND model. Is this a model fit AFTER fitting the generative model, and is used to cluster Z like a GMM ? I would appreciate a clarification about the context of this model.","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Awareness Models,"['Brandon Amos', 'Laurent Dinh', 'Serkan Cabi', 'Thomas Rothörl', 'Sergio Gómez Colmenarejo', 'Alistair Muldal', 'Tom Erez', 'Yuval Tassa', 'Nando de Freitas', 'Misha Denil']",Accept,2018,"[3, 3, 32, 30]","[7, 8, 37, 35]","[19, 25, 254, 243]","[9, 12, 115, 136]","[10, 13, 77, 83]","[0, 0, 62, 24]","The authors explore how sequence models that look at proprioceptive signals from a simulated or real-world robotic hand can be used to decode properties of objects (which are not directly observed), or produce entropy maximizing or minimizing motions.

The overall idea presented in the paper is quite nice: proprioception-based models that inject actions and encoder/pressure observations can be used to measure physical properties of objects that are not directly observed, and can also be used to create information gathering (or avoiding) behaviors. There is some related work that the authors do not cite that is highly relevant here. A few in particular come to mind:

Yu, Tan, Liu, Turk. Preparing for the Unknown: uses a sequence model to estimate physical properties of a robot (rather than unobserved objects)

Fu, Levine, Abbeel. One-Shot Learning of Manipulation Skills: trains a similar proprioception-only model and uses it for object manipulation, similar idea that object properties can be induced from proprioception

But in general the citations to relevant robotic manipulation work are pretty sparse.

The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline.

Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!

One point about style: I found the somewhat lofty claims in the introduction a bit off-putting. It's great to discuss the greater ""vision"" behind the work, but this paper suffers from a bit too much high-level vision and not enough effort put into explaining what the method actually does.","[4, 7, 4]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Monotonic Chunkwise Attention,"['Chung-Cheng Chiu*', 'Colin Raffel*']",Accept,2018,"[15, 9]","[20, 14]","[116, 112]","[59, 49]","[41, 58]","[16, 5]","The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position. Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model. In terms of the model this is a relatively small extention of Raffel et al 2017.

Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. Is the offline attention baseline unidirectional or bidirectional? In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.

My concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model. Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping. My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer. While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.

For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper. 
Sentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?

I like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model.

---
 The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices). While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper. 
","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
"Emergent Communication in a Multi-Modal, Multi-Step Referential Game","['Katrina Evtimova', 'Andrew Drozdov', 'Douwe Kiela', 'Kyunghyun Cho']",Accept,2018,"[2, 2, 6, 9]","[6, 7, 11, 14]","[4, 17, 158, 396]","[1, 8, 82, 154]","[2, 8, 72, 215]","[1, 1, 4, 27]","The paper proposes a new multi-modal, multi-step reference game, where the sender has access to visual data and the receiver has access to textual messages, and also the conversation can be terminated by the receiver when proper. 

Later, the paper describes their idea and extension in details and reports comprehensive experiment results of a number of hypotheses. The research questions seems straightforward, but it is good to see those experiments review some interesting points.  One thing I am bit concerned is that the results are based on a single dataset. Do we have other datasets that can be used?

The authors also lay out further several research directions. Overall, I think this paper is easy to read and good. 

","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Attacking Binarized Neural Networks,"['Angus Galloway', 'Graham W. Taylor', 'Medhat Moussa']",Accept,2018,"[2, 15, 24]","[7, 20, 27]","[11, 174, 50]","[2, 78, 26]","[8, 77, 8]","[1, 19, 16]","1) Summary
This paper proposes a study on the robustness of one low-precision neural networks class - binarized neural networks (BNN) - against adversarial attacks. Specifically, the authors show that these low precision networks are not just efficient in terms of memory consumption and forward computation, but also more immune to adversarial attacks than their high-precision counterparts. In experiments, they show the advantage of BNNs by conducting experiments based on black-box and white-box adversarial attacks without the need to artificially mask gradients.


2) Pros:
+ Introduced, studied, and supported the novel idea that BNNs are robust to adversarial attacks.
+ Showed that BNNs are robust to the Fast Gradient Sign Method (FGSM) and Carlini-Wagner attacks in white-box adversarial attacks by presenting evidence that BNNs either outperform or perform similar to the high-precision baseline against the attacks.
+ Insightful analysis and discussion of the advantages of using BNNs against adversarial attacks.

3) Cons:
Missing full-precision model trained with PGD in section 3.2:
The authors mention that the full-precision model would also likely improve with PGD training, but do not have the numbers. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black-box attack setting.


Additional comments:
Can the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible.


4) Conclusion:
Overall, this paper proposes great insightful information about BNNs that shows the additional benefit of using them besides less memory consumption and efficient computation. This paper shows that the used architecture for BBNs makes them less susceptible to known white-box adversarial attack techniques.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
The Kanerva Machine: A Generative Distributed Memory,"['Yan Wu', 'Greg Wayne', 'Alex Graves', 'Timothy Lillicrap']",Accept,2018,"[1, 6, 15, 11]","[6, 10, 18, 16]","[19, 55, 77, 127]","[7, 17, 42, 47]","[11, 33, 27, 70]","[1, 5, 8, 10]","The paper presents the Kanerva Machine, extending an interesting older conceptual memory model to modern usage. The review of Kanerva’s sparse distributed memory in the appendix was appreciated. While the analyses and bounds of the original work were only proven when restricted to uniform and binary data, the extensions proposed bring it to modern domain of non-uniform and floating point data.

The iterative reading mechanism which provides denoising and reconstruction when within tolerable error bounds, whilst no longer analytically provable, is well shown experimentally.
The experiments and results on Omniglot and CIFAR provide an interesting insight to the model's behaviour with the comparisons to VAE and DNC also seem well constructed.

The discussions regarding efficiency and potential optimizations of writing inference model were also interesting and indeed the low rank approximation of U seems an interesting future direction.

Overall I found the paper well written and reintroduced + reframed a relatively underutilized but well theoretically founded model for modern use.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning how to explain neural networks: PatternNet and PatternAttribution,"['Pieter-Jan Kindermans', 'Kristof T. Schütt', 'Maximilian Alber', 'Klaus-Robert Müller', 'Dumitru Erhan', 'Been Kim', 'Sven Dähne']",Accept,2018,"[7, 7, 3, 26, 13, 9, 9]","[11, 11, 8, 31, 17, 14, 10]","[39, 22, 13, 478, 57, 74, 26]","[17, 6, 3, 182, 26, 29, 9]","[16, 14, 9, 118, 26, 41, 4]","[6, 2, 1, 178, 5, 4, 13]","I found this paper an interesting read for two reasons: First, interpretability is an increasingly important problem as machine learning models grow more and more complicated. Second, the paper aims at generalization of previous work on confounded linear model interpretation in neuroimaging (the so-called filter versus patterns problem). The problem is relevant for discriminative problems: If the objective is really to visualize the generative process,  the ""filters"" learned by the discriminative process need to be transformed to correct for spatial correlated noise. 

Given the focus on extracting visualization of the generative process, it would have been meaningful to place the discussion in a greater frame of generative model deep learning (VAEs, GANs etc etc). At present the ""state of the art"" discussion appears quite narrow, being confined to recent methods for visualization of discriminative deep models.

The authors convincingly demonstrate for the linear case, that their ""PatternNet"" mechanism can produce the generative process (i.e. discard spatially correlated ""distractors""). The PatternNet is generalized to multi-layer ReLu networks by construction of node-specific pattern vectors and back-propagating these through the network. The ""proof"" (eqs. 4-6) is sketchy and involves uncontrolled approximations. The back-propagation mechanism is very briefly introduced and depicted in figure 1.

Yet, the results are rather convincing. Both the anecdotal/qualitative examples and the more quantitative patch elimination experiment figure 4a (?number missing) 

I do not understand the remark: ""However, our method has the advantage that it is not only applicable to image models but is a generalization of the theory commonly used in neuroimaging Haufe et al. (2014).""  what ??

Overall, I appreciate the general idea. However, the contribution could have been much stronger based on a detailed derivation with testable assumptions/approximations, and if based on a clear declaration of the aim.

","[6, 8, 8]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
WRPN: Wide Reduced-Precision Networks,"['Asit Mishra', 'Eriko Nurvitadhi', 'Jeffrey J Cook', 'Debbie Marr']",Accept,2018,"[11, 16, 17, 4]","[14, 21, 17, 7]","[48, 78, 15, 25]","[37, 54, 12, 17]","[8, 7, 2, 6]","[3, 17, 1, 2]","This paper presents an simple and interesting idea to improve the performance for neural nets. The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced). The paper is aiming to solve a practical problem, and has done some solid research work to validate that.  In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach.   

In addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet. The results are convincing to me. 

On the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study. So the novelty is limited.

So overall given limited novelty but the paper presents useful results, I would recommend borderline leaning towards reject.","[5, 5, 9]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the Discrimination-Generalization Tradeoff in GANs,"['Pengchuan Zhang', 'Qiang Liu', 'Dengyong Zhou', 'Tao Xu', 'Xiaodong He']",Accept,2018,"[2, 9, 18, 17, 20]","[7, 14, 21, 22, 25]","[72, 183, 62, 390, 316]","[29, 88, 37, 178, 189]","[40, 86, 17, 42, 106]","[3, 9, 8, 170, 21]","== Paper Summary ==
The paper addresses the problem of balancing capacities of generator and discriminator classes in generative adversarial nets (GANs) from purely theoretical (function analytical and statistical learning) perspective. In my point of view, the main *novel* contributions are: 
(a) Conditions on function classes guaranteeing that the induced IPMs are metrics and not pseudo-metrics (Theorem 2.2). Especially I liked an argument explaining why ReLu activations could work better in discriminator that tanh.
(b) Proving that convergence in the neural distance implies a weak convergence (Theorem 2.5)
(c) Listing particular cases when the neural distance upper bounds the so-called bounded Lipschitz distance (also know as the Fortet-Mourier distance) and the symmetrized KL-divergence (Corollary 2.8 and Proposition 2.9).

The paper is well written (although with *many* typos), the topic is clearly motivated and certainly interesting. The related literature is mainly covered well, apart from several important points listed below.

== Major comments ==
In my opinion, the authors are slightly overselling the results. Next I shortly explain why:

(1) First, point (a) above is indeed novel, but not groundbreaking. A very similar result previously appeared in [1, Theorem 5]. The authors may argue that the referenced result deals only with MMDs, that is IPMs specified to the function classes belonging to the Reproducing Kernel Hilbert Spaces. However, the technique used to prove the ""sufficient"" part of the statement is literally *identical*. 

(2) As discussed in the paragraph right after Theorem 2.5, Theorem 10 of [2] presents the same result which is on one hand stronger than Theorem 2.5 of the current paper because it allows for more general divergences than the neural distance and on the other hand weaker because in [2] the authors assumes a compact input space. Overall, Theorem 2.5 of course makes a novel contribution, because the compactness assumption is not required, however conceptually it is not that novel.

(3) In Section 3 the authors discuss the generalization properties of the neural network distance. One of the main messages (emphasized several times throughout the paper) is that surprisingly the capacity of the generator class does not enter the generalization error bound. However, this is not surprising at all as it is a consequence of the way in which the authors define the generalization. In short, the capacity of discriminators (D) naturally enters the picture, because the generalization error accounts for the mismatch between the true data distribution mu (used for testing) and its empirical version hat{mu} (used for training). However, the authors assume the model distribution (nu) is the same both during testing and training. In practice this is not true and during testing GANs use the empirical version of nu. If the authors were to account for this mismatch, capacity of G would certainly pop up as well.

(4) The error bounds of Section 3 are based on a very standard machinery (empirical processes, Rademacher complexity) and to the best of my knowledge do not lead to any new interesting conclusions in terms of GANs.

(5) Finally, I would suggest the authors to remove Section 4. I suggest this mainly because the authors admit in Remark 4.1 that the main result of this section (Theorem 4.1) is a corollary of a stronger result appearing in [2]. Also, the main part of the paper has 13 pages, while a recommended amount is 8. 

== Minor comments ==

(1) There are *MANY* typos in the paper. Only few of them are listed below.
(2) First paragraph of page 18, proof of Theorem 2.2. This part is of course well known and the authors may just cite Lemma 9.3.2. of Dudley's ""Real analysis and probability"" for instance.
(3) Theorem 2.5: ""Let ...""
(4) Page 7, ""...we may BE interested...""
(5) Corollary 3.2. I doubt that in practice anyone uses discriminator with one hidden unit. The authors may want to consider using the bound on the Rademacher complexity of DNNs recently derived in [3]. 
(6) Page 8, ""..is neural networK""
(7) Page 9: ""...interested IN evaluating...""
(8) Page 10. All most ---> almost.

[1] Gretton et al., A Kernel Two-Sample Test, JMLR 2012.
[2] Liu et al, Approximation and Convergence Properties of Generative Adversarial Learning, 2017
[3] Bartlett et al, Spectrally-normalized margin bounds for neural networks, 2017","[6, 3, 7]","[' Marginally above acceptance threshold', ' Clear rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning,"['Audrunas Gruslys', 'Will Dabney', 'Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Marc Bellemare', 'Remi Munos']",Accept,2018,"[5, 2, 9, 7, 12, 23]","[9, 7, 14, 12, 17, 28]","[18, 67, 47, 60, 102, 266]","[7, 30, 19, 31, 45, 143]","[10, 36, 26, 27, 51, 96]","[1, 1, 2, 2, 6, 27]","This paper proposes a novel reinforcement learning algorithm (« The Reactor ») based on the combination of several improvements to DQN: a distributional version of Retrace, a policy gradient update rule called beta-LOO aiming at variance reduction, a variant of prioritized experience replay for sequences, and a parallel training architecture. Experiments on Atari games show a significant improvement over prioritized dueling networks in particular, and competitive performance compared to Rainbow, at a fraction of the training time.

There are definitely several interesting and meaningful contributions in this submission, and I like the motivations behind them. They are not groundbreaking (essentially extending existing techniques) but are still very relevant to current RL research.

Unfortunately I also see it as a step back in terms of comparison to other algorithms. The recent Rainbow paper finally established a long overdue clear benchmark on Atari. We have seen with the « Deep Reinforcement Learning that Matters » paper how important (and difficult) it is to properly compare algorithms on deep RL problems. I assume that this submission was mostly written before Rainbow came out, and that comparisons to Rainbow were hastily added just before the ICLR deadline: this would explain why they are quite limited, but in my opinion it remains a major issue, which is the main reason why I am advocating for rejection.

More precisely, focusing on the comparison to Rainbow which is the main competitor here, my concerns are the following:
- There is almost no discussion on the differences between Reactor and Rainbow (actually the paper lacks a « related work » section). In particular Rainbow also uses a version of distributional multi-step, which as far as I can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « off-policyness » of the replay data), but still seems to work well on Atari.
- Rainbow is not distributed. This was a deliberate choice by its authors to focus on algorithmic comparisons. However, it seems to me that it could benefit from a parallel training scheme like Reactor’s. I believe a comparison between Reactor and Rainbow needs to either have them both parallelized or none of them (especially for a comparison on time efficiency like in Fig. 2)
- Rainbow uses the traditional feedforward DQN architecture while Reactor uses a recurrent network. It is not clear to which extent this has an impact on the results.
- Rainbow was stopped at 200M steps, at which point it seems to be overall superior to Reactor at 200M steps. The results as presented here emphasize the superiority of Reactor at 500M steps, but a proper comparison would require Rainbow results at 500M steps as well.

In addition, although I found most of the paper to be clear enough, some parts were confusing to me, in particular:
- « multi-step distributional Bellman operator » in 3.2: not clear exactly what the target distribution is. If I understand correctly this is the same as the Rainbow extension, but this link is not mentioned.
- 3.4.1 (network architecture): a simple diagram in the appendix would make it much easier to understand (Table 3 is still hard to read because it is not clear which layers are connected together)
- 3.3 (prioritized sequence replay): again a visual illustration of the partitioning scheme would in my opinion help clarify the approach

A few minor points to conclude:
- In eq. 6, 7 and the rest of this section, A does not depend (directly) on theta so it should probably be removed to avoid confusion. Note also that using the letter A may not be best since A is used to denote an action in 3.1.
- In 3.1: « Let us assume that for the chosen action A we have access to an estimate R(A) of Qπ(A) » => « unbiased estimate »
- In last equation of p.5 it is not clear what q_i^n is
- There is a lambda missing on p.6 in the equation showing that alphas are non-negative on average, just before the min
- In the equation above eq. 12 there is a sum over « i=1 »
- That same equation ends with some h_z_i that are not defined
- In Fig. 2 (left) for Reactor we see one worker using large batches and another one using many threads. This is confusing.
- 3.3 mentions sequences of length 32 but 3.4 says length 33.
- 3.3 says tree operations are in O(n ln(n)) but it should be O(ln(n))
- At very end of 3.3 it is not clear what « total variation » is.
- In 3.4 please specify the frequency at which the learner thread downloads shared parameters and uploads updates
- Caption of Fig. 3 talks about « changing the number of workers » for the left plot while it is in the right plot
- The explanation on what the variants of Reactor (ND and 500M) mean comes after results are shown in Fig. 2.
- Section 4 starts with Fig. 3 without explaining what the task is, how performance is measured, etc. It also claims that Distributional Retrace helps while this is not the case in Fig. 3 (I realize it is explained afterwards, but it is confusing when reading the sentence « We can also see... »). Finally it says priorization is the most important component while the beta-LOO ablation seems to perform just the same.
- Footnote 3 should say it is 200M observations except for Reactor 500M
- End of 4.1: « The algorithms that we compare Reactor against are » => missing ACER, A3C and Rainbow
- There are two references for « Sample efficient actor-critic with experience replay »
- I do not see the added benefit of the Elo computation. It seems to convey essentially the same information as average rank.

And a few typos:
- Just above 2.1.3: « increasing » => increasingly
- In 3.1: « where V is a baseline that depend » => depends
- p.7: « hight » => high, and « to all other sequences » => of all other sequences
- Double parentheses in Bellemare citation at beginning of section 4
- Several typos in appendix (too many to list)

Note: I did not have time to carefully read Appendix 6.3 (contextual priority tree)

Edit after revision: bumped score from 5 to 7 because (1) authors did many improvements to the paper, and (2) their explanations shed light on some of my concerns","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Towards Synthesizing Complex Programs From Input-Output Examples,"['Xinyun Chen', 'Chang Liu', 'Dawn Song']",Accept,2018,"[9, 11, 20]","[14, 14, 25]","[93, 66, 440]","[43, 40, 254]","[43, 22, 154]","[7, 4, 32]","This paper proposes a method for learning parsers for context-free languages. They demonstrate that this achieves perfect accuracy on training and held-out examples of input/output pairs for two synthetic grammars. In comparison, existing approaches appear to achieve little to no generalization, especially when tested on longer examples than seen during training.

The approach is presented very thoroughly. Details about the grammars, the architecture, the learning algorithm, and the hyperparameters are clearly discussed, which is much appreciated. Despite the thoroughness of the task and model descriptions, the proposed method is not well motivated. The description of the relatively complex two-phase reinforcement learning algorithm is largely procedural, and it is not obvious how necessary the individual pieces of the algorithm are. This is particularly problematic because the only empirical result reported is that it achieves 100% accuracy. Quite a few natural questions left unanswered, limiting what readers can learn from this paper, e.g.
- How quickly does the model learn? Is there a smooth progression that leads to perfect generalization?
- Presumably the policy learned in Phase 1 is a decent model by itself, since it can reliably find candidate traces. How accurate is it? What are the drawbacks of using that instead of the model from the second phase? Are there systematic problems, such as overfitting, that necessitate a second phase?
- How robust is the method to hyperparameters and multiple initializations? Why choose F = 10 and K = 3? Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?

Other misc. points:
- The paper mentions that ""the training curriculum is very important to regularize the reinforcement learning process."" Unless I am misunderstanding the experimental setup, this is not supported by the result, correct? The proposed method achieves perfect accuracy in every condition.
- The reimplementations of the methods from Grefenstette et al. 2015 have surprisingly low training accuracy (in some cases 0% for Stack LSTM and 2.23% for DeQueue LSTM). Have you evaluated these reimplementations on their reported tasks to tease apart differences due to varying tasks and differences due to varying implementations?","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Do GANs learn the distribution? Some Theory and Empirics,"['Sanjeev Arora', 'Andrej Risteski', 'Yi Zhang']",Accept,2018,"[29, 7, 2]","[34, 12, 6]","[217, 83, 19]","[103, 35, 9]","[76, 45, 10]","[38, 3, 0]","This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples. The main goal is to quantify mode collapse in state-of-the-art generative models. The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.
Using the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1).
The results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.
Bidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs.
Finally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator.

This is a very interesting area and exciting work. The main idea behind the proposed test is very insightful. The main theoretical contribution stimulates and motivates much needed further research in the area. In my opinion both contributions suffer from some significant limitations. However, given how little we know about the behavior of modern generative models, it is a good step in the right direction.


1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity. The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2. My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity. Anecdotally, real-life distributions are far from uniform, so this should be a common issue. I would still use the test as a part of a suite of measurements, but I would not solely rely on it. I feel that the authors should give a more prominent disclaimer to potential users of the test.

2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing. The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution. As long as they yield collisions at the same rate, these two generative models are ‘equally diverse’. Isn’t coverage of equal importance?

3. The other main contribution of the paper is Theorem 3, which shows—via a very particular construction on the generator and encoder—that bidirectional GANs can also suffer from serious mode collapse. I welcome and are grateful for any theory in the area. This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice. In light of this, the statement in the introduction that “encoder-decoder training objectives cannot avoid mode collapse” might need to be qualified. In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning One-hidden-layer Neural Networks with Landscape Design,"['Rong Ge', 'Jason D. Lee', 'Tengyu Ma']",Accept,2018,"[11, 12, 8]","[16, 17, 13]","[155, 192, 181]","[63, 77, 76]","[78, 106, 97]","[14, 9, 8]","This paper studies the problem of learning one-hidden layer neural networks and is a theory paper. A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix. Simulations confirm the findings.

Overall, pretty interesting result and solid contribution. The paper also raises good questions for future works. For instance, is designing alternative loss function useful in practice? In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos.

Two questions:
1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows? If so they should clarify this otherwise it confuses the reader a bit.
2) What can be said about rate of convergence in terms of network parameters? Currently a generic bound is employed which is not very insightful in my opinion.

","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning From Noisy Singly-labeled Data,"['Ashish Khetan', 'Zachary C. Lipton', 'Animashree Anandkumar']",Accept,2018,"[3, 5, 13]","[7, 10, 18]","[29, 203, 419]","[10, 77, 154]","[15, 117, 223]","[4, 9, 42]","This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators). The authors provide both theoretical and experimental validation of their idea. 

Pros:
+ The paper is generally very clearly written. The motivation, notation, and method are clear.
+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. 
+ The approach is a novel twist on an existing method for learning from noisy data. 

Cons: 
- All experiments use simulated workers; this is probably common but still not very convincing.
- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. ""To re (label), or not to re (label)."" HCOMP 2014.
- The authors should have compared their approach to the ""base"" approach of Natarajan et al. 
- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.
- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture.

Questions: 
- How would the model need to change to account for example difficulty? 
- Why are Joulin 2016, Krause 2016 not relevant?
- Best to clarify what the weights in the weighted sum of Natarajan are. 
- ""large training error on wrongly labeled examples"" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels? Where does this ground truth come from?
- Not clear what ""Ensure"" means in the algorithm description.
- In Sec. 4.4, why is it important that the samples are fresh?
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Understanding Short-Horizon Bias in Stochastic Meta-Optimization,"['Yuhuai Wu', 'Mengye Ren', 'Renjie Liao', 'Roger Grosse.']",Accept,2018,"[3, 4, 9, 12]","[8, 8, 14, 17]","[69, 65, 99, 123]","[28, 29, 52, 60]","[38, 36, 43, 61]","[3, 0, 4, 2]","This paper studies the issue of truncated backpropagation for meta-optimization. Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like.

This paper highlights this problem as a fundamental issue limiting meta-optimization approaches. The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.  

(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2).

The paper is generally clear and well written.

Major comments
-------------------------
I was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature? The authors suggest this is needed because ""the dynamics of training are different at the very start compared to later stages"", which is a bit vague. Perhaps the authors can expand upon  this point?

The conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective--but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, ""The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule""). Which is the real issue, noise or curvature? Would running the problem on quadratics with different condition numbers be insightful?

Minor comments
-------------------------
The stochastic gradient equation in Sec 2.2.2 is missing a subscript: ""h_i"" instead of ""h""

It would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.

It looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct? I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)?

Figure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process.

Figure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case? I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly.

Figure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls. I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: ""SMD trajectories (red) during meta-optimization of initial effective ..."". I would also change the caption to use ""meta-training losses"" instead of ""training losses"" (I believe those numbers are for the meta-loss, correct?). Finally, I would add a colorbar to indicate numerical values for the different grayscale values.

Some recent references that warrant a mention in the text:
- both of these learn optimizers using longer numbers of unrolled steps:
Learning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017
Learned optimizers that scale and generalize, Wichrowska et al, ICML 2017
- another application of unrolled optimization:
Unrolled generative adversarial networks, Metz et al, ICLR 2017

In the text discussing Figure 4 (middle of pg. 8) , ""which is obtained by using..."" should be ""which are obtained by using...""

In the conclusion, ""optimal for deterministic objective"" should be ""deterministic objectives""","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Scalable Laplace Approximation for Neural Networks,"['Hippolyt Ritter', 'Aleksandar Botev', 'David Barber']",Accept,2018,"[2, 3, 24]","[6, 8, 29]","[14, 22, 131]","[7, 11, 64]","[6, 11, 41]","[1, 0, 26]","This paper proposes a Laplace approximation to approximate the posterior distribution over the parameters of deep networks. 

The idea is interesting and the realization of the paper is good. The idea builds upon previous work in scalable Gauss-Newton methods for optimization in deep networks, notably Botev et al., ICML 2017. In this respect, I think that the novelty in the current submission is limited, as the approximation is essentially what proposed in Botev et al., ICML 2017.  The Laplace approximation requires the Hessian of the posterior, so techniques developed for Gauss-Newton optimization can straightforwardly be applied to construct Laplace approximations.

Having said that, the experimental evaluation is quite interesting and in-depth. I think it would have been interesting to report comparisons with factorized variational inference (Graves, 2011) as it is a fairly standard and widely adopted in Bayesian deep learning. This would have been an interesting way to support the claims on the poor approximation offered by standard variational inference. 

I believe that the independence assumption across layers is a limiting factor of the proposed approximation strategy. Intuitively, changes in the weights in a given layer should affect the weights in other layers, so I would expect the posterior distribution over all the weights to reflect this through correlations across layers. I wonder how these results can be generalized to relax the independence assumption. 

","[6, 9, 6]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks,"['Behnam Neyshabur', 'Srinadh Bhojanapalli', 'Nathan Srebro']",Accept,2018,"[6, 6, 18]","[10, 11, 23]","[88, 61, 269]","[39, 24, 140]","[46, 35, 115]","[3, 2, 14]","The authors prove a generalization guarantee for deep
neural networks with ReLU activations, in terms of margins of the
classifications and norms of the weight matrices.  They compare this
bound with a similar recent bound proved by Bartlett, et al.  While,
strictly speaking, the bounds are incomparable in strength, the
authors of the submission make a convincing case that their new bound
makes stronger guarantees under some interesting conditions.

The analysis is elegant.  It uses some existing tools, but brings them
to bear in an important new context, with substantive new ideas needed.
The mathematical writing is excellent.

Very nice paper.

I guess that networks including convolutional layers are covered by
their analysis.  It feels to me that these tend to be sparse, but that
their analysis still my provides some additional leverage for such
layers.  Some explicit discussion of convolutional layers may be
helpful.  ","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,"['Abram L. Friesen', 'Pedro Domingos']",Accept,2018,"[9, 25]","[14, 28]","[24, 169]","[14, 129]","[10, 16]","[0, 24]","The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it. The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy. The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems. However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem. These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such. Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently. They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet. They show superiority of their algorithm over SSTE. 

I thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds. The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting. The results are moderately convincing in favor of the proposed approach. Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem. Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community. 

There are a few flaws/weaknesses in the paper though, making it somewhat lose. 
- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1. Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier. 
- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.  
- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition
- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach. The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. 
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,"['Rudy Bunel', 'Matthew Hausknecht', 'Jacob Devlin', 'Rishabh Singh', 'Pushmeet Kohli']",Accept,2018,"[3, 10, 8, 10, 16]","[8, 15, 13, 14, 21]","[43, 53, 49, 133, 322]","[16, 25, 25, 62, 180]","[24, 24, 22, 50, 102]","[3, 4, 2, 21, 40]","This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training.

I have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted.


Specific comments/questions follow:


Figure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width.

Page 7: ""In the supervised setting..."" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7).

Is the ""Karel DSL"" in your experiments the full Karel language, or a subset designed for the paper?

For the versions of the model that use beam search, what beam width was used? Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? 
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Unbiased Online Recurrent Optimization,"['Corentin Tallec', 'Yann Ollivier']",Accept,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the limitation of truncated backpropagation through time (BPTT) of providing biased gradient estimates.

The idea is to use a forward computation of the gradient as in Williams and Zipser (1989) with an unbiased approximation of Delta s_t/Delta theta to reduce the memory and computational cost.

The proposed approach, called UORO, is tested on a few artificial datasets.

The approach is interesting and could potentially be very useful. However, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.
Rather than with truncated BPTT with smaller truncation than required, which is easy to outperform, I would have expected a comparison with some of the other methods mentioned in the Related Work Section, such as NBT, ESNs, Decoupled Neural Interfaces, etc. Also the evaluation should be extended to other challenging tasks. 

I have increased the score to 6 based on the comments and revisions from the authors.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Wasserstein Embeddings,"['Nicolas Courty', 'Rémi Flamary', 'Mélanie Ducoffe']",Accept,2018,"[13, 10, 12, 20]","[17, 15, 17, 25]","[49, 104, 66, 257]","[28, 46, 26, 120]","[2, 11, 6, 34]","[19, 47, 34, 103]","The paper proposes to use a deep neural network to embed probability distributions in a vector space, where the Euclidean distance in that space matches the Wasserstein distance in the original space of probability distributions. A dataset of pairs of probability distributions and their Wasserstein distance is collected, and serves as a target to be predicted by the deep network.

The method is straightforward, and clearly explained. Two analyses based on Wasserstein distances (computing barycenters, and performing geodesic analysis) are then performed directly in the embedded space.

The authors claim that the proposed method produces sharper barycenters than those learned using the standard (smooth) Wasserstein distance. It is unclear from the paper whether the advantage comes from the ability of the method to scale better and use more examples, or to be able to use the non-smooth Wasserstein distance, or finally, whether the learning of a deep embedding yields improved extrapolation properties. A short discussion could be added. It would also be interesting to provide some guidance on what is a good structure for the encoder (e.g. should it include spatial pooling layers?)

The term “Wasserstein deep learning” is probably too broad, “deep Wasserstein embedding” could be more appropriate.

The last line of future work in the conclusion seems to describe the experiment of Table 1.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Quantitatively Evaluating GANs With Divergences Proposed for Training,"['Daniel Jiwoong Im', 'He Ma', 'Graham W. Taylor', 'Kristin Branson']",Accept,2018,"[5, 10, 15, 14]","[8, 15, 20, 18]","[26, 77, 174, 21]","[8, 40, 78, 8]","[17, 8, 77, 11]","[1, 29, 19, 2]","Through evaluation of current popular GAN variants. 
  * useful AIS figure
  * useful example of failure mode of inception scores
   * interesting to see that using a metric based on a model’s distance does not make the model better at that distance
the main criticism that can be given to the paper is that the proposed metrics are based on trained models which do not have an independent clear evaluation metrics (as classifiers do for inception scores). However, the authors do show that the results are consistent when changing the critic architecture. Would be nice to see if this also holds for changes in learning rates. 
 * nice to see an evaluation on how models scale with the increase in training data.

Using an Independent critic for evaluation has been proposed and used in practice before, see “Comparison of Maximum Likelihood and GAN-based training of Real NVPs”, Danihelka et all, as well as Variational Approaches for Auto-Encoding Generative Adversarial Networks, Rosca at all.

Improvements to be added to the paper:
   * How about overfitting? Would be nice to mention whether the proposed metrics are useful at detecting overfitting. From algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used. However, if the generator completely memorizes the training set, the critic will not be able to learn anything useful. In that case, the test measure will not provide any information either. A way to go around this is to use validation data to train the critic, not training data. In that case, the critic can learn the difference between training and validation data and at test time the test set can be used. 
  * Using the WGAN with weight clipping is not a good baseline. The improved WGAN method is more robust to hyper parameters and is the one currently  used by the community. The WGAN with weight clipping is quite sensitive to the clipping hyperparameter, but the authors do not report having changed it from the original paper, both for the critic or for the discriminator used during training. 
  *  Is there a guidance for  which metric should be used? 

Figure 3 needs to be made a bit larger, it is quite hard to read in the current set up. ","[7, 7, 4]","[' Good paper, accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,"['Kimin Lee', 'Honglak Lee', 'Kibok Lee', 'Jinwoo Shin']",Accept,2018,"[3, 14, 4, 10]","[8, 19, 8, 15]","[79, 273, 26, 276]","[37, 145, 13, 128]","[41, 123, 13, 122]","[1, 5, 0, 26]","This paper proposes a new method of detecting in vs. out of distribution samples. Most existing approaches for this deal with detecting out of distributions at *test time* by augmenting input data and or temperature scaling the softmax and applying a simple classification rule based on the output. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure. 

The authors propose to train a generator network in combination with the classifier and an adversarial discriminator. The generator is trained to produce images that (1) fools a standard GAN discriminator and (2) has high entropy (as enforced with the pull-away term from the EBGAN). Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. 

The model is evaluated on CIFAR-10 and SVNH, where several out of distribution datasets are used in each case. Performance gains are clear with respect to the baseline methods.

This paper is clearly written, proposes a simple model and seems to outperform current methods. One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. 

 I have some clarifying questions below:
- Figure 4 is unclear: does ""Confidence loss with original GAN"" refer to the method where the classifier is pretrained and then ""Joint confidence loss"" is with joint training? What does ""Confidence loss (KL on SVHN/CIFAR-10)"" refer to?

- Why does the join training improve the ability of the model to generalize to out-of-distribution datasets not seen during training?

- Why is the pull away term necessary and how does the model perform without it? Most GAN models are able to stably train without such explicit terms such as the pull away or batch discrimination. Is the proposed model unstable without the pull-away term? 

- How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional ""out of distribution"" class? This exact approach has been used to do semi supervised learning with GANS [1][2]. More generally, could the authors comment on how this approach is related to these semi-supervised approaches? 

- Did you try combining the classifier and discriminator into one model as in [1][2]?

[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583)
[2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Synthesizing realistic neural population activity patterns using Generative Adversarial Networks,"['Manuel Molano-Mazon', 'Arno Onken', 'Eugenio Piasini*', 'Stefano Panzeri*']",Accept,2018,"[1, 11, 5, 24]","[1, 16, 9, 29]","[2, 21, 11, 84]","[1, 5, 4, 21]","[1, 9, 4, 7]","[0, 7, 3, 56]","[Summary of paper] The paper presents a method for simulating spike trains from populations of neurons which match empirically measured multi-neuron recordings. They set up a Wasserstein-GAN and train it on both synthetic and real multi-neuron recordings, using data from the salamander retina. They find that their method (Spike-GAN) can produce spike trains that visually look like the original data, and which have low-order statistics (firing rates, correlations, time-lagged-correlations, total sum of population activity) which matches those of the original data. They emphasize that their network architecture is 'semi-convolutional', i.e. convolutional in time but not across neurons. Finally, they suggest a way to analyse the fitted networks in order to gain insights into what the 'relevant' neural features are, and illustrate it on synthetic data into which they embedded these features.

[Originality] This paper falls into the category of papers that do a next obvious thing (""GANs have not been applied to population spike trains yet""), and which do it pretty well: If one wants to create simulated neural activity data which matches experimentally observed one, then this method indeed seems to do that. As far as I know, this would be the first peer-reviewed application of GANs to multi-neuron recordings of neural data (but see https://arxiv.org/abs/1707.04582 for an arxiv paper, not cited here-- should be discussed at least).  On a technical level, there is very little to no innovation here -- while the authors emphasise their 'semi-convolutional' network architecture, this is obviously the right architecture to use for multivariate time-series data, and in itself not a big technical novel. Therefore, the paper should really be evaluate as an `application' paper, and be assessed in terms of i) how important the application is, ii) how clearly it is presented, and iii) how convincing the results are relative to state of the art. 

i) [Importance of problem, potential significance] Finding statistical models for modelling and simulating population spike trains is a topic which is extensively studied in computational neuroscience, predominantly using  model-based approaches using MaxEnt models, GLMs or latent variable models. These models are typically simple and restricted, and certainly fall short of capturing the full complexity of neural data. Thus, better, and more flexible solutions for this problem would certainly be very welcome, and have an immediate impact in this community.  However, I think that the approach based on GANs actually has two shortcomings which are not stated by the authors, and which possibly limit the impact of the method: First, statistical models of neural spike trains are often used to compute probabilities e.g. for decoding analyses— this is difficult or impossible for GANs. Second, one most often does not want to simulate data which match a specific recording, but rather which have specified statistics (e.g. firing rates and correlations)— the method here is based on fitting a particular data-set, and it is actually unclear to me when that will be useful.

ii) [Clarity] The methods are presented and explained clearly and cleanly. In my view, too much emphasis is given to highlighting the ‘semi-convolutional’ network, and, conversely, practical issues (exact architectures, cost of training) should be explained more clearly, possibly in an appendix. Similarly, the method would benefit from the authors releasing their code.

iii) [Quality, advance over previous methods] The authors discuss several methods for simulating spike trains in the introduction. In their empirical comparisons, however, they completely focus on a particular model-class (maximum entropy models, ME) which they label being the ‘state-of-the-art’. This label is misleading— ME models are but one of several approaches to modelling neural spike trains, with different models having different advantages and limitations (there is no benchmark which can be used to rank them...). In particular, the only ‘gain’ of the GAN over ME  models in the results comes from their ability of the GAN to match temporal statistics. Given that the ME models used by the authors are blind to temporal correlations, this is, of course (and as pointed out by the authors) hardly surprising. How does the GAN approach fair against alternative models which do take temporal statistics into account, e.g. GLMs, or simple moment-based method e.g. Krumin et al 2009, Lyamzin 2010, Gutnisky et al 2010— setting these up would be simple, and it would provide a non-trivial baseline for the ability of spike-GAN to outperform at least these models? While it true that GANs are much more expressive than the model-based approaches used in neuroscience, a clear demonstration would have been useful.

Minor comments: 
  - p.3: The abbreviation “1D-DCGAN” is never spelled out.
  - p.3: The architecture of Spike-GAN is never explicitely given.
  - p.3: (Sec. 2.2) Statistic 2) “average time course across activity patterns” is unclear to me -- how does one select the activity patterns over which to average? Moreover, later figures do not seem to use this statistic.
  - p.4: “introduced correlations between randomly selected pairs” -- How many such pairs were formed?
  - p.7 (just above Discussion) At the beginning of this section, and for Figs. 4A,B, the texts suggests that packets fire spontaneously with a given probability. For Figs. 4C-E, a particular packet responds to a particular input. Is then the neuron population used in these figures different from the one in Figs. 4A,B? How did the authors ensure that a particular set of neurons respond to their stimulus as a packet? What stimulus did they use?
  - p.8 (Fig. 4E) Are the eight neurons with higher importance those corresponding to the packet? This is insinuated but not stated.
  - p.12 (Appendix A) 
    + The authors do not mention how they produced their “ground truth” data. (What was its firing rate? Did it include correlations? A refractory period?)
    + Generating samples from the trained Spike-GAN is ostensibly cheap. Hence it is unclear why the authors did not  produce a large enough number of samples in order to obtain a 'numerical probability', just as they did for the ground truth data? 
    + Fig. S1B: The figure shows that every sample has the same empirical frequency. This indicates more a lack of statistical power rather than any correspondence between the theoretical and empirical probabilities. This undermines the argument in the second paragraph of p.12. In the other hand, if the authors did approximate numerical probabilities for the Spike-GAN, this argument would no longer be required.
  - p.13 Fig. S1A,B: the abscissas mention “frequency”, while the ordinates mention “probability”
  - p.25 Fig. S4: This figure suggests that the first layer of the Spike-GAN critic sometimes recognizes the packet patterns in the data. However, to know whether this is true, we would need to compare this to a representation of the neurons reordered in the same way and identified by packet. I.e. one expects something something like figure like Fig. 4A, with the packets lining up with the recovered filters when neurons are ordered the same way.
","[8, 4, 6]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Kernel Implicit Variational Inference,"['Jiaxin Shi', 'Shengyang Sun', 'Jun Zhu']",Accept,2018,"[5, 4, 14]","[10, 9, 19]","[87, 36, 468]","[40, 14, 204]","[35, 13, 211]","[12, 9, 53]","Thank you for the feedback, and I think many of my concerns have been addressed.

I think the paper should be accepted.

==== original review ====

Thank you for an interesting read. 

Approximate inference with implicit distribution has been a recent focus of the research since late 2016. I have seen several papers simultaneously proposing the density ratio estimation idea using GAN approach. This paper, although still doing density ratio estimation, uses kernel estimators instead and thus avoids the usage of discriminators. 

Furthermore, the paper proposed a new type of implicit posterior approximation which uses intuitions from matrix factorisation. I do think that another big challenge that we need to address is the construction of good implicit approximations, which is not well studied in previous literature (although this is a very new topic). This paper provides a good start in this direction.

However several points need to be clarified and improved:
1. There are other ways to do implicit posterior inference such as amortising deterministic/stochastic dynamics, and approximating the gradient updates of VI. Please check the literature.
2. For kernel based density ratio estimation methods, you probably need to cite a bunch of Sugiyama papers besides (Kanamori et al. 2009). 
3. Why do you need to introduce both regression under p and q (the reverse ratio trick)? I didn't see if you have comparisons between the two. From my perspective the reverse ratio trick version is naturally more suitable to VI.
4. Do you have any speed and numerical issues on differentiating through alpha (which requires differentiating K^{-1})?
5. For kernel methods, kernel parameters and lambda are key to performances. How did you tune them?
6. For the celebA part, can you compute some quantitative metric, e.g inception score?
","[7, 5, 7]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback,"['Hal Daumé III', 'John Langford', 'Amr Sharaf']",Accept,2018,"[18, 21, 4]","[23, 26, 9]","[254, 222, 21]","[152, 112, 10]","[79, 83, 11]","[23, 27, 0]","The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles.
The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive ""reward shaping"" (the residual losses) from a feedback that is only provided at the end of the epochs.

The paper is dense but well written. 

The theoretical grounding is a bit thin or hard to follow.
The authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to ""value-aware"" contextual bandits.

The experimental section is solid. The method is evaluated on several RL environments against state of the art RL algorithms. It is also evaluated on bandit structured prediction tasks.
An interesting synthetic experiment (Figure 4) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks.


Question 1: The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning. But sometimes old and simple methods are not that bad. Could you develop a bit on the relation between RESLOPE and eligibility traces ?

Question 2: RESLOPE is built upon contextual bandits which require a stationary environment. Does RESLOPE inherit from this assumption?


Typos:
page 1 
""scalar loss that output."" -> ""scalar loss.""
"", effectively a representation"" -> "". By effective we mean effective in term of credit assignment.""
page 5
""and MTR"" -> ""and DR""
page 6
""in simultaneously."" -> ???
"".In greedy"" -> "". In greedy""
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 4, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Distributed Distributional Deterministic Policy Gradients,"['Gabriel Barth-Maron', 'Matthew W. Hoffman', 'David Budden', 'Will Dabney', 'Dan Horgan', 'Dhruva TB', 'Alistair Muldal', 'Nicolas Heess', 'Timothy Lillicrap']",Accept,2018,"[5, 14, 1, 2, 4, 2, 1, 10, 11]","[9, 19, 5, 7, 5, 5, 5, 15, 16]","[17, 46, 35, 67, 11, 6, 16, 200, 127]","[5, 21, 14, 30, 5, 1, 4, 81, 47]","[10, 22, 20, 36, 5, 5, 11, 111, 70]","[2, 3, 1, 1, 1, 0, 1, 8, 10]","The paper investigates a number of additions to DDPG algorithm and their effect on performance. The additions investigated are distributional Bellman updates, N-step returns, and prioritized experience replay.

The paper does a good job of analyzing these effects on a wide range of continuous control tasks, from the standard benchmark suite, to hand manipulation, to complex terrain locomotion and I believe these results are valuable to the community.

However, I have a concern about the soundness of using N-step returns in DDPG setting. When a sequence of length N is sampled from the replay buffer and used to calculate N-step return, this sequence is generated according a particular policy. As a result, experience is non-stationary - for the same state-action pair, early iterations of the algorithm will produce structurally different (not just due to stochasticity) N-step returns because the policy to generate those N steps has changed between algorithm iterations. So it seems to me the authors are using off-policy updates where strictly on-policy updates should be used. I would like some clarification from the authors on this point, and if it is indeed the case to bring attention to this point in the final manuscript.

It would also be useful to evaluate the effect of N for values other than 1 and 5, especially given the significance this addition has on performance. I can believe N-step returns are useful, possibly due to effectively enlarging simulation timestep, but it would be good to know at which point it becomes detrimental.

I also believe ""Distributional Policy Gradients"" is an overly broad title for this submission as this work still relies on off-policy updates and does not tackle the problem of marrying distributional updates with on-policy methods. ""Distributional DDPG"" or ""Distributional Actor-Critic"" or variant perhaps could be more fair title choices?

Aside from these concerns, lack of originality of contributions makes it difficult to highly recommend the paper. Nonetheless, I do believe the experimental evaluation if well-conducted and would be of interest to the ICLR community. ","[6, 9, 5]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,['Sebastian Nowozin'],Accept,2018,[12],[17],[121],[69],[42],[10],"This paper provides an interesting analysis of the importance sampled estimate of the LL bound and proposes to use Jackknife to correct for the bias. The experiments show that the proposed method works for model evaluation and that computing the correction is archivable at a reasonable computational cost. It also contains an insightful analysis.

","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Towards Reverse-Engineering Black-Box Neural Networks,"['Seong Joon Oh', 'Max Augustin', 'Mario Fritz', 'Bernt Schiele']",Accept,2018,"[4, 2, 17, 25]","[9, 6, 22, 30]","[69, 14, 256, 531]","[28, 6, 128, 303]","[38, 8, 114, 166]","[3, 0, 14, 62]","The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach). It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy. As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance. It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately. That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization.

The selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries? One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy.

In table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io.

In the ImageNet classifier family prediction, how different are the various families from each other? Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader.

Overall the results seem interesting, but without more insights it's difficult to judge how generally useful they are.","[7, 5, 7]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning,"['Rajarshi Das', 'Shehzaad Dhuliawala', 'Manzil Zaheer', 'Luke Vilnis', 'Ishan Durugkar', 'Akshay Krishnamurthy', 'Alex Smola', 'Andrew McCallum']",Accept,2018,"[30, 4, 8, 6, 6, 9, 23, 29]","[35, 9, 13, 10, 10, 14, 28, 34]","[94, 25, 143, 32, 23, 148, 341, 392]","[65, 13, 69, 16, 12, 69, 216, 258]","[23, 11, 73, 16, 11, 73, 83, 113]","[6, 1, 1, 0, 0, 6, 42, 21]","The paper proposes an approach for query answering/link prediction in KBs that uses RL to navigate the KB graph between a query entity and a potential answer entity. The main originality is that, unlike random walk models, the proposed approach learns to navigate the graph while being conditioned on the query relation type.

I find the method sound and efficient and the proposed experiments are solid and convincing; for what they test for.

Indeed, for each relation type that one wants to be testing on, this type of approach needs many training examples of pairs of entities (say e_1, e_2) connected both by this relation type (e_1 R e_2) and by alternative paths (e_1 R' R'' R''' e_2). Because the model needs to discover and learn that R <=> R ' R'' R''' .

The proposed model seems to be able to do that well when the number of relation types remains low (< 50). But things get interesting in KBs when the number of relation types gets pretty large (hundreds / thousands). Learning the kind of patterns described above gets much trickier then. The results on FB15k are a bit worrying in that respect. Maybe this is a matter of the dataset FB15k itself but then having experiments on another dataset with hundreds of relation types could be important. 

NELL has indeed 200 relations but if I'm not mistaken, the NELL dataset is used for fact prediction and not query answering. And as noted in the paper, fact prediction is much easier.

","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Boosting the Actor with Dual Critic,"['Bo Dai', 'Albert Shaw', 'Niao He', 'Lihong Li', 'Le Song']",Accept,2018,"[16, 2, 7, 16, 15]","[21, 6, 12, 20, 20]","[315, 12, 83, 170, 343]","[147, 6, 36, 89, 185]","[135, 6, 42, 66, 122]","[33, 0, 5, 15, 36]","This paper studies a new architecture DualAC. The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible).

There are several drawbacks of the current format of the paper:
1. The algorithm is vague. Alg 1 line 5: 'closed form': there is no closed form in Eq(14). It is just an MC approximation.
line 6: Decay O(1/t^\beta). This is indeed vague albeit easy to understand. The algorithm requires that every step is crystal clear.

2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space). There are many format errors like this throughout the paper. The author is suggested to do a careful format check.

3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief.

4. The experimental part is ok to me, but not very impressive.

Overall, this seems to be a nice paper to me.","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,"['Zhilin Yang', 'Saizheng Zhang', 'Jack Urbanek', 'Will Feng', 'Alexander Miller', 'Arthur Szlam', 'Douwe Kiela', 'Jason Weston']",Accept,2018,"[16, 6, 2, 2, 3, 10, 6, 20]","[21, 6, 7, 6, 7, 15, 11, 25]","[82, 27, 24, 6, 24, 138, 158, 237]","[34, 13, 9, 4, 11, 54, 82, 126]","[35, 13, 15, 2, 13, 74, 72, 84]","[13, 1, 0, 0, 0, 10, 4, 27]","The paper provides an interesting data collection scheme that improves upon standard collection of static databases that have multiple shortcomings -- End of Section 3 clearly summarizes the advantages of the proposed algorithm. The paper is easy to follow and the evaluation is meaningful.

In MTD, both data collection and training the model are intertwined and so, the quality of the data can be limited by the learning capacity of the model. It is possible that after some iterations, the data distribution is similar to previous rounds in which case, the dataset becomes similar to static data collection (albeit at a much higher cost and effort). Is this observed ? Further, is it possible to construct MTD variants that lead to constantly improving datasets by being agnostic to the actual model choice ? For example, utilizing only the priors of the D_{train_all}, mixing model and other humans' predictions, etc.



","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Policy Optimization by Genetic Distillation ,"['Tanmay Gangwani', 'Jian Peng']",Accept,2018,"[3, 10]","[8, 15]","[23, 166]","[11, 84]","[12, 62]","[0, 20]","The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a ""strong"" network). The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the ""crossover"" network is presumably time-consuming.

It seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment. There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider ""evolutionary"" computing. Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning. I would perhaps even call this a distillation network rather than a crossover network. In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as ""expert"" trajectories can be generated in unlimited quantities.","[6, 8, 3]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Neural Networks as Gaussian Processes,"['Jaehoon Lee', 'Yasaman Bahri', 'Roman Novak', 'Samuel S. Schoenholz', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein']",Accept,2018,"[2, 7, 5, 12, 26, 6]","[7, 12, 10, 16, 30, 10]","[21, 62, 69, 93, 310, 34]","[8, 27, 26, 55, 184, 11]","[12, 27, 39, 30, 66, 17]","[1, 8, 4, 8, 60, 6]","This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.


Pros:

The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)

The paper is clear and very well written.

The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work!


Cons:

Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article.

I suggest using the same axis limits for all subplots in Figure 3.","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
META LEARNING SHARED HIERARCHIES,"['Kevin Frans', 'Jonathan Ho', 'Xi Chen', 'Pieter Abbeel', 'John Schulman']",Accept,2018,"[2, 9, 4, 17, 8]","[6, 14, 8, 22, 13]","[10, 50, 47, 608, 66]","[3, 20, 21, 291, 27]","[7, 26, 24, 291, 37]","[0, 4, 2, 26, 2]","Please see my detailed comments in the ""official comment""

The extensive revisions addressed most of my concerns

Quality
======
The idea is interesting, the theory is hand-wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED).

Clarity
=====
The paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader's best guess (ADDRESSED).

Originality
=========
The idea looks original.

Significance
==========
If it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution.

Pros and Cons
============
+ interesting idea
- we do everything asynchronously and in parallel and it magically works (ADDRESSED)
- many open questions / missing details (ADDRESSED)","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,"['Yang Song', 'Taesup Kim', 'Sebastian Nowozin', 'Stefano Ermon', 'Nate Kushman']",Accept,2018,"[4, 10, 12, 10, 12]","[8, 15, 17, 15, 16]","[64, 34, 121, 406, 33]","[31, 16, 69, 199, 19]","[33, 17, 42, 200, 13]","[0, 1, 10, 7, 1]","The authors propose to use a generative model of images to detect and defend against adverarial examples. White-box attacks against standard models for image recognition (Resnet and VGG) are considered, and a generative model (a PixelCNN) is trained on the same data as the classifiers. The authors first show that adversarial examples created by the white-box attacks correspond to low likelihood region (according to the pixelCNN), which first gives a classification rule for detecting adversarial examples.

Then, to turn the genrative model into a defensive algorithm, the authors propose to preprocess test images by approximately maximizing the likelihood under similar constraints as the attacker of images, to ""project"" adversarial examples back to high-density regions (as estimated by the generative model). As a heuristic method, the authors propose to greedily maximize the likelihood of the incoming images pixel-by-pixel, which is possible because of the specific form of the PixelCNN likelihood in the context of l-infty attacks. An ""adaptive"" version of the algorithm, in which the preprocessing is used only when the likelihood of an example is below a certain threshold, is also proposed.

Experiments are carried out on Fashion MNIST and CIFAR-10. At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples. The main result is that this approach based on generative models seems to work even on against the strongest attacks.

Overall, the idea proposed in the paper, using a generative model to detect and filter out spurious patterns that can appear in adversarial examples, is rather intuitive. The experimental result that adversarial examples can somehow be corrected by a generative model is also interesting. The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting.

Whereas the paper is an interesting step forward, the paper still doesn't provide definitive arguments in favor of using such approaches in practice. There is a significant loss in accuracy on clean examples (2% on CIFAR-10 for a resnet), and more generally against weaker opponents such as the fast gradient sign. Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models. This somewhat weakens the result, since robustness against these methods that do not transfer well is achieved by changing the model. 
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Ensemble Adversarial Training: Attacks and Defenses,"['Florian Tramèr', 'Alexey Kurakin', 'Nicolas Papernot', 'Ian Goodfellow', 'Dan Boneh', 'Patrick McDaniel']",Accept,2018,"[4, 8, 5, 10, 26, 20]","[9, 13, 10, 12, 31, 25]","[87, 37, 150, 107, 363, 252]","[33, 13, 56, 48, 207, 154]","[49, 22, 88, 55, 121, 46]","[5, 2, 6, 4, 35, 52]","This paper describes computationally efficient methods for training adversarially robust deep neural networks for image classification. (These methods may extend to other machine learning models and domains as well, but that's beyond the scope of this paper.) 

The former standard method for generating adversarially images quickly and using them in training was to do a single gradient step to increase the loss of the true label or decrease the loss of an alternate label. This paper shows that such training methods only lead to robustness against these ""weak"" adversarial examples, leaving the adversarially-trained models vulnerable to multi-step white-box attacks and black-box attacks (adversarial examples generated to attack alternate models).

There are two proposed solutions. The first is to generate additional adversarial examples from other models and use them in training. This seems to yield robustness against black-box attacks from held-out models as well.  Of course, it requires that you have a somewhat diverse group of models to choose from. If that's the case, why not directly build an ensemble of all the models? An ensemble of neural networks can still be represented as a neural network, although a more computationally costly one. Thus, while this heuristic appears to be useful with current models against current attacks, I don't know how well it will hold up in the future.

The second solution is to add random noise before taking the gradient step.  This yields more effective adversarial examples, both for attacking models and for training, because it relies less on the local gradient. This is another simple idea that appears to be effective. However, I would be interested to see a comparison to a 2-step gradient-based attack.  R+Step-LL can be viewed as a 2-step attack: a random step followed by a gradient step. What if both steps were gradient steps instead? This interpolates between Step-LL and I-Step-LL, with an intermediate computational cost. It would be very interesting to know if R+Step-LL is more or less effective than 2+Step-LL, and how large the difference is.

I like that this paper demonstrates the weakness of previous methods, including extensive experiments and a very nice visualization of the loss landscape in two adversarial dimensions. The proposed heuristics seem effective in practice, but they're somewhat ad hoc and there is no analysis of how these heuristics might or might not be vulnerable to future attacks.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DCN+: Mixed Objective And Deep Residual Coattention for Question Answering,"['Caiming Xiong', 'Victor Zhong', 'Richard Socher']",Accept,2018,"[10, 4, 12]","[15, 8, 17]","[383, 37, 229]","[165, 18, 111]","[208, 19, 111]","[10, 0, 7]","This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. 

One question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
All-but-the-Top: Simple and Effective Postprocessing for Word Representations,"['Jiaqi Mu', 'Pramod Viswanath']",Accept,2018,"[4, 23]","[4, 28]","[10, 262]","[5, 104]","[4, 99]","[1, 59]","This paper proposes a simple post-processing technique for word representations designed to improve representational quality and performance on downstream tasks. The procedure involves mean subtraction followed by projecting out the first D principle directions and is motivated by improving isotropy of the partition function. Extensive empirical analysis supports the efficacy of the approach.

The idea of post-processing word embeddings to improve their performance is not new, but I believe the specific procedure and its connection to the concept of isotropy has not been investigated previously. Relative to other post-processing techniques, this method has a fair amount of theoretical justification, particularly as described in Appendix A. I think the experiments are reasonably comprehensive. All told, I think this is a good paper, but I do have some comments and questions that I think should be addressed before publication.

1) I think it is useful to analyze the distribution of singular values of the matrix of word vectors. However, I did not find the heuristic analysis based on the visual appearance of these distributions to be convincing. For example, in Fig. 1, it is not clear to me that there exists a separation between regimes of exponential decay and rough constancy. It would be ideal if a more quantitative metric is established that captures the main qualitative behavior alluded to here.

Furthermore, the vocabulary size is likely to have a strong effect on the shape of the distributions. Are the plots in Fig. 4 for the same vocabulary size? Related to this, the dimensionality of the representation will have a strong effect on the shape, and this should be controlled for in Fig. 8. One way to do this would be to instead plot the density of singular values. Finally, for the Gaussian matrix simulations, in the asymptotic limit, the density of singular values depends only on the ratio of dimensions, i.e. the vector dimension to the vocabulary size. Fig. 4/8 might be more revealing if this ratio were controlled for.

2) It would be useful to describe why isotropy of the partition function is the goal, as opposed to isotropy of the vectors themselves. This may be argued in Arora et al. (2016), but summarizing that argument in this paper would be helpful. In fact, an additional experiment that would be very valuable would be to investigate empirically which form of isotropy is more effective in governing performance. One way to do this would be to enforce approximate isotropy of the partition function without also enforcing isotropy of the vectors themselves. Practically speaking, one might imagine doing this by requiring I = 1 to second order without also requiring that the mean vanish. I think this would allow for \sigma_max > \sigma_min while still satisfying I = 1 to second order. (But this is just off the top of my head -- there may be better ways to conduct this experiment).

It is not clear to me why the experiment leading to Table 2 is a good proxy for the exact computation of I. It would be great if there were some mathematical justification for this approximation.

Why does Fig. 3 use D=10, 20 when much smaller D are considered elsewhere? Also I think a log scale on the x-axis might be more informative.

3) It would be good to mention other forms of post-processing, especially in the context of word similarity. For example, in the original paper, GloVe advocates averaging the target and context vector representations, and normalizing across the feature dimension before computing cosine similarity.

4) I think it's likely that there is a strong connection between the optimal value of D and the frequency distribution of words in the evaluation dataset. While the paper does mention that D may depend on specifics of the dataset, etc., I would expect frequency-dependence to be the main factor, and it might be worth exploring this effect explicitly.
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Rewiring: Training very sparse deep networks,"['Guillaume Bellec', 'David Kappel', 'Wolfgang Maass', 'Robert Legenstein']",Accept,2018,"[5, 5, 42, 19]","[9, 10, 46, 23]","[18, 22, 214, 61]","[8, 4, 76, 23]","[9, 12, 46, 20]","[1, 6, 92, 18]","This paper presents an iterative approach to sparsify a network already during training. During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold. This is a big advantage when training is performed on hardware with computational limitations, in comparison to ""post-hoc"" sparsification methods, that compress the network after training.
The method is derived by considering the ""rewiring"" of an (artificial) neural network as a stochastic process. This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. References to previous work in this area are missing, e.g.

[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network
Models, Neural Computation 2000
[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011

Especially the stochastic gradient method in [2] is strongly related to the existing approach.

Positive aspects

- The presented approach is well grounded in the theory of stochastic processes. The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process

- By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.

- The method is specifically designed for online learning with limited hardware ressources.

Negative aspects

- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C). Especially the results on MNIST suggest that this method is most advantageous for very high compression levels. However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.

- A detailled discussion of the relation to previously existing very similar work is missing (see above)


Technical Remarks

Fig. 1, 2 and 3 are referenced on the pages following the page containing the figure. Readibility could be slightly increased by putting the figures on the respective pages.
","[6, 8, 5]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Communication Algorithms via Deep Learning,"['Hyeji Kim', 'Yihan Jiang', 'Ranvir B. Rana', 'Sreeram Kannan', 'Sewoong Oh', 'Pramod Viswanath']",Accept,2018,"[9, 1, 1, 9, 11, 23]","[14, 6, 5, 14, 16, 28]","[73, 25, 10, 132, 196, 262]","[35, 12, 5, 53, 80, 104]","[27, 10, 4, 62, 89, 99]","[11, 3, 1, 17, 27, 59]","Error-correcting codes constitute a well-researched area of study within communication engineering. In communication, messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy. The codewords are then transmitted over a channel that has some random noise. At the receiving end the noisy codewords are then decoded to recover the messages. Many well known families of codes exist, notably convolutional codes and Turbo codes, two code families that are central to this paper, that achieve the near optimal possible performance with efficient algorithms. For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder. For drawing baselines, it is assumed that the random noise in channel is additive Gaussian (AWGN).

This paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. These decoders are robust to changes in noise model and blocklength - and shows near optimal performance.

It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR, both of which are optimal, iterative and runs in linear time. Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that, why their good performance surprising?

The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. The authors propose an architecture to design codes for this channel. This is a nice step. However, in the performance plot (figure 8), the RNN based code-decoder does not seem to be outperforming the existing codes except for two points. For both in high and low SNR the performance is suboptimal to  Turbo codes and a code by Schalkwijk & Kailath. The section is also super-concise to follow. I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder. This is an issue with the paper - the authors previously mentioned (8,16) polar code without mentioning what the numbers mean. 

However, I overall liked the idea of using neural nets to design codes for some non-standard channels. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing-end the Gaussian feedback channel part can be a new direction. This paper lacks theoretical aspect, as to no indication is given why RNN based design/decoders can be good. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim.
","[6, 2, 9]","[' Marginally above acceptance threshold', ' Strong rejection', ' Top 15% of accepted papers, strong accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Demystifying MMD GANs,"['Mikołaj Bińkowski', 'Danica J. Sutherland', 'Michael Arbel', 'Arthur Gretton']",Accept,2018,"[2, 7, 2, 17]","[6, 12, 7, 22]","[18, 59, 37, 201]","[9, 24, 18, 102]","[9, 33, 19, 74]","[0, 2, 0, 25]","The quality and clarity of this work are very good. The introduction of the kernel inception metric is well-motivated and novel, to my knowledge. With the mention of a bit more related work (although this is already quite good), I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo.

Pros
 - best description of MMD GANs that I have encountered
 - good contextualization of related work and descriptions of relationships, at least among the works surveyed
 - reasonable proposed metric (KID) and comparison with other scores
 - proof of unbiased gradient estimates is a solid contribution

Cons
 - although the review of related work is very good, it does focus on ~3 recent papers. As a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in
 - connection between IPMs and MMD gets a bit lost; a figure (e.g.  flow chart) would help
 - wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline
 - Figure 1 is a bit confusing; why is KID tested without replacement, and FID with? Why 100 vs 10 samples? The comparison is good to have, but it's hard to draw any insight with these differences in the subfigures. The figure caption should also explain what we are supposed to get out of looking at this figure.

Specific comments:
 - I suggest bolding terms where they are defined; this makes it easy for people to scan/find (e.g. Jensen-Shannon divergence, Integral Probability Metrics, witness functions, Wasserstein distance, etc.) 
 - Although they are common knowledge in the field, because this is a review it could be helpful to provide references or brief explanations of e.g. JSD, KL, Wasserstein distance, RKHS, etc.
 - a flow chart (of GANs, IPMs, MMD, etc., mentioning a few more models than are discussed in depth here, would be *very* helpful.
 - page 2, middle paragraph, you mention ""...constraints to ensure the kernel distribution embeddings remained injective""; it would be helpful to add a sentence here to explain why that's a good thing.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","['Pratik Chaudhari', 'Stefano Soatto']",Accept,2018,"[10, 26]","[15, 31]","[77, 445]","[29, 241]","[42, 138]","[6, 66]","This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models. It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function. However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function. The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.

I think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.

First, Assumption 4 seems a bit too abstract. It is not easy to see what the assumption means. It would be better if an example is given, which is verified to satisfy the assumption.

Another comment is related to the overall content of this paper. Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function.","[6, 8, 5]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Distributed Fine-tuning of Language Models on Private Data,"['Vadim Popov', 'Mikhail Kudinov', 'Irina Piontkovskaya', 'Petr Vytovtov', 'Alex Nevidomsky']",Accept,2018,"[2, 4, 2, 2, 2]","[6, 8, 7, 3, 3]","[11, 22, 19, 4, 3]","[7, 10, 9, 3, 2]","[4, 11, 10, 1, 1]","[0, 1, 0, 0, 0]","This paper discusses the application of word prediction for software keyboards. The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements. 

The authors propose a simple method of mixing the global model with user specific data. Collecting the user specific models and averaging them to form the next global model. 

The proposal is practical. However, I am not convinced that this is novel enough for publication at ICLR. 

One major question. The authors assume that the global model will depict general english. However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English. It is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forgetting.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Noisy Networks For Exploration,"['Meire Fortunato', 'Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Jacob Menick', 'Matteo Hessel', 'Ian Osband', 'Alex Graves', 'Volodymyr Mnih', 'Remi Munos', 'Demis Hassabis', 'Olivier Pietquin', 'Charles Blundell', 'Shane Legg']",Accept,2018,"[4, 9, 7, 2, 5, 6, 15, 13, 23, 10, 17, 9, 19]","[8, 14, 12, 6, 9, 11, 18, 17, 28, 15, 22, 14, 24]","[14, 47, 60, 21, 51, 53, 77, 54, 266, 54, 231, 88, 78]","[5, 19, 31, 9, 25, 18, 42, 27, 143, 10, 143, 35, 23]","[8, 26, 27, 12, 26, 32, 27, 24, 96, 26, 74, 48, 48]","[1, 2, 2, 0, 0, 3, 8, 3, 27, 18, 14, 5, 7]","A new exploration method for deep RL is presented, based on the idea of injecting noise into the deep networks’ weights. The noise may take various forms (either uncorrelated or factored) and its magnitude is trained by gradient descent along other parameters. It is shown how to implement this idea both in DQN (and its dueling variant) and A3C, with experiments on Atari games showing a significant improvement on average compared to these baseline algorithms.

This definitely looks like a worthy direction of research, and experiments are convincing enough to show that the proposed algorithms indeed improve on their baseline version. The specific proposed algorithm is close in spirit to the one from “Parameter space noise for exploration”, but there are significant differences. It is also interesting to see (Section 4.1) that the noise evolves in non-obvious ways across different games.

I have two main concerns about this submission. The first one is the absence of a comparison to the method from “Parameter space noise for exploration”, which shares similar key ideas (and was published in early June, so there was enough time to add this comparison by the ICLR deadline). A comparison to the paper(s) by Osband et al (2016, 2017) would have also been worth adding. My second concern is that I find the title and overall discussion in the paper potentially misleading, by focusing only on the “exploration” part of the proposed algorithm(s). Although the noise injected in the parameters is indeed responsible for the exploration behavior of the agent, it may also have an important effect on the optimization process: in both DQN and A3C it modifies the cost function being optimized, both through the “target” values (respectively Q_hat and advantage) and the parameters of the policy (respectively Q and pi). Since there is no attempt to disentangle these exploration and optimization effects, it is unclear if one is more important than the other to explain the success of the approach. It also sheds doubt on the interpretation that the agent somehow learns some kind of optimal exploration behavior through gradient descent (something I believe is far from obvious).

Estimating the impact of a paper on future research is an important factor in evaluating it. Here, I find myself in the akward (and unusual to me) situation where I know the proposed approach has been shown to bring a meaningful improvement, more precisely in Rainbow (“Rainbow: Combining Improvements in Deep Reinforcement Learning”). I am unsure whether I should take it into account in this review, but in doubt I am choosing to, which is why I am advocating for acceptance in spite of the above-mentioned concerns.

A few small remarks / questions / typos:
- In eq. 3 A(...) is missing the action a as input
- Just below: “the the”
- Last sentence of p. 3 can be misleading because the gradient is not back-propagated through all paths in the defined cost
- “In our experiments we used f(x) = sgn(x) p |x|”: this makes sense to me for eq. 9 but why not use f(x) = x in eq. 10?
- Why use factored noise in DQN and independent noise in A3C? This is presented like an arbitrary choice here.
- What is the justification for using epsilon’ instead of epsilon in eq. 15? My interpretation of double DQN is that we want to evaluate (with the target network) the action chosen by the Q network, which here is perturbed with epsilon (NB: eq. 15 should have b in the argmax, not b*)
- Section 4 should say explicitly that results are over 200M frames
- Assuming the noise is sampled similarly doing evaluation (= as in training), please mention it clearly.
- In paragraph below eq. 18: “superior performance compare to their corresponding baselines”: compared
- There is a Section 4.1 but no 4.2
- Appendix has a lot of redundant material with the main text, for instance it seems to me that A.1 is useless.
- In appendix B: “σi,j is simply set to 0.017 for all parameters” => where does this magic value come from?
- List x seems useless in C.1 and C.2
- C.1 and C.2 should be combined in a single algorithm with a simple “if dueling” on l. 24
- In C.3: (1) missing pi subscript for zeta in the “Output:” line, (2) it is not clear what the zeta’ parameters are for, in particular should they be used in l. 12 and 22?
- The paper “Dropout as a Bayesian approximation” seems worth at least  adding to the list of related work in the introduction.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,"['Ashwin Kalyan', 'Abhishek Mohta', 'Oleksandr Polozov', 'Dhruv Batra', 'Prateek Jain', 'Sumit Gulwani']",Accept,2018,"[1, 1, 5, 11, 15, 18]","[6, 4, 9, 16, 20, 23]","[24, 3, 37, 292, 216, 182]","[8, 1, 16, 134, 109, 120]","[15, 2, 18, 142, 93, 36]","[1, 0, 3, 16, 14, 26]","This is a strong paper. It focuses on an important problem (speeding up program synthesis), it’s generally very well-written, and it features thorough evaluation. The results are impressive: the proposed system synthesizes programs from a single example that generalize better than prior state-of-the-art, and it does so ~50% faster on average.

In Appendix C, for over half of the tasks, NGDS is slower than PROSE (by up to a factor of 20, in the worst case). What types of tasks are these? In the results, you highlight a couple of specific cases where NGDS is significantly *faster* than PROSE—I would like to see some analysis of the cases were it is slower, as well. I do recognize that in all of these cases, PROSE is already quite fast (less than 1 second, often much less) so these large relative slowdowns likely don’t lead to a noticeable absolute difference in speed. Still, it would be nice to know what is going on here.

Overall, this is a strong paper, and I would advocate for accepting it.


A few more specific comments:


Page 2, “Neural-Guided Deductive Search” paragraph: use of the word “imbibes” - while technically accurate, this use doesn’t reflect the most common usage of the word (“to drink”). I found it very jarring.

The paper is very well-written overall, but I found the introduction to be unsatisfyingly vague—it was hard for me to evaluate your “key observations” when I couldn’t quite yet tell what the system you’re proposing actually does. The paragraph about “key observation III” finally reveals some of these details—I would suggest moving this much earlier in the introduction.

Page 4, “Appendix A shows the resulting search DAG” - As this is a figure accompanying a specific illustrative example, it belongs in this section, rather than forcing the reader to hunt for it in the Appendix.

","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hyperparameter optimization: a spectral approach,"['Elad Hazan', 'Adam Klivans', 'Yang Yuan']",Accept,2018,"[16, 21, 16]","[21, 26, 21]","[219, 123, 104]","[103, 60, 47]","[93, 47, 34]","[23, 16, 23]","- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right. Not clear how important they are
- they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary). Not a realistic setting. In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials. 
- interesting idea but I think it's more theoretical than practical. Feels like a hammer in need of a nail. ","[6, 9, 6]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Learning with Logged Bandit Feedback,"['Thorsten Joachims', 'Adith Swaminathan', 'Maarten de Rijke']",Accept,2018,"[22, 7, 28]","[27, 11, 33]","[220, 44, 826]","[125, 25, 522]","[56, 17, 172]","[39, 2, 132]","Learning better policies from logged bandit feedback is a very important problem, with wide applications in internet, e-commerce and anywhere it is possible to incorporate controlled exploration. The authors study the problem of learning the best policy from logged bandit data. While this is not a brand new problem, the important and relevant contribution that the authors make is to do this using policies that can be learnt via neural networks. The authors are motivated by two main applications: (i) multi-class classification problems with bandit feedback (ii) ad placements problem in the contextual bandit setting. 

The main contributions of the authors is to design an output layer that allows training on logged bandit feedback data. Traditionally in the full feedback setting (setting where one gets to see the actual label and not just if our prediction is correct or incorrect) one uses cross-entropy loss function to optimize the parameters of a deep neural network. This does not work in a bandit setting, and previous work has developed various methods such as inverse-propensity scoring, self-normalized inverse propensity scoring, doubly robust estimators to handle the bandit setting. The authors in this paper work with self-normalized inverse propensity scoring as the technique to deal with bandit feedback data. the self normalized inverse propensity estimator (SNIPS) that the authors use is not a new estimator and has been previously studied in the work of Adith Swaminathan and co-authors. However, this estimator being a ratio is not an easy optimization problem to work with. The authors use a fairly standard reduction of converting ratio problems to a series of constrained optimization problems. This conversion of ratio problems to a series of constrained optimization problems is a standard textbook problem, and therefore not new. But, i like the authors handling of the constrained optimization problems via the use of Lagrangian constraints. It would have been great if the authors connected this to the REINFORCE algorithm of Williams. Unfortunately, the authors do not do a great job in establishing this connection, and I hope they do this in the full version of the paper.  The experimental results are fairly convincing and i really do not have any major comments. Here are my ""minor"" comments.

1. It would be great if the authors can establish connections to the REINFORCE algorithm in a more elaborate manner. It would be really instructive to the reader.

2.  On page 6,  the authors talk about lowering the learning rate and the learning rate schedule. I am guessing this is because of the intrinsic high variance of the problem. It would be great if the authors can explain in more detail why they did so.","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,"['Scott Reed', 'Yutian Chen', 'Thomas Paine', 'Aäron van den Oord', 'S. M. Ali Eslami', 'Danilo Rezende', 'Oriol Vinyals', 'Nando de Freitas']",Accept,2018,"[5, 10, 5, 7, 7, 8, 12, 19]","[9, 15, 9, 11, 11, 13, 17, 24]","[42, 84, 16, 74, 54, 90, 209, 199]","[20, 38, 10, 34, 18, 28, 101, 101]","[21, 32, 5, 36, 32, 58, 98, 84]","[1, 14, 1, 4, 4, 4, 10, 14]","This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning. The application is an obvious target for research and some relevant citations are missing, e.g. ""Towards a Neural Statistician"" (Edwards et al., ICLR 2017). Nonetheless, I think the current paper seems interesting enough to merit publication.

The paper is well-produced, i.e. the overall writing, visuals, and narrative flow are good. It was easy to read the paper straight through while understanding both the technical details and more intuitive motivations.

I have some concerns about the architectures and experiments presented in the paper. For architectures: the attention-based model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta PixelCNN model is a standard PixelCNN trained with the MAML approach by Finn et al. For experiments: the ImageNet flipping task is clearly tailored to the strengths of the attention-based model, and the presentation of the general Omniglot results could be improved. The image flipping experiment is neat, but the attention-based model's strong performance is unsurprising. I think the results in Tables 1/2 should be merged into a single table. It would make it clear that the MAML-based and attention-based models achieve similar performance on this task.

Overall, I think the paper makes a nice contribution. The paper could be improved significantly, e.g., by showing how to scale the attention-based architecture to problems with more data or by designing an architecture specifically for use with MAML-based inference.","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning,"['Gregory Farquhar', 'Tim Rocktäschel', 'Maximilian Igl', 'Shimon Whiteson']",Accept,2018,"[2, 7, 2, 16]","[6, 12, 6, 21]","[37, 129, 29, 274]","[18, 61, 15, 142]","[18, 63, 13, 103]","[1, 5, 1, 29]","The authors propose a new network architecture for RL that contains some relevant inductive biases about planning. This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task. The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end). This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes. The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax.

I thought the paper was clear and well-motivated. The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility. 

Experimental results seem promising but I wasn’t fully convinced of its conclusions. In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn’t clear to me that this is the right baseline. Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top. Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity? Same comment in Atari, but there it’s not really obvious that the proposed architecture is helping. Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc.

Page 5, the auxiliary loss on reward prediction seems appropriate, but it’s not clear from the text and experiments whether it actually was necessary. Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?  

Despite some shortcomings in the result section, I believe this is good work and worth communicating as is.","[8, 5, 4]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING,"['Dejiao Zhang', 'Haozhu Wang', 'Mario Figueiredo', 'Laura Balzano']",Accept,2018,"[6, 10, 27, 17]","[10, 15, 32, 21]","[33, 9, 283, 101]","[16, 3, 159, 50]","[16, 3, 51, 33]","[1, 3, 73, 18]","SUMMARY
The paper proposes to apply GrOWL regularization to the tensors of parameters between each pair of layers. The groups are composed of all coefficients associated to inputs coming from the same neuron in the previous layer. The proposed algorithm is a simple proximal gradient algorithm using the proximal operator of the GrOWL norm. Given that the GrOWL norm tend to empirically reinforce a natural clustering of the vectors of coefficients which occurs in some layers, the paper proposes to cluster the corresponding parameter vectors, to replace them with their centroid and to retrain with the constrain that some vectors are now equal. Experiments show that some sparsity is obtained by the model and that together with the clustering and high compression of the model is obtained which maintaining or improving over a good level of generalization accuracy. In comparison, plain group Lasso yields compressed versions that are too sparse, and tend to degrade performance. The method is also competitive with weight decay with much better compression.

REVIEW
Given the well known issue that the Lasso tends to select arbitrarily and in a non stable way variables
that are correlated *but* given that the well known elastic-net (and conceptually simpler than GrOWL) was proposed to address that issue already more than 10 years ago, it would seem relevant to compare the proposed method with the group elastic-net.

The proposed algorithm is a simple proximal gradient algorithm, but since the objective is non-convex it would be relevant to provide references for convergence guarantees of the algorithm.

How should the step size eta be chosen? I don't see that this is discussed in the paper.

In the clustering algorithm how is the threshold value chosen?

Is it chosen by cross validation?

Is the performance better with clustering or without?

Is the same threshold chosen for GrOWL and the Lasso?

It would be useful to know which values of p, Lambda_1 and Lambda_2 are selected in the experiments?

For Figures 5,7,8,9 given that the matrices do not have particular structures that need to be visualized but that the important thing to compare is the distribution of correlation between pairs, these figures that are hard to read and compare would be advantageously replaced by histograms of the values of the correlations between pairs (of different variables). Indeed, right now one must rely on comparison of shades of colors in the thin lines that display correlation and it is really difficult to appreciate how much of correlation of what level are present in each Figure. Histograms would extract exactly the relevant information...

A brief description of affinity propagation, if only in the appendix, would be relevant.
Why this method as opposed to more classical agglomerative clustering?

A brief reminder of what the principle of weight decay is would also be relevant for the paper to more self contained.

The proposed experiments are compelling, except for the fact that it would be nice to have a comparison with the group elastic-net. 

I liked figure 6.d and would vote for inclusion in the main paper.


TYPOS etc 

3rd last line of sec. 3.2 can fail at selecting -> fail to select

In eq. (5) theta^t should be theta^{(t)}

In section 4.1 you that the network has a single fully connected layer of hidden units -> what you mean is that the network has a single hidden layer, which is furthermore fully connected.

You cite several times Sergey (2015) in section 4.2. It seems you have exchanged first name and last name plus the corresponding reference is quite strange.

Appendix B line 5 "", while."" -> incomplete sentence.

","[8, 6, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A New Method of Region Embedding for Text Classification,"['chao qiao', 'bo huang', 'guocheng niu', 'daren li', 'daxiang dong', 'wei he', 'dianhai yu', 'hua wu']",Accept,2018,"[1, 15, 8, 9, 8, 14, 13, 15]","[5, 20, 12, 10, 13, 15, 18, 20]","[6, 211, 14, 4, 17, 23, 51, 243]","[2, 76, 8, 3, 13, 17, 19, 128]","[3, 19, 5, 0, 4, 6, 31, 102]","[1, 116, 1, 1, 0, 0, 1, 13]","The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit. The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). 

The method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?

The introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.
The related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.

The authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.  

The included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. 
I found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?

Qualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? 

Pros:
 + simple model
 + strong quantitative results

Cons:
 - notation (i.e. precise definition of r_{i,c})
 - qualitative analysis could be extended
 - writing could be improved  ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"['Wei Ping', 'Kainan Peng', 'Andrew Gibiansky', 'Sercan O. Arik', 'Ajay Kannan', 'Sharan Narang', 'Jonathan Raiman', 'John Miller']",Accept,2018,"[9, 2, 2, 8, 5, 4, 4, 4]","[14, 4, 6, 13, 10, 9, 8, 8]","[56, 15, 10, 76, 11, 33, 21, 31]","[26, 7, 5, 26, 3, 8, 9, 16]","[30, 8, 5, 41, 7, 23, 12, 15]","[0, 0, 0, 9, 1, 2, 0, 0]","This paper provides an overview of the Deep Voice 3 text-to-speech system. It describes the system in a fair amount of detail and discusses some trade-offs w.r.t. audio quality and computational constraints. Some experimental validation of certain architectural choices is also provided.

My main concern with this work is that it reads more like a tech report: it describes the workings and design choices behind one particular system in great detail, but often these choices are simply stated as fact and not really motivated, or compared to alternatives. This makes it difficult to tell which of these aspects are crucial to get good performance, and which are just arbitrary choices that happen to work okay.

As this system was clearly developed with actual deployment in mind (and not purely as an academic pursuit), all of these choices must have been well-deliberated. It is unfortunate that the paper doesn't demonstrate this. I think this makes the work less interesting overall to an ICLR audience. That said, it is perhaps useful to get some insight into what types of models are actually used in practice.

An exception to this is the comparison of ""converters"", model components that convert the model's internal representation of speech into waveforms. This comparison is particularly interesting because some of the results are remarkable, i.e. Griffin-Lim spectrogram inversion and the WORLD vocoder achieving very similar MOS scores in some cases (Table 2). I wish there would be more of that kind of thing in the paper. The comparison of attention mechanisms is also useful.

I'm on the fence as I think it is nice to get some insight into a practical pipeline which benefits from many current trends in deep learning research (autoregressive models, monotonic attention, ...), but I also feel that the paper is a bit meager when it comes to motivating all the architectural aspects. I think the paper is well written so I've tentatively recommended acceptance.


Other comments:

- The separation of the ""decoder"" and ""converter"" stage is not entirely clear to me. It seems that the decoder is trained to predict spectrograms autoregressively, but its final layer is then discarded and its hidden representation is then used as input to the converter stage instead? The motivation for doing this is unclear to me, surely it would be better to train everything end-to-end, including the converter? This seems like an unnecessary detour, what's the reasoning behind this?

- At the bottom of page 2 it is said that ""the whole model is trained end-to-end, excluding the vocoder"", which I think is an unfortunate turn of phrase. It's either end-to-end, or it isn't.

- In Section 3.3, the point of mixing of h_k and h_e is unclear to me. Why is this done?

- The gated linear unit in Figure 2a shows that speaker embedding information is only injected in the linear part. Has this been experimentally validated to work better than simpler mechanisms such as adding conditioning-dependent biases/gains?

- When the decoder is trained to do autoregressive prediction of spectrograms, is it autoregressive only in time, or also in frequency? I'm guessing it's the former, but this means there is an implicit independence assumption (the intensities in different frequency bins are conditionally independent, given all past timesteps). Has this been taken into consideration? Maybe it doesn't matter because the decoder is never used directly anyway, and this is only a ""feature learning"" stage of sorts?

- Why use the L1 loss on spectrograms?

- The recent work on Parallel WaveNet may allow for speeding up WaveNet when used as a vocoder, this could be worth looking into seeing as inference speed is used as an argument to choose different vocoder strategies (with poorer audio quality as a result).

- The title heavily emphasizes that this model can do multi-speaker TTS with many (2000) speakers, but that seems to be only a minor aspect that is only discussed briefly in the paper. And it is also something that preceding systems were already capable of (although maybe it hasn't been tested with a dataset of this size before). It might make sense to rethink the title to emphasize some of the more relevant and novel aspects of this work.


----

Revision: the authors have adequately addressed quite a few instances where I feel motivations / explanations were lacking, so I'm happy to increase my rating from 6 to 7. I think the proposed title change would also be a good idea.","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
HexaConv,"['Emiel Hoogeboom', 'Jorn W.T. Peters', 'Taco S. Cohen', 'Max Welling']",Accept,2018,"[1, 1, 5, 19]","[6, 4, 10, 24]","[31, 7, 86, 390]","[12, 3, 32, 190]","[19, 4, 49, 166]","[0, 0, 5, 34]","The paper presents an approach to efficiently implement planar and group convolutions over hexagonal lattices to leverage better accuracy of these operations due to reduced anisotropy. They show that convolutional neural networks thus built lead to better performance - reduced inductive bias - for the same parameter budget.

G-CNNs were introduced by Cohen and Welling in ICML, 2016. They proposed DNN layers that implemented equivariance to symmetry groups. They showed that group equivariant networks can lead to more effective weight sharing and hence more efficient networks as evinced by better performance on CIFAR10 & CIFAR10+ for the same parameter budget. This paper shows G-equivariance implemented on hexagonal lattices can lead to even more efficient networks. 

The benefits of using hexagonal lattices over rectangular lattices is well known in the signal processing as well as in computer vision. For example, see   

Golay M. Hexagonal parallel pattern transformation. IEEE Transactions on Computers 1969. 18(8): p. 733-740.

Staunton R. The design of hexagonal sampling structures for image digitization and their use with local operators. Image and Vision Computing 1989. 7(3): p. 162-166. 

L. Middleton and J. Sivaswamy, Hexagonal Image Processing, Springer Verlag, London, 2005

The originality of the paper lies in the practical and efficient implementation of G-Conv layers. Group-equivariant DNNs could lead to more robust, efficient and (arguably) better performing neural networks.

Pros

- A good paper that systematically pushes the state of the art towards the design of invariant, efficient and better performing  DNNs with G-equivariant representations.

- It leverages upon the existing theory in a variety of areas - signal & image processing and machine learning, to design better DNNs.

 - Experimental evaluation suffices for a proof of concept validation of the presented ideas.   

 
Cons

- The authors should relate the paper better to existing works in the signal processing and vision literature.

- The results are on simple benchmarks like CIFAR-10. It is likely but not immediately apparent if the benefits scale to more complex problems.

- Clarity could be improved in a few places

: Since * is used for a standard convolution operator, it might be useful to use *_g as a G-convolution operator.

: Strictly speaking, for translation equivariance, the shift should be cyclic etc.

: Spelling mistakes - authors should run a spellchecker.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields,"['Thomas Unterthiner', 'Bernhard Nessler', 'Calvin Seward', 'Günter Klambauer', 'Martin Heusel', 'Hubert Ramsauer', 'Sepp Hochreiter']",Accept,2018,"[8, 11, 2, 7, 12, 4, 25]","[12, 14, 5, 12, 12, 9, 30]","[38, 15, 7, 40, 6, 12, 114]","[15, 6, 3, 8, 2, 5, 38]","[19, 4, 4, 19, 2, 7, 48]","[4, 5, 0, 13, 2, 0, 28]","The paper takes an interesting approach to solve the existing problems of GAN training, using Coulomb potential for addressing the learning problem. It is also well written with a clear presentation of the motivation of the problems it is trying to address, the background and proves the optimality of the suggested solution. My understanding and validity of the proof is still an educated guess. I have been through section A.2 , but I'm unfamiliar with the earlier literature on the similar topics so I would not be able to comment on it. 

Overall, I think this is a good paper that provides a novel way of looking at and solving problems in GANs. I just had a couple of points in the paper that I would like some clarification on : 

* In section 2.2.1 : The notion of the generated a_i not disappearing is something I did not follow. What does it mean for a generated sample to ""not disappear"" ? and this directly extends to the continuity equation in (2). 

* In section 1 : in the explanation of the 3rd problem that GANs exhibit i.e.  the generator not being able to generalize the distribution of the input samples, I was hoping if you could give a bit more motivation as to why this happens. I don't think this needs to be included in the paper, but would like to have it for a personal clarification. ","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learn to Pay Attention,"['Saumya Jetley', 'Nicholas A. Lord', 'Namhoon Lee', 'Philip H. S. Torr']",Accept,2018,"[0, -4, -3, 31, 3, -3]","[5, 1, 2, 36, 8, 2]","[16, 1, 3, 104, 4, 2]","[13, 1, 3, 87, 3, 1]","[2, 0, 0, 4, 0, 0]","[1, 0, 0, 13, 1, 1]","This paper proposes a network with the standard soft-attention mechanism for classification tasks, where the global feature is used to attend on multiple feature maps of local features at different intermediate layers of CNN. The attended features at different feature maps are then used to predict the final classes by either concatenating features or ensembling results from individual attended features. The paper shows that the proposed model outperforms the baseline models in classification and weakly supervised segmentation.

Strength:
- It is interesting idea to use the global feature as a query in the attention mechanism while classification tasks do not naturally involve a query unlike other tasks such as visual question answering and image captioning.

- The proposed model shows superior performances over GAP in multiple tasks.

Weakness:
- There are a lot of missing references. There have been a bunch of works using the soft-attention mechanism in many different applications including visual question answering [A-C], attribute prediction [D], image captioning [E,F] and image segmentation [G]. Only two previous works using the soft-attention (Bahdanau et al., 2014; Xu et al., 2015) are mentioned in Introduction but they are not discussed while other types of attention models (Mnih et al., 2014; Jaderberg et al., 2015) are discussed more.

- Section 2 lacks discussions about related work but is more dedicated to emphasizing the contribution of the paper.

- The global feature is used as the query vector for the attention calculation. Thus, if the global feature contains information for a wrong class, the attention quality should be poor too. Justification on this issue can improve the paper.

- [H] reports the performance on the fine-grained bird classification using different type of attention mechanism. Comparison and justification with this method can improve the paper. The performance in [H] is almost 10 % point higher accuracy than the proposed model.

- In the segmentation experiments, the models are trained on extremely small images, which is unnatural in segmentation scenarios. Experiments on realistic settings should be included. Moreover, [G] introduces a method of using an attention model for segmentation, while the paper does not contain any discussion about it.


Overall, I am concerned that the proposed model is not well discussed with important previous works. I believe that the comparisons and discussions with these works can greatly improve the paper.

I also have some questions about the experiments:
- Is there any reasoning why we have to simplify the concatenation into an addition in Section 3.2? They are not equivalent.

- When generating the fooling images of VGG-att, is the attention module involved, or do you use the same fooling images for both VGG and VGG-att?

Minor comments:
- Fig. 1 -> Fig. 2 in Section 3.1. If not, Fig. 2 is never referred.

References
[A] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, 2016.
[B] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016.
[C] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. In CVPR, 2016.
[D] Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, and Bohyung Han. Hierarchical attention networks. arXiv preprint arXiv:1606.02393, 2016.
[E] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In CVPR, 2016.
[F] Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-Guided Attention Model for Image Captioning. AAAI, 2017.
[G] Seunghoon Hong, Junhyuk Oh, Honglak Lee and Bohyung Han, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, In CVPR, 2016.
[H] Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Spatial Transformer Networks, NIPS, 2015


","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
SMASH: One-Shot Model Architecture Search through HyperNetworks,"['Andrew Brock', 'Theo Lim', 'J.M. Ritchie', 'Nick Weston']",Accept,2018,"[3, 18, 15, 3]","[8, 22, 18, 3]","[32, 62, 38, 6]","[12, 28, 12, 2]","[19, 9, 4, 4]","[1, 25, 22, 0]","This paper tackles the problem of finding an optimal architecture for deep neural nets . They propose to solve it by training an auxiliary HyperNet to generate the main model. The authors propose the so called ""SMASH"" algorithm that ranks the neural net architectures based on their validation error. The authors adopt a memory-bank view of the network configurations for exploring a varied collection of network configurations. It is not clear whether this is a new contribution of this paper or whether the authors merely adopt this idea.  A clearer note on this would be welcome. My key concern is with the results as described in 4.1.; the correlation structure breaks down completely for ""low-budget"" SMASH in Figure 5(a) as compared Figure (4). Doesn't this then entail an investigation of what is the optimal size of the hyper network? Also I couldn't quite follow the importance of figure 5(b) - is it referenced in the text? The authors also note that SMASH is saves a lot of computation time; some time-comparison numbers would probably be more helpful to drive home the point especially when other methods out-perform SMASH. 
One final point, for the uninitiated reader- sections 3.1 and 3.2 could probably be written somewhat more lucidly for better access.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
On the regularization of Wasserstein GANs,"['Henning Petzka', 'Asja Fischer', 'Denis Lukovnikov']",Accept,2018,"[2, 9, 5]","[6, 14, 9]","[14, 79, 21]","[8, 30, 12]","[5, 36, 8]","[1, 13, 1]","This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1. The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss. The reasons for this choice are discussed and linked to theoretical properties of OT. Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.

The paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea. Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport.

Discussion:

+ The paper spends a lot of time justifying the proposed method by discussing the limits of the ""Improved training of Wasserstein GAN"" from Gulrajani et al. (2017). The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix. The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).

+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem. As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss. This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.

+ The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples). Does it lead to better stability to choose one or the other? 
 How is it implemented in practice? recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix. Numerical experiments comparing the two implementation or at least a discussion is necessary.

+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization. I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve. A discussion of the links is necessary and will clearly bring more theoretical ground to the method. Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization. In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.

+ The numerical experiments are encouraging but a bit short. The 2D example seem to work very well and the convergence curves are far better with the proposed regularization. But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix. The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1).

+ This is more of a suggestion. The comparison of the dual critic to the true Wasserstein distance is very interesting. It would be nice to see the behavior for different values of lambda.


[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447.


Review update after reply:

The authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept. 
","[7, 6, 2]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Strong rejection']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Interactive Grounded Language Acquisition and Generalization in a 2D World,"['Haonan Yu', 'Haichao Zhang', 'Wei Xu']",Accept,2018,"[9, 12, 19]","[14, 17, 23]","[46, 55, 71]","[15, 24, 32]","[24, 24, 37]","[7, 7, 2]","The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks. Agents working in this setting therefore, learn the language of the ""teacher"" and efficiently ground words to their respective concepts in the environment. The work also propose a neat model motivated by the environment and outperform various baselines. 

Further, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase. 

The robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right? As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable). Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?  

Overall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the Information Bottleneck Theory of Deep Learning,"['Andrew Michael Saxe', 'Yamini Bansal', 'Joel Dapello', 'Madhu Advani', 'Artemy Kolchinsky', 'Brendan Daniel Tracey', 'David Daniel Cox']",Accept,2018,"[13, 1, 1, 3, 9, 8, 16]","[18, 6, 5, 5, 14, 12, 21]","[43, 14, 7, 11, 34, 24, 92]","[20, 5, 4, 3, 4, 4, 44]","[19, 9, 3, 7, 22, 13, 35]","[4, 0, 0, 1, 8, 7, 13]","This paper presents a study on the Information Bottleneck (IB) theory of deep learning, providing results in contrasts to the main theory claims. According to the authors, the IB theory suggests that the network generalization is mainly due to a ‘compression phase’ in the information plane occurring after a ‘fitting phase’ and that the ‘compression phase’ is due to the stochastic gradient decent (SDG). Instead, the results provided by this paper show that: the generalization can happen even without compression; that SDG is not the primary factor in compression; and that the compression does not necessarily occur after the ‘fitting phase’. Overall, the paper tackles the IB theory claims with consistent methodology, thus providing substantial arguments against the IB theory. 

The main concern is that the paper is built to argue against another theoretical work, raising a substantial discussion with the authors of the IB theory. This paper should carefully address all the raised arguments in the main text. 

There are, moreover, some open questions that are not fully clear in this contribution:
1)	To evaluate the mutual information in the ReLu networks (sec. 2) the authors discretize the output activity in their range. Should the non-linearity of ReLu be considered as a form of compression? Do you check the ratio of ReLus that are not active during training or the ratio of inputs that fall into the negative domain of each ReLu? 
2)	Since one of today common topics is the training of deep neural networks with lower representational precision, could the quantization error due to the low precision be considered as a form of noise inserted in the network layers that influences the generalization performance in deep neural networks? 
3)	What are the main conclusions or impact of the present study in the theory of neural networks? Is it the authors aim to just demonstrate that the IB theory is not correct? Perhaps, the paper should empathize the obtained results not just in contrast to the other theory, but proactively in agreement with a new proposal. 

Finally, a small issue comes from the Figures that need some improvement. In most of the cases (Figure 3 C, D; Figure 4 A, B, C; Figure 5 C, D; Figure 6) the axes font is too small to be read. Figure 3C is also very unclear.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Adversarial Dropout Regularization,"['Kuniaki Saito', 'Yoshitaka Ushiku', 'Tatsuya Harada', 'Kate Saenko']",Accept,2018,"[3, 4, 14]","[8, 9, 19]","[60, 35, 379]","[29, 21, 210]","[25, 12, 112]","[6, 2, 57]","(Summary)
This paper is about learning discriminative features for the target domain in unsupervised DA problem. The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators.

(Pros)
The approach proposed in section 3.2 uses dropout logits and the sensitivity criterion between two softmax probability distributions which seems novel.

(Cons)
1. By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims ""achieved state of the art results on three datasets."" in sec5. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Does the proposed method outperform these state of the art methods using the same network architectures?
2. I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G,C. In the current draft, it's not immediately clear how the loss functions depend on the optimization variables. For example, in eqns 2,3,5, the minimization is over G,C but G,C do not appear anywhere in the equation. 
3. For the digits experiments, appendix B states ""we used exactly the same network architecture"". Well, which architecture was it?
4. It's not clear what exactly the ""ENT"" baseline is. The text says ""(ENT) obtained by modifying (Springenberg 2015)"". I'd encourage the authors to make this part more explicit and self-explanatory.

(Assessment)
Borderline. The method section is not very well written and the authors avoid comparing the method against the state of the art methods in unsupervised DA.","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Detecting Statistical Interactions from Neural Network Weights,"['Michael Tsang', 'Dehua Cheng', 'Yan Liu']",Accept,2018,"[17, 5, 17]","[21, 10, 22]","[25, 20, 219]","[12, 12, 132]","[9, 8, 57]","[4, 0, 30]","This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order. First the paper describes that an 'interaction strength' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers. Then, two simple networks for the main and interaction effects are modeled separately, and learned jointly with posing L1-regularization only on the interaction part to cancel out the main effect as much as possible. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. A nice series of experimental validations demonstrate the various types of interactions can be detected, while it also fairly clarifies the limitations.

In addition to the related work mentioned in the manuscript, interaction detection is also originated from so-called AID, literally intended for 'automatic interaction detector' (Morgan & Sonquist, 1963), which is also the origin of CHAID and CART, thus the tree-based methods like Additive Groves would be the one of main methods for this. But given the flexibility of function representations, the use of neural networks would be worth rethinking, and this work would give one clear example.

I liked the overall ideas which is clean and simple, but also found several points still confusing and unclear.

1) One of the keys behind this method is the architecture described in 4.1. But this part sounds quite heuristic, and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1. Absorbing the main effect is not critical to these facts? In a standard sense of statistics, interaction would be something like residuals after removing the main (additive) effect. (like a standard test by a likelihood ratio test for models with vs without interactions)

2) the description about the neural network for the main effect is a bit unclear. For example, what does exactly mean the 'networks with univariate inputs for each input variable'? Is my guessing that it is a 1-10-10-10-1 network (in the experiments) correct...? Also, do g_i and g_i' in the GAM model (sec 4.3) correspond to the two networks for the main and interaction effects respectively?

3) mu is finally fixed at min function, and I'm not sure why this is abstracted throughout the manuscript. Is it for considering the requirements for any possible criteria?

Pros:
- detecting (any order / any form of) statistical interactions by neural networks is provided.
- nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA, HierLasso, and Additive Groves.

Cons:
- some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect.
- the neural network architecture with L1 regularization is a bit heuristic, and I'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect.

","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
SCAN: Learning Hierarchical Compositional Visual Concepts,"['Irina Higgins', 'Nicolas Sonnerat', 'Loic Matthey', 'Arka Pal', 'Christopher P Burgess', 'Matko Bošnjak', 'Murray Shanahan', 'Matthew Botvinick', 'Demis Hassabis', 'Alexander Lerchner']",Accept,2018,"[7, 10, 11, 3, 2, 11, 32, 18, 10, 16]","[11, 13, 14, 3, 5, 16, 37, 23, 15, 20]","[36, 11, 28, 7, 22, 30, 98, 110, 54, 30]","[11, 3, 12, 3, 9, 15, 50, 40, 10, 11]","[20, 6, 15, 4, 13, 12, 27, 54, 26, 16]","[5, 2, 1, 0, 0, 3, 21, 16, 18, 3]","This paper introduces a VAE-based model for translating between images and text. The main way that their model differs from other multimodal methods is that their latent representation is well-suited to applying symbolic operations, such as AND and IGNORE, to the text. This gives them a more expressive language for sampling images from text.

Pros:
- The paper is well written, and it provides useful visualizations and implementation details in the appendix.

- The idea of learning compositional representations inside of a VAE framework is very appealing.

- They provide a modular way of learning recombination operations.

Cons:
- The experimental evaluation is limited. They test their model only on a simple, artificial dataset. It would also be helpful to see a more extensive evaluation of the model's ability to learn logical recombination operators, since this is their main contribution.

- The approach relies on first learning a pretrained visual VAE model, but it is unclear how robust this is. Should we expect visual VAEs to learn features that map closely to the visual concepts that appear in the text? What happens if the visual model doesn't learn such a representation? This again could be addressed with experiments on more challenging datasets.

- The paper should explain the differences and trade offs between other multimodal VAE models (such as their baselines, JMVAE and TrELBO) more clearly. It should also clarify differences between the SCAN_U baseline and SCAN in the main text.

- The paper suggests that using the forward KL-divergence is important, but this does not seem to be tested with experiments.

- The three operators (AND, IN COMMON, and IGNORE) can easily be implemented as simple transformations of a (binary) bag-of-words representation. What about more complex operations, such as OR, which seemingly cannot be encoded this way?

Overall, I am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance.
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Efficient Sparse-Winograd Convolutional Neural Networks,"['Xingyu Liu', 'Jeff Pool', 'Song Han', 'William J. Dally']",Accept,2018,"[2, 11]","[7, 16]","[22, 151]","[12, 97]","[2, 16]","[8, 38]","Summary: 
The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. 
This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way:
- weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity
- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations

This way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.

Review:
The paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. 
I only have a couple of questions/comments:
1) I’m not familiar with the term m-specific (“Matrices B, G and A are m-specific.”) and didn’t find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description.
2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs?
3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)
4) Figure 5 caption has a typo: “acrruacy”

References:
Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. ""Binaryconnect: Training deep neural networks with binary weights during propagations."" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015.
Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. ""Neural networks with few multiplications."" arXiv preprint arXiv:1510.03009 (2015).
Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. ""Xnor-net: Imagenet classification using binary convolutional neural networks."" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
mixup: Beyond Empirical Risk Minimization,"['Hongyi Zhang', 'Moustapha Cisse', 'Yann N. Dauphin', 'David Lopez-Paz']",Accept,2018,"[22, 9, 8, 8]","[27, 13, 13, 13]","[84, 35, 67, 60]","[48, 18, 32, 28]","[14, 15, 32, 27]","[22, 2, 3, 5]","Theoretical contributions: None. Moreover, there is no clear theoretical explanation for why this approach ought to work. The authors cite (Chapelle et al., 2000) and actually most of the equations are taken from there, but the authors do not justify why the proposed distribution is a good approximation for the true p(x, y). 

Practical contributions: The paper introduces a new technique for training DNNs by forming a convex combination between two training data instances, as well as changing the associated label to the corresponding convex combination of the original 2 labels. 

Experimental results. The authors show mixup provides improvement over baselines in the following settings:
  * Image Classification on Imagenet. CIFAR-10 and CIFAR-100, across architectures.
  * Speech data 
  * Memorization of corrupted labels
  * Adversarial robustness (white box and black box attacks)
  * GANs (though quite a limited example, it is hard to generalize from this setting to the standard problems that GANs are used for).
  * Tabular data.

Reproducibility: The provided website to access the source code is currently not loading. However, experiment hyperparameters are meticulously recorded in the paper. 

Key selling points:
  * Good results across the board.
  * Easy to implement.
  * Not computationally expensive. 

What is missing:
  * Convincing theoretical arguments for why combining data and labels this way is a good approach. Convex combinations of natural images does not result in natural images. 
 * Baseline in which the labels are not mixed, in order to ensure that the gains are not coming from the data augmentation only. Combining the proposed data augmentation with label smoothing should be another baseline.
  * A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space. 
  * A concrete strategy for obtaining good results using the proposed method. For example, for speech data the authors say that “For mixup, we use a warm-up period of five epochs where we train the network on original training examples, since we find it speeds up initial convergence.“ Would be good to see how this affects results and convergence speed. Apart from having to tune the lambda hyperparameter, one might also have to tune when to start mixup. 
  * Figure 2 seems like a test made to work for this method and does not add much to the paper. Yes, if one trains on convex combination between data, one expects the model to do better in that regime. 
  * Label smoothing baseline to put numbers into perspective, for example in Figure 4. 



","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Towards Neural Phrase-based Machine Translation,"['Po-Sen Huang', 'Chong Wang', 'Sitao Huang', 'Dengyong Zhou', 'Li Deng']",Accept,2018,"[8, 15, 6, 18, 28]","[12, 20, 11, 21, 30]","[75, 269, 35, 62, 329]","[44, 114, 22, 37, 188]","[26, 38, 11, 17, 36]","[5, 117, 2, 8, 105]","This paper introduces a new architecture for end to end neural machine translation. Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right. 

This kind of approach is more related to ngram based machine translation than conventional phrase based one.  

The idea is nice. The proposed approach does not rely on attention based model. This opens nice perpectives for better and faster inference. 

My first concern is about the architecture description. For instance, the swan part is not really stand alone. For reader who does not already know this net, I'm not sure this is really clear. Moreover, there is no link between notations used for the swan part and the ones used in the reordering part. 

Then, one question arises. Why don't you consider the reordering of the whole source sentence. Maybe you could motivate your choice at this point. This is the main contribution of the paper, since swan already exists.

Finally, the experimental part shows nice improvements but: 1/ you must provide baseline results with a well tuned phrase based mt system; 2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. ","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
cGANs with Projection Discriminator,"['Takeru Miyato', 'Masanori Koyama']",Accept,2018,"[3, 7]","[8, 12]","[24, 38]","[9, 14]","[14, 19]","[1, 5]","This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.  It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.

This paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). 

What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward.

The sentence containing ""assume that the network model can be shared"" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Multi-Task Learning for Document Ranking and Query Suggestion,"['Wasi Uddin Ahmad', 'Kai-Wei Chang', 'Hongning Wang']",Accept,2018,"[3, 11, 11]","[8, 16, 16]","[55, 317, 170]","[24, 146, 107]","[30, 157, 53]","[1, 14, 10]","Novelty: It looks quite straightforward to combine document ranking and query suggestion.  For the model architecture, it is a standard multi-task learning framework. For the “session encoder”, it is also proposed (at least, used) in (Sordoni et al., CIKM 2015). Therefore, I think the technical novelty of the work is limited. 

Clarify: The paper is in general well written. One minor suggestion is to replace Figure 1 with Figure 3, which is more intuitive. 

Experiments: 
1.	Why don’t you try deep LSTM models and attention mechanisms (although you mentioned them as future work)? There are many open-source tools for deep LSTM/GRU and attention models, and I see no obstacle to implement your algorithms on their top. 
2.	In Table 2, M-NSRF with regularization significantly outperforms the version without regularization. This indicates that it might be the regularization that works rather than multi-task learning. For fair comparison, the regularization trick should also be applied to the baselines. 
3.	For the evaluation metric of query suggestion, why not using BLEU score? At least, you should compare with the metrics used in (Sordoni et al., 2015) for fairness. 
4.	The experiments are not very comprehensive – currently, there is only one experiment in the paper, from which one can hardly draw convincing conclusions.
5.	How many words are there in your documents? What is the average length of each document? You only mention that “our goal is to rank candidate documents titles……” in Page 6, 2nd paragraph. It might be quite different for long document retrieval vs. short document retrievel. 
6.	How did you split the dataset into training, validation and test sets?  It seems that you used a different splitting rule from (Sordoni et al., 2015), why? ","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,"['Yeming Wen', 'Paul Vicol', 'Jimmy Ba', 'Dustin Tran', 'Roger Grosse']",Accept,2018,"[1, 4, 8, 4, 12]","[5, 9, 13, 9, 17]","[16, 30, 96, 76, 123]","[6, 16, 46, 31, 60]","[10, 14, 49, 40, 61]","[0, 0, 1, 5, 2]","The paper is well written. The proposal is explained clearly. 
Although the technical contribution of this work is relevant for network learning, several key aspects are yet to be addressed thoroughly, particularly the experiments. 

Will there be any values of alpha, beta and gamma where eq(8) and eq(9) are equivalent. In other words, will it be the case that SharedPerturbation(alpha, beta, gamma, N) = Flipout(alpha1, beta1, gamma1, N1) for some choices of alpha, alpha1, beta, beta1, ...? This needs to be analyzed very thoroughly because some experiments seem to imply that Flip and NoFlip are giving same performance (Fig 2(b)). 
It seems like small batch with shared perturbation should be similar to large batch with flipout? 
Will alpha and gamma depend on the depth of the network? Can we say anything about which networks are better? 
It is clear that the perturbations E1 and E2 are to be uniform +/-1. Are there any benefits for choosing non-uniform sampling, and does the computational overhead of sampling them depend on the network depth/size. 

The experiments seem to be inconclusive. 
Firstly, how would the proposed strategy work on standard vision problems including learning imagenet and cifar datasets (such experiments would put the proposal into perspective compared to dropout and residual net type procedures) ?
Secondly, without confidence intervals (or significance tests of any kind), it is difficult to evaluate the goodness of Flipout vs. baselines, specifically in Figures 2(b,d). 
Thirdly, it is known that small batch sizes give better performance guarantees than large ones, and so, what does Figure 1 really imply? (Needs more explanation here, relating back to description of alpha, beta and gamma; see above). 
","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Improving GAN Training via Binarized Representation Entropy (BRE) Regularization,"['Yanshuai Cao', 'Gavin Weiguang Ding', 'Kry Yik-Chau Lui', 'Ruitong Huang']",Accept,2018,"[6, 6, 1, 8]","[10, 9, 4, 11]","[32, 25, 10, 27]","[12, 11, 5, 15]","[19, 10, 5, 11]","[1, 4, 0, 1]","The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the ""model capacity"" of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator.

The paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed.

Some detailed comments / questions:

1. It seems the concept of ""binarized activation patterns"", which the proposed regularizer is designed upon, is closely coupled with rectifier nets. I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract.

2. In order for the paper to be more self-contained, maybe list at least once the formula for ""rectifier net"" (sth. like ""a^T max(0, wx + b) + c"") ? This might also help the readers better understand where the polytopes in Figure 1 come from.

3. In section 3.1, when presenting random variables (U_1, ..., U_d), I find the word ""Bernourlli"" a bit misleading because typically people would expect U_i to take values from {0, 1} whereas here you assume {-1, +1}. This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations.
Also, ""K"" is already used to denote the mini-batch size, so it's a slight abuse to reuse ""k"" to denote the ""kth marginal"".

4. In section 3.2, it may be clearer to explicitly point out the use of the ""3-sigma"" rule for Gaussian distributions here. But I don't find it justified anywhere why ""leave 99.7% of i, j pairs unpenalized"" is sth. to be sought for here?

5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015), ""n"" abruptly appears without proper introduction / context.

6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse?

7. Figure 3 is missing the sub-labels (a), (b), (c), (d).","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generalizing Hamiltonian Monte Carlo with Neural Networks,"['Daniel Levy', 'Matt D. Hoffman', 'Jascha Sohl-Dickstein']",Accept,2018,"[13, 13, 10]","[16, 17, 15]","[21, 85, 130]","[10, 49, 52]","[10, 28, 74]","[1, 8, 4]","The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction,"['Da Xiao', 'Jo-Yu Liao', 'Xingyuan Yuan']",Accept,2018,"[14, 1, 1]","[18, 1, 1]","[23, 2, 2]","[8, 1, 1]","[2, 1, 1]","[13, 0, 0]","The paper is interesting to read and gives valuable insights. 

However, the paper clearly breaks the submission guidelines. The paper is far too long, 14 pages (+refs and appendix, in total 19 pages), while the page limit is 8 pages (+refs and appendix). Therefore, the paper should be rejected. I can not foresee how the authors should be able to squeeze to content into 8 pages. The paper is more suitable for a journal, where page limit is less of an issue.","[3, 7, 7]","[' Clear rejection', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,"['Tianyi Zhou', 'Jeff Bilmes']",Accept,2018,"[10, 30]","[15, 35]","[112, 299]","[55, 211]","[48, 49]","[9, 39]","The main strength of this paper, I think, is the theoretical result in Theorem 1. This result is quite nice. I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further.

The authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*. If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected).

1) k goes to infinity
2) alpha goes to 1
3) g(w*) goes to 0

The authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*. In fact this proof shows the situation is much better than that. 

If we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero. 

There is also a remark that G(A) tends to be modular when lambda is small which is useful.
From the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly.

It would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion. Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Recasting Gradient-Based Meta-Learning as Hierarchical Bayes,"['Erin Grant', 'Chelsea Finn', 'Sergey Levine', 'Trevor Darrell', 'Thomas Griffiths']",Accept,2018,"[4, 6, 10, 32, 19]","[8, 11, 15, 37, 24]","[24, 306, 743, 653, 317]","[13, 128, 326, 376, 190]","[10, 172, 396, 230, 87]","[1, 6, 21, 47, 40]","MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure. In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation. Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task.

Casting MAML as HB seems a good idea. The paper does a good job of explaining the connection, but I think the presentation could be clarified. The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper. 
The synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method. 
Regarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review. However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here. In addition, the point made by the TCML authors is fair (""nothing prevented you from..."") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote. This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.
The experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.

I am assessing this paper as ""7"" because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small. I think the approach is good and fruitful. 


# Suggestions on readability

* I have the feeling the paper inverts $\alpha, \beta$ from their use in Finn 2017 (step size for meta- vs task-training). This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)

* I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1. This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use. (could leave the output of ML-... implicit by writing ML-...(\theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)

* Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2).

* sec6 ""Machine learning and deep learning"": I would definitely avoid this formulation, seems to tail in with all the media nonsense on ""what's the difference between ML and DL ?"". In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing.

# Typos

* sec1 second parag: did you really mean ""in the architecture or loss function""? unclear.
* sec2: over a family
* ""common structure, so that"" (not such that)
* orthgonal
* sec2.1 suggestion: clarify that \theta and \phi are in the same space
* sec2.2 suggestion: task-specific parameter $\phi_j$ is distinct from ... parameters $\phi_{j'}, j' \neq j}
* ""unless an approximate ... is provided"" (the use of the subjunctive here is definitely dated :-) )
* sec3.1 task-specific parameters $\phi_j$ (I would avoid writing just \phi altogether to distinguish in usage from \theta)
* Gaussian-noised
* approximation of the it objective
* before eq9: ""that solves"": well, it doesn't really ""solve"" the minimisation, in that it is not a minimum; reformulate this?
* sec4.1 innaccurate
* well approximated
* sec4.2 an curvature
* (Amari 1989)
* For the the Laplace
* O(n^3) : what is n ?
* sec5.2 (Ravi and L 2017)
* for the the 
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Decoupling the Layers in Residual Networks,"['Ricky Fok', 'Aijun An', 'Zana Rashidi', 'Xiaogang Wang']",Accept,2018,"[2, 25, 1, 14]","[2, 30, 4, 18]","[6, 174, 5, 22]","[1, 117, 4, 7]","[2, 14, 0, 2]","[3, 43, 1, 13]","Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100.

One thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from ""rotating"" a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Proximal Backpropagation,"['Thomas Frerix', 'Thomas Möllenhoff', 'Michael Moeller', 'Daniel Cremers']",Accept,2018,"[2, 6, 9, 19]","[5, 11, 14, 24]","[9, 31, 113, 602]","[5, 14, 40, 327]","[4, 14, 48, 186]","[0, 3, 25, 89]","The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step. The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG. The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step).

On reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case. It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results. The experiments are serious, and done using standard state-of-the-art tools and architectures. Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements.

The main drawback of this paper is that there is no theory to suggest the ProxProp algorithm has better worst-case convergence guarantees, and that the experiments do not show a consistent benefit (in terms of time) of the method. On the one hand, I somewhat agree with the authors that ""while the running time is higher... we expect that it can be improved through further engineering efforts"", but on the other hand, the idea of nested algorithms (""matrix-free"" or ""truncated Newton"") always has this issue. A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems.

In brief: quality is high, clarity is high, originality is high, and significance is medium.
Pros: interesting idea, relevant theory provided, high-quality experiments
Cons: no evidence that this is a ""break-through"" idea

Minor comments:

- Theorems seemed reasonable and I have no reason to doubt their accuracy

- No typos at all, which I find very unusual. Nice job!

- In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10).  It's nice to have it very clear, since ""gradient step"" doesn't make it clear what the stepsize is, and if this is done in a ""Jacob-like"" or ""Gauss-Seidel-like"" fashion. (c) has no reference equation, does it?

- Similarly, for Algo 2, add references. In particular, tie in the stepsizes tau and tau_theta here.

- Motivation in section 4.1 was a bit iffy. A larger stepsize is not always better, and smaller is not worse. Minimizing a quadratic f(x) = .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE).

- The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods. This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s).

- Eq (9), this is done component-wise, i.e., Hadamard product, right?

- About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap.

Prox tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, ""Proximal splitting methods in signal processing,"" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf

- Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me. It would be nice to expand this.

- Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right? and in a conv net, just a convolution? It would help the reader to be more explicit here.

- Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused? The wording here was confusing.

- Fig 2 was not that convincing since the figure with time showed that either usual BackProp or the exact ProxProp were best, so why care about the approximate ProxProp with a few CG iterations? The argument of better generalization is based on very limited experiments and without any explanation, so I find that a weak argument (and it just seems weird that inexact CG gives better generalization).  The right figure would be nice to see with time on the x-axis as well.

- Section 5.2, this was nice and contributed to my favorable opinion about the work. However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero. I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here?

- Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs. I am confused why these are different numbers. Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was? And why limit to 50 epochs then? Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
An Online Learning Approach to Generative Adversarial Networks,"['Paulina Grnarova', 'Kfir Y Levy', 'Aurelien Lucchi', 'Thomas Hofmann', 'Andreas Krause']",Accept,2018,"[5, 8, 10, 26, 16]","[8, 13, 15, 31, 21]","[11, 60, 110, 205, 476]","[3, 29, 51, 112, 250]","[6, 29, 52, 69, 192]","[2, 2, 7, 24, 34]","It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing. Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). The present paper proposes to obtain mixed strategy through an online learning approach. Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss. One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M).In particular, the concave player plays the FTRL algorithm with standard L2 regularization term. The regret of concave player can be bounded using existing result for FTRL. The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. Then a lemma by Kalai and Vempala can be used. The theory part of the paper is reasonable and quite well written. 

Based on the theory developed, the paper presents a practical algorithm. Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration. The paper claims that this may help to prevent model collapsing.

However, the experimental part is less satisfying. From figure 2, I don't see much advantage of Checkhov GAN. In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA).The paper didn't really compare other popular GAN models, especially WGAN and its improved version, which is already quite popular by now and should be compared with.

Overall, I think it is a borderline paper.

-------------------------
I read the response and the new experimental results regarding WGAN.
The experimental results make more sense now.
It would be interesting to see whether the idea can be applied to more recent GAN models and still perform better.
I raised my score to 7.

","[7, 8, 5]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,"['Yujun Lin', 'Song Han', 'Huizi Mao', 'Yu Wang', 'Bill Dally']",Accept,2018,"[3, 4, 4, 13, 34]","[7, 9, 8, 18, 39]","[34, 150, 24, 404, 274]","[16, 66, 13, 255, 174]","[13, 67, 10, 58, 23]","[5, 17, 1, 91, 77]","The paper is thorough and on the whole clearly presented. However, I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle. The methods proposed are heuristic in nature, and it's not clear what the guiding principle is. E.g., ""momentum correction"". What exactly is the problem without this correction? The authors describe it qualitatively, ""When the gradient sparsity is high, the interval dramatically increases, and thus the significant momentum effect will harm the model performance"". Can the issue be described more precisely? Similarly for gradient clipping, ""The method proposed by Pascanu et al. (2013) rescales the gradients whenever the sum of their L2-norms exceeds a threshold. This step is conventionally executed after gradient aggregation from all nodes. Because we accumulate gradients over iterations on each node independently, we perform the gradient clipping locally before adding the current gradient... "" What exactly is the issue here? It reads like a story of what the authors did, but it's not really clear why they did it.

The experiments seem quite thorough, with several methods being compared. What is the expected performance of the 1-bit SGD method proposed by Seide et al.?

re. page 2: What exactly is ""layer normalization""?

re. page 4: What are ""drastic gradients""?","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Mixed Precision Training of Convolutional Neural Networks using Integer Operations,"['Dipankar Das', 'Naveen Mellempudi', 'Dheevatsa Mudigere', 'Dhiraj Kalamkar', 'Sasikanth Avancha', 'Kunal Banerjee', 'Srinivas Sridharan', 'Karthik Vaidyanathan', 'Bharat Kaul', 'Evangelos Georganas', 'Alexander Heinecke', 'Pradeep Dubey', 'Jesus Corbal', 'Nikita Shustrov', 'Roma Dubtsov', 'Evarist Fomenko', 'Vadim Pirogov']",Accept,2018,"[14, 2, 9, 12, 18, 8, 12, 16, 7, 7, 12, 40, 21, 1, 6, 1, 4]","[17, 7, 14, 17, 22, 13, 17, 21, 12, 12, 17, 45, 21, 1, 6, 1, 4]","[40, 12, 55, 34, 48, 55, 31, 53, 37, 39, 76, 123, 20, 2, 3, 4, 4]","[20, 1, 22, 19, 18, 35, 17, 42, 14, 20, 43, 57, 13, 1, 2, 2, 2]","[15, 11, 31, 12, 19, 12, 12, 5, 21, 15, 18, 24, 1, 1, 1, 2, 1]","[5, 0, 2, 3, 11, 8, 2, 6, 2, 4, 15, 42, 6, 0, 0, 0, 1]","This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations. These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10. However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported. In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16. 

The reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs. However, I was hoping to see a direct comparison between FP16 and INT16.  

The paper is written clearly and the English is fine.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Gradient Estimators for Implicit Models,"['Yingzhen Li', 'Richard E. Turner']",Accept,2018,"[7, 12]","[12, 17]","[66, 147]","[28, 67]","[38, 70]","[0, 10]","This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.

This is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. 

- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term ""Stein identity"" is appropriate to express the equation (8). 
- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 4, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,"['Bo Zong', 'Qi Song', 'Martin Renqiang Min', 'Wei Cheng', 'Cristian Lumezanu', 'Daeki Cho', 'Haifeng Chen']",Accept,2018,"[10, 13, 12, 10, 13, 9, 18]","[15, 18, 17, 15, 17, 12, 23]","[54, 179, 79, 112, 50, 5, 234]","[37, 94, 37, 64, 44, 4, 145]","[13, 28, 32, 24, 4, 1, 42]","[4, 57, 10, 24, 2, 0, 47]","1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection
2. It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods
3. Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process? can it be a trainable parameter?
4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  ","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,"['Seyed-Mohsen Moosavi-Dezfooli', 'Alhussein Fawzi', 'Omar Fawzi', 'Pascal Frossard', 'Stefano Soatto']",Accept,2018,"[4, 8, 9, 20, 26]","[9, 12, 14, 25, 31]","[50, 46, 86, 586, 445]","[21, 19, 33, 262, 241]","[27, 20, 35, 160, 138]","[2, 7, 18, 164, 66]","The paper is written well and clear.   The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist.  

Pros: the intuition and geometry is rather clearly presented.  

Cons: 
References to ""CaffeNet""  and ""LeNet"" (even though the latter is well-known) are missing.  In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained. 

It is not clear why the authors have decided to use out-dated 5-layer ""LeNet""  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures). It would be nice to see how the behavior and boundaries look in these cases.  

The conclusion is speculative:
""Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it
is key to suppress this subspace of shared positive directions, which can possibly be done through
regularization of the objective function. This will be the subject of future works."" 

It is clear that regularization should play a significant role in shaping the decision boundaries. Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used. All these factors should play a very significant role in the experimental validation of their hypothesis.

Notes: I did not check the proofs of the theorems in detail. 
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,"['Feiwen Zhu', 'Jeff Pool', 'Michael Andersch', 'Jeremy Appleyard', 'Fung Xie']",Accept,2018,"[8, 11, 7, 5, 1]","[12, 14, 11, 9, 1]","[7, 24, 9, 9, 2]","[4, 13, 6, 5, 1]","[3, 10, 2, 3, 1]","[0, 1, 1, 1, 0]","The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper). It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal.

Pros:
- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs
- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well

Cons:
- This work may be better reviewed at a more GPU-centric conference
- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 2, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Not-So-Random Features,"['Brian Bullins', 'Cyril Zhang', 'Yi Zhang']",Accept,2018,"[3, 2, 26]","[8, 7, 31]","[34, 42, 1300]","[15, 18, 574]","[16, 24, 160]","[3, 0, 566]","
In this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together. The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem. They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity. Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity. Empirically, they show the performances are better than random feature and the LKRF. 

I like the way they handle the nonconvexity component of the model. However, there are several issues need to be addressed. 

1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly. 

2, In the paper, there is an assumption about the peak of random feature ""it is a natural assumption on realistic data that the largest peaks are close to the origin"". I was wondering where this assumption is used? Could you please provide more justification for such assumption?

3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning. Please clarify this in the paper explicitly. 

4, The experiment is weak. The algorithm is closely related to boosting and MKL, while there is no such comparison. Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison. 

Suggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning. 
","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Mitigating Adversarial Effects Through Randomization,"['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Zhou Ren', 'Alan Yuille']",Accept,2018,"[3, 20, 4, 9, 36]","[8, 25, 9, 14, 41]","[69, 181, 40, 48, 702]","[26, 63, 18, 26, 337]","[42, 48, 21, 17, 263]","[1, 70, 1, 5, 102]","The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.

Some of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results.


Typically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images?

The proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes.

Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. 

In section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model?

","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning an Embedding Space for Transferable Robot Skills,"['Karol Hausman', 'Jost Tobias Springenberg', 'Ziyu Wang', 'Nicolas Heess', 'Martin Riedmiller']",Accept,2018,"[6, 7, 7, 10, 26]","[11, 12, 10, 15, 31]","[83, 76, 53, 200, 168]","[36, 37, 20, 81, 100]","[40, 35, 29, 111, 47]","[7, 4, 4, 8, 21]","The paper presents a new approach for hierarchical reinforcement learning which aims at learning a versatile set of skills. The paper uses a variational bound for entropy regularized RL to learn a versatile latent space which represents the skill to execute. The variational bound is used to diversify the learned skills as well as to make the skills identifyable from their state trajectories. The algorithm is tested on a simple point mass task and on simulated robot manipulation tasks.

This is a very intersting paper which is also very well written. I like the presented approach of learning the skill embeddings using the variational lower bound. It represents one of the most principled approches for hierarchical RL. 

Pros: 
- Interesting new approach for hiearchical reinforcement learning that focuses on skill versatility
- The variational lower bound is one of the most principled formulations for hierarchical RL that I have seen so far
- The results are convincing

Cons:
- More comparisons against other DRL algorithms such as TRPO and PPO would be useful

Summary: This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills. This is a good paper.

More comments:
- There are several papers that focus on learning versatile skills in the context of movement primitive libraries, see [1],[2],[3]. These papers should be discussed.

[1] Daniel, C.; Neumann, G.; Kroemer, O.; Peters, J. (2016). Hierarchical Relative Entropy Policy Search, Journal of Machine Learning Research (JMLR),
[2] End, F.; Akrour, R.; Peters, J.; Neumann, G. (2017). Layered Direct Policy Search for Learning Hierarchical Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).
[3] Gabriel, A.; Akrour, R.; Peters, J.; Neumann, G. (2017). Empowered Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to cluster in order to transfer across domains and tasks,"['Yen-Chang Hsu', 'Zhaoyang Lv', 'Zsolt Kira']",Accept,2018,"[4, 3, 15]","[9, 7, 20]","[38, 24, 130]","[16, 11, 54]","[22, 13, 71]","[0, 0, 5]","The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class. Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide ""soft labels"" for whether or not pairs of target data belong to the same class. A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points. 

The proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest. The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task.

The way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on. To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters. However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear. Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO. Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works.

The experimental evaluation is very thorough, spanning a variety of tasks and settings. Strong results in multiple settings indicate that the proposed method is effective and generalizable. Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup.

At times, the experimental details are a little unclear. Consistent use of the A, T, and S' dataset abbreviations would help. Also, the results section seems to switch off between calling the method CCN and LCO interchangeably. Finally, a few of the experimental settings differ from their baselines in nontrivial ways. For the Office experiment, the LCO appears to be trained on ImageNet data. While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data. Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting. In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature.

Overall, the paper seems to have both a novel contribution and strong technical merit. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.

=========================
Update after author rebuttal:
=========================
I have read the author's response and have looked at the changes to the manuscript. I am satisfied with the improvements to the paper and have changed my review to 'accept'. ","[7, 9, 5]","[' Good paper, accept', ' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Spatially Transformed Adversarial Examples,"['Chaowei Xiao', 'Jun-Yan Zhu', 'Bo Li', 'Warren He', 'Mingyan Liu', 'Dawn Song']",Accept,2018,"[5, 7, 10, 7, 20, 20]","[10, 12, 15, 11, 25, 25]","[83, 121, 300, 27, 261, 440]","[34, 47, 124, 16, 130, 254]","[46, 59, 148, 9, 59, 154]","[3, 15, 28, 2, 72, 32]","This paper proposes a new way to create adversarial examples. Instead of changing pixel values they perform spatial transformations. 

The authors obtain a flow field that is optimized to fool a target classifier. A regularization term controlled by a parameter tau is ensuring very small visual difference between the adversarial and the original image. 

The used spatial transformations are differentiable with respect to the flow field (as was already known from previous work on spatial transformations) it is easy to perform gradient descent to optimize the flow that fools classifiers for targeted and untargeted attacks. 

The obtained adversarial examples seem almost imperceivable (at least for ImageNet). 
This is a new direction of attacks that opens a whole new dimension of things to consider. 

It is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets, CAM attention visualization and also additional materials with high-res attacks. 

This is a very creative new and important idea in the space of adversarial attacks. 

Edit: After reading the other reviews , the replies to the reviews and the revision of the paper with the human study on perception, I increase my score to 9. This is definitely in the top 15% of ICLR accepted papers, in my opinion.   

Also a remark: As far as I understand, a lot of people writing comments here have a misconception about what this paper is trying to do: This is not about improving attack rates, or comparing with other attacks for different epsilons, etc. 
This is a new *dimension* of attacks. It shows that limiting l_inf of l_2 is not sufficient and we have to think of human perception to get the right attack model. Therefore, it is opening a new direction of research and hence it is important scholarship. It is asking a new question, which is frequently more important than improving performance on previous benchmarks. 

","[9, 7, 7]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets,"['Jinsung Yoon', 'James Jordon', 'Mihaela van der Schaar']",Accept,2018,"[4, 1, 19]","[9, 5, 24]","[75, 29, 829]","[27, 16, 344]","[33, 13, 230]","[15, 0, 255]","This paper introduces a generative adversarial network (GAN) for estimating individualized treatment effects (ITEs) by (1) learning a generator that tries to fool a discriminator with feature, treatment, potential outcome- vectors, and (2) by learning a GAN for the treatment effect. In my view, the counterfactual component is the interesting and original component, and the results show that the ITE GAN component further improves performance (marginally but not significant). The analysis is conducted on semi-synthetic data sets created to match real data distributions with synthetically introduced selection bias and conducts extensive experimentation. While the results show worse performance compared to existing literature in the experiment with small data sizes, the work does show improvements in larger data sets. However, Table 5 in the appendix suggests these results are not significant when considering average treatment effect estimation (eATE and eATE).

Quality: good. Clarity: acceptable. Originality: original. Significance: marginal.

The ITE GAN does not significantly outperform the counterfactual GAN alone (in the S and GAN loss regime), and in my understanding the counterfactual GAN is the particularly innovative component here, i.e., can the algorithm effectively enough generate indistinguishable counterfactual outcomes from x and noise. I wonder if the paper should focus on this in isolation to better understand and characterize this contribution.

What is the significance of bold in the tables? I'd remove it if it's just to highlight which method is yours.

Discussion section should be called ""Conclusion"" and a space permitting a Discussion section should be written.
E.g. exploration of the form of the loss when k>2, or when k is exponential e.g. a {0,1}^c hypercube for c potentially related treatment options in an order set. 
E.g. implications of underperformance in settings with small data sets. We have lots of large data sets where ground truth is unknown, and relatively more small data sets where we can identify ground truth at some cost.
E.g. discussion of Table 2 (ITEs) where GANITE is outperforming the methods (at least on large data sets) and Table 5 (ATEs) which does not show the same result is warranted. Why might we expect this to the case?","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,"['Xu He', 'Herbert Jaeger']",Accept,2018,"[15, 29]","[20, 34]","[142, 43]","[64, 12]","[18, 7]","[60, 24]","This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors. This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is ""unused"" until the present task in hand.

The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem. In Section 2 the authors review conceptors. This method is algebraic method closely related to spanning sub spaces and SVD. The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally. In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation. The authors provide a version with batch SGD as well.

In Section 4, the authors show their method on permuted MNIST. They compare the method to EWC with the same architecture. They show that their method more efficiently suffers on permuted MNIST from less degradation. Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance.

In general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life. Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.

I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.  What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods. Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   ","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,"['Krzysztof Choromanski', 'Carlton Downey', 'Byron Boots']",Accept,2018,"[7, 9, 14]","[12, 14, 19]","[129, 22, 221]","[49, 13, 112]","[68, 7, 98]","[12, 2, 11]","This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space.
While PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features.

In particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness. 

Authors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM. They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training.

Question:
-	What is the cost of constructing orthogonal random features compared to RF?
-	What is the definition of H the Hadamard matrix in the discrete orthogonal joint definition?
-	What are the hyperparameters values use for the LSTM
-	Empirical evaluations seem to use relatively small datasets composed by few dozens of temporal trajectories. Did you consider larger dataset for evaluation? 
-	How did you select the maximum number of epochs in Figure 5? It seems that the validation error is still decreasing after 25 epochs?

Pros:
-	Provide theoretical guarantee for the use of orthogonal random features in the context of PSRNN
Cons:
-	Empirical evaluation only on small scale datasets.
","[7, 4, 8]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Fidelity-Weighted Learning,"['Mostafa Dehghani', 'Arash Mehrjou', 'Stephan Gouws', 'Jaap Kamps', 'Bernhard Schölkopf']",Accept,2018,"[7, 3, 9, 21, 24]","[12, 8, 11, 26, 29]","[117, 41, 16, 281, 777]","[50, 11, 9, 217, 380]","[56, 27, 7, 24, 286]","[11, 3, 0, 40, 111]","The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.  This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study.  My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work. Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines.
(4) the idea of using surrogate labels to learn representation is also not new. One example work is ""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"". The authors didn't compare their method with this one.","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,"['Sandeep Subramanian', 'Adam Trischler', 'Yoshua Bengio', 'Christopher J Pal']",Accept,2018,"[6, 3, 31, 22]","[10, 8, 36, 27]","[36, 82, 975, 230]","[16, 39, 405, 98]","[18, 43, 454, 109]","[2, 0, 116, 23]","This paper shows that learning sentence representations from a diverse set of tasks (skip-thought objective, MT, constituency parsing, and natural language inference) produces .
The main contribution of the paper is to show learning from multiple tasks improves the quality of the learned representations.
Experiments on various text classification and sentiment analysis datasets show that the proposed method is competitive with existing approaches.
There is an impressive number of experiments presented in the paper, but the results are a bit mixed, and it is not always clear that adding more tasks help.

I think this paper addresses an important problem of learning general purpose sentence representations. 
However, I am unable to draw a definitive conclusion from the paper. 
From Table 2, the best performing model is not always the one with more tasks. 
For example, adding a parsing objective can either improve or lower the performance quite significantly.
Could it be that datasets such as MRPC, SICK, and STSB require more understanding of syntax?
Even if this is the case, why adding this objective hurt performance for other datasets?
Importantly, it is also not clear whether the performance improvement comes from having more unlabeled data (even if it is trained with the same training objective) or having multiple training objectives.
Another question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations, given that it seems to be the easiest one to collect training data for.

The results for transfer learning and low resource settings are more positive.
However, it is not surprising that pretraining parts of the model on a large amount of unlabeled data helps when there is not a lot of labeled examples.

Overall, while the main contribution of the paper is that having multiple training objectives help learning better sentence, I am not yet convinced by the experiments that this is indeed the case.","[4, 8, 8]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Understanding Deep Neural Networks with Rectified Linear Units,"['Raman Arora', 'Amitabh Basu', 'Poorya Mianjy', 'Anirbit Mukherjee']",Accept,2018,"[12, 15, 3, 3]","[17, 20, 7, 8]","[118, 86, 21, 17]","[66, 21, 12, 2]","[39, 21, 9, 13]","[13, 44, 0, 2]","This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks. I summarize the main results as below:

(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN. Any smooth function can be approximated by such networks.

(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.

(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces

(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.

Among these results (1), (2), (4) are sort of known in the literature. This paper extends the existing results in some subtle ways. For (1), the authors show that the DNN has a tighter bound on the depth. For (2), the ""hard"" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum.

The stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU. Other than that, I don't get much more insight from the theoretical result. When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient. Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough.

Result (3) is more interesting as it is a new result. The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.  However, the construction seems artificial and these functions don't seem to be visually very complex.

Overall, this is an incremental work in the direction of studying the representation power of neural networks. The results might be of theoretical interest, but I doubt if a pragmatic ReLU network user will learn anything by reading this paper.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Improving GANs Using Optimal Transport,"['Tim Salimans', 'Han Zhang', 'Alec Radford', 'Dimitris Metaxas']",Accept,2018,"[7, 8, 3, 30]","[12, 13, 7, 35]","[60, 79, 28, 683]","[21, 33, 9, 428]","[36, 41, 19, 109]","[3, 5, 0, 146]","The paper introduces a new algorithm for training GANs based on the Earth Mover’s distance. In order to avoid biased gradients, the authors use the dual form of the distance on mini-batches, to make it more robust. To compute the distance between mini batches, they use the Sinkhorn distance. Unlike the original Sinkhorn distance paper, they use the dual form of the distance and do not have biased gradients. Unlike the Cramer GAN formulation, they use a mini-batch distance allowing for a better leverage of the two distributions, and potentially decrease variance in gradients.

Evaluation: the paper shows good results a battery of tasks, including a standard toy example, CIFAR-10 and conditional image generation, where they obtain better results than StackGAN. 

The paper is honest about its shortcomings, in the current set up the model requires a lot of computation, with best results obtained using a high batch size.

Would like to see: 
  * a numerical comparison with Cramer GAN, to see whether the additional  computational cost is worth the gains. 
  * Cramer GAN shows an increase in diversity, would like to see an analog experiment for conditional generation, like figure 3 in the Cramer GAN paper.
","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,"['Clemens Rosenbaum', 'Tim Klinger', 'Matthew Riemer']",Accept,2018,"[3, 15, 4]","[7, 20, 9]","[12, 38, 46]","[5, 22, 20]","[7, 15, 25]","[0, 1, 1]","Summary:
The paper suggests to use a modular network with a controller which makes decisions, at each time step, regarding the next nodule to apply. This network is suggested a tool for solving multi-task scenarios, where certain modules may be shared and others may be trained independently for each task. It is proposed to learn the modules with standard back propagation and the controller with reinforcement learning techniques, mostly tabular. 

-	page 4: 
In algorithm 2, line 6, I do not understand the reward computation. It seems that either a _{k+1} subscript index is missing for the right hand side R, or an exponent of n-k is missing on \gamma. In the current formula, the final reward affects all decisions without a decay based on the distance between action and reward gain. This issue should be corrected or explicitly stated.

The ‘collaboration reward’ is not clearly justified: If I understand correctly, It is stated that actions which were chosen often in the past get higher reward when chosen again. This may create a ‘winner takes all’ effect, but it is not clear why this is beneficial for good routing. Specifically, this term is optimized when a single action is always chosen with high probability – but such a single winner does not seem to be the behavior we want to encourage.

-	Page 5: It is not described clearly (and better: defined formally) what exactly is the state representation. It is said to include the current network output (which is a vector in R^d), the task label and the depth, but it is not stated how this information is condensed into a single integer index for the tabular methods. If I understand correctly, the state representation used in the tabular algorithms includes only the current depth. If this is true, this constitutes a highly restricted controller, making decisions only based on depth without considering the current output. 
-	The functional approximation versions are even less clear: Again it is not clear what information is contained in the state and how it is represented. In addition it is not clear in this case what network architecture is used for computation of the policy (PG) or valkue (Q-learning), and how exactly they are optimized.
-	The WPL algorithm is not clear to me
o	 In algorithm box 3, what is R_k? I do not see it defined anywhere. Is it related to \hat{R}? how?
o	Is it assumed that the actions are binary? 
o	I do not understand why positive gradients are multiplied with the action probability and negative gradients with 1 minus this probability. What is the source of a-symmetry between positive and negative gradients?  

-	Page 6:
o	It is not clear why MNist is tested over 200 examples, where there is a much larger test set available
o	In MIN-MTL I do not understand the motivation from creating superclasses composed of 5 random classes each: why do we need such arbitrary and un-natural class definitions? 

-	Page 7: 
The results on Cifar-100 are compared to several baselines, but not to the standard non-MTL solution: Solve the multi-class classification problem using a softmax loss and a unified, non routing architecture in which all the layers are shared by all classes, with the only distinction in the last classification layer. If the routing solution does not beat this standard baseline, there is no justification for its more complex structure and optimization.

-	Page 8: The author report that when training the controller with single agent methods the policy collapses into choosing a single module for most tasks. However, this is not-surprising, given that the action-based reward (whos strength is unclear) seems to promote such winner-takes-all behavior.

Overall:
-	The paper is highly unclear in its method representation
o	There is no unified clear notation. The essential symbols (states, actions, rewards) are not formally defined, and often it is not clear even if they are integers, scalars, or vectors. In notation existing, there are occasional notation errors. 
o	The reward is a) not clear, and b) not well motivated when it is explained, and c) not explicitly stated anywhere: it is said that the action-specific reward may be up to 10 times larger than the final reward, but the actual tradeoff parameter between them is not stated. Note that this parameter is important, as using a 10-times larger action-related reward means that the classification-related reward becomes insignificant.
o	The state representation used is not clear, and if I understand correctly, it includes only the current depth. This is a severely limited state representation, which does not enable to learn actions based on intermediate results
o	The continuous versions of the RL algorithms are not explained at all: no state representation, nor optimization is described.
o	The presentation suffers from severe over-generalization and lack of clarity, which disabled my ability to understand the network and algorithms for a specific case. Instead, I would recommend that in future versions of this document a single network, with a specific router  and set of decisions, and  with a single algorithm, will be explained with clear notation end-to-end

Beyond the clarity issues, I suspect also that the novelty is minor (if the state does not include any information about the current output) and that the empirical baseline is lacking. However, it is hard to judge these due to lack of clarity.


After revision:
- Most of the clarity issues were handled well, and the paper now read nicely
- It is now clear that routing is not done based on the current input (an example is not dynamically routed based on its current representation). Instead routing depends on the task and depth only.  This is still interesting, but is far from reaching context-dependent routing.
- The results presented are nice and show that task-dependent routing may be better than plain baseline or the stiching alternative.  However, since this is a task transfer issue, I believe several data size points should be tested. For example, as data size rises, the task-specific-all-fc alternative is expected to get stronger (as with more data, related task are less required for good performance).
 

- ","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On Unifying Deep Generative Models,"['Zhiting Hu', 'Zichao Yang', 'Ruslan Salakhutdinov', 'Eric P. Xing']",Accept,2018,"[6, 9, 16, 18]","[11, 14, 21, 23]","[112, 60, 419, 625]","[51, 24, 207, 339]","[54, 23, 201, 218]","[7, 13, 11, 68]","The authors develops a framework interpreting GAN algorithms as performing a form of variational inference on a generative model reconstructing an indicator variable of whether a sample is from the true of generative data distributions. Starting from the ‘non-saturated’ GAN loss the key result (lemma 1) shows that GANs minimizes the KL divergence between the generator(inference) distribution and a posterior distribution implicitly defined by the discriminator. I found the paper IWGAN and especially the AAVAE experiments quite interesting.  However the paper is also very dense and quite hard to follow at times - In general I think the paper would benefit from moving some content (like the wake-sleep part of the paper) to the appendix and concentrating more on the key results and a few more experiments as detailed in the comments / questions below.

Q1) What would happen if the KL-divergence minimizing loss proposed by Huszar (see e.g http://www.inference.vc/an-alternative-update-rule-for-generative-adversarial-networks/) was used instead of the “non-saturated” GAN loss - would the residial JSD terms in Lemma 1 cancel out then?

Q2) In Lemma 1 the negative JSD term looks a bit nasty to me e.g. in addition to KL divergence the GAN loss also maximises the JSD between the data and generative distributions. This JSD term acts in a somewhat opposite direction of the KL-divergence that we are interested in minimizing. Can the authors provide some more detailed comments / analysis on these two somewhat opposed terms - I find this quite important to include given the opposed direction of the JSD versus the KL term and that the JSD is ignored in e.g. section 4.1? secondly did the authors do any experiments on the the relative sizes of these two terms? I imagine it would be possible to perform some low-dimensional toy experiments where both terms were tractable to compute numerically?

Q3) I think the paper could benefit from some intuition / discussion of the posterior term q^r(x|y) in lemma 1 composed on the prior p_theta0(x) and discriminator q^r(y|x). The terms drops out nicely in math however i had a bit of a hard time wrapping my head around what minimizing the KL-divergence between this term and the inference distribution p(xIy). I know this is a kind of open ended question but i think it would greatly aid the reader in understanding the paper if more ‘guidance’ is provided instead of just writing “..by definition this is the posterior.’

Q4) In a similar vein to the above. It would be nice with some more discussion / definitions of the terms in Lemma 2. e.g what does “Here most of the components have exact correspondences (and the same definitions) in GANs and InfoGAN (see Table 1)” mean? 

Q5) The authors state that there is ‘strong connections’ between VAEs and GANs. I agree that both (after some assumptions) both minimize a KL-divergence (table 1) however to me it is not obvious how strong this relation is. Could the authors provide some discussion / thoughts on this topic?

Overall i like this work but also feel that some aspects could be improved: My main concern is that a lot of the analysis hinges on the JSD term being insignificant, but the authors to my knowledge does but provide any prof / indications that this is actually true. Secondly I think the paper would greatly benefit from concentration on fewer topics (e.g. maybe drop the RW topic as it feels a bit like an appendix) and instead provide a more throughout discussion of the theory (lemma 1 + lemma 2) as well as some more experiments wrt JSD term.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect,"['Xiang Wei', 'Boqing Gong', 'Zixia Liu', 'Wei Lu', 'Liqiang Wang']",Accept,2018,"[23, 10, 4, 5, 16]","[28, 15, 7, 10, 21]","[56, 154, 11, 59, 181]","[18, 75, 9, 18, 98]","[4, 72, 2, 3, 31]","[34, 7, 0, 38, 52]","This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al.  [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty ""encouraging"" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning.  

In terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator’s response on the perturbed points.  The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10.  They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. 

The authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. 

Where the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. 
[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan.

[2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. 

","[7, 4, 6]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Unsupervised Neural Machine Translation,"['Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre', 'Kyunghyun Cho']",Accept,2018,"[2, 17]","[6, 22]","[3, 149]","[3, 84]","[0, 4]","[0, 61]","The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest. While the results are interesting I find very few original ideas in this paper. Please find my comments/questions/suggestions below:

1) The authors mention that there are 3 important aspects in which their model differs from a standard NMT architecture. All the 3 differences have been adapted from existing works. The authors clearly acknowledge and cite the sources. Even sharing the encoder using cross lingual embeddings has been explored in the context of multilingual NER (please see https://arxiv.org/abs/1607.00198). Because of this I find the paper to be a bit lacking on the novelty quotient. Even backtranslation has been used successfully in the past (as acknowledged by the authors). Unsupervised MT in itself is not a new idea (again clearly acknowledged by the authors).

2) I am not very convinced about the idea of denoising. Specifically, I am not sure if it will work for arbitrary language pairs. In fact, I think there is a contradiction even in the way the authors write this. On one hand, they want to ""learn the internal structure of the languages involved"" and on the other hand they deliberately corrupt this structure by adding noise. This seems very counter-intuitive and in fact the results in Table 1 suggest that it leads to a drop in performance. I am not very sure that the analogy with autoencoders holds in this case.

3) Following up on the above question, the authors mention that ""We emphasize, however, that it is not possible to use backtranslation alone without denoising"". Again, if denoising itself leads to a drop in the performance as compared to the nearest neighbor baseline then why use backtranslation in conjunction with denoising and not in conjunction with the baseline itself. 

4) This point is more of a clarification and perhaps due to my lack of understanding. Backtranslation to generate a pseudo corpus makes sense only after the model has achieved a certain (good) performance. Can you please provide details of how long did you train the model (with denoising?) before producing the backtranslations ?

5) The authors mention that 100K parallel sentences may be insufficient for training a NMT system. However, this size may be decent enough for  a PBSMT system. It would be interesting to see the performance of a PBSMT system trained on 100K parallel sentences. 

6) How did you arrive at the beam size of 12 ? Was this a hyperparameter? Just curious.

7) The comparable NMT set up is not very clear. Can you please explain it in detail ? In the same paragraph, what exactly do you mean by ""the supervised system in this paper is relatively small?""","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,"['Aleksandar Bojchevski', 'Stephan Günnemann']",Accept,2018,"[2, 10]","[7, 15]","[38, 274]","[17, 145]","[19, 104]","[2, 25]","This paper proposes Graph2Gauss (G2G), a node embedding method that embeds nodes in attributed graphs (can work w/o attributes as well) into Gaussian distributions rather than conventionally latent vectors. By doing so, G2G can reflect the uncertainty of a node's embedding. The authors then use these Gaussian distributions and neighborhood ranking constraints to obtain the final node embeddings. Experiments on link prediction and node classification showed improved performance over several strong embedding methods. Overall, the paper is well-written and the contributions are remarkable. The reason I am giving a less possible rating is that some statements are questionable and can severely affect the conclusions claimed in this paper, which therefore requires the authors' detailed response. I am certainly willing to change my rating if the authors clarify my questions.

Major concern 1: Is the latent vector dimension L really the same for G2G and other compared methods? 
In the first paragraph of Section 4, it is stated that ""in all experiments if the competing techniques use an embedding of
dimensionality L, G2G’s embedding is actually only half of this dimensionality so that the overall number of ’parameters’ per node (mean vector + variance terms) matches L.""  This setting can be wrong since the degree of freedom of a L-dim Gaussian distribution should be L+L(L-1)/2, where the first term corresponds to the mean and the second term corresponds to the covariance. If I understand it correctly, when any compared embedding method used an L-dim vector, the authors used the dimension of L/2. But this setting is wrong if one wants the overall number of ’parameters’ per node (mean vector + variance terms) matches L, as stated by the authors. Fixing L, the equivalent dimension L_G2G for G2G should be set such that L_G2G +L_G2G (L_G2G -1)/2=L, not 2*L_G2G=L.  Since this setting is universal to the follow-up analysis and may severely degrade the performance of GSG due to less embedding dimensions, I hope the authors can clarify this point.

Major concern 2: The claim on inductive learning
Inductive learning is one of the major contributions claimed in this paper. The authors claim G2G can learn an embedding of an unseen node solely based on their attributes. However, is it not clear why this can be done. In the learning stage of Sec. 3.3, the attributes do not seem to play a role in the energy function. Also, since no algorithm descriptions are available, it's not clear how using only an unseen node's attributes can yield a good embedding under G2G work (so does Sec. 4.5). 
Moreover, how does it compare to directly using raw user attributes for these tasks?

Minor concern/suggestions: The ""similarity"" measure in section 3.1 using KL divergence should be better rephased by ""dissimilarity"" measure. Otherwise, one has a similarity measure $Delta$ and wants it to increase as the hop distance k decreases (closer nodes are more similar). But the ranking constraints are somewhat counter-intuitive because you want $Delta$ to be small if nodes are closer. There is nothing wrong with the ranking condition, but rather an inconsistency between the use of ""similarity"" measure for KL divergence. 
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Parametrized Hierarchical Procedures for Neural Programming,"['Roy Fox', 'Richard Shin', 'Sanjay Krishnan', 'Ken Goldberg', 'Dawn Song', 'Ion Stoica']",Accept,2018,"[12, 9, 5, 32, 20, 24]","[17, 13, 10, 37, 25, 29]","[51, 38, 96, 498, 440, 431]","[21, 21, 47, 282, 254, 276]","[29, 16, 33, 114, 154, 97]","[1, 1, 16, 102, 32, 58]","In the paper titled ""Parameterized Hierarchical Procedures for Neural Programming"", the authors proposed ""Parametrized Hierarchical Procedure (PHP)"", which is a representation of a hierarchical procedure by differentiable parametrization. Each PHP is represented with two multi-layer perceptrons with ReLU activation, one for its operation statement and one for its termination statement. With two benchmark tasks (NanoCraft and long-hand addition), the authors demonstrated that PHPs are able to learn neural programs accurately from smaller amounts of strong/weak supervision. 

Overall the paper is well-written with clear logic and accurate narratives. The methodology within the paper appears to be reasonable to me. Because this is not my research area, I cannot judge its technical contribution. ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[1, 2, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Eigenoption Discovery through the Deep Successor Representation,"['Marlos C. Machado', 'Clemens Rosenbaum', 'Xiaoxiao Guo', 'Miao Liu', 'Gerald Tesauro', 'Murray Campbell']",Accept,2018,"[8, 3, 6, 8, 32, 38]","[13, 7, 11, 13, 37, 42]","[51, 12, 86, 50, 118, 78]","[19, 5, 39, 24, 67, 36]","[28, 7, 38, 23, 28, 23]","[4, 0, 9, 3, 23, 19]","Eigenoption Discovery Through the Deep Successor Representation

The paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called “eigenoptions”. In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options. They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features. The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one. Interestingly, the way they do so is through the successor representation (SR). Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP. Thus, if we have the SR matrix we can replace the Laplacian mentioned above. This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs. Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation. The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over).

This is a well-written paper with interesting (and potentially useful) insights. I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated.

One question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options). In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions. The way the authors propose to do it is to have the agent follow a random policy. So, in order to have options that lead to more direct, ""purposeful"" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space. 

This problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue. One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about. Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me. To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space. In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper. Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible). More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance).

Another point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2. In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than “random options” built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals. This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well. 

Interestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative. As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies. Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them.","[7, 6, 9]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Skip Connections Eliminate Singularities,"['Emin Orhan', 'Xaq Pitkow']",Accept,2018,"[2, 8]","[7, 13]","[17, 33]","[4, 12]","[12, 17]","[1, 4]","This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function. The discussion is largely based on a sequence of experiments, some of which are interesting and insightful. The discussion here can be useful for other researchers. 

My main concern is that the result here is purely empirical, with no concrete theoretical justification. What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation. Therefore, i found many of the discussion to be questionable. I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.
 
Do you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues? Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper. 

All the conclusions are based on one or two datasets. Could you consider testing the result on more different datasets to verify if the results are generalizable? ","[6, 8, 8]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Distributed Prioritized Experience Replay,"['Dan Horgan', 'John Quan', 'David Budden', 'Gabriel Barth-Maron', 'Matteo Hessel', 'Hado van Hasselt', 'David Silver']",Accept,2018,"[4, 2, 9, 20]","[9, 7, 14, 25]","[40, 68, 499, 631]","[6, 18, 124, 206]","[4, 28, 170, 111]","[30, 22, 205, 314]","This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner. The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers. Using this system, the authors are able to harness much more compute to learn very high quality policies in little time. The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow. 

It’s hard to take issue with a paper that has such overwhelmingly convincing experimental results. However, there are a couple additional experiments that would be quite nice:
•	In order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X). 
•	It would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time. For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?

Pros:
•	Well written and clear.
•	Very impressive results.
•	It’s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.

Cons:
•	Hard to replicate experiments without the deep computational pockets of DeepMind.
","[9, 6, 7]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,"['Shiyu Liang', 'Yixuan Li', 'R. Srikant']",Accept,2018,"[5, 4, 29]","[10, 9, 34]","[20, 55, 378]","[9, 23, 165]","[7, 29, 75]","[4, 3, 138]","
-----UPDATE------

The authors addressed my concerns satisfactorily. Given this and the other reviews I have bumped up my score from a 5 to a 6.

----------------------


This paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs. This is a novel use of existing methods.

Some roughly chronological comments follow:

In the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.

The paper is quite well written aside from some grammatical issues. In particular, articles are frequently missing from nouns. Some sentences need rewriting (e.g. in 4.1 ""which is as well used by Hendrycks..."", in 5.2 ""performance becomes unchanged"").

 It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven.

I'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images. I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications). Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?

When you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again.

In terms of experimentation it would be interesting to see the reciprocal of the results between two datasets. For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?

Section 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results. This may just be a matter of taste.

I did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one? (This is in part, addressed in the CIFAR80 20 experiments in the appendices).

This appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods).

Pros:
- Baseline performance is exceeded by a large margin
- Novel use of adversarial perturbation and temperature
- Interesting analysis

Cons:
- Doesn't introduce and novel methods of its own
- Could do with additional experiments (as mentioned above)
- Minor grammatical errors
","[6, 6, 9]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"['Jie Chen', 'Tengfei Ma', 'Cao Xiao']",Accept,2018,"[11, 3, 4]","[16, 8, 9]","[79, 81, 149]","[23, 38, 66]","[33, 37, 62]","[23, 6, 21]","The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. It further explores variance reduction to speed up training via importance sampling.  The idea comes with theoretical support and experimental studies.

Some questions are as follows:

1) could you elaborate on n/t_l  in (5) that accounts for the normalization difference between matrix form (1) and the integral form (2) ?

2) In Prop.2., there seems no essential difference between the two parts, as e(v) also depends on how the u_j's are sampled.

3) what loss g is used in experiments?","[7, 6, 8, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[2, 4, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Model-Ensemble Trust-Region Policy Optimization,"['Thanard Kurutach', 'Ignasi Clavera', 'Yan Duan', 'Aviv Tamar', 'Pieter Abbeel']",Accept,2018,"[4, 2, 11, 8, 17]","[7, 4, 15, 13, 22]","[19, 18, 52, 108, 608]","[9, 8, 28, 47, 291]","[10, 10, 19, 57, 291]","[0, 0, 5, 4, 26]","This paper presents a simple model-based RL approach, and shows that with a few small tweaks to more ""typical"" model-based procedures, the methods can substantially outperform model-free methods on continuous control tasks.  In particular, the authors show that by 1) using an ensemble of models instead of a single models, 2) using TRPO to optimize the policy based upon these models (rather that analytical gradients), and 3) using the model ensemble to validate when to stop policy optimization, then a simple model-based approach actually can outperform model-free methods.

Overall, I think this is a nice paper, and worth accepting.  There is very little actually new here, of course: the actual model-based method is entirely standard except with the additions above (which are also all fairly standard approaches in isolation).  But at a higher level, the fact that such simple model-based approaches work better than somewhat complex model free approaches actually is the point of the paper to me.  While the general theme of model-based RL outperforming model-free RL is not new (Atkeson and Santamaria (1997) comes to a similar conclusion) its good to see this same pattern demonstrated ""officially"" on modern RL benchmarks, especially since the _completely_ naive strategy of using a single model and more standard policy optimization doesn't perform as well.

Naturally, there is some question as to whether the work here is novel enough to warrant publication, but I think the overall message of the paper is strong enough to overcome fairly minimal contribution from an algorithmic perspective.  I did also have a few general concerns that I think could be discussed with a bit more detail in the paper:
1) The choice of this particular model ensemble to represent uncertainty seems rather ad-how.  Why is it sufficient to simply learn N models with different initial weights?  It seems that the likely cause for this is that the random initial weights may lead to very different behavior in the unobserved parts of the space (i.e., portions of the state space where we have no samples), and thus.  But it seems like there are much more principled ways of overcoming this same problem, e.g. by using an actual Bayesian neural net, directly modeling uncertainty in the forward model, or using generative model approaches.  There's some discussion of this point in the introduction, but I think a bit more explanation about why the model ensemble is expected to work well for this purpose.
2) Likewise, the fact the TRPO outperforms more standard gradient methods is somewhat surprising.  How is the model ensemble being treated during BPTT?  In the described TRPO method, the authors use a different model at each time step, sampling uniformly.  But it seems like a single model is used for each rollout in the proposed BPTT method?  If so, it's not surprising that this approach performs worse.  But it seems like one could backprop through the different per-timestep models just as easily, and it would remove one additional source of difference between the two settings.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks,"['Jinsung Yoon', 'William R. Zame', 'Mihaela van der Schaar']",Accept,2018,"[4, 22, 19]","[9, 26, 24]","[75, 45, 829]","[27, 16, 344]","[33, 17, 230]","[15, 12, 255]","This paper proposes a novel method to solve the problem of active sensing from a new angle (Essentially, the active sensing is a kind of method that decides when (or where) to take new measurements and what measurements we should conduct at that time or (place)). By taking advantage of the characteristics of long-term memory and Bi-directionality of Bi-RNN and M-RNN, deep sensing can model multivariate time-series signals for predicting future labels and estimating the values of new measurements. The architecture of Deep Sensing basically consists of three components: 
1. Interpolation and imputation for each of channels where missing points exist;
2. Prediction for the future labels in terms of the whole multivariate signals (The signal is a time-series data and made up of multiple channels, there is supposed to be a measured label for each moment of the signal); 
3. Active sensing for the future moments of each of the channels. 

Pros

The novelty of this paper lies in using a neural network structure to solve a traditional statistical problem which was usually done by a Bayesian approach or using the idea of the stochastic process. 

A detailed description of the network architecture is provided and each of the configurations has been fully illustrated.  The explanation of the structure of the combined RNNs is rigorous but clear enough of understanding. 

The method was tested on a large real dataset and got a really promising result based several rational assumptions (such as assuming some of the points are missing for evaluating the error of the interpolation & imputation).

Cons

How and why the architecture is designed in this way should be further discussed or explained. Some of the details of the design could be inferred indirectly. But somewhere like the structure of the interpolation in Fig.3 doesn't have any further discussion. For example, why using GRU based RNN, and how Bi-RNN benefits here. 
","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Graph Attention Networks,"['Petar Veličković', 'Guillem Cucurull', 'Arantxa Casanova', 'Adriana Romero', 'Pietro Liò', 'Yoshua Bengio']",Accept,2018,['only informal'],['only informal'],[],[],[],[],"This paper has proposed a new method for classifying nodes of a graph. Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing (Transductive) and in the scenario that the test is on a completely new graph (Inductive).
Each layer of the network consists of feature representations for all of the nodes in the Graph. A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours (including the node). The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k. They also can incorporate the multi-head attention mechanism and average/concatenate the output of each head.

Originality:
Authors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism. However, the significance of the attention mechanism has not been studied in the experiments. For example by reporting the results when attention is turned off (1/|N_i| for every node) and only a 0-1 mask for neighbours is used. They have compared with GraphSAGE only on PPI dataset. I would change my rating if they show that the 33% gain is mainly due to the attention in compare to other hyper-parameters. [The experiments are now more informative. Thanks]
Also, in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and doesn't aggregate over all the neighbours in order to keep its computational footprint consistent. However, the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE.

Pros:
- Interesting combination of attention and local graph representation learning. 
- Well written paper. It conveys the idea clearly.
- State-of-the-art results on three datasets.

Cons:
- When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network. Therefore, limiting its adaptability to broader class of graph datasets. 
- Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Unsupervised Representation Learning by Predicting Image Rotations,"['Spyros Gidaris', 'Praveer Singh', 'Nikos Komodakis']",Accept,2018,"[4, 5, 15]","[9, 10, 19]","[38, 30, 130]","[18, 8, 71]","[19, 17, 28]","[1, 5, 31]","Strengths:
* Very simple strategy for unsupervised learning of deep image features. Simplicity of approach is a good quality in my view.
* The rationale for the effectiveness of the approach is explained well.
* The representation learned from unlabeled data is shown to yield strong results on image categorization (albeit mostly in scenarios where the unsupervised features have been learned from the *same* dataset where classification is performed -- more on this below).
* The image rotations are implemented in terms of flipping and transposition, which do not create visual artifacts easily recognizable by deep models.

Weaknesses:
* There are several obvious additional experiments that, in my view, would greatly strengthen this work:
1. Nearly all of the image categorization results (with the exception of those in Table 4) are presented for the contrived scenario where the unsupervised representation is learned from the same training set as the one used for the final supervised training of the categorization model. This is a useless application scenario. If labels for the training examples are available, why not using them for feature learning given that this leads to improved performance (see results in Tables)? More importantly, this setup does not allow us to understand how general the unsupervised features are. Maybe they are effective  precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish... I would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or, even better, from a dataset of randomly selected images that lack categorical coherence (e.g., photos randomly picked from the Web, such as Flickr pics).
2. In nearly all the experiments, the classifier is built on top of the frozen unsupervised features. This is in contrast with the common practice of finetuning the entire pretrained unsupervised net on the supervised task. It'd be good to know why the authors opted for the different setup and to see in any case some supervised finetuning results.
3. It would be useful to see the accuracy per class both when using unsupervised features as well as fully-supervised features. There are many objects that have a canonical pose/rotation in the world. Forcing the unsupervised features to distinguish rotations of such objects may affect the recognition accuracy for these classes. Thus, my request for seeing how the unsupervised learning affects class-specific accuracy.
4. While the results in Table 2 are impressive, it appears that the different unsupervised learning methods reported in this table are based on different architectures. This raises the question of whether performance gains are due to the better mechanism for unsupervised learning or rather the better network architecture.
5. I do understand that using only 0, 90, 180 and 270 degree rotations eliminates the issue of potentially recognizable artifacts. Nevertheless, it'd be interesting to see what happens empirically when the number of discrete rotations is increased, e.g., by including 45, 135, 225 and 315 degree rotations. And what happens if you use only 0 and 180? Or only 90 and 270?
* While the paper is easy to understand, at times the writing is poor and awkward (e.g., opening sentence of intro, first sentence in section 2.2).","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,"['Tomer Galanti', 'Lior Wolf', 'Sagie Benaim']",Accept,2018,"[2, 19, 6]","[7, 24, 11]","[37, 413, 53]","[13, 213, 20]","[22, 165, 28]","[2, 35, 5]","Quality:
The paper appears to be correct

Clarity:
the paper is clear, although more formalization would help sometimes

Originality
The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know.

Significance
The points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.

Pros/cons
Pros
-Adresses an important problem in representation learning
-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings
-A new cross domain mapping is proposed
-Large set of experiments
Cons
-Some parts deserve more formalization/justification
-Too many materials for a conference paper
-The cost of the algorithm seems high 

Summary:
This paper studies the problem of unsupervised learning of semantic mappings. It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping. A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.
A large set of experiments are provided to support the claims of the paper.


Comments:

-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect.

-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss.
The approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.

-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.

Typo:
Section 5.1 is build of -> is built of
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Lifelong Learning with Dynamically Expandable Networks,"['Jaehong Yoon', 'Eunho Yang', 'Jeongtae Lee', 'Sung Ju Hwang']",Accept,2018,"[2, 13, 2, 9]","[7, 18, 2, 14]","[26, 122, 2, 207]","[10, 68, 1, 95]","[15, 49, 1, 108]","[1, 5, 0, 4]","The topic is of great interest to the community, and the ideas explored by the authors are reasonable, but I found the conclusion less-than-clear. Mainly, I was not sure how to interpret the experimental findings, and did not have a clear picture of the various models being investigated (e.g. ""base DNN regularized with l2""), or even of the criteria being examined. What is ""learning capacity""? (If it's number of model parameters, the authors should just say, ""number of parameters""). The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was.

What does the ""batch model"" refer to?

re. "" 11.9%p − 51.8%p""; remove ""p""?

Reference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?

re. ""... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL"": isn't the number of parameters matched? If so, why is the ""learning capacity"" different? What do the authors mean exactly by ""learning capacity""?

re. Figure 3, e.g. ""Average per-task performance of the models over number of task t"": this is a general point, but usually the expression ""<f(x)> vs. <x>"" is used rather than ""<f(x)> over <x>"" when describing a plot.

""DNN: dase (sic) DNN"": how is this trained?


","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Parallelizing Linear Recurrent Neural Nets Over Sequence Length,"['Eric Martin', 'Chris Cundy']",Accept,2018,"[24, 2]","[29, 7]","[17, 13]","[12, 5]","[1, 8]","[4, 0]","This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure.

The paper is written very well, with explanation (as opposed to obfuscation) as the goal. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.

The paper provides argument and experimental evidence against the rotation used typically in RNNs. While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.

While the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Robust Rewards with Adverserial Inverse Reinforcement Learning,"['Justin Fu', 'Katie Luo', 'Sergey Levine']",Accept,2018,"[4, 2, 10]","[8, 7, 15]","[34, 17, 743]","[15, 8, 326]","[18, 9, 396]","[1, 0, 21]","The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce ""disentangled rewards"", demonstrating that they transfer well to the same task under different transition dynamics.

The need for ""state-only"" rewards is a useful insight and is covered fairly well in the paper. The need for an ""adversarial"" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak.

Detailed feedback:

""deployed in at test-time on environments"" -> ""deployed at test time in environments""?

""which can effectively recover disentangle the goals"" -> ""which can effectively disentangle the goals""?

""it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy"": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it?

""r high-dimensional (Finn et al., 2016b) Wulfmeier"" -> ""r high-dimensional (Finn et al., 2016b). Wulfmeier"".

""also consider learning cost function with"" -> ""also consider learning cost functions with""?

""o learn nonlinear cost function have"" -> ""o learn nonlinear cost functions have"".

"" are not robust the environment changes"" -> "" are not robust to environment changes""?

""We present a short proof sketch"": It is unclear to me what is being proven here. Please state the theorem.

""In the method presented in Section 4, we cannot learn a state-only reward function"": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here.

""AIRLperforms"" -> ""AIRL performs"".

Figure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret.

""must reach to goal"" -> ""must reach the goal""?

""pointmass"" -> ""point mass"". (Multiple times.)

Amin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.)

Perhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare.
","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Reinforcement Learning Algorithm Selection,"['Romain Laroche', 'Raphael Feraud']",Accept,2018,"[1, -3, 14, 0, 0]","[5, 1, 18, 4, 4]","[23, 1, 2, 4, 4]","[23, 1, 1, 4, 4]","[0, 0, 0, 0, 0]","[0, 0, 1, 0, 0]","The authors consider the problem of dynamically choosing between several reinforcement learning algorithms for solving a reinforcement learning with discounted rewards and episodic tasks. The authors propose the following solution to the problem:
- During epochs of exponentially increasing size (this technique is well known in the bandit litterature and is called a ""doubling trick""), the various reinforcement learning algorithms are ""frozen"" (i.e. they do not adapt their policy) and the K available algorithms are sampled using the UCB1 algorithm  in order to discover the one which yields the highest mean reward.

Overall the paper is well written, and presents some interesting novel ideas on aggregating reinforcement learning algorithms. Below are some remarks:

- An alternative and perhaps simpler formalization of the problem would be learning with expert advice (using algorithms such as ""follow the perturbed leader""), where each of the available reinforcement learning algorithms acts as an expert. What is more, these algorithms usually yield O(sqrt(T)log(T)), which is the regret obtained by the authors in the worse case (where all the learning algorithms do converge to the optimal policy at the optimal speed O(1/sqrt(T)). It would have been good to see how those approaches perform against the proposed algorithms. 
- The authors use UCB1, but they did not try KL-UCB, which is stricly better (in fact it is optimal for bounded rewards). In particular the numerical performance of the latter is usually vastly better than the former, especially when rewards have a small variance.
- The performance measure used by the authors is rather misleading (""short sighted regret""): they compare what they obtain to what the policy discovered by the best reainforcement learning algorithm \underline{based on the trajectories they have seen}, and the trajectories themselves are generated by the choices made by the algorthms at previous time. Ie in general, there might be cases in which one does not explore enough with this approach (i.e one does not try all state-action pairs enough), so that while this performance measure is low, the actual regret is very high and the algorithm does not learn the optimal policy at all (while this could be done by simply exploring at random log(T) times ...).
","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"['Wieland Brendel *', 'Jonas Rauber *', 'Matthias Bethge']",Accept,2018,"[8, 2, 20]","[13, 5, 25]","[63, 18, 142]","[22, 4, 52]","[34, 12, 58]","[7, 2, 32]","The authors identify a new security threat for deep learning: Decision-based adversarial attacks. This new class of attacks on deep learning systems requires from an attacker only the knowledge of class labels (previous attacks required more information, e.g., access to a gradient oracle). Unsurprisingly, since the attacker has so few information, such kind of attacks involves quite a lot trial and error. The authors propose one specific attack instance out of this class of attacks. It works as follows.

First, an initial point outside of the benign region is guessed. Then multiple steps towards the decision boundary is taken, finally reaching the boundary (I am not sure about the precise implementation, but it seems not crucial; the author may please check whether their description of the algorithm is really reproducable). Then, in a nutshell, a random walk on a sphere centered around the original, benign point is performed, where after each step, the radius of the sphere is slightly reduced (drawing the point closer to the original point), if and only if the resulting point still is outside of the benign region.

The algorithm is evaluated on the following datasets: MNIST, CIFAR, VGG19, ResNet50, and InceptionV3.

The paper is rather well written and structured. The text was easy to follow. I suggest that a self-contained description of the problem setting (assumptions on attacker and defender; aim?) shall be added to the camera-ready version (being not familiar with the area, I had to read a couple of papers to get a feeling for the setting, before reviewing this paper). As in many DL papers these days, there really isn't any math in it worth a mention; so no reason here to say anything about mathematical soundness. The authors employ a reasonable evaluation criterion in their experiments: the median squared Euclidean distance between the original and adversarially modified data point. The results show consistent improvement for most data sets. 

In summary, this is an innovative paper, proposing a new class of attacks that totally makes sense in my opinion. Apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready, this is a nice, fresh paper, that might spur more attacks (and of course new defenses) from the new class of decision-based attacks. It is worth to note that the authors show that distillation is not a useful defense against such attacks, so we may expect follow-up proposing useful defenses against the new attack (which BTW is shown to be about a factor of 10 in terms of iterations more costly than the SOTA).","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generating Wikipedia by Summarizing Long Sequences,"['Peter J. Liu*', 'Mohammad Saleh*', 'Etienne Pot', 'Ben Goodrich', 'Ryan Sepassi', 'Lukasz Kaiser', 'Noam Shazeer']",Accept,2018,"[3, 14, 4, 10, 1, 14, 9]","[8, 19, 8, 13, 5, 18, 13]","[32, 31, 11, 14, 12, 84, 62]","[9, 11, 4, 7, 5, 44, 23]","[21, 11, 6, 6, 7, 32, 36]","[2, 9, 1, 1, 0, 8, 3]","The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences. The outputs are evaluated by ROUGE-L and test perplexity. There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences.

This paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.

Evaluation:
Currently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task?

Extractiveness analysis:
I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores? This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help. 

A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not. 

I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here.

Overall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work.

----
After reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper. This is a very nice contribution.
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Variational Message Passing with Structured Inference Networks,"['Wu Lin', 'Nicolas Hubacher', 'Mohammad Emtiyaz Khan']",Accept,2018,"[9, 1, 15]","[14, 1, 20]","[37, 1, 72]","[19, 1, 34]","[10, 0, 36]","[8, 0, 2]","This paper presents a variational inference algorithm for models that contain
deep neural network components and probabilistic graphical model (PGM)
components.
The algorithm implements natural-gradient message-passing where the messages
automatically reduce to stochastic gradients for the non-conjugate neural
network components. The authors demonstrate the algorithm on a Gaussian mixture
model and linear dynamical system where they show that the proposed algorithm
outperforms previous algorithms. Overall, I think that the paper proposes some
interesting ideas, however, in its current form I do not think that the novelty
of the contributions are clearly presented and that they are not thoroughly
evaluated in the experiments.

The authors propose a new variational inference algorithm that handles models
with deep neural networks and PGM components. However, it appears that the
authors rely heavily on the work of (Khan & Lin, 2017) that actually provides
the algorithm. As far as I can tell this paper fits inference networks into
the algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an
inference network to generate potentials for a conditionally-conjugate
distribution and ii) introducing new PGM parameters to decouple the inference
network from the model parameters. These ideas are a clever solution to work
inference networks into the message-passing algorithm of (Khan & Lin, 2017),
but I think the authors may be overselling these ideas as a brand new algorithm.
I think if the authors sold the paper as an alternative to (Johnson, et al., 2016)
that doesn't suffer from the implicit gradient problem the paper would fit into
the existing literature better.

Another concern that I have is that there are a lot of conditiona-conjugacy
assumptions baked into the algorithm that the authors only mention at the end
of the presentation of their algorithm. Additionally, the authors briefly state
that they can handle non-conjugate distributions in the model by just using
conjugate distributions in the variational approximation. Though one could do
this, the authors do not adequately show that one should, or that one can do this
without suffering a lot of error in the posterior approximation. I think that
without an experiment the small section on non-conjugacy should be removed.

Finally, I found the experimental evaluation to not thoroughly demonstrate the
advantages and disadvantages of the proposed algorithm. The algorithm was applied
to the two models originally considered in (Johnson, et al., 2016) and the
proposed algorithm was shown to attain lower mean-square errors for the two
models. The experiments do not however demonstrate why the algorithm is
performing better. For instance, is the (Johnson, et al., 2016) algorithm
suffering from the implicit gradient? It also would have been great to have
considered a model that the (Johnson, et. al., 2016) algorithm would not work
well on or could not be applied to show the added applicability of the proposed
algorithm.

I also have some minor comments on the paper:
- There are a lot of typos.
- The first two sentences of the abstract do not really contribute anything
  to the paper. What is a powerful model? What is a powerful algorithm?
- DNN was used in Section 2 without being defined.
- Using p() as an approximate distribution in Section 3 is confusing notation
  because p() was used for the distributions in the model.
- How is the covariance matrix parameterized that the inference network produces?
- The phrases ""first term of the inference network"" are not clear. Just use The
  DNN term and the PGM term of the inference networks, and better still throw
  in a reference to Eq. (4).
- The term ""deterministic parameters"" was used and never introduced.
- At the bottom of page 5 the extension to the non-conjugate case should be
  presented somewhere (probably the appendix) since the fact that you can do
  this is a part of your algorithm that's important.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Sobolev GAN,"['Youssef Mroueh', 'Chun-Liang Li', 'Tom Sercu', 'Anant Raj', 'Yu Cheng']",Accept,2018,"[1, 5]","[2, 10]","[4, 86]","[2, 39]","[2, 40]","[0, 7]","Summary: The authors provide another type of GAN--the Sobolev GAN--which is the typical setup of a GAN but using a function class F for which f belongs to F iff \grad f belongs to L^2(mu). They relate this MMD to the Cramer and Fisher distance and then produce a recipe for training GANs with this sort of function class. In their empirical examples, they show it has similar performance to the WGAN-GP.

Overall, the paper has some interesting mathematical relationships to other MMDs. However, I finished reading the paper wondering why one would want to trust this GAN over any of the other GANs. I may have missed it, but I didn't see any compelling theoretical reason the gradients from this method would prove superior to many of the other GANs in existence today. The authors argue ""from [equation 5] we see that we are comparing CDFs, which are better behaved on discrete distributions,"" but I wasn't sure what exactly to make of this comment.

Nits:
* The ""Stein metric"" is actually called the Stein discrepancy [see Gorham & Mackey (2015) Measuring Sample Quality using Stein's Method].","[6, 7, 8, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[3, 3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,"['Tianmin Shu', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[4, 10, 12]","[9, 15, 17]","[47, 383, 229]","[21, 165, 111]","[22, 208, 111]","[4, 10, 7]","This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar. The experiments show that the method is better than a flat policy for learning a simple set of block-related skills in minecraft (find, get, put, stack) and generalizes better to a modification of the environment (size of room). The sequence of subtasks generated by the policy are interpretable.

Strengths:
- The grammar and policies are trained using a sparse reward upon task completion. 
- The method is well ablated; Figures 4 and 5 answered most questions I had while reading.
- Theoretically, the method makes few assumptions about the environment and the relationships between tasks.
- The interpretability of the final behaviors is a good result. 

Weaknesses:
- The implementation gives the agent a -0.5 reward if it generates a currently unexecutable goal g’. Providing this reward requires knowing the full state of the world. If this hack is required, then this method would not be useful in a real world setting, defeating the purpose of the sparse reward mentioned above. I would really like to see how the method performs without this hack. 
- There are no comparisons to other multitask or hierarchical methods. Progressive Networks or Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning seem like natural comparisons.
- A video to show what the environments and tasks look like during execution would be helpful.
- The performances of the different ablations are rather close. Please a standard deviation over multiple training runs. Also, why does figure 4.b not include a flat policy?
- The stages are ordered in a semantically meaningful order (find is the first stage), but the authors claim that the order is arbitrary. If this claim is going to be included in the paper, it needs to be proven (results shown for random orderings) because right now I do not believe it. 

Quality:
The method does provide hierarchical and interpretable policies for executing instructions, this is a meaningful direction to work on.

Clarity:
Although the method is complicated, the paper was understandable.

Originality and significance:
Although the method is interesting, I am worried that the environment has been too tailored for the method, and that it would fail in realistic scenarios. The results would be more significant if the tasks had an additional degree of complexity, e.g. “put blue block next to the green block” “get the blue block in room 2”. Then the sequences of subtasks would be a bit less linear (e.g., first need to find blue, then get, then find green, then put). At the moment the tasks are barely more than the actions provided in the environment.

Another impedance to the paper’s significance is the number of hacks to make the method work (ordering of stages, alternating policy optimization, first training each stage on only tasks of previous stage). Because the method is only evaluated on one simple environment, it unclear which hacks are for the method generally, and which hacks are for the method to work on the environment.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Memory Architectures in Recurrent Neural Network Language Models,"['Dani Yogatama', 'Yishu Miao', 'Gabor Melis', 'Wang Ling', 'Adhiguna Kuncoro', 'Chris Dyer', 'Phil Blunsom']",Accept,2018,"[10, 7, 5, 9, 3, 20, 15]","[14, 11, 9, 13, 7, 24, 20]","[74, 45, 22, 68, 29, 268, 178]","[33, 25, 7, 41, 12, 161, 91]","[34, 19, 13, 21, 15, 90, 70]","[7, 1, 2, 6, 2, 17, 17]","The main contribution of this paper are:
(a) a proposed extension to continuous stack model to allow multiple pop operation,
(b) on a language model task, they demonstrate that their model gives better perplexity than comparable LSTM and attention model, and 
(c) on a syntactic task (non-local subject-verb agreement), again, they demonstrate better performance than comparable LSTM and attention model.

Additionally, the paper provides a nice introduction to the topic and casts the current models into three categories -- the sequential memory access, the random memory access and the stack memory access models. 

Their analysis in section (3.4) using the Venn diagram and illustrative figures in (3), (4) and (5) provide useful insight into the performance of the model.","[8, 6, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Wavelet Pooling for Convolutional Neural Networks,"['Travis Williams', 'Robert Li']",Accept,2018,"[1, 18]","[6, 23]","[20, 55]","[8, 24]","[9, 15]","[3, 16]","I think this paper presents an interesting take on feature pooling. In particular, the idea is to look at pooling as some form of a lossy process, and try to find such a process such that it discards less information given some decimation criterion. Once formulating the problem like this, it becomes obvious that wavelets are a very good candidate.

Pros:
- The nice thing about this method is that average pooling is in some sense a special case of this method, so we can see a clear connection.
- Lots of experiments, and results, which show the method both performing the best in some cases, and not the best in others. I applaud the authors for publishing all the experiments they ran because some may have been tempted to ""forget"" about the experiments in which the proposed method did not perform the best.

Cons:
- No comparison to non-wavelet methods. For example, one obvious comparison would have been to look at using a DCT or FFT transform where the output would discard high frequency components (this can get very close to the wavelet idea!).
- This method has the potential to show its potential on larger pooling windows than 2x2. I would have loved to see some experiments that prove/disprove this.

Other comments:
- Given that this method's flexibility, I could imagine this generate a new class of pooling methods based on lossy transforms. For example, given a MxNxK input, the wavelet idea can be made to output (M/D)x(N/D)x(K/D) (where D is decimation factor). Of interest is the fact that channels can be treated just like any other dimension, since information will be preserved!

Final comments:
- I like the idea and it seems novel it may lead to some promising research directions related to lossy pooling methods/channel aggregation. As such, I think it will be a nice addition to ICLR, especially if the authors decide to run some of the experiments I was suggesting, namely: show what happens when larger pooling windows are used (say 4x4 instead of 2x2), and compare to other lossy techniques (such as Fourier or cosine-transforms).","[9, 7, 4]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design,"['Yoav Levine', 'David Yakira', 'Nadav Cohen', 'Amnon Shashua']",Accept,2018,"[2, 2, 7, 31]","[7, 5, 12, 36]","[25, 8, 55, 142]","[7, 2, 23, 83]","[18, 5, 27, 39]","[0, 1, 5, 20]","The authors try to bring in two seemingly different areas and try
to leverage the results in one for another.
First authors show that the equivalence of the function realized(in
tensor form, given in earlier work) by a ConvAC and
the function used to model n-body quantum system. After establishing
the equivalence of two, the authors argue that
quantum entanglement measures used to measure correlations in n-body
quantum systems can be used as an expressive measure
(how much correlation in input they can handle) of the function
realized by a ConvAC. Separation Rank analysis, which was done
earlier, becomes a special case. As the functional equivalence is
established, authors adopt Tensor Network framework,
to analyze the properties of the ConvAC. The main result being able
to quantify the expressiveness to some extend to the min
cut of the underlying Tensor Network graph corresponding to ConvAC.
This is further used to argue about guide-lining the
width of various parts of ConvAC, if some prior correlation
structure is known about the input. This is also validated
experimentally.

Although I do not see major results at this moment, this work can be
of great significance. The attempt to bring in two areas
have to be appreciated. This work opens up a footing to do graph
theoretical analysis of deep learning architectures and from
the perspective of Quantum entanglement, this could lead to open up new directions. 
The paper is lucidly written, comprehensively covering the
preliminaries. I thoroughly enjoyed reading it, and I think the
paper and the work would be of great contribution to the community.

(There are some typos  (preform --> perform ))","[8, 6, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 2, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Variational Inference of Disentangled Latent Concepts from Unlabeled Observations,"['Abhishek Kumar', 'Prasanna Sattigeri', 'Avinash Balakrishnan']",Accept,2018,"[9, 10, 2]","[13, 15, 4]","[44, 102, 13]","[24, 44, 6]","[18, 47, 6]","[2, 11, 1]","******
Update: revising reviewer score to 6 after acknowledging revisions and improved manuscript
******

The authors propose a new regularization term modifying the VAE (Kingma et al 2013) objective to encourage learning disentangling representations.
Specifically, the authors suggest to add penalization to ELBO in the form of -KL(q(z)||p(z)) , which encourages a more global criterion than the local ELBOs.
In practice, the authors decide that the objective they want to optimize is unwieldy and resort to moment matching of covariances of q(z) and p(z) via gradient descent.
The final objective uses a persistent estimate of the covariance matrix of q and upgrades it at each mini-batch to perform learning.

The authors use this objective function to perform experiments measuring disentanglement and find minor benefits compared to other objectives in quantitative terms.

Comments:
1. The originally proposed modification in Equation (4) appears to be rigorous and as far as I can tell still poses a lower bound to log(p(x)). The proof could use the result posed earlier: KL(q(z)||p(z)) is smaller than E_x KL(q(z|x)||p(z|x)).
2. The proposed moment matching scheme performing decorrelation resembles approaches for variational PCA and especially independent component analysis. The relationship to these techniques is not discussed adequately. In addition, this paper could really benefit from an empirical figure of the marginal statistics of z under the different regularizers in order to establish what type of structure is being imposed here and what it results in.
3. The resulting regularizer with the decorrelation terms could be studied as a modeling choice. In the probabilistic sense, regularizers can be seen as structural and prior assumptions on variables. As it stands, it is unnecessarily vague which assumptions this extra regularizer is making on variables.
4. Why is using the objective in Equation (4) not tried and tested and compared to? It could be thought that subsampling would be enough to evaluate this extra KL term without any need for additional variational parameters \psi. The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq (4) has problems.
5. The model seems to be making on minor progress in its stated goal, disentanglement. It would be more convincing to clarify the structural properties of this regularizer in a statistical sense more clearly given that experimentally it seems to only have a minor effect.
6. Is there a relationship to NICE (Laurent Dinh et al)?
7. The infogan is also an obvious point of reference and comparison here.
8. The authors claim that there are no models which can combine GANs with inference in a satisfactory way, which is obviously not accurate nowadays given the progress on literature combining GANs and variational inference.

All in all I find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,"['Yaguang Li', 'Rose Yu', 'Cyrus Shahabi', 'Yan Liu']",Accept,2018,"[7, 8, 26, 17]","[12, 13, 31, 22]","[46, 88, 399, 219]","[35, 35, 248, 132]","[3, 50, 41, 57]","[8, 3, 110, 30]","The paper proposes to build a graph where the edge weight is defined using the road network distance which is shown to be more realistic than the Euclidean distance. The defined diffusion convolution operation is essentially conducting random walks over the road segment graph. To avoid the expensive matrix operation for the random walk, it empirically shows that K = 3 hops of the random walk can give a good performance. The outputs of the graph convolutionary operation are then fed into the sequence to sequence architecture with the GRU cell to model the temporal dependency. Experiments show that the proposed architecture can achieve good performance compared to classic time series baselines and several simplified variants of the proposed model. 

Although the paper argues that several existing deep-learning based approaches may not be directly applied in the current setting either due to using Euclidean distance or undirected graph structure, the comparisons are not persuasive. For example, the approach in the paper ""DeepTransport: Learning Spatial-Temporal Dependency for Traffic Condition Forecasting"" also consider directed graph and a diffusion effect from 2 or 3 hops away in the neighboring subgraph of a target road segment. 

Furthermore, the paper proposes to use two convolution components in Equation 2, each of which corresponds to out-degree and in-degree direction, respectively. This effectively increase the number of model parameters to learn. Compared to the existing spectral graph convolution approach, it is still not clear how its performance will be by using the same number of parameters. The experiments will be improved if it can compare with ""Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting"" using roughly the same number of parameters.","[5, 9, 4]","[' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
i-RevNet: Deep Invertible Networks,"['Jörn-Henrik Jacobsen', 'Arnold W.M. Smeulders', 'Edouard Oyallon']",Accept,2018,"[3, 37, 5]","[8, 42, 9]","[40, 295, 40]","[17, 161, 15]","[21, 45, 22]","[2, 89, 3]","ICLR I-Revnet


This paper build on top of ReVNets (Gomez et al., 2017)  and introduce a variant that is fully 
invertible. The model performs comparable to its variants without any loss of information.
They analyze the model and its learned representations from multiple perspectives in detail. 
 
It is indeed very interesting an thought provoking to see that contrary to popular belief in the community no information loss is necessary to learn good generalizable features. What is missing, is more motivation for why such a property is desirable. As the authors mentions the model size has almost doubled compared to comparable ResNet. And the study of the property of the learned futures might probably limited to this i-RevNet only. It would be good to see more motivation, beside the valuable insight of knowing it’s possible.

Generally the paper is well written and readable, but few minor comments:
1-Better formatting such as putting results in model sizes, etc in tables will make them easier to find.
2-Writing down more in detail 3.1, ideally in algorithm or equation than all in text as makes it hard to read in current format.","[8, 9, 8]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unsupervised Cipher Cracking Using Discrete GANs,"['Aidan N. Gomez', 'Sicong Huang', 'Ivan Zhang', 'Bryan M. Li', 'Muhammad Osama', 'Lukasz Kaiser']",Accept,2018,"[2, 1, 1, 1, 4, 14]","[6, 6, 4, 6, 9, 18]","[29, 14, 5, 7, 44, 84]","[9, 6, 2, 1, 20, 44]","[18, 6, 3, 4, 15, 32]","[2, 2, 0, 2, 9, 8]","The paper shows an application of GANs to deciphering text. The goal is to arrive at a ```""hands free"" approach to this problem; i.e., an approach that does not require any knowledge of the language being deciphered such as letter frequency and such. The authors start from a CycleGAN architecture, which may be used to learn mapping between two probability spaces. They point out that using GANs for discrete distributions is a challenging problem since it can lead to uninformative discriminants. They propose to  resolve this issue by using a continuous embedding space to approximate (or convert) the discrete random variables into continuous random variables. The new proposed algorithm, called CipherGAN, is then shown to be stable and achieve deciphering of substitution ciphers and Vigenere ciphers.

I did not completely understand how the embedding was performed, so perhaps the authors could elaborate on that a bit more. Apart from that, the paper is well written and well motivated. It used some recent ideas in deep learning such as Cycle GANs and shows how to tweak them to make them work for discrete problems and also make them more stable. One comment would be that the paper is decidedly an applied paper (and not much theory) since certain steps in the algorithm (such as training the discriminator loss along with the Lipschitz conditioning term) are included because it was experimentally  observed to lead to stability. ","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[1, 4, 4]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Parametric Closed-Loop Policies for Markov Potential Games,"['Sergio Valcarcel Macua', 'Javier Zazo', 'Santiago Zazo']",Accept,2018,"[8, 5, 25]","[13, 10, 29]","[35, 23, 115]","[16, 11, 60]","[12, 8, 9]","[7, 4, 46]","While it is not very surprising that in a potential game it is easy to find Nash equilibria (compare to normal form static games, in which local maxima of the potential are pure Nash equilibria), the idea of approaching these stochastic games from this direction is novel and potentially (no pun intended) fruitful. The paper is well written, the motivation is clear, and some of the ideas are non-trivial. However, the connection to learning representations is a little tenuous. ","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 3, 1]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Temporal Difference Models: Model-Free Deep RL for Model-Based Control,"['Vitchyr Pong*', 'Shixiang Gu*', 'Murtaza Dalal', 'Sergey Levine']",Accept,2018,"[3, 7, 1, 10]","[7, 12, 6, 15]","[23, 89, 17, 743]","[11, 38, 7, 326]","[12, 49, 10, 396]","[0, 2, 0, 21]","
This paper proposes a ""temporal difference model learning"", a method that aims to combine the benefits of model-based and model-free RL.  The proposed method essentially learns a time-varying goal-conditional value function for a specific reward formulation, which acts as a surrogate for a model in an MPC-like setting.  The authors show that the method outperforms some alternatives on three continuous control domains and real robot system.

I believe this paper to be borderline, but ultimately below the threshold for acceptance.  On the positive side, there are certainly some interesting ideas here: the notion of goal-conditioned value functions as proxies for a model, and as a means of merging model-free and model-based approaches is very really interesting, and hints at a deeper structure to goal-conditioned value functions in general.  Ultimately, though, I feel that there are two main issues that make this research feel as though it is still ultimately in the earlier stages: 1) the very large focus on the perspective that this approach is unifying model-based and model-free RL, when it fact this connection seems a bit tenuous; and 2) the rather lackluster experimental results, which show only marginal improvement over purely model-based methods (at the cost of much additional complexity), and which make me wonder if there's an issue with their implementation of prior work (namely the Highsight Experience Replay algorithm).

To address the first point, although the paper stresses it to a very high degree, I can't help but feel that the connection that the claimed advance of ""unifying model-based and model-free RL"" is overstated.  As far as I can tell, the connection is as follows: the learned quantity here is a time-varying goal-conditioned value function, and under some specific definition of reward, we can interpret the constraint that this value function equal zero as a proxy for the dynamics constraint in MPC.  But the exact correspondence between this and the MPC formulation only occurs for a horizon of size zero: longer horizons require a multi-step MPC for the definition of the model-free and model-based correspondence.  The fact that the action selection of a model-based method and this approach have some function which looks similar (but only under certain conditions), just seems like a fairly odd connection to highlight so heavily.

Rather, it seems to me that what's happening here is really quite simple: the authors are extending goal-conditioned value functions to the case of non-stationary finite horizon value functions (the claimed ""key insight"" in eq (5) is a completely standard finite-horizon MDP formulation).  This seems to describe perfectly well what is happening here, and it does also seem intuitive that this provides an advantage over stationary goal-conditioned value functions: just as goal conditioned value functions offer the advantage of considering ""every state as a goal"", this method can consider ""every state as a goal for every time horizon"".  This seems interesting enough on its own, and I admit I don't see the need for the method to be yet another claimed unification of model-free and model-based RL.

I would also suggest that the authors look into the literature on how TD methods implicitly learn models (see e.g. Boyan 1997 ""Least-squares temporal difference learning"", and Parr et al., 2007 ""An analysis of linear models..."").  In these works it has been shown that least squares TD methods (at least in the linear feature setting), implicitly learn a dynamics model in feature space, but only the ""projection"" of the reward function is actually needed to learn the TD weights.  In building the proposed value functions, it seems like the authors are effectively solving for multiple rewards simultaneously, which would effectively preserve the learned dynamics model.  I feel like this may be an interesting line of analysis for the paper if the authors _do_ want to stick with the notion of the method as unifying model-free and model-based RL.

All these points may ultimately just be a matter of interpretation, though, if not for the second issue with the paper, which is that the results seem quite lackluster, and the claimed performance of HER seems rather suspicious.  But instead, the authors evaluate the algorithm on just three continuous control tasks (and a real robot, which is more impressive, but the task here is still so extremely simple for a real robot system that it really just qualifies as a real-world demonstration rather than an actual application).  And in these three settings, a model-based approach seems to work just as well on two of the tasks, and may soon perform just as well after a few more episodes on the last task (it doesn't appear to have converged yet).  And despite the HER paper showing improvement over traditional policy approaches, in these experiments plain DDPG consistently performs as well or better than HER.  ","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Model compression via distillation and quantization,"['Antonio Polino', 'Razvan Pascanu', 'Dan Alistarh']",Accept,2018,"[8, 2, 20]","[13, 7, 25]","[33, 39, 207]","[15, 19, 86]","[16, 20, 86]","[2, 0, 35]","The paper proposes to combine two approaches to compress deep neural networks - distillation and quantization. The authors proposed two methods, one largely relying on the distillation loss idea then followed by a quantization step, and another one that also learns the location of the quantization points. Somewhat surprisingly, nobody has combined the two approaches before, which makes this paper interesting. Experiments show that both methods work well in compressing large deep neural network models for applications where resources are limited, like on mobile devices. 

Overall I am mostly OK with this paper but not impressed by it.  Detailed comments below.

1. Quantizing with respect to the distillation loss seems to do better than with the normal loss - this needs more discussion. 
2. The idea of using the gradient with respect to the quantization points to learn them is interesting but not entirely new (see, e.g., ""Matrix Recovery from Quantized and Corrupted Measurements"", ICASSP 2014 and ""OrdRec: An Ordinal Model for Predicting Personalized Item Rating Distributions"", RecSys 2011, although in a different context). I also wonder if it would work better if you can also allow the weights to move a little bit (it seems to me from Algorithm 2 that you only update the quantization points). How about learning them altogether? Also this differentiable quantization method does not really depend on distillation, which is kind of confusing given the title.
3. I am a little bit confused by how the bits are redistributed in the second method, as in the end it seems to use more than the proposed number of bits shown in the table (as recognized in section 4.2). This makes the comparison a little bit unfair (especially for the CIFAR 100 case, where the ""2 bits"" differentiable quantization is actually using 3.23 bits). This needs more clarification.
4. The writing can be improved. For example, the concepts of ""teacher"" and ""student"" is not clear at all in the abstract - consider putting the first sentence of Section 3 in there instead. Also, the first sentence of the paper reads as ""... have showed tremendous performance"", which is not proper English. At the top of page 3 I found ""we will restrict our attention to uniform and non-uniform quantization"". What are you not restricting to, then?

Slightly increased my rating after reading the rebuttal and the revision. ","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis,"['Yi Zhou', 'Zimo Li', 'Shuangjiu Xiao', 'Chong He', 'Zeng Huang', 'Hao Li']",Accept,2018,"[23, 5, 14, 5, 10, 11]","[28, 9, 19, 10, 15, 16]","[374, 17, 51, 34, 24, 113]","[177, 10, 46, 12, 12, 49]","[61, 5, 1, 4, 10, 33]","[136, 2, 4, 18, 2, 31]","The problem of learning auto-regressive (data-driven) human motion models that have long-term stability
is of ongoing interest. Steady progress is being made on this problem, and this paper adds to that.
The paper is clearly written. The specific form of training (a fixed number of self-conditioned predictions,
followed by a fixed number of ground-truth conditioned steps) is interesting for simplicity and its efficacy.
The biggest open question for me is how it would compare to the equally simple stochastic version proposed
by the scheduled sampling approach of [Bengio et al. 2015].

PROS:  The paper provides a simple solution to a problem of interest to many.
CONS:  It is not clear if it improves over something like scheduled sampling, which is a stochastic predecessor
       of the main idea introduced here. The ""duration of stability"" is a less interesting goal than
       actually matching the distribution of the input data.

The need to pay attention to the distribution-mismatch problem for sequence prediction problems
has been known for a while. In particular, the DAGGER (see below) and scheduled sampling algorithms (already cited) 
target this issue, in addition to the addition of progressively increasing amounts of noise during training
(Fragkiadaki et al). Also see papers below on Professor Forcing, as well as ""Learning Human Motion Models
for Long-term Predictions"" (concurrent work?), which uses annealing over dropout rates to achieve stable long-term predictions.

  DAGGER algorithm (2011):  http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf
  ""A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning""

  Professor Forcing (NIPS 2016)
  http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf

  Learning Human Motion Models for Long-term Predictions (2017)
  https://arxiv.org/abs/1704.02827
  https://www.youtube.com/watch?v=PgJ2kZR9V5w
  
While the motions do not freeze, do the synthesized motion distributions match the actual data distributions?
This is not clear, and would be relatively simple to evaluate.  Is the motion generation fully deterministic?
It would be useful to have probabilistic transition distributions that match those seen in the data.
An interesting open issue (in motion, but also of course NLP domains) is that of how to best evaulate
sequence-prediction models.  The duration of ""stable prediction"" does not directly capture the motion quality. 

Figure 1:  Suggest to make u != v for the purposes of clarity, so that they can be more easily distinguished.

Data representation:
Why not factor out the facing angle, i.e., rotation about the vertical axis, as done by Holden et al, and in a variety of
previous work in general?
The representation is already made translation invariant. Relatedly, in the Training section,
data augmentation includes translating the sequence: ""rotate and translate the sequence randomly"".
Why bother with the translation if the representation itself is already translation invariant?

The video illustrates motions with and without ""foot alignment"".
However, no motivation or description of ""foot alignment"" is given in the paper.

The following comment need not be given much weight in terms of evaluation of the paper, given that the
current paper does not use simulation-based methods. However, it is included for completeness.
The survey of simulation-based methods for modeling human motions is not representative of the body of work in this area
over the past 25 years.  It may be more useful to reference a survey, such as 
""Interactive Character Animation Using Simulated Physics: A State‐of‐the‐Art Review"" (2012)
An example of recent SOTA work for modeling dynamic motions from motion capture, including many
highly dynamic motions, is ""Guided Learning of Control Graphs for Physics-Based Characters"" (2016)
More recent work includes ""Learning human behaviors from motion capture by adversarial imitation"", 
""Robust Imitation of Diverse Behaviors"", and ""Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning"", all of which demonstrate imitation of various motion styles to various degrees.

It is worthwhile acknowledging that the synthesized motions are still low quality, particular when rendered with more human-like looking models, and readily distinguishable from the original motions.  In this sense, they are not comparable to the quality of results demonstrated in recent works by Holden et al. or some other recent works.  However, the authors should be given credit for including some results with fully rendered characters, which much more readily exposes motion flaws.

The followup work on [Lee et al 2010 ""Motion Fields""] is quite relevant:
""Continuous character control with low-dimensional embeddings""
In terms of usefulness, being able to provide some control over the motion output is a more interesting problem than
being able to generate long uncontrolled sequences.  A caveat is that the methods are not applied to large datasets.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
On the Expressive Power of Overlapping Architectures of Deep Learning,"['Or Sharir', 'Amnon Shashua']",Accept,2018,"[4, 31]","[8, 36]","[22, 142]","[7, 83]","[15, 39]","[0, 20]","The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size; the so called overlapping convolutional architectures. The main object of study is to quantify the benefits of overlap in convolutional architectures.

The main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e., there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture; whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures. The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network. 

(1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions. It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case. That this is the case should be made clear in the title and abstract. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case, but since it is left as future work, the paper should make it clear throughout.

(2) The experiment is minimal and even the given experiment is not described well. What data augmentation was used for the CIFAR-10 dataset? It is only mentioned that the data is augmented with translations and horizontal flips. What is the factor of augmentation? How much translation? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily. Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips. 

(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations (a) why are non-overlapping architectures so common? (b) why only slight overlap is used in practice?  The paper seems to claim that since overlapping architectures have higher expressivity that answers (a). It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point. It seems the right resolution will be to show that after the overlap is set to a certain small value, there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper. 

(4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3. 

The paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper). However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments. ","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Regularizing and Optimizing LSTM Language Models,"['Stephen Merity', 'Nitish Shirish Keskar', 'Richard Socher']",Accept,2018,"[8, 4, 12]","[9, 8, 17]","[14, 46, 229]","[6, 16, 111]","[8, 27, 111]","[0, 3, 7]","Clearly presented paper, including a number of reasonable techniques to improve LSTM-LMs. The proposed techniques are heuristic, but are reasonable and appear to yield improvements in perplexity. Some specific comments follow.

re. ""ASGD"" for Averaged SGD: ASGD usually stands for Asynchronous SGD, have the authors considered an alternative acronym? AvSGD?

re. Optimization criterion on page 2, note that SGD is usually taken to minimizing expected loss, not just empirical loss (Bottou thesis 1991).

Is there any theoretical analysis of convergence for Averaged SGD?

re. paragraph starting with ""To prevent such inefficient data usage, we randomly select the sequence length for the forward and backward pass in two steps"": the explanation is a bit unclear. What is the ""base sequence length"" exactly? Also, re. the motivation above this paragraph, I'm not sure what ""elements"" really refers to, though I can guess.

What is the number of training tokens of the datasets used, PTB and WT2?

Can the authors provide more explanation for what ""neural cache models"" are, and how they relate to ""pointer models""?

Why do the sections ""Pointer models"", ""Ablation analysis"", and ""AWD-QRNN"" come after the Experiments section?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,"['Youngjin Kim', 'Minjung Kim', 'Gunhee Kim']",Accept,2018,"[24, 18, 16]","[29, 23, 21]","[94, 39, 150]","[41, 22, 95]","[14, 9, 45]","[39, 8, 10]","MemoryGAN is proposed to handle structural discontinuity (avoid unrealistic samples) for the generator, and the forgetting behavior of the discriminator. The idea to incorporate memory mechanism into GAN is interesting, and the authors make nice interpretation why this needed, and clearly demonstrate which component helps (including the connections to previous methods).   

My major concerns:

Figure 1 is questionable in demonstrating the advantage of proposed MemoryGAN. My understanding is that four z's used in DCGAN and MemoryGAN are ""randomly sampled"" and fixed, interpolation is done in latent space, and propagate to x to show the samples.  Take MNIST for example, It can be seen that the DCGAN has to (1) transit among digits in different classes, while MemoryGAN only (2) transit among digits in the same class. Task 1 is significantly harder than task 2, it is not surprise that DCGAN generate unrealistic images. A better experiment is to fix four digits from different class at first, find their corresponding latent codes, do interpolation, and propagate back to sample space to visualize results. If the proposed technique can truly handle structural discontinuity, it will ""jump"" over the sample manifold from one class to another, and thus avoid unrealistic samples. Also, the current illustration also indicates that the generated samples by MemoryGAN is not diverse.

It seems the memory mechanism can bring major computational overhead, is it possible to provide the comparison on running time?

To what degree the MemoryGAN can handle structural discontinuity? It can be seen from Table 2 that larger improvement is observed when tested on a more diverse dataset. For example, the improvement gap from MNIST to CIFAR is larger. If the MemoryGAN can truly deal with structural discontinuity, the results on generating a wide range of different images for ImageNet may endow the paper with higher impact.

The authors should consider to make their code reproducible and public. 


Minor comments:

In Section 4.3, Please fix ""Results in 2"" as ""Results in Table 2"".


","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
An image representation based convolutional network for DNA classification,"['Bojian Yin', 'Marleen Balvert', 'Davide Zambrano', 'Alexander Schoenhuth', 'Sander Bohte']",Accept,2018,"[1, 1, 10, 17, 19]","[6, 4, 15, 22, 24]","[14, 10, 23, 66, 85]","[5, 2, 11, 15, 37]","[7, 5, 7, 18, 23]","[2, 3, 5, 33, 25]","The authors of this manuscript transformed the k-mer representation of DNA fragments to a 2D image representation using the space-filling Hilbert curves for the classification of chromatin occupancy. In generally, this paper is easy to read. The components of the proposed model mainly include Hilbert curve theory and CNN which are existing technologies. But the authors make their combination useful in applications. Some specific comments are:

1. In page 5, I could not understand the formula d_kink < d_out. d_link ;
2. There may exist some new histone modification data that were captured by the next-generation sequencing (e.g. ChIP-seq) and are more accurate;  
3. It seems that the authors treat it as a two-class problem for each data set. It would be more useful in real applications if all the data sets are combined to form a multi-class problem.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration,"['Evan Zheran Liu', 'Kelvin Guu', 'Panupong Pasupat', 'Tianlin Shi', 'Percy Liang']",Accept,2018,"[2, 4, 7, 6, 14]","[7, 9, 12, 6, 19]","[19, 37, 50, 17, 346]","[8, 13, 26, 9, 166]","[11, 23, 24, 6, 170]","[0, 1, 0, 2, 10]","This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called ""Workflow Guided Exploration"". Workflows are defined through a DSL unique to the domain.

The paper is clear, very well written, and well-motivated. Exploration is still a challenging problem for RL. The workflows remind me of options though in this paper they appear to be hand-crafted. In that sense, I wonder if this has been done before in another domain. The results suggest that WGE sometimes helps but not consistently. While the experiments show that DOMNET improves over Shi et al, that could be explained as not having to train on raw pixels or not enough episodes.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Thermometer Encoding: One Hot Way To Resist Adversarial Examples,"['Jacob Buckman', 'Aurko Roy', 'Colin Raffel', 'Ian Goodfellow']",Accept,2018,"[3, 5, 9, 10]","[7, 9, 14, 12]","[16, 28, 112, 107]","[7, 10, 49, 48]","[8, 14, 58, 55]","[1, 4, 5, 4]","The authors present an in-depth study of discretizing / quantizing the input as a defense against adversarial examples. The idea is that the threshold effects of discretization make it harder to find adversarial examples that only make small alterations of the image, but also that it introduces more non-linearities, which might increase robustness. In addition, discretization has little negative impact on the performance on clean data. The authors also propose a version of single-step or multi-step attacks against models that use discretized inputs, and present extensive experiments on MNIST, CIFAR-10, CIFAR-100 and SVHN, against standard baselines and, on MNIST and CIFAR-10, against a version of quantization in which the values are represented by a small number of bits.

The merits of the paper is that the study is rather comprehensive: a large number of datasets were used, two types of discretization were tried, and the authors propose an attack mechanism better that seems reasonable considering the defense they consider. The two main claims of the paper, namely that discretization doesn't hurt performance on natural test examples and that better robustness (in the author's experimental setup) is achieved through the discretized encoding, are properly backed up by the experiments.

Yet, the applicability of the method in practice is still to be demonstrated. The threshold effects might imply that small perturbations of the input (in the l_infty sense) will not have a large effect on their discritized version, but it may also go the other way: an opponent might be able to greatly change the discretized input without drastically changing the input. Figure 8 in the appendix is a bit worrysome on that point, as the performance of the discretized version drops rapidly to 0 when the opponents gets a bit stronger. Did the authors observe the same kind of bahavior on other datasets? What would the authors propose to mitigate this issue? To what extend the good results that are exhibited in the paper are valid over the wide range of opponent's strengths?

minor comment:
- the experiments on CIFAR-100 in Appendix E are carried out by mixing adversarial / clean examples while training, whereas those on SVHN in Appendix F use adversarial examples only.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
On the State of the Art of Evaluation in Neural Language Models,"['Gábor Melis', 'Chris Dyer', 'Phil Blunsom']",Accept,2018,"[5, 20, 15]","[9, 24, 20]","[22, 268, 178]","[7, 161, 91]","[13, 90, 70]","[2, 17, 17]","The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures.

I have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.","[7, 5, 8]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[2, 5, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Sensitivity and Generalization in Neural Networks: an Empirical Study,"['Roman Novak', 'Yasaman Bahri', 'Daniel A. Abolafia', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein']",Accept,2018,"[25, 2, 1, 8, 10]","[29, 6, 3, 13, 15]","[46, 18, 6, 61, 130]","[21, 7, 2, 33, 52]","[15, 10, 4, 27, 74]","[10, 1, 0, 1, 4]","This work investigates sensitivity and generalisation properties of neural networks with respect to a number of metrics aimed at quantifying the robustness with respect to data variability, varying parameters and representativity of training/testing data. 
The validation is based on the Jacobian of the network, and in the detection of the “transitions” associated to the data space. These measures are linked, as the former quantifies the sensitivity of the network respect to infinitesimal data variations, while the latter quantifies the complexity of the modelled data space. 
The study explores a number of experimental setting, where the behaviour of the network is analysed on synthetic paths around training data, from pure random data points, to curves interpolating different/same data classes.
The experimental results are performed on CIFAR10,CIFAR100, and MNIST. Highly-parameterised networks seem to offer a better generalisation, while lower Jacobian norm are usually associated to better generalisation and fewer transitions, and can be obtained with data augmentation.

The paper proposes an interesting analysis aimed at the empirical exploration of neural network properties, the proposed metrics provide relevant insights to understand the behaviour of a network under varying data points. 

Major remarks.

The proposed investigation is to my opinion quite controversial. Interesting data variation does not usually corresponds to linear data change. When considering the linear interpolation of training data, the authors are actually creating data instances not compatible with the original data source: for example, the pixel-wise intensity average of digits is not a digit anymore. For this reason, the conclusions drawn about the model sensitivity are to my opinion based a potentially uninteresting experimental context. Meaningful data variation can be way more complex and high-dimensional, for example by considering spatial warps of digits, or occlusions and superpositions of natural images. This kind of variability is likely to correspond to real data changes, and may lead to more reliable conclusions. For this reason, the proposed results may provide little indications of the true behaviour of the models data in case of meaningful  data variations. 

Moreover, although performed within a cross-validation setting, training and testing are still applied to the same dataset. Cross-validation doesn’t rule out validation bias, while it is also known that the classification performance significantly drops when applied to independent “unseen” data, provided for example in different cohorts. I would expect that highly parameterised models would lead to worse performance when applied to genuinely independent cohorts, and I believe that this work should extend the investigation to this experimental setting.

Minor remarks.

The authors should revise the presentation of the proposed work. The 14 figures(!) of main text are not presented in the order of appearance. The main one (figure 1) is provided in the first paragraph of the introduction and never discussed in the rest of the paper. 
","[5, 4, 8]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Simulating Action Dynamics with Neural Process Networks,"['Antoine Bosselut', 'Omer Levy', 'Ari Holtzman', 'Corin Ennis', 'Dieter Fox', 'Yejin Choi']",Accept,2018,"[3, 7, 2, 2, 24, 14]","[8, 12, 7, 2, 29, 19]","[77, 122, 47, 2, 449, 344]","[35, 55, 22, 1, 244, 175]","[41, 62, 25, 1, 146, 160]","[1, 5, 0, 0, 59, 9]","SUMMARY.

The paper presents a novel approach to procedural language understanding.
The proposed model reads food recipes and updates the representation of the entities mentioned in the text in order to reflect the physical changes of the entities in the recipe.
The authors also propose a manually annotated dataset where each passage of a recipe is annotated with entities, actions performed over the entities, and the change in state of the entities after the action.
The authors tested their model on the proposed dataset and compared it with several baselines.


----------

OVERALL JUDGMENT
The paper is very well written and easy to read.
I enjoyed reading this paper, I found the proposed architecture very well thought for the proposed task.
I would have liked to see a little bit more of analysis on the results, it would be interesting to see what are the cases the model struggles the most.

I am wondering how the model would perform without intermediate losses i.e., entity selection loss and action selection loss.
It would also be interesting to see the impact of the amount of 'intermediate' supervision on the state change prediction.

The setup for generation is a bit unclear to me.
The authors mentioned to encode entity vectors with a biGRU, do the authors encode it in order of appearance in the text? would not it be better to encode the entities with some structure-agnostic model like Deep Sets?

","[9, 6, 8]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,"['Murat Kocaoglu', 'Christopher Snyder', 'Alexandros G. Dimakis', 'Sriram Vishwanath']",Accept,2018,"[7, 19, 17, 19]","[12, 21, 22, 24]","[41, 10, 297, 333]","[22, 4, 123, 158]","[14, 4, 132, 116]","[5, 2, 42, 59]","In their paper ""CausalGAN: Learning Causal implicit Generative Models with adv. training"" the authors address the following issue: Given a causal structure between ""labels"" of an image (e.g. gender, mustache, smiling, etc.), one tries to learn a causal model between these variables and the image itself from observational data. Here, the image is considered to be an effect of all the labels. Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions. These tasks are clearly different, as nicely shown by the authors' example of ""do(mustache = 1)"" versus ""given mustache = 1"" (a sample from the latter distribution contains only men). The paper does not aim at learning causal structure from data (as clearly stated by the authors). The example images look convincing to me.

I like the idea of this paper. IMO, it is a very nice, clean, and useful approach of combining causality and the expressive power of neural networks. The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area. For me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task. The presentation of the paper, however, should be improved significantly before publication. (In fact, because of the presentation of the paper, I was hesitating whether I should suggest acceptance.) Below, I give some examples (and suggest improvements), but there are many others. There is a risk that in its current state the paper will not generate much impact, and that would be a pity. I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper. 


- I believe that I understand the authors' intention of the caption of Fig. 1, but ""samples outside the dataset"" is a misleading formulation. Any reasonable model does more than just reproducing the data points. I find the argumentation the authors give in Figure 6 much sharper. Even better: add the expression ""P(male = 1 | mustache = 1) = 1"". Then, the difference is crystal clear.
- The difference between Figures 1, 4, and 6 could be clarified.    
- The list of ""prior work on learning causal graphs"" seems a bit random. I would add Spirtes et al 2000, Heckermann et al 1999, Peters et al 2016, and Chickering et al 2002. 
- Male -> Bald does not make much sense causally (it should be Gender -> Baldness)... Aha, now I understand: The authors seem to switch between ""Gender"" and ""Male"" being random variables. Make this consistent, please. 
- There are many typos and comma mistakes. 
- I would introduce the do-notation much earlier. The paragraph on p. 2 is now written without do-notation (""intervening Mustache = 1 would not change the distribution""). But this way, the statements are at least very confusing (which one is ""the distribution""?).
- I would get rid of the concept of CiGM. To me, it seems that this is a causal model with a neural network (NN) modeling the functions that appear in the SCM. This means, it's ""just"" using NNs as a model class. Instead, one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN? (This would also clarify the paper's contribution.)
- many realizations = one sample (not samples), I think. 
- Fig 1: which model is used to generate the conditional sample?  
- The notation changes between E and N and Z for the noises. I believe that N is supposed to be the noise in the SCM, but then maybe it should not be called E at the beginning. 
- I believe Prop 1 (as it is stated) is wrong. For a reference, see Peters, Janzing, Scholkopf: Elements of Causal Inference: Foundations and Learning Algorithms (available as pdf), Definition 6.32. One requires the strict positivity of the densities (to properly define conditionals). Also, I believe the Z should be a vector, not a set. 
- Below eq. (1), I am not sure what the V in P_V refers to.
- The concept of data probability density function seems weird to me. Either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf.
- Many subscripts are used without explanation. r -> real? g -> generating? G -> generating? Sometimes, no subscripts are used (e.g., Fig 4 or figures in Sec. 8.13)
- I would get rid of Theorem 1 and explain it in words for the following reasons. (1) What is an ""informal"" theorem? (2) It refers to equations appearing much later. (3) It is stated again later as Theorem 2. 
- Also: the name P_g does not appear anywhere else in the theorem, I think. 
- Furthermore, I would reformulate the theorem. The main point is that the intervention distributions are correct (this fact seems to be there, but is ""hidden"" in the CIGN notation in the corollary).
- Re. the formulation in Thm 2: is it clear that there is a unique global optimum (my intuition would say there could be several), thus: better write ""_a_ global minimum""?
- Fig. 3 was not very clear to me. I suggest to put more information into its caption. 
- In particular, why is the dataset not used for the causal controller? I thought, that it should model the joint (empirical) distribution over the labels, and this is part of the dataset. Am I missing sth?
- IMO, the structure of the paper can be improved. Currently, Section 3 is called ""Background"" which does not say much. Section 4 contains CIGMs, Section 5 Causal GANs, 5.1. Causal Controller, 5.2. CausalGAN, 5.2.1. Architecture (which the causal controller is part of) etc. An alternative could be: 
Sec 1: Introduction 
Sec 1.1: Related Work
Sec 2: Causal Models
Sec 2.1: Causal Models using Generative Models (old: CIGM)
Sec 3: Causal GANs
Sec 3.1: Architecture (including controller)
Sec 3.2: loss functions 
...
Sec 4: Empricial Results (old: Sec. 6: Results)
- ""Causal Graph 1"" is not a proper reference (it's Fig 23 I guess). Also, it is quite important for the paper, I think it should be in the main part. 
- There are different references to the ""Appendix"", ""Suppl. Material"", or ""Sec. 8"" -- please be consistent (and try to avoid ambiguity by being more specific -- the appendix contains ~20 pages). Have I missed the reference to the proof of Thm 2?
- 8.1. contains copy-paste from the main text.
- ""proposition from Goodfellow"" -> please be more precise
- What is Fig 8 used for? Is it not sufficient to have and discuss Fig 23? 
- IMO, Section 5.3. should be rewritten (also, maybe include another reference for BEGAN).
- There is a reference to Lemma 15. However, I have not found that lemma.
- I think it's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model, see Fig 16. This is the case since for the generated realizations, the noise values are known. The authors may think about including a comment on that issue.
- Since this paper's main proposal is a methodological one, I would make the publication conditional on the fact that code is released. 


","[7, 6, 9]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,"['Fabrizio Pedersoli', 'George Tzanetakis', 'Andrea Tagliasacchi']",Accept,2018,"[7, 20, 11]","[11, 24, 16]","[10, 150, 116]","[6, 114, 36]","[2, 6, 53]","[2, 30, 27]","This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures. It also provides optimizations that substantially improve the speed of the forward pass: packing layer bits along the channel dimension, pre-allocation of CUDA resources and binary-optimized CUDA kernels for matrix multiplications. The authors compare their framework to BinaryNET and Nervana/Neon and show a 8x speedup for 8092 matrix-matrix multiplication and a 68x speedup for MLP networks. For CNN, they a speedup of 5x is obtained from the GPU to binary-optimizimed-GPU. A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.

The main contribution of this paper is an optimized code for Binary CNN. The authors provide the code with permissive licensing. As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups. The authors should provide a table with actual numbers instead of the hard-to-read bar graphs. Otherwise the paper is well written and relatively clear, although the flow is somewhat unwieldy. 

Overall, i think it makes a good contribution to a field that is gaining importance for mobile and embedded applications of deep convnets. I think it is a good fit for a poster.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Towards Deep Learning Models Resistant to Adversarial Attacks,"['Aleksander Madry', 'Aleksandar Makelov', 'Ludwig Schmidt', 'Dimitris Tsipras', 'Adrian Vladu']",Accept,2018,"[14, 2, 5, 4, 9]","[19, 2, 10, 9, 14]","[127, 2, 104, 40, 42]","[55, 1, 50, 17, 20]","[64, 1, 50, 21, 21]","[8, 0, 4, 2, 1]","This paper consolidates and builds on recent work on adversarial examples and adversarial training for image classification. Its contributions:

 - Making the connection between adversarial training and robust optimization more explicit.

 - Empirical evidence that:
   * Projected gradient descent (PGD) (as proposed by Kurakin et al. (2016)) reasonably approximates the optimal attack against deep convolutional neural networks
   * PGD finds better adversarial examples, and training with it yields more robust models, compared to FGSM 

 - Additional empirical analysis:
   * Comparison of weights in robust and non-robust MNIST classifiers
   * Vulnerability of L_infty-robust models to to L_2-bounded attacks

The evidence that PGD consistently finds good examples is fairly compelling -- when initialized from 10,000 random points near the example to be disguised, it usually finds examples of similar quality. The remaining variance that's present in those distributions shouldn't hurt learning much, as long as a significant fraction of the adversarial examples are close enough to optimal.

Given the consistent effectiveness of PGD, using PGD for adversarial training should yield models that are reliably robust (for a specific definition of robustness, such as bounded L_infinity norm). This is an improvement over purely heuristic approaches, which are often less robust than claimed.

The comparison to R+FGSM is interesting, and could be extended in a few small ways. What would R+FGSM look like with 10,000 restarts? The distribution should be much broader, which would further demonstrate how PGD works better on these models. Also, when generating adversarial examples for testing, how well would R+FGSM work if you took the best of 2,000 random restarts? This would match the number of gradient computations required by PGD with 100 steps and 20 restarts. Again, I expect that PGD would be better, but this would make that point clearer. I think this analysis would make the paper stronger, but I don't think it's required for acceptance, especially since R+FGSM itself is such a recent development.

One thing not discussed is the high computational cost: performing a 40-step optimization of each training example will be ~40 times slower than standard stochastic gradient descent. I suspect this is the reason why there are results on MNIST and CIFAR, but not ImageNet. It would be very helpful to add some discussion of this.

The title seems unnecessarily vague, since many papers have been written with the same goal -- make deep learning models resistant to adversarial attacks. (This comment does not affect my opinion about whether or not the paper should be accepted, and is merely a suggestion for the authors.)

Also, much of the paper's content is in the appendices. This reads like a journal article where the references were put in the middle. I don't know if that's fixable, given conference constraints.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning to Teach,"['Yang Fan', 'Fei Tian', 'Tao Qin', 'Xiang-Yang Li', 'Tie-Yan Liu']",Accept,2018,"[-2, 6, 0, -4, 3, 14]","[3, 11, 5, 1, 8, 19]","[5, 36, 9, 1, 11, 198]","[3, 19, 4, 1, 8, 112]","[2, 15, 3, 0, 3, 49]","[0, 2, 2, 0, 0, 37]","This paper focuses on the problem of ""machine teaching"", i.e., how to select a good strategy to select training data points to pass to a machine learning algorithm, for faster learning. The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns, and use policy gradient to update the teacher parameters. I find the definition of the ""state"" in this case very interesting. The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster. 

Overall I think that this paper is decent. The angle the authors took is interesting (essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup). The problem formulation is mostly reasonable, and the evaluation seems quite convincing. The paper is well-written: I enjoyed the mathematical formulation (Section 3). The authors did a good job of using different experiments (filtration number analysis, and teaching both the same architecture and a different architecture) to intuitively explain what their method actually does. 

At the same time, though, I see several important issues that need to be addressed if this paper is to be accepted. Details below. 

1. As much as I enjoyed reading Section 3, it is very redundant. In some cases it is good to outline a powerful and generic framework (like the authors did here with defining ""teaching"" in a very broad sense, including selecting good loss functions and hypothesis spaces) and then explain that the current work focuses on one aspect (selecting training data points). However, I do not see it being the case here. In my opinion, selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it. But that is not very interesting (if you can propose new loss functions, that would be way cooler). I also do not see how to define an intuitive set of ""states"" in that case. Therefore, I think this section should be shortened. I also think that the authors should not discuss the general framework and rather focus on ""data teaching"", which is the only focus of the current paper. The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions. 
2. The authors should do a better job at explaining the details of the state definition, especially the student model features and the combination of data and current learner model. 
3. There is only one definition of the reward - related to batch number when the accuracy first exceeds a threshold. Is accuracy stable, can it drop back down below the threshold in the next epoch? The accuracy on a held-out test set is not guaranteed to be monotonically increasing, right? Is this a problem in practice (it seems to happen on your curves)? What about other potential reward definitions? And what would they potentially lead to? 
4. Experimental results are averaged over 5 repeated runs - a bit too small in my opinion. 
5. Can the authors show convergence of the teacher parameter \theta? I think it is important to see how fast the teacher model converges, too. 
6. In some of your experiments, every training method converges to the same accuracy after enough training (Fig.2b), while in others, not quite (Fig. 2a and 2c). Why is this the case? Does it mean that you have not run enough iterations for the baseline methods? My intuition is that if the learner algorithm is convex, then ultimately they will all get to the same accuracy level, so the task is just to get there quicker. I understand that since the learner algorithm is an NN, this is not the case - but more explanation is necessary here - does your method also reduces the empirical possibility to get stuck in local minima? 
7. More explanation is needed towards Fig.4c. In this case, using a teacher model trained on a harder task (CIFAR10) leads to much improved student training on a simpler task (MNIST). Why?
8. Although in terms of ""effective training data points"" the proposed method outperforms the other methods, in terms of time (Fig.5) the difference between it and say, NoTeach, is not that significant (especially at very high desired accuracy). More explanation needed here. 

Read the rebuttal and revision and slightly increased my rating.","[8, 5, 9]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning from Between-class Examples for Deep Sound Recognition,"['Yuji Tokozume', 'Yoshitaka Ushiku', 'Tatsuya Harada']",Accept,2018,"[2, 9, 22]","[2, 14, 27]","[5, 88, 294]","[3, 47, 171]","[2, 34, 106]","[0, 7, 17]","Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes. Previous approaches to training data mixing are (1) from random classes, or (2) from the same class. The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network. Results look like significant improvements over standard learning setups.

Detailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks. In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power. The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters.

Minor Asks:
(1) Clarification on how the error rates are defined. Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label).
(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes. Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales. I.e. are we talking about a space of 10 total classes or 10000 total classes? How does num required epochs get impacted as we increase this class space?
(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels.
(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation... what happens if you just randomly pick one of the 10 crops for prediction?","[9, 4, 8]","[' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hierarchical Representations for Efficient Architecture Search,"['Hanxiao Liu', 'Karen Simonyan', 'Oriol Vinyals', 'Chrisantha Fernando', 'Koray Kavukcuoglu']",Accept,2018,"[4, 11, 12, 16, 10]","[9, 15, 17, 20, 14]","[74, 102, 209, 51, 101]","[35, 44, 101, 27, 46]","[33, 51, 98, 13, 46]","[6, 7, 10, 11, 9]","The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy.

Manually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. 

Overall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. 

The fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. 

Nevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3.

Another concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A.

Finally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values.

References:

[1] Grosse, Roger, et al. ""Exploiting compositionality to explore a large space of model structures."" UAI (2012).
[2] Duvenaud, David, et al. ""Structure discovery in nonparametric regression through compositional kernel search."" ICML (2013).
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization,"['Ozsel Kilinc', 'Ismail Uysal']",Accept,2018,"[4, 14]","[8, 19]","[12, 37]","[3, 18]","[7, 5]","[2, 14]","This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9). Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use multiple data sets to study different aspects of the proposed approach.

I updated my scores based on the reviewers responses. It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv! Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR). So my main problem with this paper, lack of novelty, is addressed and my score has changed. Thanks to the reviewer for clarifying this.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Divide-and-Conquer Reinforcement Learning,"['Dibya Ghosh', 'Avi Singh', 'Aravind Rajeswaran', 'Vikash Kumar', 'Sergey Levine']",Accept,2018,"[2, 4, 4, 17, 10]","[7, 8, 9, 22, 15]","[22, 29, 60, 111, 743]","[10, 13, 25, 51, 326]","[12, 15, 33, 42, 396]","[0, 1, 2, 18, 21]","This paper presents a reinforcement learning method for learning complex tasks by dividing the state space into slices, learning local policies within each slice, while ensuring that they don't deviate too far from each other, while simultaneously learning a central policy that works across the entire state space in the process. The most closely related works to this one are Guided Policy Search (GPS) and ""Distral"", and the authors compare and contrast their work with the prior work suitably.

The paper is written well, has good insights, is technically sound, and has all the relevant references. The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline). The paper and included experiments are a valuable contribution to the community interested in solving harder and harder tasks using reinforcement learning.

For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all. If the local policies are trained to convergence, (and the context omega is provided by an oracle), how well does this mixture of local policies perform? This result would be instructive to see for each of the tasks.

The partitioning of each task must currently be designed by hand. It would be interesting (in future work) to explore how the partitioning could perhaps be discovered automatically.","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,"['Anubhav Ashok', 'Nicholas Rhinehart', 'Fares Beainy', 'Kris M. Kitani']",Accept,2018,"[2, 5, 9, 11]","[3, 10, 9, 16]","[3, 48, 10, 287]","[1, 24, 6, 155]","[2, 23, 3, 109]","[0, 1, 1, 23]","Summary:
The manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model. After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding. This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection. The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy. As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models.

Review:
The manuscript describes the proposed algorithm in great detail and the description is easy to follow. The experimental analysis of the approach is very convincing and confirms the author’s claims. 
Using the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section.

A few questions/comments:
1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to? Are these multiple rollouts of the policies? If so, shouldn’t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)? I think I might have missed something here.
2) Minor: some of the citations are a bit awkward, e.g. on page 7: “algorithm from Williams Williams (1992). I would use the \citet command from natbib for such citations and \citep for parenthesized citations, e.g. “... incorporate dark knowledge (Hinton et al., 2015)” or “The MNIST (LeCun et al., 1998) dataset...” 
3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations.
4) Appendix: Section 8 states “Below are the results”, but the figure landed on the next page. I would either try to force the figures to be output at that position (not in or after Section 9) or write ""Figures X-Y show the results"". Also in Section 11, Figure 13 should be referenced with the \ref command
5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)?
6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments?
7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution?

Overall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.","[9, 4, 5]","[' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Gaussian Process Behaviour in Wide Deep Neural Networks,"['Alexander G. de G. Matthews', 'Jiri Hron', 'Mark Rowland', 'Richard E. Turner', 'Zoubin Ghahramani']",Accept,2018,"[4, 2, 19, 12, 26]","[9, 7, 24, 17, 30]","[19, 23, 79, 147, 310]","[9, 10, 38, 67, 184]","[9, 13, 38, 70, 66]","[1, 0, 3, 10, 60]","- Summary

The paper is well written and proves how deep, wide, fully connected NNs are equivalent to GPs in the limit. This result, which was well known for single-layer NNs, is now extended to the multilayer case. Although there was already previous work suggesting GP this behavior, there was no formal proof under the specific conditions presented here.

The convergence to a GP is also verified experimentally on some toy examples.


- Relevance

The result itself does not feel very novel because variants of it were already available.

Unfortunately, although making other researchers aware of this is worthy, the application of this result seems limited, since in fact it describes and lets us know more about a regime that we would rather avoid, rather than one we want to exploit. Most of the applications of deep learning benefit from strong structured priors that cannot be represented as a GP. This is properly acknowledged in the paper.

The lack of practical relevance combined with the not-groundbreaking novelty of the result makes this paper less appealing.


- Other comments

Page 6: ""It does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for Bayesian approaches and in this context our results are directly relevant.""

Although that argument can hold for datasets that are large in terms of amount of data points, it doesn't for datasets that are large in terms of number of dimensions. The empirical study could have used very high-dimensional datasets with comparatively low amounts of training data. That would maintain a regime were the prior does matter but and better show the generality of the results.

Page 6: ""We use rectified linear units and correct the variances to avoid a loss of prior variance as depth is increased as discussed in Section 3"" 

Are you sure this is discussed in Section 3?

Page 4: ""This is because for finite H the input activations do not have a multivariate normal distribution"". 

Can you elaborate on this? Since we are interested in the infinite limit, why is this a problem?","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Alternating Multi-bit Quantization for Recurrent Neural Networks,"['Chen Xu', 'Jianqiang Yao', 'Zhouchen Lin', 'Wenwu Ou', 'Yuanbin Cao', 'Zhirong Wang', 'Hongbin Zha']",Accept,2018,"[18, 1, 20, 2, 1, 19, 31]","[23, 1, 25, 6, 1, 23, 36]","[221, 2, 373, 62, 2, 25, 386]","[93, 1, 135, 30, 1, 13, 282]","[32, 1, 116, 30, 1, 5, 22]","[96, 0, 122, 2, 0, 7, 82]","I have read the comments and clarifications from the authors. They have added extra experiments, and clarified the speed-ups concern raised by others. I keep my original rating of the paper.

---------------
ORIGINAL REVIEW:

This paper introduces a multi-bit quantization method for recurrent neural networks, which is built on alternating the minimization formulated by Guo et al. 2017 by first fixing the \alpha values and then finding the optimal binary codes b_i with a BST, to then estimate \alpha with the refined approximation by Guo et al. 2017, iteratively. The observation that the optimal binary code can be computed with a BST is simple and elegant.

The paper is easy to follow and the topic of reducing memory and speeding up computations for RNN and DNN is interesting and relevant to the community.

The overall contribution on model quantization is based on existing methods, which makes the novelty of the paper suffer a bit. Said that, applying it to RNN is a convincing and a strong motivation. Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. This is, not only saves memory storage and usage, but also on runtime calculation using CPU, which is an important characteristic when there are limited computational resources.

Results on language models show that the models with quantized weights with 3 bits obtain the same or even slightly better performance on the tested datasets with impressive speed-ups and memory savings.

For completeness, it would be interesting, and I would strongly encourage to add a discussion or even an experiment using feedforward DNN with a simple dataset as MNIST, as most of previous work discussed in the paper report experiments on DNN that are feedforward. Would the speed-ups and memory savings obtained for RNN hold also for feedforward networks?




","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Divide and Conquer Networks,"['Alex Nowak', 'David Folqué', 'Joan Bruna']",Accept,2018,"[-2, 23, 23]","[1, 27, 27]","[1, 48, 91]","[1, 38, 72]","[0, 0, 0]","[0, 10, 19]","This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.  However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story.

I will be happy to revisit the rating if the experimental section is enriched.

Pros:
- very easy to follow idea and model
- simple merge or RL and SL in an end-to-end trainable model
- improvements over previous solutions

Cons:
- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. 
- TSP experiments show that ""in distribution"" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). 
- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments

Side notes:
- DCN is already quite commonly used abbreviation for ""Deep Classifier Network"" as well as ""Dynamic Capacity Network"", thus might be a good idea to find different name.
- please fix \cite calls to \citep, when authors name is not used as part of the sentence, for example:
Graph Neural Network Nowak et al. (2017) 
should be
Graph Neural Network (Nowak et al. (2017))

# After the update

Evaluation section has been updated threefold:
- TSP experiments are now in the appendix rather than main part of the paper
- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering
- Knapsack problem has been added

Paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. I can see the benefit of trainable approach here, the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that.

I increased rating for the paper, however in order to put the ""clear accept"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).

","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,"['Will Grathwohl', 'Dami Choi', 'Yuhuai Wu', 'Geoff Roeder', 'David Duvenaud']",Accept,2018,"[3, 2, 3, 2, 9]","[8, 4, 8, 6, 13]","[28, 9, 69, 13, 103]","[12, 4, 28, 5, 52]","[15, 5, 38, 8, 49]","[1, 0, 3, 0, 2]","This paper introduces LAX/RELAX, a method to reduce the variance of the REINFORCE gradient estimator. The method builds on and is directly inspired by REBAR. Similarly to REBAR, RELAX is an unbiased estimator, and the idea is to introduce a control variate that leverages the reparameterization gradient. In contrast to REBAR, RELAX learns a free-from control variate, which allows for low-variance gradient estimates for both discrete and continuous random variables. The method is evaluated on a toy experiment, as well as the discrete VAE and reinforcement learning. It effectively reduces the variance of state-of-the-art methods (namely, REBAR and actor-critic).

Overall, I enjoyed reading the paper. I think it is a neat idea that can be of interest for researchers in the field. The paper is clearly explained, and I found the experiments convincing. I have minor comments only.

+ Is there a good way to initialize c_phi prior to optimization? Given that c_phi must be a proxy for f(), maybe you can take advantage of this observation to find a good initialization for phi?

+ I was confused with the Bernoulli example in Appendix B. Consider the case theta=0.5. Then, b=H(z) takes value 1 if z>0, and 0 otherwise. Thus, p(z|b,theta) should assign mass zero to values z>0 when b=0, which does not seem to be the case with the proposed sampling scheme in page 11, since v*theta=0.5*v, which gives values in [0,0.5]. And similarly for the case b=1.

+ Why is the method called LAX? What does it stand for?

+ In Section 3.3, it is unclear to me why rho!=phi. Given that c_phi(z)=f(sigma_lambda(z))+r_rho(z), with lambda being a temperature parameter, why isn't rho renamed as phi? (the first term doesn't seem to have any parameters). In general, this section was a little bit unclear if you are not familiar with the REBAR method; consider adding more details.

+ Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach.

+ In the abstract, consider adding two of the main ideas that the estimator relies on: control variates and reparameterization gradients. This would probably be more clear than ""based on gradients of a learned function.""

+ In the first paragraph of Section 3, the sentence ""f is not differentiable or not computable"" may be misleading, because it is unclear what ""not computable"" means (one may think that it cannot be evaluated). Consider replacing with ""not analytically computable.""

+ In Section 3.3, it reads ""differentiable function of discrete random variables,"" which does not make sense.

+ Before Eq. 11, it reads ""where epsilon_t does not depend on theta"". I think it should be the distribution over epsilon_t what doesn't depend on theta.

+ In Section 6.1, it was unclear to me why t=.499 is a more challenging setting.

+ The header of Section 6.3.1 should be removed, as Section 6.3 is short.

+ In Section 6.3.1, there is a broken reference to a figure.

+ Please avoid contractions (doesn't, we'll, it's, etc.)

+ There were some other typos; please read carefully the paper and double-check the writing. In particular, I found some missing commas, some proper nouns that are not capitalized in Section 5, and others (e.g., ""an learned,"" ""gradient decent"").","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Modular Continual Learning in a Unified Visual Environment,"['Kevin T. Feigelis', 'Blue Sheffer', 'Daniel L. K. Yamins']",Accept,2018,"[2, 2, 15]","[5, 6, 20]","[7, 4, 78]","[3, 2, 40]","[4, 1, 33]","[0, 1, 5]","Reading this paper feels like reading at least two closely-related papers compressed into one, with overflow into the appendix (e.g. one about the EMS module, one about the the recurrent voting, etc).

There were so many aspects/components, that I am not entirely confident I fully understood how they all work together, and in fact I am pretty confident there was at least some part of this that I definitely did not understand. Reading it 5-20 more times would most likely help.

For example, consider the opening example of Section 3. In principle, this kind of example is great, and more of these would be very useful in this paper. This particular one raises a few questions:
-Eq 5 makes it so that $(W \Psi)$ and $(a_x)$ need to be positive or negative together.  Why use ReLu's here at all? Why not just $sign( (W \Psi) a_x) $? Multiplying them will do the same thing, and is much simpler. I am probably missing something here, would like to know what it is... (Or, if the point of the artificial complexity is to give an example of the 3 basic principles, then perhaps point this out, or point out why the simpler version I just suggested would not scale up, etc)
-what exactly, in this example, does $\Psi$ correspond to? In prev discussion, $\Psi$ is always written with subscripts to denote state history (I believe), so this is an opportunity to explain what is different here. 
-Nitpick: why is a vector written as $W$? (or rather, what is the point of bold vs non-bold here?)
-a non-bold version of $Psi$, a few lines below, seems to correspond to the 4096 features of VGG's FC6, so I am still not sure what the bold version represents

-The defs/eqns at the beginning of section 3.1 (Sc, CReLu, etc) were slightly hard to follow and I wonder whether there were any typos, e.g. was CReS meant to refer directly to Sc, but used the notation ${ReLu}^2$ instead? 

Each of these on its own would be easier to overlook, but there is a compounding effect here for me, as a reader, such that by further on in the paper, I am rather confused.

I also wonder whether any of the elements described, have more ""standard"" interpretations/notations. For example, my slight confusion propagated further: after above point, I then did not have a clear intuition about $l_i$ in the EMS module. I get that symmetry has been built in, e.g. by the definitions of CReS and CReLu, etc, but I still don't see how it all works together, e.g. are late bottleneck architectures *exactly* the same as MLPs, but where inputs have simply been symmetrized, squared, etc? Nor do I have intuition about multiplicative symmetric interactions between visual features and actions, although I do get the sense that if I were to spend several hours implementing/writing out toy examples, it would clarify it significantly (in fact, I wouldn't be too surprised if it turns out to be fairly straightforward, as in my above comment indicating a seeming equivalence to simply multiplying two terms and taking the resulting sign). If the paper didn't need to be quite as dense, then I would suggest providing more elucidation for the reader, either with intuitions or examples or clearer relationships to more familiar formulations.

Later, I did find that some of the info I *needed* in order to understand the results (e.g. exactly what is meant by a ""symmetry ablation"", how was that implemented?) was in fact in the appendices (of which there are over 8 pages).

I do wonder how sensitive the performance of the overall system is to some of the details, like, e.g. the low-temp Boltzmann sampling rather than identity function, as described at the end of S2.

My confidence in this review is somewhere between 2 and 3.

The problem is an interesting one, the overall approach makes sense, it is clear the authors have done a very substantial  amount of work, and very diligently so (well-done!), some of the ideas are interesting and seem creative, but I am not sure I understand the glue of the details, and that might be very important here in order to assess it effectively.","[8, 8, 6]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[2, 3, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop,"['Yaniv Taigman', 'Lior Wolf', 'Adam Polyak', 'Eliya Nachmani']",Accept,2018,"[10, 19, 4, 3]","[15, 24, 9, 8]","[42, 413, 41, 47]","[19, 213, 16, 14]","[21, 165, 23, 30]","[2, 35, 2, 3]","This paper present the application of the memory buffer concept to speech synthesis, and additionally learns a ""speaker vector"" that makes the system adaptive and work reasonably well on ""in-the-wild"" speech data. This is a relevant problem, and a novel solution, but synthesis is a wicked problem to evaluate, so I am not sure if ICLR is the best venue for this paper. I see two competing goals:

- If the focus is on showing that the presented approach outperforms other approaches under given conditions, a different task would be better (for example recognition, or some sort of trajectory reconstruction)
- If the focus is on showing that the system outperforms other synthesis systems, then a speech oriented venue might be best (and it is unfortunate that optimized hyper-parameters for the other systems are not available for a fair comparsion)
- If fair comparisons with the other appraoches cannot be made, my sense is that the multi-speaker (post-training fitting) option is really the most interesting and novel contribution here, which could be discussed in mroe detail

Still, the approach is creative and interesting and deserves to be presented. I have a few questions/ suggestions:

Introduction

- The link to Baddeley's ""phonological loop"" concept seems weak at best. There is nothing phonological about the features that this model stores and retrieves, and no evidence that the model behaves in a way consistent with ""phonologcial"" (or articulatory) assumptions or models - maybe best to avoid distracting the reader with this concept and strengthen the speaker adaptation aspect?
- The memory model is not an RNN, but it is a recurrently called structure (as the name ""phonological loop"" also implies) - so I would also not highlight this point much
- Why would the four properties of the proposed method (mid of p. 2, end of introduction: memory buffer, shared memory, shallow fully connected networks, and simple reader mechanism) lead to better robustness and improve performance on noisy and limited training data? Maybe the proposed approach works better for any speech synthesis task? Why specifically for ""in-the-wild"" data? The results in Table 2 show that the proposed system outperforms other systems on Blizzard 2013, but not Blizzard 2011 - does this support the previous argument?
- Why not also evaluate MCD scores? This should be a quick and automatic way to diagnose what the system is doing? Or is this not meaningful with the noisy training data?

Previous work

- Please introduce abbreviations the first time they are used (""CBHG"" for example)
- There is other work on using ""in-the-wild"" speech as well: Pallavi Baljekar and Alan W Black. Utterance Selection Techniques for TTS Systems using Found Speech, SSW 2016, Sunnyvale, USA Sept 2016

The architecture
- Please explain the ""GMM"" (Gaussian Mixture Model?) attention mechanism in a bit more detail, how does back-propagation work in this case?
- Why was this approach chosen? Does it promise to be robust or good for low data situations specifically?
- The fonts in Figure 2 are very small, please make them bigger, and the Figure may not print well in b/w. Why does the mean of the absolute weights go up for high buffer positions? Is there some ""leaking"" from even longer contexts?
- I don't understand ""However, human speech is not deterministic and one cannot expect [...] truth"". You are saying that the model cannot be excepted to reproduce the input exactly? Or does this apply only to the temporal distribution of the sequence (but not the spectral characteristics)? The previous sentence implies that it does. And how does teacher-forcing help in this case?
- what type of speed is ""x5""? Five times slower or faster than real-time?

Experiments
- Table 2: maybe mention how these results were computed, i.e. which systems use optimized hyper parameters, and which don't? How do these results support the interpretation of hte results in the introruction re in-the-wild data and found data?
- I am not sure how to read Figure 4. Maybe it would be easier to plot the different phone sequences against each other and show how the timings are off, i.e. plot the time of the center of panel one vs the time of the center of panel 2 for the corresponding phone, and show how this is different from a straight line. Or maybe plot phones as rectangles that get deformed from square shape as durations get learned?
- Figure 5: maybe provide spectrograms and add pitch contours to better show the effect of the dfifferent intonations? 
- Figure 4 uses a lot of space, could be reduced, if needed

Discussion
- I think the first claim is a bit to broad - nowhere is it shown that the method is inherently more robust to clapping and laughs, and variable prosody. The authors will know the relevant data-sets better than I do, maybe they can simply extend the discussion to show that this is what happens. 
- Efficiency: I think Wavenet has also gotten much faster and runs in less than real-time now - can you expand that discussion a bit, or maybe give estimates in times of FLOPS required, rather than anecdotal evidence for systems that may or may not be comparable?

Conclusion
- Now the advantage of the proposed model is with the number of parameters, rather than the computation required. Can you clarify? Are your models smaller than competing models?
","[5, 6, 8]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"['Taesik Na', 'Jong Hwan Ko', 'Saibal Mukhopadhyay']",Accept,2018,"[11, 4, 17]","[15, 9, 22]","[35, 69, 311]","[19, 36, 178]","[5, 13, 37]","[11, 20, 96]","The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar. The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks. The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks. 

The idea proposed is fairly straight-forward. Despite being a simple approach, the experimental results are quite promising.  The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.  

As pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings. Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?

Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive. Do the authors have any insight on this? 

Comments:
1. The writing of the paper could be improved. For example, ""Transferability analysis"" in section 1 is barely understandable;
2. Arrow in Figure 3 are not quite readable;
3. The paper is over 11 pages. The authors might want to consider shrink it down the recommended length. ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN,"['Xu Chen', 'Jiang Wang', 'Hao Ge']",Accept,2018,"[21, 1, 12]","[26, 1, 17]","[481, 2, 67]","[205, 1, 31]","[83, 1, 8]","[193, 0, 28]","This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem.
As a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point.
Additionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models.
But this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1.
As follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem.
Most important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it. But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Emergent Communication through Negotiation,"['Kris Cao', 'Angeliki Lazaridou', 'Marc Lanctot', 'Joel Z Leibo', 'Karl Tuyls', 'Stephen Clark']",Accept,2018,"[3, 8, 12, 8, 17, 20]","[8, 12, 17, 13, 22, 25]","[19, 64, 105, 92, 242, 139]","[7, 32, 46, 29, 145, 94]","[11, 30, 48, 54, 53, 28]","[1, 2, 11, 9, 44, 17]","The authors describe a variant of the negotiation game in which agents of different type, selfish or prosocial, and with different preferences. The central feature is the consideration of a secondary communication (linguistic) channel for the purpose of cheap talk, i.e. talk whose semantics are not laid out a priori. 

The essential findings include that prosociality is a prerequisite for effective communication (i.e. formation of meaningful communication on the linguistic channel), and furthermore, that the secondary channel helps improve the negotiation outcomes.

The paper is well-structured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel. Finally, agent societies are represented by injecting individuals' ID into the input representation.

The positive:
- The authors attack the challenging task of given agents a means to develop communication patterns without apriori knowledge.
- The paper presents the problem in a well-structured manner and sufficient clarity to retrace the essential contribution (minor points for improvement).
- The quality of the text is very high and error-free.
- The background and results are well-contextualised with relevant related work. 

The problematic:
- By the very nature of the employed learning mechanisms, the provided solution provides little insight into what the emerging communication is really about. In my view, the lack of interpretable semantics hardly warrants a reference to 'cheap talk'. As such the expectations set by the well-developed introduction and background sections are moderated over the course of the paper.
- The goal of providing agents with richer communicative ability without providing prior grounding is challenging, since agents need to learn about communication partners at runtime. But it appears as of the main contribution of the paper can be reduced to the decomposition of the learnable feature space into two communication channels. The implicit relationship of linguistic channel on proposal channel input based on the time information (Page 4, top) provides agents with extended inputs, thus enabling a more nuanced learning based on the relationship of proposal and linguistic channel. As such the well-defined semantics of the proposal channel effectively act as the grounding for the linguistic channel. This, then, could have been equally achieved by providing agents with a richer input structure mediated by a single channel. From this perspective, the solution offers limited surprises. The improvement of accuracy in the context of agent societies based on provided ID follows the same pattern of extending the input features.
- One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents. However, apart from this initial statement, this feature is not explicitly picked up. In combination with the previous point, the necessity/value of the additional communication channel is unclear.

Concrete suggestions for improvement:

- Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper.
- Figure 3 is really hard to read/interpret. The same applies to Figure 4 (although less critical in this case).
- Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities (e.g. in appendix).
- Emergent communication is effectively unidirectional, with one agent as listener. Have you observed other outcomes in your evaluation?

In summary, the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well-established negotiation game. The problem is addressed systematically and well-presented, but can leave the reader with the impression that the secondary channel, apart from decomposing the model, does not provide conceptual benefit over introducing a richer feature space that can be exploited by the learning mechanisms. Combined with the lack of specific cheap talk features, the use of actual cheap talk is rather abstract. Those aspects warrant justification.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to Multi-Task by Active Sampling,"['Sahil Sharma*', 'Ashutosh Kumar Jha*', 'Parikshit S Hegde', 'Balaraman Ravindran']",Accept,2018,"[6, 1, 1, 21]","[11, 1, 6, 26]","[36, 1, 10, 253]","[12, 1, 7, 142]","[12, 0, 3, 86]","[12, 0, 0, 25]","
The authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic 
change to the distribution to UCB to feature-based neural-network based RL. The domain is video games.   All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).


Pros:

- very promising results with an interesting active learning approach to multitask RL

- a number of approaches developed for the basic idea

- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)

- paper is overall well written/clear

Cons:

- Comparison only to a very basic baseline (i.e. uniform sampling)
Couldn't comparisons be made, in some way, to other multitask work?



Additional  comments:

- The assumption of the availability of a target score goes against
the motivation that one need not learn individual networks ..  authors
say instead one can use 'published' scores, but that only assumes
someone else has done the work (and furthermore, published it!).

The authors do have a section on eliminating the need by doubling an
estimate for each task) which makes this work more acceptable (shown
for 6 tasks or MT1, compared to baseline uniform sampling).

Clearly there is more to be done here for a future direction (could be
mentioned in future work section).

- The averaging metrics (geometric, harmonic vs arithmetic, whether
  or not to clip max score achieved) are somewhat interesting, but in
  the main paper, I think they are only used in section 6 (seems like
  a waste of space). Consider moving some of the results, on showing
  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to
  the main paper.


- The can be several benefits to multitask learning, in particular
  time and/or space savings in learning new tasks via learning more
  general features. Sections 7.2 and 7.3 on specificity/generality of
  features were interesting.



--> Can the authors show that a trained network (via their multitask
    approached) learns significantly faster on a brand new game
    (that's similar to games already trained on), compared to learning from
    scratch?

--> How does the performance improve/degrade (or the variance), on the
    same set of tasks, if the different multitask instances (MT_i)
    formed a supersets hierarchy, ie if MT_2 contained all the
    tasks/games in MT_1, could training on MT_2 help average
    performance on the games in MT_1 ? Could go either way since the network
   has to allocate resources to learn other games too.  But is there a pattern?



- 'Figure 7.2' in section 7.2 refers to Figure 5.


- Can you motivate/discuss better why not providing the identity of a
  game as an input is an advantage? Why not explore both
  possibilities? what are the pros/cons? (section 3)




","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Variational Network Quantization,"['Jan Achterhold', 'Jan Mathias Koehler', 'Anke Schmeink', 'Tim Genewein']",Accept,2018,"[9, 17, -3]","[13, 21, 1]","[5, 38, 2]","[3, 24, 1]","[0, 0, 0]","[2, 14, 1]","This paper presents Variational Network Quantization; a variational Bayesian approach for quantising neural network weights to ternary values post-training in a principled way. This is achieved by a straightforward extension of the scale mixture of Gaussians perspective of the log-uniform prior proposed at [1]. The authors posit a mixture of delta peaks hyperprior over the locations of the Gaussian distribution, where each peak can be seen as the specific target value for quantisation (including zero to induce sparsity). They then further propose an approximation for the KL-divergence, necessary for the variational objective, from this multimodal prior to a factorized Gaussian posterior by appropriately combining the approximation given at [2] for each of the modes. At test-time, the variational posterior for each weight is replaced by the target quantisation value that is closest, w.r.t. the squared distance, to the mean of the Gaussian variational posterior. Encouraging experimental results are shown with performance comparable to the state-of-the-art for ternary weight neural networks.

This paper presented a straightforward extension of the work done at [1, 2] for ternary networks through a multimodal quantising prior. It is generally well-written, with extensive preliminaries and clear equations. The visualizations also serve as a nice way to convey the behaviour of the proposed approach. The idea is interesting and well executed so I propose for acceptance. I only have a couple of minor questions: 
- For the KL-divergence approximation you report a maximum difference of 1 nat per weight that seems a bit high; did you experiment with the `naive` Monte Carlo approximation of the bound (e.g. as done at Bayes By Backprop) during optimization? If yes, was there a big difference in performance?
- Was pre-training necessary to obtain the current results for MNIST? As far as I know, [1] and [2] did not need pre-training for the MNIST results (but did employ pre-training for CIFAR 10).
- How necessary was each one of the constraints during optimization (and what did they prevent)? 
- Did you ever observe posterior means that do not settle at one of the prior modes but rather stay in between? Or did you ever had issues of the variance growing large enough, so that q(w) captures multiple modes of the prior (maybe the constraints prevent this)? How sensitive is the quantisation scheme?

Other minor comments / typos:
(1) 7th line of section 2.1 page 2, ‘a unstructured data’ -> ‘unstructured data’
(2) 5th line on page 3, remove ‘compare Eq. (1)’ (or rephrase it appropriately).
(3) Section 2.2, ’Kullback-Leibler divergence between the true and the approximate posterior’; between implies symmetry (and the KL isn’t symmetric) so I suggest to change it to e.g. ‘from the true to the approximate posterior’ to avoid confusion. Same for the first line of Section 3.3.
(4) Footnote 2, the distribution of the noise depends on the random variable so I would suggest to change it to a general \epsilon \sim p(\epsilon).
(5) Equation 4 is confusing.

[1] Louizos, Ullrich & Welling, Bayesian Compression for Deep Learning.
[2] Molchanov, Ashukha & Vetrov, Variational Dropout Sparsifies Deep Neural Networks.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Variational Continual Learning,"['Cuong V. Nguyen', 'Yingzhen Li', 'Thang D. Bui', 'Richard E. Turner']",Accept,2018,"[19, 8, 5, 23]","[24, 13, 9, 28]","[209, 56, 16, 222]","[106, 27, 9, 134]","[22, 20, 5, 49]","[81, 9, 2, 39]","The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning. The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations. 

The paper is well written, the problem is described neatly in conjunction with the past work, and the proposed algorithm is supported by experiments. The work is a useful addition to the community.

My main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach,"['Tsui-Wei Weng*', 'Huan Zhang*', 'Pin-Yu Chen', 'Jinfeng Yi', 'Dong Su', 'Yupeng Gao', 'Cho-Jui Hsieh', 'Luca Daniel']",Accept,2018,"[3, 3, 9, 7, 13, 5, 11, 18]","[8, 8, 14, 12, 18, 9, 16, 23]","[45, 102, 390, 125, 37, 13, 355, 136]","[16, 51, 150, 64, 12, 8, 171, 63]","[28, 50, 199, 55, 11, 4, 158, 39]","[1, 1, 41, 6, 14, 1, 26, 34]","In this work, the objective is to analyze the robustness of a neural network to any sort of attack.

This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function. This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community.

This is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[1, 3, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,"['Artemij Amiranashvili', 'Alexey Dosovitskiy', 'Vladlen Koltun', 'Thomas Brox']",Accept,2018,"[1, 6, 19, 17]","[5, 10, 24, 22]","[11, 96, 295, 297]","[4, 41, 135, 155]","[7, 44, 103, 101]","[0, 11, 57, 41]","This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length. Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity. The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings. There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations. They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom.

Overall, I find the paper important for furthering the understanding of fundamental RL algorithms. However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head. May this helped Q_MC have better perceptual capabilities?

A couple of other questions:
- I couldn't find any mention of eligibility traces - why?
- Why was the async RL framework used? It would be nice to have a discussion on whether this choice may have affected the results.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Bayesian Perspective on Generalization and Stochastic Gradient Descent,['Samuel L. Smith and Quoc V. Le'],Accept,2018,"[3, 14]","[8, 19]","[30, 299]","[10, 143]","[20, 145]","[0, 11]","Summary:
This paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017). It does so from the perspective of ""Bayesian model comparison"", where two models are compared based on their ""marginal likelihood"" (aka, their ""evidence"" --- the expected probability of the training data under the model, when parameters are drawn from the prior).  It first shows that a simple weakly regularized (linear) logistic regression model over 200 dimensional data can perfectly memorize a random training set with 200 points, while also generalizing well when the class labels are not random (eg, when a simple linear model explains the class labels); this provides a much simpler example of a model generalizing well in spite of high capacity, relative to the experiments presented by Zhang et al (2017). It shows that in this very simple setting, the ""evidence"" of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data).

The paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior).  Thus, Bayesian evidence prefers minima that are both deep and broad.  This provides a way of comparing models in a way which is independent of the model parametrization (unfortunately, however, computing the evidence is intractable for large networks). The paper then discusses how SGD can be seen as an algorithmic way of finding minima with large ""evidence"" --- the ""noise"" in the gradient estimation helps the model avoid ""sharp"" minima, while the gradient helps the model find ""deep"" minima.  The paper shows that SGD can be understood using stochastic differential equations, where the noise scale is approximately aN/((1-m)B) (a = learning rate, N = size of training set, B = batch size, m = momentum).  It argues that because there should be an optimal noise scale (which maximizes test performance), the batch size should be taken proportional to the learning rate, as well as the training set size, and proportional to 1/(1-m).  These scaling rules are confirmed experimentally (DNN trained on MNIST).  Thus, this Bayesian perspective can also help explain the observation that models trained with smaller batch sizes (noisier gradient estimates) often generalize better than those with larger batch sizes (Kesker et al, 2016). These scaling rules provide guidance on how to increase the batch size, which is desirable for increasing the parralelism of SGD training.

Review:
Quality: The quality of the work is high.  Experiments and analysis are both presented clearly.

Clarity: The paper is relatively clear, though some of the connections between the different parts of the paper felt unclear to me:
1) It would be nice if the paper were to explain, from a theoretical perspective, why large evidence should correspond to better generalization, or provide an overview of the work which has shown this (eg, Rissanen, 1983).
2) Could margin-based generalization bounds explain the superior generalization performance of the linear model trained on random vs. non-random data?  It seems to me that the model trained on meaningful data should have a larger margin.
3) The connection between the work on Bayesian evidence, and the work on SGD, felt very informal. The link seems to be purely intuitive (SGD should converge to minima with high evidence, because its updates are noisy).  Can this be formalized?  There is a footnote on page 7 regarding Bayesian posterior sampling -- I think this should be brought into the body of the paper, and explained in more detail.
4) The paper does not give any background on stochastic differential equations, and why there should be an optimal noise scale 'g', which remains constant during the stochastic process, for converging to a minima with high evidene.  Are there any theoretical results which can be leveraged from the stochastic processes literature? For example, are there results which prove anything regarding the convergence of a stochastic process under different amounts of noise?
5) It was unclear to me why momentum was used in the MNIST experiments.  This seems to complicate the experimental setting.  Does the generalization gap not appear when no momentum is used?  Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4?  If the learning rate were optimized together with batch size (eg, keeping aN/B constant), would the generalization gap still appear?  Figure 5a seems to suggest that it would not appear (peaks appear to all have the same test accuracy).
6) It was unclear to me whether the analysis of SGD as a stochastic differential equation with noise scale aN/((1-m)B) was a contribution of this paper.  It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original.
7) Some small feedback: The notation $< x_i > = 0$ and $< x_i^2 > = 1$ is not explained.  Is each feature being normalized to be zero mean, unit variance, or is each training example being normalized?

Originality: The works seems to be relatively original combination of ideas from Bayesian evidence, to deep neural network research.  However, I am not familiar enough with the literature on Bayesian evidence, or the literature on sharp/broad minima, and their generalization properties, to be able to confidently say how original this work is.

Significance: I believe that this work is quite significant in two different ways:
1) ""Bayesian evidence"" provides a nice way of understanding why neural nets might generalize well, which could lead to further theoretical contributions.
2) The scaling rules described in section 5 could help practitioners use much larger batch sizes during training, by simultaneously increasing the learning rate, the training set size, and/or the momentum parameter.  This could help parallelize neural network training considerably.

Some things which could limit the significance of the work:
1) The paper does not provide a way of measuring the (approximate) evidence of a model.  It simply says it is prohibitively expensive to compute for large models.  Can the ""Gaussian approximation"" to the evidence (equation 10) be approximated efficiently for large neural networks?
2) The paper does not prove that SGD converges to models of high evidence, or formally relate the noise scale 'g' to the quality of the converged model, or relate the evidence of the model to its generalization performance.

Overall, I feel the strengths of the paper outweight its weaknesses.  I think that the paper would be made stronger and clearer if the questions I raised above are addressed prior to publication.","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Fix your classifier: the marginal value of training the last weight layer,"['Elad Hoffer', 'Itay Hubara', 'Daniel Soudry']",Accept,2018,"[4, 5, 9]","[8, 9, 14]","[35, 26, 90]","[14, 11, 35]","[19, 14, 42]","[2, 1, 13]","This paper proposes replacing the weights of the final classifier layer in a CNN with a fixed projection matrix.  In particular a Hadamard matrix can be used, which can be represented implicitly.

I'd have liked to see some discussion of how to efficiently implement the Hadamard transform when the number of penultimate features does not match the number of classes, since the provided code does not do this.

How does this approach scale as the number of classes grows very large (as it would in language modeling, for example)?

An interesting experiment to do here would be to look this technique interacts with distillation, when used in the teacher or student network or both.   Does fixing the features make it more difficult to place dog than on boat when classifying a cat?  Do networks with fixed classifier weights make worse teachers for distillation?
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Compositional Obverter Communication Learning from Raw Visual Input,"['Edward Choi', 'Angeliki Lazaridou', 'Nando de Freitas']",Accept,2018,"[13, 8, 19]","[18, 12, 24]","[102, 64, 199]","[41, 32, 101]","[56, 30, 84]","[5, 2, 14]","This paper proposes to apply the obverter technique of Batali (1998) to a multi-agent communication game. The main novelty with respect to Batali's orginal work is that the agents in this paper have to communicate about images represented at the raw pixel level. The paper presents an extensive analysis of the patterns learnt by the agents, in particular in terms of how compositional they are.

I greatly enjoyed reading the paper: while the obverter idea is not new, it's nice to see it applied in the context of modern deep learning architectures, and the analysis of the results is very interesting.

The writeup is somewhat confusing, and in particular the reader has to constantly refer to the supplementary materials to make sense of the models and experiments. At least the model architecture could be discussed in more detail in the main text.

It's a pity that there is no direct quantitative comparison between the obverter technique and a RL architecture.

Some more detailed comments:

It would be good if a native speaker could edit the paper for language and style (I only annotated English problems in the abstract, since there were just too many of them).

Abstract:

distinguishing natures -> aspects

describe the complex environment -> describe complex environments

with the accuracy -> with accuracy

Introduction:

have shown great progress -> has...

The claim that language learning in humans is entirely based on communication needs hedging. See, e.g., cultures where children must acquire quite a bit of their language skills from passive listening to adult conversations (Schieffelin & Ochs 1983; Shore 1997).

Also, while it's certainly the case that humans do not learn from the neatly hand-crafted features favored in early language emergence studies, their input is most definitely not pixels (and, arguably, it is something more abstract than raw visual stimuli, see, e.g., the work on ""object biases"" in language acquisition).

Method:

The description of the experiment sometimes refers to the listener having to determine whether it is seeing the same image as the speaker, whereas the game consists in the listener telling whether it sees the same object as the speaker. This is important, since, crucially, the images can present the same object in different locations.

Section 2.2 is very confusing, since the obverter technique has not been introduced, yet, so I kept wondering why you were only describing the listener architecture. Perhaps, current section 2.3 should be moved before 2.2.

Also in 2.2: what is the ""meaning vector"" that BAtali's RNN hidden vector has to be close to?

It's confusing to refer to the binary 0 and 1 vectors as outputs of the sigmoid: they are rather the binary categories you want the agents to approximate with the sigmoid function.

The obverter technique should be described in more detail in the main text. In particular, with max message length 20 and a vocabulary of 5 symbols, you'll have an astronomical number of possible inputs to evaluate at each step: is this really what you're doing?

Experiments:

distinctness: better call it distinctiveness

Is training accuracy in Fig 4 averaged across both agents? And what were the values the Jaccard similarity was computed over?

It's nice that the agents discovered some non-strictly agglutinative morphology, consisting in removing symbols from the prefix. Also, it looks like they are developing some notion of ""inflectional class"" (Aronoff 1994) (the color groups), which is also intriguing. However, I did not understand rules such as: ""remove two as and add one a""... isn't that the same as: ""add one a""?

Discussion and future work:

I didn't follow the discussion about partial truth and using negations.

There is another ICLR submission that proposes a quantitative measure of compositionality you might find interesting: https://openreview.net/pdf?id=HJGv1Z-AW
","[9, 6, 3]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Temporally Efficient Deep Learning with Spikes,"[""Peter O'Connor"", 'Efstratios Gavves', 'Matthias Reisser', 'Max Welling']",Accept,2018,"[22, 9, 1, 19]","[23, 14, 6, 24]","[32, 137, 9, 390]","[20, 64, 3, 190]","[5, 62, 6, 166]","[7, 11, 0, 34]","This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This approach extends the method presented on Arxiv on Sigma delta quantized networks (Peter O’Connor and Max Welling. Sigma delta quantized networks. arXiv preprint arXiv:1611.02024, 2016b.). Overall, the paper is interesting and promising; only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing. The paper, however, is not flawless. The authors demonstrate the method on just two datasets, and effectively they show results of training only for Feed-Forward Neural Nets (the authors claim that “the entire spiking network end-to-end works” referring to their pre-trained VGG19, but this paper presents only training for the three top layers). Furthermore, even if suitable datasets are not available, the authors could have chosen to train different architectures. The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST. Although it is a common base-line, some choices are not clear: why using a FFNN instead that a CNN which performs better on this dataset; how data is presented in terms of temporal series – this applies to the Temporal MNIST too; why performances for Temporal MNIST – which should be a more suitable dataset — are worse than for the standard MNIST; what is the meaning of the right column of Figure 5 since it’s just a linear combination of the GOps results. For the second dataset, some points are not clear too: why the labels and the pictures seem not to match (in appendix E); why there are more training iterations with spikes w.r.t. the not-spiking case. Overall, the paper is mathematically sound, except for the “future updates” meaning which probably deserves a clearer explanation. Moreover, I don’t see why the learning rule equations (14-15) are described in the appendix, while they are referred constantly in the main text. The final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in Appendix D: perhaps this problem affects CCNs more than FFN. 
Finally, there are some minor issues here and there (the authors show quite some lack of attention for just 7 pages):
-	Two times “get” in “we get get a decoding scheme” in the introduction;
-	Two times “update” in “our true update update as” in Sec. 2.6;
-	Pag3 correct the capital S in 2.3.1
-	Pag4 Figure 1 increase font size (also for Figure2); close bracket after Equation 3; N (number of spikes) is not defined
-	Pag5 “one-hot” or “onehot”; 
-	in the inline equation the sum goes from n=1 to S, while in eq.(8) it goes from n=1 to N;
-	Eq(10)(11)(12) and some lines have a typo (a \cdot) just before some of the ws;
-	Pag6 k_{beta} is not defined in the main text;
-	Pag7 there are two “so that” in 3.1; capital letter “It used 32x10^12..”; beside, here, why do not report the difference in computation w.r.t. not-spiking nets?
-	Pag7 in 3.2 “discussed in 1” is section 1?
-	Pag14 Appendix E, why the labels don’t match the pictures;
-	Pag14 Appendix F, explain better the architecture used for this experiment.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Countering Adversarial Images using Input Transformations,"['Chuan Guo', 'Mayank Rana', 'Moustapha Cisse', 'Laurens van der Maaten']",Accept,2018,"[5, 7, 9, 14]","[10, 7, 13, 19]","[78, 3, 35, 118]","[27, 2, 18, 57]","[42, 1, 15, 48]","[9, 0, 2, 13]","Summary: This works proposes strategies to make neural networks less sensitive to adversarial attacks. They consist into applying different transformations to the images, such as quantization, JPEG compression, total variation minimization and image quilting. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images.
Experiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).
60% of attacks are countered in this last most difficult setting.
The previous best approach for this task consists in ensemble training and is attack specific. It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25%.  

Comments: The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results.
 
The discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice.
Minor: the bibliography should be uniformed.","[8, 4, 7]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Fraternal Dropout,"['Konrad Zolna', 'Devansh Arpit', 'Dendi Suhubdy', 'Yoshua Bengio']",Accept,2018,"[3, 8, 2, 31]","[7, 13, 2, 36]","[35, 45, 6, 975]","[14, 20, 2, 405]","[19, 25, 4, 454]","[2, 0, 0, 116]","The paper proposes “fraternal dropout”, which passes the same input twice through a model with different dropout masks. The L2 norm of the differences is then used as an additional regulariser. As the authors note, this implicitly minimises the variance of the model under the dropout mask.

The method is well presented and adequately placed within the related work. The text is well written and easy to follow.

I have only two concerns. The first is that the method is rather incremental and I am uncertain how it will stand the test of time and will be adopted.

The second is that of the experimental evaluation. They authors write that a full hyper parameter search was not conducted in the fear of having a more thorough evaluation than the base lines, erroneously reporting superior results.

To me, this is not an acceptable answer. IMHO, the evaluation should be thorough for both the base lines and the proposed method. If authors can get away with a sub standard evaluation because the competing method did, the field might converge to sub standard evaluations overall. This is clearly not in anyones interest. I am open to the author's comments on this, as I understand that spending weeks on tuning a competing method is also not unbiased and work that could be avoided if all software was published.
","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
MGAN: Training Generative Adversarial Nets with Multiple Generators,"['Quan Hoang', 'Tu Dinh Nguyen', 'Trung Le', 'Dinh Phung']",Accept,2018,"[2, 6, 9, 17]","[4, 10, 14, 22]","[6, 61, 145, 393]","[4, 36, 86, 228]","[2, 18, 47, 105]","[0, 7, 12, 60]","Summary:

The paper proposes a mixture of  generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added.


Quality/clarity:

The paper is well written and easy to follow.

clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text.

Originality:

 Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator.

General review:

- when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!

- Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?  
 
- in the tied weight case, in the synthetic example, can you show what each ""generator"" of the mixture learn? are they really learning modes of the data? 

- the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior.  would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generative Models of Visually Grounded Imagination,"['Ramakrishna Vedantam', 'Ian Fischer', 'Jonathan Huang', 'Kevin Murphy']",Accept,2018,"[5, 13, 27, 24]","[10, 17, 32, 29]","[31, 45, 94, 164]","[14, 19, 53, 91]","[15, 23, 33, 57]","[2, 3, 8, 16]","The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified, and when some are left undefined, creating a more abstract generation task. 

Pros:
+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing. Both quantitative and qualitative results are provided. 
+ The paper is fairly clear.

Cons:
- It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b).
- Fig. 5 could be more convincing; ""bushy eyebrows"" is a difficult attribute to judge, and in the abstract generation when that is the only attribute specified, it is not clear how good the results are.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Multi-View Data Generation Without View Supervision,"['Mickael Chen', 'Ludovic Denoyer', 'Thierry Artières']",Accept,2018,"[3, 16, 26]","[8, 20, 30]","[18, 147, 95]","[8, 88, 72]","[9, 40, 9]","[1, 19, 14]","This paper firstly proposes a GAN architecture that aim at decomposing the underlying distribution of a particular class into ""content"" and ""view"". The content can be seen as an intrinsic instantiation of the class that is independent of certain types of variation (eg viewpoint), and a view is the observation of the object under a particular variation. The authors additionally propose a second conditional GAN that learns to generate different views given a specific content. 

I find the idea of separating content and view interesting and I like the GMV and CGMV architectures. Not relying on manual attribute/class annotation for the views is also positive. The approach seems to work well for a relatively clean setup such as the chair dataset, but for the other datasets the separation is not so apparent. For example, in figure 5, what does each column represent in terms of view? It seems that it depends heavily on the content. That raises the question of how useful it is to have such a separation between content and views; for some datasets their diversity can be a bottleneck for this partition, making the interpretation of views difficult. 

A missing (supervised) reference that considers also the separation of content and views.
[A] Learning to generate chairs with convolutional neural networks, Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, CVPR 15

Q:Figure 5, you mean ""all images in a column were generated with the same view vector""
Q: Why on Figure 7 you use different examples for CGAN?","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Multi-level Residual Networks from Dynamical Systems View,"['Bo Chang', 'Lili Meng', 'Eldad Haber', 'Frederick Tung', 'David Begert']",Accept,2018,"[13, 9, 21, 10, 2]","[18, 14, 26, 15, 2]","[78, 81, 100, 47, 4]","[44, 23, 22, 21, 2]","[14, 18, 43, 18, 2]","[20, 40, 35, 8, 0]","This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way. On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy. The paper is interesting and easy to follow. 

I have several comments:
1.	It would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance. Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?
2.	The mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models. It would be interesting to see a comparison or discussion. 
[1] C Gulcehre, Mollifying Networks, 2016
3.	Could you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge,"['Emmanuel de Bezenac', 'Arthur Pajot', 'Patrick Gallinari']",Accept,2018,"[2, 2, 31]","[7, 6, 36]","[26, 7, 382]","[12, 3, 256]","[13, 3, 54]","[1, 1, 72]","The paper ‘Deep learning for Physical Process: incorporating prior physical knowledge’ proposes
to question the use of data-intensive strategies such as deep learning in solving physical 
inverse problems that are traditionally solved through assimilation strategies. They notably show
how physical priors on a given phenomenon can be incorporated in the learning process and propose 
an application on the problem of estimating sea surface temperature directly from a given 
collection of satellite images.

All in all the paper is very clear and interesting. The results obtained on the considered problem
are clearly of great interest, especially when compared to state-of-the-art assimilation strategies
such as the one of Béréziat. While the learning architecture is not original in itself, it is 
shown that a proper physical regularization greatly improves the performance. For these reasons I 
believe the paper has sufficient merits to be published at ICLR. That being said, I believe that 
some discussions could strengthen the paper:
 - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating
uncertainties in the observation or physical evolution models. It is still unclear how those uncertainties 
can be integrated in the model;
 - Assimilation methods are usually independent of the type of data at hand. It is not clear how the model
learnt on one particular type of data transpose to other data sequences. Notably, the question of transfer
and generalization is of high relevance here. Does the learnt model performs well on other dataset (for instance,
acquired on a different region or at a distant time). I believe this type of issue has to be examinated 
for this type of approach to be widely use in inverse physical problems. 
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Natural Language Inference over Interaction Space,"['Yichen Gong', 'Heng Luo', 'Jian Zhang']",Accept,2018,"[1, 25]","[6, 30]","[7, 221]","[2, 97]","[0, 8]","[5, 116]","Pros: 
The paper proposes a “Densely Interactive Inference Network (DIIN)” for NLI or NLI alike tasks. Although using tensors to capture high-order interaction and performing dimension reduction over that are both not novel, the paper explores them for NLI. The paper is written clearly and is very easy to follow. The ablation experiments in Table 5 give a good level of details to help observe different components' effectiveness.
Cons:
1) The differences of performances between the proposed model and the previous models are not very clear. With regard to MultiNLI, since the previous results (e.g., those in Table 2) did not use cross-sentence attention and had to represent a premise or a hypothesis as a *fixed-length* vector, is it fair to compare DIIN with them? Note that the proposed DIIN model does represent a premise or a hypothesis by variable lengths (see interaction layer in Figure 1), and tensors provide some sorts of attention between them. Can this (Table 2) really shows the advantage of the proposed models? However, when a variable-length representation is allowed (see Table 3 on SNLI), the advantage of the model is also not observed, with no improvement as a single model (compared with ESIM) and being almost same as previous models (e.g., model 18 in Table 3) in ensembling.
2) Method-wise, as discussed above, using tensors to capture high-order interaction and performing dimension reduction over that are both not novel.
3) The paper mentions the use of untied parameters for premise and hypothesis, but it doesn’t compare it with tied version in the experiment section. 
4) In Table 6, for CONDITIONAL tag, why the baseline models (lower total accuracies) have a 100% accuracy, but DIIN only has about a 60% accuracy?
","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning Sparse Neural Networks through L_0 Regularization,"['Christos Louizos', 'Max Welling', 'Diederik P. Kingma']",Accept,2018,['title not found'],['title not found'],[],[],[],[],"Learning sparse neural networks through L0 regularisation

Summary: 

The authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations. 

Pros:

The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. 

Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. 

The work is put in context and related to some previous relaxation approaches to sparsity. 

The method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. 

Cons:

The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods. Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered, e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. 

It was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. 

Minor point: it wasn’t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don’t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. In the context here this didn’t seem a particularly relevant addition to the paper. 

","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Loss-aware Weight Quantization of Deep Networks,"['Lu Hou', 'James T. Kwok']",Accept,2018,"[4, 26]","[9, 31]","[70, 256]","[31, 136]","[22, 45]","[17, 75]","This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights). 

The paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose. The experiments are very clearly presented and solidly designed.

In fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.

Finally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm. However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \in Q^{n_l}$). This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
"Don't Decay the Learning Rate, Increase the Batch Size","['Samuel L. Smith', 'Pieter-Jan Kindermans', 'Chris Ying', 'Quoc V. Le']",Accept,2018,"[3, 7, 2, 14]","[8, 11, 3, 19]","[30, 39, 9, 299]","[10, 17, 4, 143]","[20, 16, 5, 145]","[0, 6, 0, 11]","The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant. This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time. The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent. In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size. It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance.

COMMENTS:

The paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales. The observation is explained well and substantiated by clear experimental evidence. The main issue I have is with the part about momentum. The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased. It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$. The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. This effect is well known, but it can easily be remedied. For example, the update equations in Adam were specifically designed to correct for this effect. The mechanism is called ""bias-corrected moment estimate"" in the Adam paper, arXiv:1412.6980. The correction requires only two extra multiplications per model parameter and update step. Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size? It would be great to see the equivalent of Figure 7 with correctly rescaled $A$.

Minor issues:
* The last paragraph of Section 5 refers to a figure 8, which appears to be missing.
* In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below).
* It appears that a minus sign is missing in Eq. 7. The update steps describe gradient ascent.
* Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting. This suggests that the number of updates in this segment was chosen unnecessarily large to begin with. It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.
* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs. While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs.
* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,"['Shankar Krishnan', 'Ying Xiao', 'Rif. A. Saurous']",Accept,2018,"[24, 16, 3]","[29, 21, 8]","[79, 54, 43]","[41, 19, 18]","[11, 12, 24]","[27, 23, 1]","The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper.

To me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it.  Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works.

Otherwise, the paper is clearly written.
The topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to).

I am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \alpha, \beta, \gamma, \mu, \eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters?


 ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Approximate Inference Networks for Structured Prediction,"['Lifu Tu', 'Kevin Gimpel']",Accept,2018,"[3, 11]","[8, 16]","[26, 161]","[11, 90]","[14, 66]","[1, 5]","The paper proposes training ``inference networks,'' which are neural network structured predictors. The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network.

The idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization. However, it remains unclear why SPENs are the right choice for an energy function.

Experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent. However, the experimental results are not clearly presented. The clarity is poor enough that the paper might not be ready for publication.

Comments and questions:

1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors. The setup focuses on using SPENs as an inference network, but this seems inessential. Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below).

2) The confusion over the motivation is confounded by the fact that the experiments are very unclear. Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6). In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS? It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator).

3) The third and fourth columns of Table 5 are identical. The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers.

4) It is also unclear how to compare Tables 4 and 5. The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance. What is the takeaway from these two tables supposed to be?

5) Part of the motivation for the work is said to be the increasing interest in inference networks: ""In these and related settings, gradient descent has started to be replaced by inference networks. Our results below provide more evidence for making this transition."" However, no other work on inference networks is directly cited.","[5, 9, 7]","[' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Good paper, accept']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
When is a Convolutional Filter Easy to Learn?,"['Simon S. Du', 'Jason D. Lee', 'Yuandong Tian']",Accept,2018,"[3, 12, 13]","[8, 17, 18]","[178, 192, 150]","[74, 77, 69]","[100, 106, 73]","[4, 9, 8]","This paper studies the problem of learning a single convolutional filter using SGD. The main result is: if the ""patches"" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling). The convergence rate, and how ""sufficiently aligned"" depend on some quantities related to the underlying data distribution. A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.

Detailed comments:
1. It would be good to clarify what the angle requirement means on page 2. It says the angle between Z_i, Z_j is at most \rho, is this for any i,j? From the later part it seems that each Z_i should be \rho close to the average, which would imply pairwise closeness (with some constant factor).
2. The paper first proves result for a single neuron, which is a clean result. It would be interesting to see what are values of \gamma(\phi) and L(\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions. 
3. The convergence rate depends on \gamma(\phi_0), from the initialization, \phi_0 is probably very close to \pi/2 (the closeness depend on dimension), which is  also likely to make \gamma(\phi_0) depend on dimension (this is especially true of Gaussian). 
4. More precisely, \gamma(\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data. Also \gamma(\phi_0) depends on \gamma_{avg}(\phi_0) and \rho, when \rho is reasonable (say a constant), \gamma(\phi_0) really needs to be a constant that is independent of dimension. On the other hand, in Theorem 3.4 we can see that the upperbound on \alpha (the quality of initialization) depends on the dimension. 
5. Even assuming \rho is a constant strictly smaller than \pi/2 seems a bit strong. It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average. Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average. 

Overall I feel the result is interesting but hard to interpret correctly. The details of the theorem do not really support the high level claims very strongly. The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees. The reviewer tried to do that for Gaussian and as I mentioned above (esp. 4) the result does not seem very impressive, maybe there are other distributions where this result works better?

After reading the response, I feel the contribution for the single neuron case does not require too much assumptions and is itself a reasonable result. I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from ""general distribution"". Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted. I hope the revised version will clearly discuss the limitations of the approach and potential future directions as the response did.","[6, 8, 9]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Self-ensembling for visual domain adaptation,"['Geoff French', 'Michal Mackiewicz', 'Mark Fisher']",Accept,2018,"[2, 14, 21]","[7, 19, 26]","[16, 28, 30]","[6, 15, 18]","[7, 8, 1]","[3, 5, 11]","This paper presents a domain adaptation algorithm based on the self-ensembling method proposed by [Tarvainen & Valpola, 2017]. The main idea is to enforce the agreement between the predictions of the teacher and the student classifiers on the target domain samples while training the student to perform well on the source domain. The teacher network is simply an exponential moving average of different versions of the student network over time.   

Pros:
+ The paper is well-written and easy to read
+ The proposed method is a natural extension of the mean teacher semi-supervised learning model by [Tarvainen & Valpola, 2017]
+ The model achieves state-of-the-art results on a range of visual domain adaptation benchmarks (including top performance in the VisDA17 challenge)

Cons:
- The model is tailored to the image domain as it makes heavy use of the data augmentation. That restricts its applicability quite significantly. I’m also very interested to know how the proposed method works when no augmentation is employed (for fair comparison with some of the entries in Table 1).
- I’m not particularly fond of the engineering tricks like confidence thresholding and the class balance loss. They seem to be essential for good performance and thus, in my opinion, reduce the value of the main idea.
- Related to the previous point, the final VisDA17 model seems to be engineered too heavily to work well on a particular dataset. I’m not sure if it provides many interesting insights for the scientific community at large.

In my opinion, it’s a borderline paper. While the best reported quantitative results are quite good, it seems that achieving those requires a significant engineering effort beyond just applying the self-ensembling idea. 

Notes:
* The paper somewhat breaks the anonymity of the authors by mentioning the “winning entry in the VISDA-2017”. Maybe it’s not a big issue but in my opinion it’s better to remove references to the competition entry.
* Page 2, 2.1, line 2, typo: “stanrdard” -> “standard”

Post-rebuttal revision:
After reading the authors' response to my review, I decided to increase the score by 2 points. I appreciate the improvements that were made to the paper but still feel that this work a bit too engineering-heavy, and the title does not fully reflect what's going on in the full pipeline.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Automatically Inferring Data Quality for Spatiotemporal Forecasting,"['Sungyong Seo', 'Arash Mohegh', 'George Ban-Weiss', 'Yan Liu']",Accept,2018,"[3, 1, 1, 17]","[7, 1, 1, 22]","[22, 1, 1, 219]","[13, 1, 1, 132]","[9, 0, 0, 57]","[0, 0, 0, 30]","Update:

I have read the rebuttal and the revised manuscript. Paper reads better and comparison to Auto-regression was added. This work presents a novel way of utilizing GCN and I believe it would be interesting to the community. In this regard, I have updated my rating.

On the downside, I still remain uncertain about the practical impact of this work. Results in Table 1 show that proposed method is capable of forecasting next hour temperature with about 0.45C mean absolute error. As no reference to any state of the art temperature forecasting method is given (i.e. what is the MAE of a weather app on a modern smartphone), I can not judge whether 0.45C is good or bad. Additionally, it would be interesting to see how well proposed method can deal with next day temperature forecasting.

---------------------------------------------
In this paper authors develop a notion of data quality as the function of local variation of the graph nodes. The concept of local variation only utilizes the signals of the neighboring vertices and GCN is used to take into account broader neighborhoods of the nodes. Data quality then used to weight the loss terms for training of the LSTM network to forecast temperatures at weather stations.

I liked the idea of using local variations of the graph signals as quality of the signal. It was new to me, but I am not very familiar with some of the related literature. I have one methodological and few experimental questions.

Methodology:
Why did you decide to use GCN to capture the higher order neighborhoods? GCN does so intuitively, but it is not clear what exactly is happening due to non-linearities. What if you use graph polynomial filter instead [1] (i.e. linear combination of powers of the adjacency)? It can more evidently capture the K-hop neighborhood of a vertex.

Experiments:
- Could you please formalize the forecasting problem more rigorously. It is not easy to follow what information is used for training and testing. I'm not quite certain what ""Temperature is used as a target measurement, i.e., output of LSTM, and others including Temperature are used as input signals."" means. I would expect that forecasting of temperature tomorrow is solely performed based on today's and past information about temperature and other measurements.
- What are the measurement units in Table 1?
- I would like to see comparison to some classical time series forecasting techniques, e.g. Gaussian Process regression and Auto-regressive models. Also some references and comparisons are needed to state-of-the-art weather forecasting techniques. These comparisons are crucial to see if the method is indeed practical.

Please consider proofreading the draft. There are occasional typos and excessively long wordings.

[1] Aliaksei Sandryhaila and José MF Moura. Discrete signal processing on graphs. IEEE transactions
on signal processing, 61(7):1644–1656, 2013.","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Differentially Private Recurrent Language Models,"['H. Brendan McMahan', 'Daniel Ramage', 'Kunal Talwar', 'Li Zhang']",Accept,2018,"[16, 17, 18, 22]","[21, 21, 23, 27]","[98, 42, 182, 102]","[41, 23, 98, 52]","[52, 16, 67, 29]","[5, 3, 17, 21]","This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.

The idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility.

The presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.
","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Auto-Encoding Sequential Monte Carlo,"['Tuan Anh Le', 'Maximilian Igl', 'Tom Rainforth', 'Tom Jin', 'Frank Wood']",Accept,2018,"[4, 2, 4, 1, 23]","[8, 6, 9, 1, 28]","[28, 29, 83, 1, 153]","[13, 15, 38, 1, 71]","[15, 13, 44, 0, 71]","[0, 1, 1, 0, 11]","[After author feedback]
I think the approach is interesting and warrants publication. However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments. In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al. 

[Original review]
The authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure. The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.

The approach is interesting and the paper is well-written, however, I have some comments and questions:

- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. This seems to mean that we should not expect for q -> p when K increases?
- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made. If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC? Have you estimated the probability for positive vs. negative gradient values for  K=10? To me it looks like the probability of it being larger than zero is something like 2/3. K>10 is difficult to see from this plot alone.
- Is there a typo in the bound given by eq. (17)? Seems like there are two identical terms. Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?
- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS. Have you tried this for high-dimensional models as well? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO. For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1? Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles? I suspect this lower number of particles might be model-dependent.

Minor comments:
- Section 1, first paragraph, last sentence, ""that"" -> ""than""?
- Section 3.2, ""... using which..."" formulation in two places in the firsth and second paragraph was a bit confusing
- Page 7, second line, just ""IS""?
- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?
- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING,"['Wen Sun', 'J. Andrew Bagnell', 'Byron Boots']",Accept,2018,"[6, 18, 14]","[11, 23, 19]","[76, 195, 221]","[35, 117, 112]","[39, 58, 98]","[2, 20, 11]","This work proposes to use the value function V^e of some expert policy \pi^e in order to speed up learning of an RL agent which should eventually do better than the expert. The emphasis is put on using k-steps (with k>1) Bellman updates using bootstrapping from V^e. 

It is claimed that the case k=1 does not allow the agent to outperform the expert policy, whereas k>1 does (Section 3.1, paragraph before Lemma 3.2).

I disagree with this claim. Indeed a policy gradient algorithm (similar to (10)) with a 1-step advantage c(s,a) + gamma V^e(s_{t+1}) - V^e(s_t) will converge (say in the tabular case, or in the case you consider of a rich enough policy space \Pi) to the greedy policy with respect to V^e, which is strictly better than V^e (if V^e is not optimal). So you don’t need to use k>1 to improve the expert policy. Now it’s true that this will not converge to the optimal policy (since you keep bootstrapping with V^e instead of the current value function), but neither the k-step advantage will. 

So I don’t see any fundamental difference between k=1 and k>1. The only difference being that the k-step bootstrapping will implement a k-step Bellman operator which contracts faster (as gamma^k) when k is large. But the best choice of k has to be discussed in light of a bias-variance discussion, which is missing here. So I find that the main motivation for this work is not well supported. 

Algorithmic suggestion:
Instead of bootstrapping with V^e, why not bootstrap with min(V^e, V), where V is your current approximation of the value function. In that way you would benefit from (1) fast initialization with V^e at the beginning of learning, (2) continual improvement once you’ve reached the performance of the expert. 

Other comments:
Requiring that we know the value function of the expert on the whole state space is a very strong assumption that we do not usually make in Imitation learning. Instead we assume we have trajectories from expert (from which we can compute value function along those trajectories only). Generalization of the value function to other states is a hard problem in RL and is the topic of important research.

The overall writing lacks rigor and the contribution is poor. Indeed the lower bound (Theorem 3.1) is not novel (btw, the constant hidden in the \Omega notation is 1/(1-gamma)). Theorems 3.2 and 3.3 are not novel either. Please read [Bertsekas and Tsitsiklis, 96] as an introduction to dynamic programming with approximation.

The writing could be improved, and there are many typos, such as:
- J is not defined (Equation (2))
- Why do you call A a disadvantage function whereas this quantity is usually called an advantage?
- You are considering a finite (ie, k) horizon setting, so the value function depend on time. For example the value functions defined in (11) depend on time. 
- All derivations in Section 4, before subsection 4.1 are very approximate and lack rigor.
- Last sentence of Proof of theorem 3.1. I don’t understand H -> 2H epsilon. H is fixed, right? Also your example does not seem to be a discounted problem.
","[3, 6, 7]","[' Clear rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Understanding image motion with group representations ,"['Andrew Jaegle', 'Stephen Phillips', 'Daphne Ippolito', 'Kostas Daniilidis']",Accept,2018,"[5, 11, 3, 29]","[10, 13, 8, 34]","[40, 13, 38, 331]","[13, 6, 13, 190]","[24, 4, 24, 85]","[3, 3, 1, 56]","The authors propose to learn the rigid motion group (translation and rotation) from a latent representation of image sequences without the need for explicit labels.
Within their data driven approach they pose minimal assumptions on the model, requiring the group properties (associativity, invertibility, identity) to be fulfilled.
Their model comprises CNN elements to generate a latent representation in motion space and LSTM elements to compose these representations through time.
They experimentally demonstrate their method on sequences of MINST digits and the KITTI dataset.

Pros:
- interesting concept of combining algebraic structure with a data driven method
- clear idea development and well written
- transparent model with enough information for re-implementation
- honest pointers to scenarios where the method might not work well

Cons:
- the method is only intrinsically evaluated (Tables 2 and 3), but not compared with results from other motion estimation methods","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Latent Permutations with Gumbel-Sinkhorn Networks,"['Gonzalo Mena', 'David Belanger', 'Scott Linderman', 'Jasper Snoek']",Accept,2018,"[5, 20, 5, 13]","[7, 24, 10, 18]","[9, 51, 40, 80]","[5, 26, 26, 36]","[2, 22, 13, 34]","[2, 3, 1, 10]","The idea on which the paper is based - that the limit of the entropic regularisation over Birkhoff polytope is on the vertices = permutation matrices -, and the link with optimal transport, is very interesting. The core of the paper, Section 3, is interesting and represents a valuable contribution.

I am wondering whether the paper's approach and its Theorem 1 can be extended to other regularised versions of the optimal transport cost, such as this family (Tsallis) that generalises the entropic one:

https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14584/14420

Also, it would be good to keep in mind the actual proportion of errors that would make a random choice of a permutation matrix for your Jigsaws. When you look at your numbers, the expected proportion of parts wrong for a random assignment could be competitive with your results on the smallest puzzles (typically, 2x2). Perhaps you can put the *difference* between your result and the expected result of a random permutation; this will give a better understanding of what you gain from the non-informative baseline.
(also, it would be good to define ""Prop. wrong"" and ""Prop. any wrong"". I think I got it but it is better to be written down)

There should also be better metrics for bigger jigsaws -- for example, I would accept bigger errors if pieces that are close in the solution tend also to be put close in the err'ed solution.

Typos:

* Rewrite definition 2 in appendix. Some notations do not really make sense.","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Framework for the Quantitative Evaluation of Disentangled Representations,"['Cian Eastwood', 'Christopher K. I. Williams']",Accept,2018,"[1, 28]","[5, 33]","[10, 140]","[5, 67]","[5, 32]","[0, 41]","The authors consider the metrics for evaluating disentangled representations. They define three criteria: Disentanglement, Informativeness, and Completeness. They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.

I think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at. Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me.  

A second limitation of the work is the reliance on a ""true"" set of disentangled factors. We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?

Overall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response. 

Minor Commments:

- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. 

- It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
A Neural Representation of Sketch Drawings,"['David Ha', 'Douglas Eck']",Accept,2018,"[3, 20]","[7, 24]","[57, 89]","[29, 59]","[25, 22]","[3, 8]","The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches.

+ new and large dataset
+ novel algorithm
+ well written
- no evaluation of dataset
- virtually no evaluation of algorithm
- no baselines or comparison

The paper is well written, and easy to follow. The presented algorithm sketch-rnn seems novel and significantly different from prior work.
In addition, the authors collected the largest sketch dataset, I know of. This is exciting as it could significantly push the state of the art in sketch understanding and generation. 

Unfortunately the evaluation falls short. If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short.
For ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop. Remove the variational component, and phrasing it simply as an auto-encoder. Table 1 is good, but not sufficient. Training loss alone likely does not capture the quality of a sketch.
A comparison the Graves 2013 is absolutely required, more comparisons are desired.
Finally, it would be nice to see where the algorithm falls short, and where there is room for improvement.

If the authors wish to push their dataset, it would help to first evaluate the quality of the dataset. For example, how well do humans classify these sketches? How diverse are the sketches? Are there any obvious modes? Does the discretization into strokes matter?
Additionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on? Are there any good automated metrics, and how well do they correspond to human judgement?

In summary, I'm both excited about the dataset and new architecture, but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset. I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation.","[5, 8, 8]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Polar Transformer Networks,"['Carlos Esteves', 'Christine Allen-Blanchette', 'Xiaowei Zhou', 'Kostas Daniilidis']",Accept,2018,"[2, 4, 13, 29]","[7, 8, 18, 34]","[27, 11, 175, 331]","[12, 4, 79, 190]","[14, 6, 67, 85]","[1, 1, 29, 56]","This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The idea is to jointly train
- a network predicting a polar origin,
- a module transforming the image into a log-polar representation according to the predicted origin,
- a final classifier performing the desired classification task.
A (not too large) translation of the input image therefore does not change the log-polar representation.
Rotation and scale from the polar origin result in translation of the log-polar representation. As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image. Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge.

(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs.

(-) The evaluation, however, is quite limited. The approach is evaluated on:
 1) several variants of MNIST. The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST. This variant is of course very well suited to the proposed method, and a bit artificial.
 2) 3d voxel occupancy grids with a small resolution. The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation.

(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale? Then the input image could be rectified to a canonical orientation and scale, without needing equivariance. My intuition is that this simpler approach would work better. It should at least be evaluated.

Despite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Neural Speed Reading via Skim-RNN,"['Minjoon Seo', 'Sewon Min', 'Ali Farhadi', 'Hannaneh Hajishirzi']",Accept,2018,"[5, 2, 16, 12]","[10, 7, 21, 17]","[69, 51, 240, 237]","[26, 23, 117, 116]","[43, 28, 109, 118]","[0, 0, 14, 3]","Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn’t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient.

Contribution:
- The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN.

Pros:
- Models that dynamically decide the amount of computation make intuitive sense and are of general interests.
- The paper presents solid experimentation on various text classification and question answering datasets.
- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks).
- The paper is well written, and the presentation is good.

Cons:
- Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.
- The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps.
- Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental.

Questions:
- Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.
- One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary.

Conclusion:
- Based on the comments above, I recommend Accept","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,"['Adams Wei Yu', 'David Dohan', 'Minh-Thang Luong', 'Rui Zhao', 'Kai Chen', 'Mohammad Norouzi', 'Quoc V. Le']",Accept,2018,"[8, 21, 9, 27, 15, 10, 14]","[13, 26, 14, 32, 16, 15, 19]","[41, 29, 53, 140, 33, 136, 299]","[17, 13, 25, 69, 21, 59, 143]","[20, 16, 26, 27, 5, 72, 145]","[4, 0, 2, 44, 7, 5, 11]","This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally.

Firstly, I suggest the authors rewrite the end of the introduction. The current version tends to mix everything together and makes the misleading claim. When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1. But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method. 

Secondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:

(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC. The novelty is limited but it is a good idea to speed up the RC models. However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs. Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct.

(2) I feel that the model design is the main reason for the good overall RC performance. However, in the paper there is no motivation about why the architecture was designed like this. Moreover, the whole model architecture is only evaluated on the SQuAD dataset. As a result, it is not convincing that the system design has good generalization. If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself.

(3) I like the idea of data augmentation with paraphrasing. Currently, the improvement is only marginal, but there seems many other things to play with. For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.

I am looking forward to the test performance of this work on SQuAD.","[5, 6, 8]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,"['Carlos Riquelme', 'George Tucker', 'Jasper Snoek']",Accept,2018,"[5, 6, 13]","[10, 10, 18]","[38, 75, 80]","[14, 34, 36]","[22, 39, 34]","[2, 2, 10]","This paper presents the comparison of a list of algorithms for contextual bandit with Thompson sampling subroutine. The authors compared different methods for posterior estimation for Thompson sampling. Experimental comparisons on contextual bandit settings have been performed on a simple simulation and quite a few real datasets.

The main paper + appendix are clearly written and easy to understand. The main paper itself is very incomplete. The experimental results should be summarized and presented in the main context. There is a lack of novelty of this study. Simple comparisons of different posterior estimating methods do not provide insights or guidelines for contextual bandit problem. 

What's the new information provided by running such methods on different datasets? What are the newly observed advantages and disadvantages of them? What could be the fundamental reasons for the variety of behaviors on different datasets? No significant conclusions are made in this work.

Experimental results are not very convincing. There are lots of plots show linear cumulative regrets within the whole time horizon. Linear regrets represent either trivial methods or not long enough time horizon.
","[5, 7, 6]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The power of deeper networks for expressing natural functions,"['David Rolnick', 'Max Tegmark']",Accept,2018,"[8, 25]","[13, 30]","[63, 54]","[16, 8]","[35, 35]","[12, 11]","Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons. 
The paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth.

+ves:
Explaining the power of depth in NNs is fundamental to an understanding of deep learning. The paper is very easy to follow. and the proofs are clearly written. The theorems provide exponential gaps for very simple polynomial functions.

-ves:
1. My main concern with the paper is the novelty of the contribution to the techniques. The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas. 
2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.).
3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(δx)| <= δ^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation.

Despite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DORA The Explorer: Directed Outreaching Reinforcement Action-Selection,"['Lior Fox', 'Leshem Choshen', 'Yonatan Loewenstein']",Accept,2018,"[1, 1, 11]","[5, 6, 16]","[6, 62, 17]","[3, 25, 1]","[3, 36, 4]","[0, 1, 12]","This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. This generalization is achieved by relying on the convergence rate of SARSA updates on an auxiliary MDP.

The method presented in the paper trains a parallel ""E-value"" MDP, with initial value of 1 for all state-action pairs. It applies SARSA (on-policy) update rule to the E-value MDP, where the acting policy is selected on the original MDP. While the E-value MDP is training, the proposed method uses a 1/log transformation applied to E-values to get the corresponding exploration bonus term for the original MDP. This bonus term is shown to be equivalent counter-based methods for finite MDPs when the discount factor of the E-MDP is set to 0. The paper has minimal theoretical analysis of the proposed algorithm, essentially only showing convergence with infinite visiting. In that regard, the presented method seems like a useful heuristic with anecdotal empirical benefits.

What is crucially lacking from the paper is any reference to model-free Bayesian methods that have very similar intuition behind them: taking into account the long term exploratory benefits of actions (passed on through the Bayesian inference). A comparison would have been trivial to do (with a generic non-informative prior) for the finite MPD setting (section 3.4). Even for the function approximation case one could use Gaussian process methods as the Bayesian baseline. There are also several computationally tractable approximations of Bayesian RL that can be used as baseline for empirical analysis.

It would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. Unlike the finite case, updates to the value of one E-value can change the value for another state-action pair and the convergence could be faster than (1-alpha)^n. Given the lack of any theory on this, an empirical analysis is certainly valuable. (Update: experiment added in the a later revision to study this effect)

Notes:
- The plots are horrible in a print. I had to zoom 400% into the PDF file to be able to read the plots. Please scale them at least by 200% and use a larger font for the legends.

- Add a minimal description of the initial setup for E-value neural network to section 4.1 (i.e. how the initializing is achieved to have a constant value for all state-action pairs as described in the appendix).

* Note: This review and rating has been partially revised with the updates to the paper after the initial comments. ","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning a Generative Model for Validity in Complex Discrete Structures,"['Dave Janz', 'Jos van der Westhuizen', 'Brooks Paige', 'Matt Kusner', 'José Miguel Hernández-Lobato']",Accept,2018,"[3, 2, 6, 7, 13]","[7, 2, 11, 12, 18]","[13, 6, 53, 71, 163]","[5, 1, 25, 35, 82]","[8, 5, 27, 35, 70]","[0, 0, 1, 1, 11]","The authors use a recurrent neural network to build generative models of sequences in domains where the vast majority of sequences is invalid. The basic idea, outlined in Eq. 2, is moderately straightforward: at each step, use an approximation of the Q function for subsequences of the appropriate length to pick a valid extension. There are numerous details to get right. The writing is mostly clear, and the examples are moderately convincing. I wish the paper had more detailed arguments and discussions.

I question the appropriateness of Eq. 2 as a target. A correctly learned model will put positive weight on valid sequences, but it may be an arbitrarily slow way to generate diverse sequences, depending on the domain. For instance, imagine a domain of binary strings where the valid sequences are the all 1 sequence, or any sequence beginning with a 0. Half the generated sequences would be all 1's in this situation, right? And it's easy to construct further examples that are much worse than this?

The use of Bayesian active learning to generate the training set feels like an elegant idea. However, I wish there were more clarity about what was ad hoc and what wasn't. For instance, I think the use of  dropout to get q is suspect (see for instance https://arxiv.org/abs/1711.02989), and I'd prefer a little more detail on statements like ""The nonlinearity of g(·) means that our Monte
Carlo approximation is biased, but still consistent."" Do we have any way of quantifying the bias? Is the statement about K=16 being reasonable a statement about bias, variance, or both?

For Python strings: 
- Should we view the fact that high values of tau give a validity of 1.0 as indicative that the domain's constraints are fairly easy to learn?
- ""The use of a Boltzmann policy allows us to tune the temperature parameter to identify policies
which hit high levels of accuracy for any learned Q-function approximation."" This is only true to the extent the domain is sufficiently ""easy"" right? Is the argument that even in very hard domains, you might get this by just having an RNN which memorized a single valid sequence (assuming at least one could be found)?
- What's the best explanation for *why* the active model has much higher diversity? I understand that the active model is picking examples that tell us more about the uncertainty in w, but it's not obvious to me that means higher diversity. Do we think this is a universal property of domains?
- The highest temperature active model is exploring about half of valid sequences (modulo the non-tightness of the bound)? Have you tried gaining some insight by generating thousands of valid sequences manually and seeing which ones the model is rejecting?
- The coverage bound is used only for for Python expressions, right? Why not just randomly sample a few thousand positives and use that to get a better estimate of coverage? Since you can sample from the true positive set, it seems that your argument from the appendix about the validation set being ""too similar to the training set"" doesn't apply?
- It would be better to see a comparison to a strong non-NN baseline. For instance, I could easily make a PCFG over Python math expressions, and use rejection sampling to get rid of those that aren't exactly length 25, etc.?

I question how easy the Python strings example is. In particular, it might be that it's quite an easy example (compared to the SMILES) example. For SMILES, it seems like the Bayesian active learning technique is not by itself sufficient to create a good model? It is interesting that in the solubility domain the active model outperforms, but it would be nice to see more discussion / explanation.

Minor note: The incidence of valid strings in the Python expressions domain is (I believe) > 1/5000, although I guess 1 in 10,000 is still the right order of magnitude.

If I could score between ""marginal accept"" and ""accept"" I would. ","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control,"['Glen Berseth', 'Cheng Xie', 'Paul Cernek', 'Michiel Van de Panne']",Accept,2018,"[6, 15, 1, 29]","[11, 20, 1, 33]","[59, 69, 2, 125]","[25, 33, 1, 55]","[22, 6, 1, 24]","[12, 30, 0, 46]","This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially. The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task. The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously. 

Questions:
- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?
- What data do you use for the distillation? Section 4.1 states""We use a method similar to the DAGGER algorithm"", but what is your method. If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?
- I do not understand the purpose of ""input injection"" nor where it is used in the paper. 

Strengths:
- The method is simple but novel. The results support the method's utility.
- The testbed is nice; the tasks seem significantly different from each other. It seems that no reward shaping is used.
- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.

Weaknesses:
- Figure 2: the plots are too small.
- Distilling may hurt performance ( Figure 2.d)
- The method lacks details (see Questions above)
- No comparisons with prior work are provided. The paper cites many previous approaches to this but does not compare against any of them. 
- A second testbed (such as navigation or manipulation) would bring the paper up a notch. 

In conclusion, the paper's approach to multitask learning is a clever combination of prior work. The method is clear but not precisely described. The results are promising. I think that this is a good approach to the problem that could be used in real-world scenarios. With some filling out, this could be a great paper.","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Expressive power of recurrent neural networks,"['Valentin Khrulkov', 'Alexander Novikov', 'Ivan Oseledets']",Accept,2018,"[2, 5, 11]","[7, 9, 16]","[34, 32, 233]","[13, 10, 41]","[19, 17, 117]","[2, 5, 75]","In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. Here, the expressive power refers to the rank of tensor decomposition, i.e., the number of latent components. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks. It is proved that the space of TT-type networks with rank O(r)  can be complex as the same as the space of CP-type networks with rank poly(r).

The paper is clearly written and easy to follow. 

The contribution is clear and it is distinguished from previous studies.

Though I enjoyed reading this paper, I have several concerns.

1. The authors compare the complexity of TT representation with CP representation (and HT representation). However, CP representation does not have universality (i.e., some tensors cannot be expressed by CP representation with finite rank, see [1]), this comparison may not make sense. It seems the comparison with Tucker-type representation makes much more sense because it has universality. 

2. Connecting RNN and TT representation is a bit confusing. Specifically, I found two gaps.
   (a) RNNs reuse the same parameter against all the input x_1 to x_d. This means that G_1 to G_d in Figure 1 are all the same. That's why RNNs can handle size-varying sequences. 
   (b) Standard RNNs do not use the multilinear units shown in Figure 3, but use a simple addition of an input and the output from the previous layer (i.e., h_t = f(Wx_t + Vh_{t-1}), where h_t is the t-th hidden unit, x_t is the t-th input, W and V are weights, and f is an activation function.) 
Due to the gaps, the analysis used in this paper seems not applicable to RNNs. If this is true, the story of this paper is somewhat misleading. Or, is your theory still applicable?

[1] Hackbusch, Wolfgang. Tensor spaces and numerical tensor calculus. Vol. 42. Springer Science & Business Media, 2012.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Training wide residual networks for deployment using a single bit for each weight,['Mark D. McDonnell'],Accept,2018,[15],[19],[71],[36],[13],[22],"The authors propose to train neural networks with 1bit weights by storing and updating full precision weights in training, but using the reduced 1bit version of the network to compute predictions and gradients in training. They add a few tricks to keep the optimization numerically efficient. Since right now more and more neural networks are deployed to end users, the authors make an interesting contribution to a very relevant question.

The approach is precisely described although the text sometimes could be a bit clearer (for example, the text contains many important references to later sections).

The authors include a few other methods for comparision, but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint. For example, weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32. Additionally, for practical applications, methods like weight pruning might be more promising since they reduce both the memory load and the computational load.

Side mark: the manuscript has quite a few typos.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Discrete Weights Using the Local Reparameterization Trick,"['Oran Shayer', 'Dan Levi', 'Ethan Fetaya']",Accept,2018,"[5, 13, 8]","[7, 18, 13]","[6, 32, 61]","[3, 19, 27]","[3, 8, 32]","[0, 5, 2]","This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization. The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution. As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick. The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting. The method is evaluated on multiple experiments.

This paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]. While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward. I would thus suggest that the authors update the paper accordingly. 

Other than that, I have some other comments:
- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve? 
- For the binary setting you mentioned that you had to reduce the entropy thus added a “beta density regulariser”. Did you add R(p) or log R(p) to the objective function? Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid. To force the probability near the endpoints you have to use alpha, beta < 1 which results into a “bowl” shaped Beta distribution. I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.  
- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT). 

[1] Wang & Manning, Fast Dropout Training.

Edit: After the authors rebuttal I have increased the rating of the paper: 
- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].
- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p. The lambda term would then serve as an indicator to how much entropy is necessary.
- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).
- The authors showed benefits compared to a continuous relaxation baseline.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Can Neural Networks Understand Logical Entailment?,"['Richard Evans', 'David Saxton', 'David Amos', 'Pushmeet Kohli', 'Edward Grefenstette']",Accept,2018,"[4, 35, 5, 16, 8]","[9, 36, 10, 21, 13]","[21, 20, 12, 322, 104]","[4, 5, 2, 180, 44]","[10, 7, 7, 102, 54]","[7, 8, 3, 40, 6]","This is a wonderful and a self-contained paper. In fact, it introduces a very important problem and it solves it. 

The major point of the paper is demonstrating that it is possible to model logical entailment in neural networks. Hence, a corpus and a NN model are introduced. The corpus is used to demonstrate that the model, named PossibleWorld, is nearly perfect for the task. A comparative analysis is done with respect to state of the art recurrent NN. So far, so good.

Yet, what is the take home message? In my opinion, the message is that generic NN should not be used for specific formal tasks whereas specific neural networks that model the task are desirable. This seems to be a trivial claim, but, since the PossibleWorld nearly completely solves the task, it is worth to be investigated. 

The point that the paper leaves unexplained is: what is in the PossibleWorld Network that captures what we need? The description of the network is in fact very criptic. No examples are given and a major effort is required to the reader. Can you provide examples and insights on why this is THE needed model?

Finally, the paper does not discuss a large body of research that has been done in the past by Plate. Plate has investigated how symbolic predicates can be described in distributed representations. This is strictly related to the problem this paper investigates. As discussed in ""Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey"", 2017, the link between symbolic and distributed representations has to be better investigated in order to propose innovative NN models. Your paper can be one of the first NN model that takes advantage of this strict link.","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Scalable Private Learning with PATE,"['Nicolas Papernot', 'Shuang Song', 'Ilya Mironov', 'Ananth Raghunathan', 'Kunal Talwar', 'Ulfar Erlingsson']",Accept,2018,"[5, 6, 18, 10, 18, 23]","[10, 11, 23, 14, 23, 26]","[150, 39, 86, 29, 182, 58]","[56, 17, 45, 14, 98, 33]","[88, 22, 37, 14, 67, 17]","[6, 0, 4, 1, 17, 8]","This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. The novelty of this work is a refined aggregation process, which is improved in three ways:
a) Gaussian instead of Laplace noise is used to achieve differential privacy.
b) Queries to the aggregator are ""filtered"" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong.
c) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query.

I think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. On this basis, I think the paper should be accepted. However, I think some clarification is needed with regard to item c above:

Theorem 2 gives a data-dependent privacy guarantee. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low. This data-dependent privacy guarantee is likely to be much tighter than the data-independent guarantee.
However, since the privacy guarantee now depends on the data, it is itself sensitive information. How is this issue resolved? If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. This would resemble the ""privacy odometer"" setting of Rogers-Roth-Ullman-Vadhan [ https://arxiv.org/abs/1605.08294 ]. 
Another way to resolve this would be to have an output-dependent privacy guarantee. That is, the privacy guarantee would depend only on public information, rather than the private data. The widely-used ""sparse vector"" technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this.
In any case, this is an important issue that needs to be clarified, as it is not clear to me how this is resolved.

The algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a ""student"" being trained using sensitive data with queries being answered in a differentially private manner. And, in particular, these works also filter out uninformative queries using the sparse vector technique. It would be helpful to add a comparison.
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 1, 4]","[' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Emergent Complexity via Multi-Agent Competition,"['Trapit Bansal', 'Jakub Pachocki', 'Szymon Sidor', 'Ilya Sutskever', 'Igor Mordatch']",Accept,2018,"[5, 7, 4, 12, 12]","[9, 11, 8, 17, 17]","[31, 42, 14, 99, 109]","[19, 15, 4, 49, 48]","[12, 21, 9, 45, 56]","[0, 6, 1, 5, 5]","This paper demonstrates that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi-agent tasks with competing goals in a 3D world with simulated physics. It utilizes a decentralized training approach and use distributed implementation of PPO for very large scale multiagent training. This paper addresses the challenges in applying distributed PPO to train multiple competitive agents, including the problem of exploration with sparse reward by using full roll-outs and use the dense exploration reward which is gradually annealed to zero in favor of the sparse competition reward. It makes training more stable by selecting random old parameters for the opponent. 
 
Although the technical contributions seem to be not quite significant, this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning. The paper also makes it clear regarding the connections and distinctions to many existing work. 

Minor issues:

E[Loss] in table 1 is undefined.

In the notation section, the observation model is missing, and the policy is restricted to be reactive.
 
Uniform (v, \deta v) -> Uniform (\deta v, v)
","[7, 3, 9]","[' Good paper, accept', ' Clear rejection', ' Top 15% of accepted papers, strong accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Non-Autoregressive Neural Machine Translation,"['Jiatao Gu', 'James Bradbury', 'Caiming Xiong', 'Victor O.K. Li', 'Richard Socher']",Accept,2018,"[15, 15, 13, 0, -3, 3, -3]","[20, 19, 18, 5, 1, 8, 1]","[51, 45, 69, 28, 5, 27, 5]","[25, 40, 38, 25, 5, 16, 5]","[5, 0, 14, 0, 0, 9, 0]","[21, 5, 17, 3, 0, 2, 0]","This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs. To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations. For training, apply knowledge distilation for better training followed by fine tuning by reinforce. Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.

The motivation is clear and proposed methods are very sound. Experiments are carried out very carefully.

I have only minor concerns to this paper:

- The experiments are designed to achieve comparable BLEU with improved latency. I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough.

- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment.

- What is the impact of the external word aligner quality? For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner. 

- The positional attention is rather unclear and it would be better to revise it. Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Generative networks as inverse problems with Scattering transforms,"['Tomás Angles', 'Stéphane Mallat']",Accept,2018,"[3, 30]","[5, 35]","[7, 117]","[3, 46]","[3, 39]","[1, 32]","The authors introduce scattering transforms as image generative models in the context of Generative Adversarial Networks and suggest why they could be seen as Gaussianization transforms with controlled information loss and invertibility.
Writing is suggestive and experimental results are interesting, so I clearly recommend acceptation. 

I would appreciate more intuition on some claims (e.g. relation between Lipschitz continuity and wavelets) but they refer to the appropriate reference to Mallat, so this is not a major problem for the interested reader.

However, related to the above non-intuitive claim, here is a question on a related Gaussianization transform missed by the authors that (I feel) fulfils the conditions defined in the paper but it is not obviously related to wavelets. Authors cite Chen & Gopinath (2000) and critizise that their approach suffers from the curse of dimensionality because of the ICA stage. However, other people [Laparra et al. Iterative Gaussianization from ICA to random rotations IEEE Trans.Neural Nets 2011] proved that the ICA stage is not required (but only marginal operations followed by even random rotations). That transform seems to be Lipschitz continuous as well -since it is smooth and derivable-. In fact it has been also used for image synthesis. However, it is not obviously related to wavelets... Any comment?

Another relation to previous literature: in the end, the proposed analysis (or Gaussianization) transform is basically a wavelet transform where the different scale filters are applied in a cascade (fig 1). This is similar to Gaussian Scale Mixture  models for texture analysis [Portilla & Simoncelli Int. J. Comp. Vis. 2000] in which after wavelet transform, local division is performed to obtain Gaussian variables, and these can be used to synthesize the learned textures. That is similar to Divisive Normalization models of visual neuroscience that perform similar normalization alfter wavelets to factorize the PDF (e.g. [Lyu&Simoncelli Radial Gaussianization Neur.Comput. 2009], or [Malo et al. Neur.Comput. 2010]).

Minor notation issues: authors use a notation for functions that seems confusing (to me) since it looks like linear products. For instance: GZ for G(Z) [1st page] and phiX for phi(X) [2nd page] Sx for S(x) [in page 5]... 
","[8, 6, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
SpectralNet: Spectral Clustering using Deep Neural Networks,"['Uri Shaham', 'Kelly Stanton', 'Henry Li', 'Ronen Basri', 'Boaz Nadler', 'Yuval Kluger']",Accept,2018,"[11, 2, 13, 31, 20, 18]","[15, 2, 17, 35, 25, 23]","[25, 6, 15, 159, 79, 60]","[6, 1, 7, 77, 28, 15]","[14, 2, 6, 34, 27, 30]","[5, 3, 2, 48, 24, 15]","PAPER SUMMARY

This paper aims to address two limitations of spectral clustering: its scalability to large datasets and its generalizability to new samples. The proposed solution is based on designing a neural network called SpectralNet that maps the input data to the eigenspace of the graph Laplacian and finds an orthogonal basis for this eigenspace. The network is trained by alternating between orthogonalization and gradient descent steps, where scalability is achieved by using a stochastic optimization scheme that instead of computing an eigendecomposition of the entire data (as in vanilla spectral clustering) uses a Cholesky decomposition of the mini batch to orthogonalize the output. The method can also handle out-of-sample data by applying the learned embedding function to new data. Experiments on the MNIST handwritten digit database and the Reuters document database demonstrate the effectiveness of the proposed SpectralNet.

COMMENTS

1) I find that the output layer (i.e. the orthogonalization layer) is not well-justified. In principle, different batches require different weights on the output layer. Although the authors observe empirically that orthogonalization weights are roughly shared across different batches, the paper lacks a convincing argument for why this can happen. Moreover, it is not clear why an output layer designed to orthogonalized batches from the training set would also orthogonalize batches from the test set?

2) One claimed contribution of this work is that it extends spectral clustering to large scale data. However, the paper could have commented more on what makes spectral clustering not scalable, and how the method in this paper addresses that. The authors did mention that spectral clustering requires computing eigenvectors for large matrices, which is prohibitive. However, this argument is not entirely true, as eigen-decomposition for large sparse matrices can be carried out efficiently by tools such as ARPACK. On the other hand, computing the nearest neighbor affinity or Gaussian affinity is N^2 complexity, which could be the bottleneck of computation for spectral clustering on large scale data. But this issue can be addressed using approximate nearest neighbors obtained, e.g., via hashing. Overall, the paper compares only to vanilla spectral clustering, which is not representative of the state of the art. The paper should do an analysis of the computational complexity of the proposed method and compare it to the computational complexity of both vanilla as well as scalable spectral clustering methods to demonstrate that the proposed approach is more scalable than the state of the art. 

3)  Continuing with the point above, an experimental comparison with prior work on large scale spectral clustering (see, e.g. [a] and the references therein) is missing. In particular, the result of spectral clustering on the Reuters database is not reported, but one could use other scalable versions of spectral clustering as a baseline.

4)  Another benefit of the proposed method is that it can handle out-of-sample data. However, the evaluation of such benefit in experiments is rather limited. In reporting the performance on out-of-sample data, there is no other baseline to compare with. One can at least compare with the following baseline: apply k-means to the training data in input space, and classify each test data to the nearest centroid.

5) The reason for using an autoencoder to extract features is unclear. In subspace clustering, it has been observed that features extracted from a scattering transform network [b] can significantly improve clustering performance, see e.g. [c] where all methods have >85% accuracy on MNIST. The methods in [c] are also tested on larger datasets.

[a] Choromanska, et. al., Fast Spectral Clustering via the Nystrom Method, International conference on algorithmic learning theory, 2013

[b] Bruna, Mallat, Invariant Scattering Convolution Networks, arXiv 2012

[c] You, et. al., Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering, CVPR 2016","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,"['Alon Brutzkus', 'Amir Globerson', 'Eran Malach', 'Shai Shalev-Shwartz']",Accept,2018,"[4, 18, 2, 17]","[8, 23, 7, 22]","[21, 169, 33, 178]","[11, 100, 14, 81]","[9, 61, 18, 67]","[1, 8, 1, 30]","Paper studies an interesting phenomenon of overparameterised models being able to learn well-generalising solutions. It focuses on a setting with three crucial simplifications:
- data is linearly separable
- model is 1-hidden layer feed forward network with homogenous activations
- **only input-hidden layer weights** are trained, while the hidden-output layer's weights are fixed to be (v, v, v, ..., v, -v, -v, -v, ..., -v) (in particular -- (1,1,...,1,-1,-1,...,-1))
While the last assumption does not limit the expressiveness of the model in any way, as homogenous activations have the property of f(ax)=af(x) (for positive a) and so for any unconstrained model in the second layer, we can ""propagate"" its weights back into first layer and obtain functionally equivalent network. However, learning dynamics of a model of form 
 z(x) = SUM( g(Wx+b) ) - SUM( g(Vx+c) ) + d
and ""standard"" neural model
 z(x) = Vg(Wx+b)+c
can be completely different.
Consequently, while the results are very interesting, claiming their applicability to the deep models is (at this point) far fetched. In particular, abstract suggests no simplifications are being made, which does not correspond to actual result in the paper. The results themselves are interesting, but due to the above restriction it is not clear whether it sheds any light on neural nets, or simply described a behaviour of very specific, non-standard shallow model.

I am happy to revisit my current rating given authors rephrase the paper so that the simplifications being made are clear both in abstract and in the text, and that (at least empirically) it does not affect learning in practice. In other words - all the experiments in the paper follow the assumption made, if authors claim is that the restriction introduced does not matter, but make proofs too technical - at least experimental section should show this. If the claims do not hold empirically without the assumptions made, then the assumptions are not realistic and cannot be used for explaining the behaviour of models we are interested in.

Pros:
- tackling a hard problem of overparametrised models, without introducing common unrealistic assumptions of activations independence
- very nice result of ""phase change"" dependend on the size of hidden layer in section 7

Cons:
- simplification with non-trainable second layer is currently not well studied in the paper; and while not affecting expressive power - it is something that can change learning dynamics completely

# After the update

Authors addressed my concerns by:
- making simplification assumption clearer in the text
- adding empirical evaluation without the assumption
- weakening the assumptions

I find these modifications satisfactory and rating has been updated accordingly. 
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Depthwise Separable Convolutions for Neural Machine Translation,"['Lukasz Kaiser', 'Aidan N. Gomez', 'Francois Chollet']",Accept,2018,"[14, 2, 17]","[18, 6, 21]","[84, 29, 23]","[44, 9, 6]","[32, 18, 8]","[8, 2, 9]","This paper presents the SliceNet architecture, an sequence-to-sequence model based on super-dilated convolutions, which allow to reduce the computational cost of the model compared to standard convolution. The proposed model is then evaluated on machine translation and yields competitive performance compared to state-of-the-art approaches.

In terms of clarity, the paper is overall easy to follow, however I am a bit confused by Section 2 about what is related work and what is a novel contribution, although the section is called “Our Contribution”. For instance, it seems that the separable convolution presented in Section 2.1 were introduced by (Chollet, 2016) and are not part of the contribution of this paper. The authors should thus clarify the contributions of the paper.

In terms of significance, the SliceNet architecture is interesting and is a solid contribution for reducing computation cost of sequence-to-sequence models. The experiments on NMT are convincing and gives interesting insights, although I would like to see some pointers about why in Table 3 the Transformer approach (Vaswani et al. 2017) outperforms SliceNet.

I wonder if the proposed approach could be applied to other sequence-to-sequence tasks in NLP or even in speech recognition ? 

Minor comment: 
* The equations are not easy to follow, they should be numbered. The three equations just before Section 2.2 should also be adapted as they seem redundant with Table 1.
","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Measuring the Intrinsic Dimension of Objective Landscapes,"['Chunyuan Li', 'Heerad Farkhoor', 'Rosanne Liu', 'Jason Yosinski']",Accept,2018,"[8, 1, 9, 9]","[13, 1, 13, 14]","[167, 2, 36, 57]","[71, 1, 20, 26]","[82, 1, 16, 28]","[14, 0, 0, 3]","This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. Taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace. Performance on this subspace is then evaluated relative to that over the full parameter space (the baseline). As an empirical standard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline. The authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks, for several well-known datasets, and draw some interesting conclusions.

Pros:

* This paper continues the recent research trend towards a better characterization of neural networks and their performance. The authors show a good awareness of the recent literature, and to the best of my knowledge, their empirical characterization of the number of latent parameters is original. 

* The characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect. For example, as reported by the authors, when training a fully-connected network on the MNIST image dataset, shuffling pixels does not result in a change in their intrinsic dimensionality. For a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet.

* The proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space (the baseline), and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training.

* Except for the occasional typo or grammatical error, the paper is well-written and organized. The issues are clearly identified, for the most part (but see below...).

Cons:

* In the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections. Variance should be taken into account explicitly, in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself. How often does a random projection lead to a high-quality solution, and how often does it not?

* The authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed). In their experiments (FC networks of varying depths and layer widths for the MNIST dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed. This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network.

* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension. However, I don't think that they make a convincing case for this approach. Again, variation is the difficulty: two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality. How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality, even when its dimensionality greatly exceeds the intrinsic dimension?

* The authors argue for a relationship between intrinsic dimensionality and the minimum description length (MDL) of their solution, in that the intrinsic dimensionality should serve as an upper bound on the MDL. However they don't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting, with some parameters potentially requiring many more bits than others. And given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that ""there is some rigor behind"" their conclusion that LeNet is better than FC networks for classification on MNIST because its empirical intrinsic dimensionality score is lower.

* The experimental validation of their measure of intrinsic dimension could be made more extensive. In the main paper, they use three image datasets - MNIST, CIFAR-10 and ImageNet. In the supplemental information, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.

Overall, I think that this characterization does have the potential to give insights into the performance of neural networks, provided that variation across projections is properly taken into account. For now, more work is needed.

====================================================================================================
Addendum:

The authors have revised their paper to take into account the effect of variation across projections, with results that greatly strengthen their results and provide a much better justification of their approach. I'm satisfied too with their explanations, and how they incorporated them into their revised version. I've adjusted my rating of the paper accordingly.

One point, however: the revisions seem somewhat rushed, due to the many typos and grammatical errors in the updated sections. I would like to encourage the authors to check their manuscript once more, very carefully, before finalizing the paper.
====================================================================================================","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Imitation Learning from Visual Data with Multiple Intentions,"['Aviv Tamar', 'Khashayar Rohanimanesh', 'Yinlam Chow', 'Chris Vigorito', 'Ben Goodrich', 'Michael Kahane', 'Derik Pridmore']",Accept,2018,"[8, 18, 8, 1, 10, 18, 1]","[13, 23, 13, 3, 13, 18, 1]","[108, 18, 71, 2, 14, 7, 1]","[47, 14, 34, 1, 7, 5, 1]","[57, 3, 32, 1, 6, 0, 0]","[4, 1, 5, 0, 1, 2, 0]","This paper focuses on imitation learning with intentions sampled 
from a multi-modal distribution. The papers encode the mode as a hidden 
variable in a stochastic neural network and suggest stepping around posterior 
inference over this hidden variable (which is generally required to 
do efficient maximum likelihood) with a biased importance 
sampling estimator. Lastly, they incorporate attention for large visual inputs. 

The unimodal claim for distribution without randomness is weak. The distribution 
could be replaced with a normalizing flow. The use of a latent variable 
in this setting makes intuitive sense, but I don't think multimodality motivates it.

Moreover, it really felt like the biased importance sampling approach should be 
compared to a formal inference scheme. I can see how it adds value over sampling 
from the prior, but it's unclear if it has value over a modern approximate inference 
scheme like a black box variational inference algorithm or stochastic gradient MCMC.

How important is using the pretrained weights from the deterministic RNN?

Finally, I'd also be curious about how much added value you get from having 
access to extra rollouts.
","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Emergent Translation in Multi-Agent Communication,"['Jason Lee', 'Kyunghyun Cho', 'Jason Weston', 'Douwe Kiela']",Accept,2018,"[3, 9, 20, 6]","[6, 14, 25, 11]","[15, 396, 237, 158]","[6, 154, 126, 82]","[8, 215, 84, 72]","[1, 27, 27, 4]","Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2. 

Please find my detailed comments/questions/suggestions below:

1) IMO, the paper could have been written much better. At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}. There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works. IMO, this leads to unnecessary confusion and does more harm than good. For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside).

2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation. However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al.: Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references)

3) This omission of related work also weakens the experimental section. At least for the word translation task many of these common representation learning frameworks could have been easily evaluated. For example, find the nearest german neighbour of the word ""dog"" in the common representation space. The authors instead compare with very simple baselines.

4) Even when comparing with simple baselines, the proposed model does not convincingly outperform them. In particular,  the P@5 and P@20 numbers are only slightly better. 

5) Some of the choices made in the Experimental setup seem questionable to me:
   - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ?
   - It is mentioned that ""While their model unit-normalizes the output of every encoder, we found this to consistently hurt performance, so do not use normalization for fair comparison with our models."" I don't think this is a fair comparison. The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance. Please mention the numbers with unit normalization to give a better picture. It does not make sense to weaken an existing baseline and then compare with it.

6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph. This again leads to poor readability and it is hard to read and compare the corresponding numbers from Table 1.  I am not sure why this cannot be accommodated in the Table itself.

7) In Figure 2, what exactly do you mean by ""Results are averaged over 30 translation scenarios"". Can you please elaborate ?","[5, 8, 7]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Emergence of grid-like representations by training recurrent neural networks to perform spatial localization,"['Christopher J. Cueva', 'Xue-Xin Wei']",Accept,2018,"[2, 7]","[5, 11]","[6, 13]","[2, 7]","[4, 4]","[0, 2]","This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task.

On the positive side: 

This is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand. I enjoyed reading the paper which is in general clearly written. I have a few, mostly cosmetic, complaints but this can easily be addressed in a revision.

On the negative side: 

The manuscript is not written in a way that is suitable for the target ICLR audience which will include, for the most part, readers that are not expert on the entorhinal cortex and/or spatial navigation. 

First, the contributions need to be more clearly spelled out. In particular, the authors tend to take shortcuts for some of their statements. For instance, in the introduction, it is stated that previous attractor network type of models (which are also recurrent networks) “[...] require hand-crafted and fined tuned connectivity patterns, and the evidence of such specific 2D connectivity patterns has been largely absent.” This statement is problematic for two reasons: 

(i) It is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data. This is not to say that demonstrating that these patterns can arise as a byproduct is not important, on the contrary. These are just two complementary lines of work. In the same vein, it would be silly to dismiss the present work simply because it lacks spikes. 

(ii) the authors do not seem to address one of the main criticisms they make about previous work and in particular ""[a lack of evidence] of such specific 2D connectivity patterns"". My understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connectivity. I would argue that there is a lot of evidence for local inhibitory connection in the cortex. Somewhat related to this point, it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity.

Second, the navigation task used needs to be better justified. Why training a network to predict 2D spatial location from velocity inputs? Why is this a reasonable starting point to study the emergence of grid cells? It might be obvious to the authors but it will not be to the ICLR audience. Dead-reckoning (i.e., spatial localization from velocity inputs) is of critical ecological relevance for many animals. This needs to be spelled out and a reference needs to be added.  As a side note, I would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on ""modified Brownian motion”. This seems like an important assumption of the manuscript but the issue is brushed off and not discussed. Why is this a reasonable assumption to make? Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random?

Figure 4 seems kind of strange. I do not understand how the “representative units” are selected and where the “late” selectivity on the far right side in panel a arises if not from “early” units that would have to travel “far” from the left side… Apologies if I am missing something obvious.

I found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated. It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells. ","[8, 9, 8]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Memory Augmented Control Networks,"['Arbaaz Khan', 'Clark Zhang', 'Nikolay Atanasov', 'Konstantinos Karydis', 'Vijay Kumar', 'Daniel D. Lee']",Accept,2018,"[3, 2, 7, 7, 30, 23]","[7, 5, 12, 12, 35, 28]","[20, 15, 137, 76, 655, 200]","[5, 7, 53, 34, 369, 130]","[12, 7, 64, 30, 101, 45]","[3, 1, 20, 12, 185, 25]","The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. My main concern is that the structure of these domains is very similar - essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general POMDPs (or, in their continuous counterparts, state-space models). The authors claim that what is remembered by the planner does not take the form of a map, but isn't the map estimate \hat{m} introduced at the end of Section 2.1 precisely such a map? From Section 2.4, it appears that these map estimates are essential in computing the low-level policies from which the final, high-level policy is computed. If the ability to maintain and use such local maps is essential for this method, its applicability is likely restricted to this specific geometric structure of domains and their observability. 

Some additional comments:

P. 2, Section 2.1: does H(s) contain 0s for non-observable and 1s for observable states? If yes, please state it.

P. 3: the concatenation of state and observation histories is missing from the definition of the transition function.

P. 3, Eq. 1: overloaded notation - if T is the transition function for the large MDP on histories, it should not be used for the transition function between states. Maybe the authors meant to use f() for that transition?

P. 3, Eq. 3: the sum is over i, but it is not clear what i indexes.

P.3, end of Section 2.1: when computing the map estimate \hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)?

P.5: the description of the reward function is inconsistent - is it 0 at the goal state, or >0?

P. 11, above Fig. 9: typo, ""we observe that the in the robot world""
 
 ","[6, 4, 9]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The High-Dimensional Geometry of Binary Neural Networks,"['Alexander G. Anderson', 'Cory P. Berg']",Accept,2018,"[3, 3]","[7, 3]","[10, 4]","[6, 2]","[4, 2]","[0, 0]","This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that:

(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet.

(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.

(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer.

The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge.

The second observation is much less clear to me. Specifically,
a.	The author claim that “A sufficient condition for \delta u to be the same in both cases is L’(x = f(u)) ~ L’(x = g(u))”. However, I’m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. 
b.	Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified.
c.	For BNNs, where both the weights and activations are binarized, shouldn’t we compare weights*activations to (binarized weights)*(binarized activations)?
d.	To make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3.
e.	It is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. 

The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.

To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. 

%%% After Author's response %%%
a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation).

Following the author's response and revisions, I have raised my grade.
","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Simple Neural Attentive Meta-Learner,"['Nikhil Mishra', 'Mostafa Rohaninejad', 'Xi Chen', 'Pieter Abbeel']",Accept,2018,"[3, 2, 4, 17]","[8, 2, 8, 22]","[12, 5, 47, 608]","[7, 3, 21, 291]","[5, 2, 24, 291]","[0, 0, 2, 26]","The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning. Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods. The experiment results seem solid and the proposed structure is with simple design and highly generalizable. The concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts, which could be impactful. Please see the major comment below.

One major comment:
- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side. 
Specifically, for two folds: 
(1) in general, some more work in investigating the task space would be nice. The paper assumes the tasks are “related” or “similar” and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution. But what exactly should the distribution be like to be learnable and how to quantify such “related” or “similar” relationship across tasks? 
(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods? The result from the paper seems to answer with “in all cases” but then that always brings the issue of “overfitting” or parameter tuning issue. I believe the paper would have been much stronger if either of the two above are further investigated.

More detailed comments:
- On Page 1, “the optimal strategy for an arbitrary range of tasks” lacks definition of “range”; also, in the setting in this paper, these tasks should share “similarity” or follow the same “distribution” and thus such “arbitrariness” is actually constrained.

- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined. It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the “task distribution” is too large to be meaningful. One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable.

- For results in Table 1 and Table 2, how are the confidence intervals computed? Is it over multiple runs or within the same run? It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests. The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming “not statistically-significantly different” because no significance test is reported. 

- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Semi-parametric topological memory for navigation,"['Nikolay Savinov', 'Alexey Dosovitskiy', 'Vladlen Koltun']",Accept,2018,"[5, 6, 19]","[9, 10, 24]","[24, 96, 295]","[10, 41, 135]","[14, 44, 103]","[0, 11, 57]","The paper introduces a graph based memory for navigation agents. The memory graph is constructed using nearest neighbor heuristics based on temporal adjacency and visual similarity. The agent uses Dijkstra's algorithm to plan a a path through the graph in order to solve the navigation task. 

There are several major problems with this paper. My overall impression is that the the proposed agent is a nearly hard-coded solution (which I think might be the correct approach to such problems), but a poorly implemented one. Specific points: 1-There are only 5 test mazes, and the proposed agent doesn't even solve all of them. 2-The way in which the maze is traversed in the exploration phase determines the accuracy of the graph that is constructed (i.e. traversing each location exactly once using a space-filling curve). 3-Of the two heuristics used in Equation 1 how many edges are actually constructed using the visual similarity heuristic? 4-How does the visual similarity heuristic handle visually similar map elements that correspond to distinct locations? 5- The success criteria of solving a maze is arbitrarily defined -- why exactly 2.4 min?   ","[3, 7, 7]","[' Clear rejection', ' Good paper, accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hierarchical Density Order Embeddings,"['Ben Athiwaratkun', 'Andrew Gordon Wilson']",Accept,2018,"[4, 9]","[9, 14]","[21, 163]","[8, 74]","[12, 87]","[1, 2]","The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one. The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data.
+++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!)
---cons: 1) Badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves

COMMENTS:
The introduction puzzled me:  the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t. their hyponyms), proposed a solution which seems not to directly solve this problem. So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem.

This aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset.

How the 4000 hypernyms have been selected? Why a diagonal covariance has been estimated, and not a full covariance one? 

n Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets. It shouldn't be, but the authors should spend a little on this.

Apart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out. 

As a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. ","[6, 4, 8]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
MaskGAN: Better Text Generation via Filling in the _______,"['William Fedus', 'Ian Goodfellow', 'Andrew M. Dai']",Accept,2018,"[2, 10, 8]","[7, 12, 13]","[38, 107, 74]","[12, 48, 28]","[24, 55, 42]","[2, 4, 4]","Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words. Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues. As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed. But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging.

Clarity: The mechanism of generating the text samples using the proposed methodology has been described clearly. However the description of the reinforcement learning step could have been made a bit more clear.

Originality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings. There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers. How this current work compares with the existing such literature?

Significance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings. Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing.

pros:
(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples.
(b) Using a novel in-filling procedure to overcome the complexities in GAN training.
(c) generation of high quality samples even with higher perplexity on ground truth set.

cons:
(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive.
(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words?
(c) Depending on the Mask quality GAN can produce low quality samples. Any practical way of choosing the mask?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Boundary Seeking GANs,"['R Devon Hjelm', 'Athul Paul Jacob', 'Adam Trischler', 'Gerry Che', 'Kyunghyun Cho', 'Yoshua Bengio']",Accept,2018,"[5, 3, 3, 1, 9, 31]","[9, 7, 8, 1, 14, 36]","[71, 13, 82, 1, 396, 975]","[31, 6, 39, 1, 154, 405]","[37, 7, 43, 0, 215, 454]","[3, 0, 0, 0, 27, 116]","Thank you for the feedback, and I have read the revision.

I would say the revised version has more convincing experimental results (although I'm not sure about the NLP part). The authors have also addressed my concerns on variance reduction, although it's still mysterious to me that the density ratio estimation method seems to work very well even at the begining stage.

Also developing GAN approaches for discrete variables is an important and unsolved problem.

Considering all of the above, I would like to raise the rating to 7, but lower my confidence to 3 (as I'm not an expert for NLP which is the main task for discrete generative models).

==== original review ====

Thank you for an interesting read.

My understanding of the paper is that:

1. the paper proposes a density-ratio estimator via the f-gan approach;
2. the paper proposes a training criterion that matches the generator's distribution to a self-normalised importance sampling (SIS) estimation of the data distribution;
3. in order to reduce the variance of the REINFORCE gradient, the paper seeks out to do matching between conditionals instead.

There are a few things that I expect to see explanations, which are not included in the current version:

1. Can you justify your variance reduction technique either empirically or experimentally? Because your method requires sampling multiple x for a single given z, then in the same wall-clock time I should be able to obtain more samples for the vanilla version eq (8). How do they compare?

2. Why your density ratio estimation methods work in high dimensions, even when at the beginning p and q are so different?

3. It's better to include some quantitative metrics for the image and NLP experiments rather than just showing the readers images and sentences!

4. Over-optimising generators is like solving a max-min problem instead. You showed your method is more robust in this case, can you explain it from the objective you use, e.g. the convex/concavity of your approach in general?

Typo: eq (3) should be min max I believe?

BTW I'm not an expert of NLP so I won't say anything about the quality of the NLP experiment.","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Active Learning for Convolutional Neural Networks: A Core-Set Approach,"['Ozan Sener', 'Silvio Savarese']",Accept,2018,"[7, 18]","[12, 23]","[46, 364]","[22, 194]","[19, 143]","[5, 27]","After reading rebuttals from the authors: The authors have addressed all of my concerns. THe additional experiments are a good addition.

************************
The authors provide an algorithm-agnostic active learning algorithm for multi-class classification. The core technique is to construct a coreset of points whose labels inform the labels of other points.  The coreset construction requires one to construct a set of  points which can cover the entire dataset. While this is NP-hard problem in general, the greedy algorithm is 2-approximate. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried. The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. 
The experimental results are convincing enough to show that it outperforms other active learning algorithms. However, I have a few major and minor comments.

Major comments:

1. The proof of Lemma 1 is incomplete. We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label. The proof of lemma 1 only establishes the Lipschitz constant of the CNN function. Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function. 

2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset. Why is this term included. Is this term not equal to 0?

3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning.
UPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf
Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf
A bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf

4.  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function. I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper. For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training.

Minor-comment: 
1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. It would have been great had the authors mentioned that u_j \in {0,1}. 

2. The authors write on page 4, ""Moreover, zero training error can be enforced by converting average loss into maximal loss"". It is not clear to me what the authors mean here. For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that? Why would that result in zero training error?

On the whole this is interesting work and the results are very nice. But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified. Also, important references in active learning literature are missing.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Deep Reinforced Model for Abstractive Summarization,"['Romain Paulus', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[5, 10, 12]","[8, 15, 17]","[8, 383, 229]","[4, 165, 111]","[3, 208, 111]","[1, 10, 7]","This is a very clearly written paper, and a pleasure to read.

It combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L. The model is trained by a policy gradient algorithm. 

While the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments. ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention). The best method finally outperforms the lea-3d baseline for summarization. What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail.

Some minor points that I wonder about:
 - The heuristic against repeating trigrams seems quite crude. Is there a more sophisticated method that can avoid redundancy without this heuristic?
 - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)? If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well.
 - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward? Do you have any insights are speculations regarding this?","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Semantic Interpolation in Implicit Models,"['Yannic Kilcher', 'Aurelien Lucchi', 'Thomas Hofmann']",Accept,2018,"[3, 10, 26]","[8, 15, 31]","[20, 110, 205]","[6, 51, 112]","[13, 52, 69]","[1, 7, 24]","The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs. The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution. The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines. To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution). This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations.

While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. The paper is clearly written and presents the theory and experimental results nicely. I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score.","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Active Neural Localization,"['Devendra Singh Chaplot', 'Emilio Parisotto', 'Ruslan Salakhutdinov']",Accept,2018,"[2, 12, 19]","[3, 16, 24]","[2, 16, 195]","[2, 11, 120]","[0, 2, 30]","[0, 3, 45]","I have evaluated this paper for NIPS 2017 and gave it an ""accept"" rating at the time, but the paper was ultimately not accepted. This resubmission has been massively improved and definitely deserves to be published at ICLR.

This paper formulates the problem localisation on a known map using a belief network as an RL problem. The goal of the agent is to minimise the number of steps to localise itself (the agent needs to move around to accumulate evidence about its position), which corresponds to reducing the entropy of the joint distribution over a discretized grid over theta (4 orientations), x and y. The model is evaluated on a grid world, on textured 3D mazes with simplified motion (Doom environment) and on a photorealistic environment using the Unreal engine. Optimisation is done through A3C RL. Transfer from the crude simulated Doom environment to the photorealistic Unreal environment is achieved.

The belief network consists of an observation model, a motion prediction model that allows for translations along x or y and 90deg rotation, and an observation correction model that either perceives the depth in front of the agent (a bold and ambiguous choice) and matches it to the 2D map, or perceives the image in front of the agent. The map is part of the observation.

The algorithm outperforms Bayes filters for localisation in 2D and 3D and the idea of applying RL to minimise the entropy of position estimation is brilliant. Minor note: I am surprised that the cognitive map reference (Gupta et al, 2017) was dropped, as it seemed relevant.","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Critical Percolation as a Framework to Analyze the Training of Deep Networks,"['Zohar Ringel', 'Rodrigo Andrade de Bem']",Accept,2018,"[2, 16]","[5, 19]","[9, 15]","[2, 12]","[7, 2]","[0, 1]","This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution. They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing ""bugs"" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths. This analysis is conducted by drawing on results from the field of critical percolation in physics.

Overall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays. The analysis is very thorough and the methods described may find use in analyzing other tasks. In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms. It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one. This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions. I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds.  

With that being said, there is some work that needs to be done to make the paper clearer. In particular, many parts are quite technical and may not be accessible to a broader machine learning audience. It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix. Specifically:
- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution. 
- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.
- Section 6: there is lots of math here, but the main results don't obviously stand out. I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are. Interested readers can then work through the math if they want to. Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful. 

Also, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs. This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs. Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary. However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks). 

There are lots of small typos, please fix them. Here are a few:
- ""For L=16, batch size of 20, ..."": not a complete sentence. 
- Right before 6.1.1: ""when the these such"" -> ""when such""
- Top of page 8: ""it also have a"" -> ""it also has a"", ""when encountering larger dataset"" -> ""...datasets""
-  First sentence of 6.2: ""we turn to the discuss a second"" -> ""we turn to the discussion of a second""
- etc. 

Quality: High
Clarity: medium-low
Originality: high
Significance: medium-high

References:
[1] https://arxiv.org/pdf/1602.02867.pdf
[2] https://arxiv.org/pdf/1612.08810.pdf
[3] https://arxiv.org/pdf/1707.03497.pdf","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Improving Search Through A3C Reinforcement Learning Based Conversational Agent,"['Milan Aggarwal', 'Aarushi Arora', 'Shagun Sodhani', 'Balaji Krishnamurthy']",Reject,2018,"[2, 2, 4, 31]","[7, 2, 9, 36]","[22, 2, 42, 97]","[8, 1, 12, 45]","[14, 1, 27, 51]","[0, 0, 3, 1]","The paper describes reinforcement learning techniques for digital asset search.  The RL techniques consist of A3C and DQN.  This is an application paper since the techniques described already exist.  Unfortunately, there is a lack of detail throughout the paper and therefore it is not possible for someone to reproduce the results if desired.  Since there is no corpus of message response pairs to train the model, the paper trains a simulator from logs to emulate user behaviours.  Unfortunately, there is no description of the algorithm used to obtain the simulator.  The paper explains that the simulator is obtained from log data, but this is not sufficient.  The RL problem is described at a very high level in the sense that abstract states and actions are listed, but there is no explanation about how those abstract states are recognized from the raw text and there is no explanation about how the actions are turned into text.  There seems to be some confusion in the notion of state.  After describing the abstract states, it is explained that actions are selected based on a history of states.  This suggests that the abstract states are really abstract observations.   In fact, this becomes obvious when the paper introduces the RNN where a hidden belief is computed by combining the observations.  The rewards are also described at a hiogh level, but it is not clear how exactly they are computed.  The digital search application is interesting, however a detailed description with comprehensive experiments are needed for the publication of an application paper.","[3, 5, 2]","[' Clear rejection', ' Marginally below acceptance threshold', ' Strong rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,"['Victor Zhong', 'Caiming Xiong', 'Richard Socher']",Reject,2018,"[4, 10, 12]","[8, 15, 17]","[37, 383, 229]","[18, 165, 111]","[19, 208, 111]","[0, 10, 7]","The authors have addressed the problem of translating natural language queries to SQL queries. They proposed a deep neural network based solution which combines the attention based neural semantic parser and pointer networks. They also released a new dataset WikiSQL for the problem. The proposed method outperforms the existing semantic parsing baselines on WikiSQL dataset.

Pros:
1. The idea of using pointer networks for reducing search space of generated queries is interesting. Also, using extrinsic evaluation of generated queries handles the possibility of paraphrasing SQL queries.
2. A new dataset for the problem.
3. The experiments report a significant boost in the performance compared to the baseline. The ablation study is helpful for understanding the contribution of different component of the proposed method.

Cons:
1. It would have been better to see performance of the proposed method in other datasets (wherever possible). This is my main concern about the paper.
2. Extrinsic evaluation can slow down the overall training. Comparison of running times would have been helpful.
3. More details about training procedure (specifically for the RL part) would have been better.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning to select examples for program synthesis,"['Yewen Pu', 'Zachery Miranda', 'Armando Solar-Lezama', 'Leslie Pack Kaelbling']",Reject,2018,"[8, 2, 14, 33]","[13, 2, 19, 38]","[32, 4, 157, 298]","[15, 2, 81, 168]","[15, 2, 49, 91]","[2, 0, 27, 39]","The paper proposes a method for identifying representative examples for program
synthesis to increase the scalability of existing constraint programming
solutions. The authors present their approach and evaluate it empirically.

The proposed approach is interesting, but I feel that the experimental section
does not serve to show its merits for several reasons. First, it does not
demonstrate increased scalability. Only 1024 examples are considered, which is
by no means large. Even then, the authors approach selects the highest number of
examples (figure 4). CEGIS both selects fewer examples and has a shorter median
time for complete synthesis. Intuitively, the authors' method should scale
better, but they fail to show this -- a missed opportunity to make the paper
much more compelling. This is especially true as a more challenging benchmark
could be created very easily by simply scaling up the image.

Second, there is no analysis of the representativeness of the found sets of
constraints. Given that the results are very close to other approaches, it
remains unclear whether they are simply due to random variations, or whether the
proposed approach actually achieves a non-random improvement.

In addition to my concerns about the experimental evaluation, I have concerns
about the general approach. It is unclear to me that machine learning is the
best approach for modeling and solving this problem. In particular, the
selection probability of any particular example could be estimated through a
heuristic, for example by simply counting the number of neighbouring examples
that have a different color, weighted by whether they are in the set of examples
already, to assess its ""borderness"", with high values being more important to
achieve a good program. The border pixels are probably sufficient to learn the
program perfectly, and in fact this may be exactly what the neural net is
learning. The above heuristic is obviously specific to the domain, but similar
heuristics could be easily constructed for other domains. I feel that this is
something the authors should at least compare to in the empirical evaluation.

Another concern is that the authors' approach assumes that all parameters have
the same effect. Even for the example the authors give in section 2, it is
unclear that this would be true.

The text says that rand+cegis selects 70% of examples of the proposed approach,
but figure 4 seems to suggest that the numbers are very close -- is this initial
examples only?

Overall the paper appears rushed -- the acknowledgements section is left over
from the template and there is a reference to figure ""blah"". There are typos and
grammatical mistakes throughout the paper. The reference to ""Model counting"" is
incomplete.

In summary, I feel that the paper cannot be accepted in its current form.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Adversarial Learning for Semi-Supervised Semantic Segmentation,"['Wei-Chih Hung', 'Yi-Hsuan Tsai', 'Yan-Ting Liou', 'Yen-Yu Lin', 'Ming-Hsuan Yang']",Reject,2018,"[30, 11, 7, 11]","[35, 16, 12, 16]","[203, 205, 78, 156]","[79, 112, 43, 96]","[18, 6, 2, 5]","[106, 87, 33, 55]","This paper describes techniques for training semantic segmentation networks. There are two key ideas:

- Attach a pixel-level GAN loss to the output semantic segmentation map. That is, add a discriminator network that decides whether each pixel in the label map belongs to a real label map or not. Of course, this loss alone is unaware of the input image and would drive the network to produce plausible label maps that have no relation to the input image. An additional cross-entropy loss (the standard semantic segmentation loss) is used to tie the network to the input and the ground-truth label map, when available.

- Additional unlabeled data is utilized by using a trained semantic segmentation network to produce a label map with associated confidences; high-confidence pixels are used as ground-truth labels and are fed back to the network as training data.

The paper is fine and the work is competently done, but the experimental results never quite come together. The technical development isn’t surprising and doesn’t have much to teach researchers working in the area. Given that the technical novelty is rather light and the experimental benefits are not quite there, I cannot recommend the paper for publication in a first-tier conference.

Some more detailed comments:

1. The GAN and the semi-supervised training scheme appear to be largely independent. The GAN can be applied without any unlabeled data, for example. The paper generally appears to present two largely independent ideas. This is fine, except they don’t convincingly pan out in experiments.

2. The biggest issue is that the experimental results do not convincingly indicate that the presented ideas are useful.
2a. In the “Full” condition, the presented approach does not come close to the performance of the DeepLab baseline, even though the DeepLab network is used in the presented approach. Perhaps the authors have taken out some components of the DeepLab scheme for these experiments, such as multi-scale processing, but the question then is “Why?”. These components are not illegal, they are not cheating, they are not overly complex and are widely used. If the authors cannot demonstrate an improvement with these components, their ideas are unlikely to be adopted in state-of-the-art semantic systems, which do use these components and are doing fine.
2b. In the 1/8, 1/4, and 1/2 conditions, the performance of the baselines is not quoted. This is wrong. Since the authors are evaluating on the validation sets, there is no reason not to train the baselines on the same amount of labeled data (1/8, 1/4, 1/2) and report the results. The training scripts are widely available and such training of baselines for controlled experiments is commonly done in the literature. The reviewer is left to suspect, with no evidence given to the contrary, that the presented approach does not outperform the DeepLab baseline even in the reduced-data conditions.

A somewhat unflattering view of the work would be that this is another example of throwing a GAN at everything to see if it sticks. In this case, the experiments do not indicate that it did.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Ground-Truth Adversarial Examples,"['Nicholas Carlini', 'Guy Katz', 'Clark Barrett', 'David L. Dill']",Reject,2018,"[7, 8, 23, 33]","[12, 13, 28, 37]","[110, 97, 203, 174]","[44, 52, 121, 135]","[66, 38, 56, 7]","[0, 7, 26, 32]","The paper describes a method for generating so called ground truth adversarial examples: adversaries that have minimal (L1 or L_inf) distance to the training example used to generate them. The technique uses the recently developed reluplex, which can be used to verify certian properties of deep neural networks that use ReLU activations. The authors show how the L1 distance can be formulated using a ReLU and therefore extend the reluplex also work with L1 distances. The experiments on MNIST suggest that the C&W attack produces close to optimal adversarial examples, although it is not clear if these findings would transfer to larger more complex networks. The evaluation also suggests that training with iterative adversarial examples does not overfit and does indeed harden the network to attacks in many cases.

In general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being. Incidentally, it would have been useful if the authors provided indicative information on the computational cost (e.g. in the form of time on a standard GPU) for generating these ground truths and carrying out experiments.

The experiments are quite small scale, which I expect is due to the computational cost of generating the adversarial examples. It is difficult to say how far the findings can be generalized from MNIST to more realistic situations. Tests on another dataset would have been welcomed.

Also, while interesting, are adversarial examples that have minimal L_p distance from training examples really that useful in practice? Of course, it's nice that we can find these, but it could be argued that L_p norms are not a good way of judging the similarity of an adversarial example to a true example. I think it would be more useful to investigate attacks that are perceptually insignificant, or attacks that operate in the physical world, as these are more likely to be a concern for real world systems. 

In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning,"['Matt Riemer', 'Michele Franceschini', 'and Tim Klinger']",Reject,2018,"[4, 14, 15]","[9, 15, 20]","[46, 36, 38]","[20, 25, 22]","[25, 3, 15]","[1, 8, 1]","The paper proposes an architecture for efficient deep lifelong learning. The key idea is to use recollection generator (autoencoder) to remember the previously processed data in a compact representation. Then when training a reasoning model, recollections generated from the recollection generator are used with real-world examples as input data. Using the recollection, it can avoid forgetting previous data. In the experiments, it has been shown that the proposed approach is efficient for transfer knowledge with small data compared to random sampling approach.

It is an interesting idea to remember previous examples using the compact representation from autoencoder and use it for transfer learning. However, I think the paper would be improved if the following points are clarified.

1. It seems that reconstructed data from autoencoder does not contain target values. It is not clear to me how the reasoning model can use the reconstructed data (recollections) for supervised learning tasks. 

2. It seems that the proposed framework can be better presented as a method for data compression for deep learning. Ideally, for lifelong learning, the reasoning model should not forget previously learned kwnoledge embeded in their weights. 
However, under the current architecture, it seems that the reasoning model does not have such mechanisms.

3. For lifelong learning, it would be interesting to test if the same reasoning model can deal with increasing number of tasks from different datasets using the recollection mechanisms.

 



","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Trace norm regularization and faster inference for embedded speech recognition RNNs,"['Markus Kliegl', 'Siddharth Goyal', 'Kexin Zhao', 'Kavya Srinet', 'Mohammad Shoeybi']",Reject,2018,"[2, 11, 17, 2, 13]","[1, 16, 22, 7, 18]","[4, 26, 63, 15, 46]","[1, 13, 21, 4, 16]","[3, 10, 5, 11, 27]","[0, 3, 37, 0, 3]","Paper is well written and clearly explained. The paper is a experimental paper as it has more content on the experimentation and less content on problem definition and formulation. The experimental section is strong and it has evaluated across different datasets and various scenarios. However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue. It only considers a slight modification into the loss function by adding a trace norm regularization.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Comparison of Paragram and GloVe Results for Similarity Benchmarks,"['Jakub Dutkiewicz', 'Czesław Jędrzejek']",Reject,2018,"[6, 21]","[8, 23]","[17, 41]","[15, 37]","[1, 1]","[1, 3]","This paper proposes a ranking-based similarity metric for distributional semantic models. The main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""Ranking-based Exponential Similarity Measure"" (RESM), which is based on the recently proposed APSyn measure.

I think the work has several important issues:

1. The work is very light on references. There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method. None of this work is cited, which I find inexcusable. 

2. The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work. The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper. It is unclear what exactly helps, in which case, and why. 

3. There are technical issues with what is presented, with some seemingly factual errors. For example, ""In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance. Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product). Another example, ""are obtained using the GloVe vector, not using PPMI"" - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work). 

4. Then there is the additional question, why should we care? The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings. In other words, what is supposed to be the take-away, and why should we care?

As such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference.

Minor points:
- Typo in Eq 10
- Typo on page 6 (/cite instead of \cite)","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Optimal transport maps for distribution preserving operations on latent spaces of Generative Models,"['Eirikur Agustsson', 'Alexander Sage', 'Radu Timofte', 'Luc Van Gool']",Reject,2018,"[5, 2, 10, 35]","[10, 3, 15, 40]","[56, 4, 468, 1396]","[29, 2, 250, 801]","[25, 2, 194, 420]","[2, 0, 24, 175]","This paper is concerned with the mismatch between the input distribution used for training and interpolated input. It extends the discussion on this phenomenon and the correction method proposed by White (2016), and proposes an optimal transport-based approach, which essentially makes use of the trick of change of variables. The discussion of the phenomenon is interesting, and the proposed method seems well motivated and useful. There are a number of errors or inconsistencies in the paper, and the experiments results, compared to those given by SLERP, see rather weak. My big concern about the paper is that it seems to be written in a rush and needs a lot of improvement before being published. Below please see more detailed comments.

- In Introduction, the authors claim that ""This is problematic, since the generator G was trained on a fixed prior and expects to see inputs with statistics consistent with that distribution."" Here the learned generative network might still apply even if the input distribution changes (e.g., see the covariate shift setting); should one claim that the support of the test input distribution may not be contained in the support of the input distribution for training? Is there any previous result supporting this? 
- Moreover, I am wondering whether Sections 2.2 and 2.3 can be simplified or improved--the underlying idea seems intuitive, but some of the statements seem somewhat confusing. For instance, what does equation (6) mean?
- Note that a parenthesis is missing in line 3 below (4). In (6), the dot should follow the equation.
- Line 1 of page 7: here it would be nice to make it clear what p_{y|x} means. How did you obtain values of f(x) from this conditional distribution?
- Theorem 2: here does one assume that F_Y is invertible? (Maybe this is not necessary according to the definition of F_Y^{[-1]}...)
- Line 4 above Section 4.2: the sentence is not complete.
- Section 4.2: It seems that Figure 3 appears in the main text earlier than Figure 2. Please pay attention to the organization.
- Line 3, page 10: ""slightly different, however...""
- Line 3 below Figure 2: I failed to see ""a slight loss in detain for the SLERP version."" Perhaps the authors could elaborate on it?
- The paragraph above Figure 3 is not complete.","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Tandem Blocks in Deep Convolutional Neural Networks,"['Chris Hettinger', 'Tanner Christensen', 'Jeff Humpherys', 'Tyler J Jarvis']",Reject,2018,"[2, 2, 13, 15]","[2, 2, 16, 19]","[3, 4, 22, 7]","[0, 2, 9, 0]","[3, 2, 3, 5]","[0, 0, 10, 2]","This paper performs an analysis of shortcut connections in ResNet-like architectures. The authors hypothesize that the success of shortcut connections comes from the combination of linear and non-linear features at each layer and propose to substitute the identity shortcuts with a convolutional one (without non-linearity). This alternative is referred to as tandem block. Experiments are performed on a variety of image classification tasks such as CIFAR-10, CIFAR-100, SVHN and Fashion MNIST.

The paper is well structured and easy to follow. The main contribution of the paper is the comparison between identity skip connections and skip connections with one convolutional layer.

My main concerns are related to the contribution of the paper and experimental pipeline followed to perform the comparison. First, the idea of having convolutional shortcuts was already explored in the ResNet paper (see https://arxiv.org/pdf/1603.05027.pdf). Second, given Figures 3-4-5-6, it would seem that the authors are monitoring the performance on the test set during training. Moreover, results on Table 2 are reported as the ones with “the highest test accuracy achieved with each tandem block”. Could the authors give more details on how the hyperparameters of the architectures/optimization were chosen and provide more information on how the best results were achieved?

In section 3.5, the authors mention that batchnorm was not useful in their experiments, and was more sensitive to the learning rate value. Do the authors have any explanation/intuition for this behavior?

In section 4, authors claim that their results are competitive with the best published results for a similar number of parameters. It would be beneficial to add the mentioned best performing models in Table 2 to back this statement. Moreover, it seems that in some cases such as SVHN the differences between all the proposed blocks are too minor to draw any strong conclusions. Could those differences be due to, for example, luck in picking the initialization seed? How many times was each experiment run? If more than once, what was the std?

The experiments were performed on relatively shallow networks (8 to 26 layers). I wonder how the conclusions drawn scale to much deeper networks (of 100 layers for example) and on larger datasets such as ImageNet.

Figures 3-5 are not referenced nor discussed in the text.

Following the design of the tandem blocks proposed in the paper, I wonder why the tandem block B3x3(2,w) was not included.

Finally, it might be interesting to initialize the convolutions in the shortcut connections with the identity, and check what they have leant at the end of the training.

Some typos that the authors might want to fix:

- backpropegation -> backpropagation (Introduction, paragraph 3)
- dropout is a kind of regularization as well (Introduction, second to last paragraph)
- nad -> and (Sect 3.1. paragraph 1)
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy,"['Gintare Karolina Dziugaite', 'Daniel M. Roy']",Reject,2018,"[4, 29]","[9, 34]","[51, 108]","[21, 54]","[30, 49]","[0, 5]","1) I would like to ask for the clarification regarding the generalization guarantees. The original Entropy-SGD paper shows improved generalization over SGD using uniform stability, however the analysis of the authors rely on an unrealistic assumption regarding the eigenvalues of the Hessian (they are assumed to be away from zero, which is not true at least at local minima of interest). What is the enabling technique in this submission that avoids taking this assumption? (to clarify: the analysis is all-together different in both papers, however this aspect of the analysis is not fully clear to me).
2) It is unclear to me what are the unrealistic assumptions made in the paper. Please, list them all in one place in the paper and discuss in details.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Graph2Seq: Scalable Learning Dynamics for Graphs,"['Shaileshh Bojja Venkatakrishnan', 'Mohammad Alizadeh', 'Pramod Viswanath']",Reject,2018,"[5, 9, 23]","[10, 14, 28]","[37, 159, 262]","[14, 92, 104]","[17, 53, 99]","[6, 14, 59]","This paper proposes a novel way of embedding graph structure into a sequence that can have an unbounded length. 

There has been a significant amount of prior work (e.g. d graph convolutional neural networks) for signals supported on a specific graph. This paper on the contrary tries to encode the topology of a graph using a dynamical system created by the graph and randomization. 

The main theorem is that the created dynamical system can be used to reverse engineer the graph topology for any digraph. 
As far as I understood, the authors are doing essentially reverse directed graphical model learning. In classical learning of directed graphical models (or causal DAGs) one wants to learn the structure of a graph from observed data created by this graph inducing conditional independencies on data. This procedure is creating a dynamical system that (following very closely previous work) estimates conditional directed information for every pair of vertices u,v and can find if an edge is present from the observed trajectory. 
The recovery algorithm is essentially previous work (but the application to graph recovery is new).

The authors state:
``Estimating conditional directed information efficiently from samples is itself an active area of research Quinn et al. (2011), but simple plug-in estimators with a standard kernel density estimator will be consistent.''

One thing that is missing here is that the number of samples needed could be exponential in the degrees of the graph. Therefore, it is not clear at all that high-dimensional densities or directed information can be estimated from a number of samples that is polynomial in the dimension (e.g. graph degree).

This is related to the second limitation, that there is no sample complexity bounds presented only an asymptotic statement. 

One remark is that there are many ways to represent a finite graph with a sequence that can be decoded back to the graph (and of course if there is no bound on the graph size, there will be no bound on the size of the sequence). For example, one could take the adjacency matrix and sequentially write down one row after the other (perhaps using a special symbol to indicate 'next row'). Many other simple methods can be obtained also, with a size of sequence being polynomial (in fact linear) in the size of the graph. I understand that such trivial representations might not work well with RNNs but they would satisfy stronger versions of Theorem 1 with optimal size. 
On the contrary it was not clear how the proposed sequence will scale in the graph size. 


Another remark is that it seems that GCNN and this paper solve different problems. 
GCNNs want to represent graph-supported signals (on a fixed graph) while this paper tries to represent the topology of a graph, which seems different. 


The experimental evaluation was somewhat limited and that is the biggest problem from a practical standpoint. It is not clear why one would want to use these sequences for solving MVC. There are several graph classification tasks that try to use the graph structure (as well as possibly other features) see eg the bioinformatics 
and other applications. Literature includes for example:
Graph Kernels by S.V.N. Vishwanathan et al. 
Deep graph kernels (Yanardag & Vishwanathan and graph invariant kernels (Orsini et al.),
which use counts of small substructures as features. 

The are many benchmarks of graph classification tasks where the proposed representation could be useful but significantly more validation work would be needed to make that case. 

","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Residual Gated Graph ConvNets,"['Xavier Bresson', 'Thomas Laurent']",Reject,2018,"[17, 13]","[22, 18]","[97, 39]","[36, 10]","[35, 23]","[26, 6]","The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version.

Summary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs. It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees. The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms. They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering.  

Main comments:
It would strengthen the paper to also compare all these network learning based approaches to variational ones. For instance, to a spectral clustering method for the semi supervised clustering, or
solving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006.

The abstract and the conclusion should be revised, they are very vague.
- The abstract should be self contained and should not contain citations.
- The authors should clarify which problem they are dealing with.
- instead of the ""numerical result show the performance of the new model"", give some numerical results here, otherwise, this sentence is useless.
- we propose ... as propose -> unclear: what do you propose?
 

Minor comments:
- You should make sentences when using references with the author names format. Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997)
- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)
- Don't start sentences with And, or But

","[7, 3, 6]","[' Good paper, accept', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning to Optimize Neural Nets,"['Ke Li', 'Jitendra Malik']",Reject,2018,"[4, 36]","[8, 41]","[32, 446]","[11, 242]","[20, 154]","[1, 50]","[Main comments]

* I would advice the authors to explain in more details in the intro
what's new compared to Li & Malik (2016) and Andrychowicz et al. (2016).
It took me until section 3.5 to figure it out.

* If I understand correctly, the only new part compared to Li & Malik (2016) is
section 3.5, where block-diagonal structure is imposed on the learned matrices.
Is that correct?

* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without
  block-diagonal structure)

* Please clarify whether the objective value shown in the plots is wrt the training
  set or the test set. Reporting the training objective value makes little
sense to me, unless the time taken to train on MNIST is taken into account in
the comparison. 

* Please clarify what are the hyper-parameters of your meta-training algorithm
  and how you chose them.

I will adjust my score based on the answer to these questions.

[Other comments]

* ""Given this state of affairs, perhaps it is time for us to start practicing
  what we preach and learn how to learn""

This is in my opinion too casual for a scientific publication...

* ""aim to learn what parameter values of the base-level learner are useful
  across a family of related tasks""

If this is essentially multi-task learning, why not calling it so?  ""Learning
what to learn"" does not mean anything.  I understand that the authors wanted to
have ""what"", ""which"" and ""how"" sections but this is not clear at all.

What is a ""base-level learner""? I think it would be useful to define it more
precisely early on.

* I don't see the difference between what is described in Section 2.2
  (""learning which model to learn"") and usual machine learning (searching for
the best hypothesis in a hypothesis class).

* Typo: p captures the how -> p captures how

* The L-BFGS results reported in all Figures looked suspicious to me.  How do you
  explain that it converges to a an objective value that is so much worse?
Moreover, the fact that there are huge oscillations makes me think that the
authors are measuring the function value during the line search rather than
that at the end of each iteration.
","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Multi-Advisor Reinforcement Learning,"['Romain Laroche', 'Mehdi Fatemi', 'Joshua Romoff', 'Harm van Seijen']",Reject,2018,"[1, 14, 21, 12]","[6, 19, 26, 17]","[22, 198, 135, 114]","[9, 112, 79, 45]","[10, 49, 33, 62]","[3, 37, 23, 7]","This paper presents MAd-RL, a method for decomposition of a single-agent RL problem into a simple sub-problems, and aggregating them back together. Specifically, the authors propose a novel local planner - emphatic, and analyze the newly proposed local planner along of two existing ones - egocentric and agnostic. The MAd-RL, and theoretical analysis, is evaluated on the Pac-Boy task, and compared to DQN and Q-learning with function approximation.

Pros:
1. The paper is well written, and well-motivated.
2. The authors did an extraordinary job in building the intuition for the theoretical work, and giving appropriate examples where needed.
3. The theoretical analysis of the paper is extremely interesting. The observation that a linearly weighted reward, implies linearly weighted Q function, analysis of different policies, and local minima that result is the strongest and the most interesting points of this paper.

Cons:
1. The paper is too long. 14 pages total - 4 extra pages (in appendix) over the 8 page limit, and 1 extra page of references. That is 50% overrun in the context, and 100% overrun in the references. The most interesting parts and the most of the contributions are in the Appendix, which makes it hard to assess the contributions of the paper. There are two options: 
  1.1 If the paper is to be considered as a whole, the excessive overrun gives this paper unfair advantage over other ICLR papers. The flavor and scope and quality of the problems that can be tackled with 50% more space is substantially different from what can be addressed within the set limit. If the extra space is necessary, perhaps this paper is better suited for another publication? 
  1.2 If the paper is assessed only based on the main part without Appendix, then the only novelty is emphatic planner, and the theoretical claims with no proofs. The results are interesting, but are lacking implementation details. Overall, a substandard paper.
2. Experiments are disjoint from the method’s section. For example:
  2.1 Section 5.1 is completely unrelated with the material presented in Section 4.
  2.2 The noise evaluation in Section 5.3 is nice, but not related with the Section 4. This is problematic because, it is not clear if the focus of the paper is on evaluating MAd-RL and performance on the Ms.PacMan task, or experimentally demonstrating claims in Section 4.

Recommendations:
1. Shorten the paper to be within (or close to the recommended length) including Appendix.
2. Focus paper on the analysis of the advisors, and Section 5. on demonstrating the claims.
3. Be more explicit about the contributions.
4. How does the negative reward influence the behavior the agent? The agent receives negative reward when near ghosts.
5. Move the short (or all) proofs from Appendix into the main text.
6. Move implementation details of the experiments (in particular the short ones) into the main text.
7. Use the standard terminology (greedy and random policies vs. egoistic and agnostic) where possible. The new terms for well-established make the paper needlessly more complex. 
8. Focus the literature review on the most relevant work, and contrast the proposed work with existing peer reviewed methods.
9. Revise the literature to emphasize more recent peer reviewed references. Only three references are recent (less than 5 years), peer reviewed references, while there are 12 historic references. Try to reduce dependencies on non-peer reviewed references (~10 of them).
10. Make a pass through the paper, and decouple it from the van Seijen et al., 2017a
11. Minor: Some claims need references:
  11.1 Page 5: “egocentric sub-optimality  does not come from the actions that are equally good, nor from the determinism of the policy, since adding randomness…” - Wouldn’t adding epsilon-greediness get the agent unstuck?
  11.2 Page 1. “It is shown on the navigation task ….” - This seems to be shown later in the results, but in the intro it is not clear if some other work, or this one shows it.  
12. Minor:
  12.1 Mix genders when talking about people. Don’t assume all people that make “complex and important problems”, or who are “consulted for advice”, are male.
  12.2 Typo: Page 5: a_0 sine die
  12.3 Page 7 - omit results that are not shown
  12.4 Make Figures larger - it is difficult, if not impossible to see
  12.5 What is the difference between Pac-Boy and Ms. Pacman task? And why not use Ms. Packman?
 
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Joint autoencoders: a flexible meta-learning framework,"['Baruch Epstein', 'Ron Meir', 'Tomer Michaeli']",Reject,2018,"[2, 31, 12]","[6, 35, 17]","[5, 106, 95]","[2, 46, 43]","[3, 18, 38]","[0, 42, 14]","The work proposed a generic framework for end-to-end transfer learning / domain adaptation with deep neural networks. The idea is to learn a joint autoencoders, containing private branch with task/domain-specific weights, as well as common branch consisting of shared weights used across tasks/domains, as well as task/domain-specific weights.  Supervised losses are added after the encoders to utilize labeled samples from different tasks. Experiments on the MNIST and CIFAR datasets showed improvements over baseline models. Its performance is comparable to / worse than several existing deep domain adaptation works on the MNIST, USPS and SVHN digit datasets.

The structure of the paper is good, and easy to read.  The idea is fairly straight-forward. It reads as an extension of ""frustratingly easy domain adaptation"" to DNN (please cite this work). Different from most existing work on DNN for multi-task/transfer learning, which focuses on weight sharing in bottom layers, the work emphasizes the importance of weight sharing in deeper layers. The overall novelty of the work is limited though. 

The authors brought up two strategies on learning the shared and private weights at the end of section 3.2. However, no follow-up comparison between the two are provided. It seems like most of the results are coming from the end-to-end learning. 

Experimental results:
section 4.1: Figure 2 is flawed. The colors do not correspond to the sub-tasks. For example, there are digits 1, 4 in color magenta, which is supposed to be the shared branch of digits of 5~9. Vice versa. 
From reducing the capacity of JAE to be the same as the baseline, most of the improvement is gone. It is not clear how much of the improvement will remain if the baseline model gets to see all the samples instead of just those from each sub-task. 

section 4.2.1: The authors demonstrate the influence of shared layer depth in table 2. While it does seem to matter for tasks of dissimilar inputs, have the authors compare having a completely shared branch or sharing more than just a single layer?

The authors suggested in section 4.1 CIFAR experiment that the proposed method provides more performance boost when the two tasks are more similar, which seems to be contradicting to the results shown in Figure 3, where its performance is worse when transferring between USPS and MNIST, which are more similar tasks vs between SVHN and MNIST. Do the authors have any insight?","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Flexible Prior Distributions for Deep Generative Models,"['Yannic Kilcher', 'Aurelien Lucchi', 'Thomas Hofmann']",Reject,2018,"[3, 10, 26]","[8, 15, 31]","[20, 110, 205]","[6, 51, 112]","[13, 52, 69]","[1, 7, 24]","Summary:

The paper proposes to learn new priors for latent codes z  for GAN training.  for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator . To fix this the paper proposes to learn a second GAN to learn the prior distributions of ""real latent code"" of the first GAN. The first GAN then uses the second GAN as prior to generate the z codes. 
 
Quality/clarity:

The paper is well written and easy to follow.

Originality:

pros:
-The paper while simple sheds some light on important problem with the prior distribution used in GAN.
- the second GAN solution trained on reverse codes from real data is interesting 
- In general the topic is interesting, the solution presented is simple but needs more study

cons:

- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement. 
- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder)

General Review:

More experimentation with the latent codes will be interesting:

- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator? Is this data low rank? how does this change depending on the dimensionality of the latent codes? Maybe adding plots to the paper can help.

- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.  Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior?

- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc? Maybe also rotating the codes with the singular vector matrix V or \Sigma^{0.5} V?

- What architecture did you use for the prior generator GAN?

- Have you thought of an end to end way to learn the prior generator GAN? 

****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.  *****

","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Prediction Under Uncertainty with Error Encoding Networks,"['Mikael Henaff', 'Junbo Zhao', 'Yann Lecun']",Reject,2018,"[8, 4, 31]","[12, 5, 36]","[24, 12, 315]","[11, 5, 162]","[12, 7, 113]","[1, 0, 40]","Summary: 

I like the general idea of learning ""output stochastic"" noise models in the paper, but the idea is not fully explored (in terms of reasonable variations and their comparative performance).  I don't fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN's failure (GANs are not easy to train and this seems to be reflected in the results); the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample.  I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded.

Comments:

The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).  The trained residual function can be used to predict a residual z_i for x_i.  Then for out-of-sample prediction for x*, the paper appears to propose sampling a z uniformly from the training data {z_i}_i (it is not clear from the description on page 3 that this uniformly sampled z* = z_i depends on the actual x* -- as far as I can tell it does not).  The paper does suggest learning a p(z|x) but does not provide implementation details nor experiment with this approach.

I like the idea of learning an ""output stochastic"" model -- it is much simpler to train than an ""input stochastic"" model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable.  However, I don't think the authors explore the idea well enough -- they simply appear to propose a non-parametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches.  To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.  Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input?  One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x.  If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.

The experiments range over 4 video datasets.  PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model ""EEN"" is compared to a deterministic model and conditional GAN.  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?  Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.  If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples.  Have we learned anything?
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Deep ResNet Blocks Sequentially using Boosting Theory,"['Furong Huang', 'Jordan T. Ash', 'John Langford', 'Robert E. Schapire']",Reject,2018,"[9, 8, 21, 32]","[14, 13, 26, 36]","[105, 23, 222, 202]","[36, 10, 112, 121]","[60, 13, 83, 38]","[9, 0, 27, 43]","Summary:
This paper considers a learning method for the ResNet using the boosting framework. More precisely, the authors view the structure of the ResNet as a (weighted) sum of base networks (weak hypotheses) and apply the boosting framework. The merit of this approach is to decompose the learning of complex networks to that of small to large networks in a moderate way and it uses less computational costs. The experimental results are good. The authors also show training and generalization error bounds for the proposed approach.

Comments: 
The idea of the paper is natural and interesting. Experimental results are somewhat impressive. However, I am afraid that theoretical results in the paper contain several mistakes and does not hold. The details are below.

I think the proof of Theorem 4.2 is wrong. More precisely, there are several possibly wrong arguments as follows:
- In the proof, \alpha_t+1 is chosen so as to minimize an upper bound of Z_t, while the actual algorithm is chosen to minimize Z_t. The minimizer of Z_t and that of an upper bound are different in general. So, the obtained upper bound does not hold for the training error of the actual algorithm. 
- It is not a mistake, but, there is no explanation why the equality between (27) and (28) holds. Please add an explanation. Indeed, equation (21) matters. 

Also, the statement of Theorem 4.2 looks somewhat cheating: The statement seems to say that it holds for any iteration T and the training error decays exponentially w.r.t. T. However, the parameter T is determined by the parameter gamma, so it is some particular iteration, which might be small and the bound could be large. 

The generalization error bound Corollary 4.3 seems to be wrong, too. More precisely, Lemma 2 of Cortes et al. is OK, but the application of Lemma 2 is not. In particular, the proof does not take into account of the function \sigma. In other words, the proof considers the Rademacher complexity R_m(\calF_t), of the class \calF_t, but, acutually, I think it should consider R_m(\sigma(\calF_t)), where the class \sigma(\calF_t) consists of the composition of functions \sigma and f_t in \calF_t. Talagrand’s lemma (see, e.g., Mohri et al.’ s book: Foundation of Machine Learning) can be used to analyze the complexity of the composite class. But, the resulting bound would depend on the Lipschizness of \sigma in an exponential way. 

The explanation of the generalization ability is not sufficient. While the latter weak hypotheses are complex enough and would have large edges, the complexity of the function class of weak hypotheses grows exponentially w.r.t. the iteration T, which should be mentioned. 

As a summary, the paper contains nice ideas and experimental results are promising, but has non-negligible mistakes in theoretical parts which degrade the contribution of the paper.

Minor Comments:
-In Algorithm 1, \gamma_t is not defined when a while-loop starts. So, the condition of the while-loop cannot be checked.

 
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Multimodal Sentiment Analysis To Explore the Structure of Emotions,"['Anthony Hu', 'Seth Flaxman']",Reject,2018,"[1, 10]","[5, 15]","[13, 40]","[5, 13]","[8, 20]","[0, 7]","The paper presents a multi-modal CNN model for sentiment analysis that combines images and text.  The model is trained on a new dataset collected from Tumblr.

Positive aspects:
+ Emphasis in model interpretability and its connection to psychological findings in emotions
+ The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative. 

Weaknesses:
- A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary. 
- The proposed method is not compared with other methods that combine text and image for sentiment analysis.
-  The study is limited to just one dataset.

The paper presents interesting ideas and findings in an important challenging area. The main novelties of the paper are: (1) the use of Tumblr data, (2) the proposed CNN architecture, combining images and text (using word embedding. 

I missed a ""related work section"", where authors clearly mention previous works on similar datasets. Some related works are mentioned in the paper, but those are spread in different sections. It's hard to get a clear overview of the previous research: datasets, methods and contextualization of the proposed approach in relation with previous work. I think authors should cite Sentibanks. Also, at some point authors should compare their proposal with previous work. 

More comments:

- Some figures could be more complete: to see more examples in Fig 1, 2, 3 would help to understand better the dataset and the challenges. 
- In table 4, for example, it would be nice to see the performance on the different emotion categories.
- It would be interesting to see qualitative visual results on recognitions.

I like this work, but I think authors should improve the aspects I mention for its publication.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Generating Adversarial Examples with Adversarial Networks,"['Chaowei Xiao', 'Bo Li', 'Jun-Yan Zhu', 'Warren He', 'Mingyan Liu', 'Dawn Song']",Reject,2018,"[1, 17]","[6, 22]","[6, 118]","[3, 57]","[3, 43]","[0, 18]","This paper describes AdvGAN, a conditional GAN plus adversarial loss. AdvGAN is able to generate adversarial samples by running a forward pass on generator. The authors evaluate AdvGAN on semi-white box and black box setting.

AdvGAN is a simple and neat solution to for generating adversary samples. The author also reports state-of-art results.

Comment:

1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap. I am wondering this method is trying to mixture several samples into one to generate adversary samples. For real color samples, it is harder to figure out the mixture.
2. Based on mixture assumption, I suggest the author add one more comparison to other method, which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).



","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Discriminative k-shot learning using probabilistic models,"['Matthias Bauer', 'Mateo Rojas-Carulla', 'Jakub Bartłomiej Świątkowski', 'Bernhard Schölkopf', 'Richard E. Turner']",Reject,2018,"[3, 2, 2, 24, 12]","[8, 6, 1, 29, 17]","[16, 11, 1, 777, 147]","[7, 3, 0, 380, 67]","[9, 6, 1, 286, 70]","[0, 2, 0, 111, 10]","This paper presents a procedure to efficiently do K-shot learning in a classification setting by creating informative priors from information learned from a large, fully labeled dataset.  Image features are learned using a standard convolutional neural network---the last layer form image features, while the last set of weights are taken to be image ""concepts"".  The method treats these weights as data, and uses these data to construct an informative prior over weights for new features.

- Sentence two: would be nice to include a citation from developmental psychology.

- Probabilistic modeling section: treating the trained weights like ""data"" is a good way to convey intuition about your method.  It might be good to clarify some specifics earlier on in the ""Probabilistic Modeling"" paragraph, e.g. how many ""observations"" are associated with this matrix. 
 
- In the second phase, concept transfer, is the only information from the supervised weights the mean and estimated covariance?  For instance, if there are 80 classes and 256 features from the supervised phase, the weight ""data"" model is 80 conditionally IID vectors of length 256 ~ Normal(\mu, \Sigma).  The posterior MAP for \mu and \Sigma are then used as a prior for weights in the K-shot task.  How many parameters are estimated for 

  * gauss iso: mu = 256-length vector, \sigma = scalar variance value of weights
  * log reg: mu = 256-length zero vector, \sigma = scalar variance value of weights
  * log reg cross val: mu = 256-length zero vector, \sigma = cross validated value

If the above is correct, the information boosts K-shot accuracy is completely contained in the 256-length posterior mean vector and the scalar weight variance value?

- Is any uncertainty about \mu_MAP or \Sigma_MAP propagated through to uncertainty in the K-shot weights?  If not, would this influence the choice of covariance structure for \Sigma_MAP? How sensitive are inferences to the choice of Normal inverse-Wishart hyper parameters? 

- What do you believe is the source of the mis-calibration in the ""predictied probability vs. proportion of times correct"" plot in Figure 2?  

Technical: The method appears to be technically correct.

Clarity: The paper is pretty clearly written, however some specific details of the method are difficult to understand.

Novel: I am not familiar with K-shot learning tasks to assess the novelty of this approach. 

Impact: While the reported results seem impressive and encouraging, I believe this a relatively incremental approach. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,"['Martin Simonovsky', 'Nikos Komodakis']",Reject,2018,"[3, 15]","[5, 19]","[13, 130]","[6, 71]","[6, 28]","[1, 31]","The authors propose a variational auto encoder architecture to generate graphs.  

Pros:
- the formulation of the problem as the modeling of a probabilistic graph is of interest 
- some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix)
- notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs 

Cons: 
- the work is quite preliminary
- many crucial elements  in graph generation are not dealt with: 
 a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the 'relational links' between the various components
 b) the boundaries between a feasible and an infeasible graph are sharp: one edge or one label can be sufficient for acting the transition independently of the graph size, this makes it a difficult task for a continuous model. The authors acknowledge this but do not offer ways to tackle the issue
 c) conditioning on the label histogram should make the problem easy: one is giving away the number of nodes and the label identities after all; however even in this setup the approach fails more often than not 
 d) the graph matching procedure proposed is a rough patch for a much deeper problem
- the evaluation should include a measure of the capacity of the architecture to :
 a) reconstruct perfectly the input
 b) denoise perturbations over node labels and additional/missing  edges  ","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Time Limits in Reinforcement Learning,"['Fabio Pardo', 'Arash Tavakoli', 'Vitaly Levdik', 'Petar Kormushev']",Reject,2018,"[2, 3, 2, 10]","[5, 8, 4, 15]","[11, 32, 6, 99]","[4, 13, 2, 50]","[7, 17, 4, 24]","[0, 2, 0, 25]","Summary: This paper explores how to handle two practical issues in reinforcement learning. The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way. The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration. The ideas are illustrated through several well-worked micro-world experiments.

Overall the paper is well written and polished. They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice.

My main issue with the paper is that these two topics are actually not new and are well covered by the existing RL formalisms. That is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists.

The first idea of the paper is to include time-remaining in the state. This is of course always possible in the MDP formalism. If it was not done, as in your examples, the state would not be Markov and thus it would not be an MDP at all. In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H). It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate. The paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case, to motivate your work. See Dynamic Programming and Optimal Control and references too it.

The second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return. I could not tell from the text but I assume, the next transition to the start state is fully discounted to zero, otherwise the value function would link the values of S_T and the next state, which I assume you do not want. The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states. This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue). If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior. The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]

Another way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2]. In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs). However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP. To clearly establish your contributions, the authors must do a better job of relating their work to [1] and [2].

[1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017.

[2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768. 

Small comments that did not impact paper scoring:
1) eq 1 we usually don't use the superscript \gamma
2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return. You should mention this
3) Last paragraph of page 2 should not be in the intro
4) in section 2.2 why is the behavior policy random instead of epsilon greedy?
5) It would be useful to discuss the average reward setting and how it relates to your work.
6) Fig 5. What does good performance look like in this domain. I have no reference point to understand these graphs
7) page 9, second par outlines alternative approaches but they are not presented as such. Confusing ","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Priors for Adversarial Autoencoders,"['Hui-Po Wang', 'Wei-Jan Ko', 'Wen-Hsiao Peng']",Reject,2018,"[1, 1, 17]","[6, 5, 22]","[13, 5, 108]","[6, 3, 67]","[7, 2, 21]","[0, 0, 20]","This paper propose a simple extension of the adversarial auto-encoders for (conditional) image generation. The general idea is that instead of using Gaussian prior, the propose algorithm uses a ""code generator"" network  to warp the gaussian distribution, such that the internal prior of the latent encoding space is more expressive and complicated. 

Pros:
- The proposed idea is simple and easy to implement
- The results show improvement in terms of visual quality

Cons:
- I agree that the proposed prior should better capture the data distribution. However, incorporating a generic prior over the latent space plays a vital role as regularisation, this helps avoid model collapse. Adding a complicated code generation network brings too much flexibility for the prior part. This makes the prior and posterior learnable, which makes it easier to fool the regularisation discriminator (think about the latent code and prior code collapsed to two different points). As a result, this weakens the regularisation over the latent encoder space.  
- The above mentioned could be verified through qualitative results. As shown in Fig. 5. I believe this is a result due to the fact that the adversarial loss in the regularisation phase does not a significant influence there. 
- I have some doubts over why AAE works so poorly when the latent dimension is 2000. How to make sure it's not a problem of implementation or the model wasn't trapped into a bad local optima / saddle points. Could you justify this?
- Contributions; this paper propose an improvement over a existing model. However, neither the idea/insights it brought can be applied onto other generative models, nor the improvement bring a significant improvement over the-state-of-the-arts. I am wondering what the community will learn from this paper, or what the author would like to claim as significant contributions. ","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Lifelong Learning by Adjusting Priors,"['Ron Amit', 'Ron Meir']",Reject,2018,"[7, 31]","[11, 35]","[9, 106]","[5, 46]","[4, 18]","[0, 42]","I personally warmly welcome any theoretically grounded methods to perform deep learning. I read the paper with interest, but I have two concerns about the main theoretical result (Theorem 1, lifelong learning PAC-Bayes bound).
* Firstly, the bound is valid for a [0,1]-valued loss, which does not comply with the losses used in the experiments (Euclidean distance and cross-entropy). This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. However, this should clearly appear in the theorem statement.
* Secondly, and more importantly, I doubt that the uaw of the meta-posterior as a distribution over priors for each task is valid. In Proposition 1 (the classical single-task PAC-Bayes bound), the bound is valid with probability 1-delta for one specific choice of prior P, and this choice must be independent of the learning sample S. However, it appears that the bound should be valid uniformly for all P in order to be used in Theorem 1 proof (see Equation 18). From a learning point of view, it seems counterintuitive that the prior used in the KL term to learn from a task relies on the training samples (i.e., the same training samples are used to learn the meta-posterior over priors, and the task specific posterior).  

A note about the experiments:
I am slightly disappointed that the authors compared their algorithm solely with methods learning from fewer tasks. I would like to see the results obtained by another method using five tasks. A simple idea would be to learn a network independently for each of the five tasks, and consider as a meta-prior an isotropic Gaussian distribution centered on the mean of the five learned weight vectors.

Typos and minor comments:
- Equation 1: \ell is never explicitly defined.
- Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau).
- Page 4, before Equation 5: A dot is missing between Q and ""This"".
- Page 7, line 3: Missing parentheses around equation number 12.
- Section 5.1.1, line 5: ""The hypothesis class is a the set of...""
- Equation 17: Q_1, ... Q_n are irrelevant.

=== UPDATE ===
I increased my score after author's rebuttal. See my other post.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,['Hang Wu'],Reject,2018,"[14, 16]","[19, 21]","[91, 204]","[46, 156]","[5, 8]","[40, 40]","This paper studies off-policy learning in the bandit setting. It develops a new learning objective where the empirical risk is regularized by the squared Chi-2 divergence between the new and old policy. This objective is motivated by a bound on the empirical risk, where this divergence appears. The authors propose to solve this objective by using generative adversarial networks for variational divergence minimization (f-GAN). The algorithm is then evaluated on settings derived from supervised learning tasks and compared to other algorithms.

I find the paper well written and clear. I like that the proposed method is both supported by theory and empirical results. 

Minor point: I do not really agree with the discussion on the impact of the stochasticity of the logging policy in section 5.6. Based on Figure 5 a and b, it seems that the learned policy is performing equally well no matter how stochastic the logging policy is. So I find it a bit misleading to suggest that the learned policy are not being improved when the logging policy is more deterministic. Rather, the gap reduces between the two policies because the logging policy gets better. In order to better showcase this mechanism, perhaps you could try using a logging policy that does not favor the best action.

quality and clarity:
++ code made available
+ well written and clear
- The proof of theorem 2 is not in the paper nor appendix (the authors say it is similar to another work).


originality
+ good extension of the work by Swaminathan & Joachims (2015a): derivation of an alternative objective and use of a deep networks
. This paper leverages a set of diverse results

significance
- The proposed method can only be applied if propensity scores were recorded when the data was generated.
- no test on a real setting
++ The proposed method is supported both by theoretical insights and empirical experiments.
+ empirical improvement with respect to previous methods


details/typos:

3.1, p3: R^(h) has an indexed parenthesis
5.2; and we more details
5.3: so that results more comparable","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Self-Organizing Memory Network,"['Callie Federer', 'Joel Zylberberg']",Reject,2018,"[19, 15, 30]","[24, 20, 34]","[223, 121, 121]","[111, 62, 76]","[11, 20, 0]","[101, 39, 45]","A neural network model consisting of recurrently connected neurons and one or more readouts is introduced which aims to retain some output over time. A plasticity rule for this goal is derived. Experiments show the robustness of the network with respect to noisy weight updates, number of non-plastic connections, and sparse connectivity. Multiple consecutive runs increase the performance; furthermore, remembering multiple stimuli is possible. Finally, ideas for the biological implementation of the rule are suggested.

While the presentation is generally comprehensible a number of errors and deficits exist (see below). In general, this paper addresses a question that seems only relevant from a neuroscience perspective. Therefore, I wonder whether it is relevant in terms of the scope of this conference. I also think that the model is rather speculative. The authors argue that the resulting learning rule is biologically plausible. But even if this is the case, it does not imply that it is implemented in neuronal circuits in the brain. As far as I can see, there exists no experimental evidence for this rule. 

The paper shows the superiority of the proposed model over the approach of Druckmann & Chkolvskii (2012), however, it lacks in-depth analysis of the network behavior. Specifically, it is not clear how the information is stored. Do neurons show time-varying responses as in Druckmann & Chkolvskii (2012) or do all neuron stabilize within the first 50 ms (as in Fig. 2A, it is not detailed how the neurons shown there have been selected)? Do the weights change continuously within the delay period or do they also converge rapidly? This question is particularily important when considering multiple consecutive trials (cf. Fig. 5) as it seem that a specific but constant network architecture can retain the desired stimulus without further plasticity. Weight histograms should be presented for the different cases and network states. Also, since autapses are allowed, an analysis of their role should be performed. This information is vital to compare the model to findings from neuroscience and judge the biologic realism.

The target used is \hat{s}(t) / \hat{s}(t = 0), this is dubbed ""fraction of stimulus retained"". In most plots, the values for this measure are <= 1, but in Fig. 3A, the value (for the FEVER network) is > 1. Thus, the name is arguably not well-chosen: how can a fraction of remembrance be greater than one? Also, in a realistic environment, it is not clear that the neuronal activities decay to zero (resulting in \hat{s}(t) also approaching zero). A squared distances measure should therefore be considered.

It is not clear from the paper when and how often weight updates are performed. Therefore, the biologic plausability cannot be assessed, since the learning rule might lead to much more rapid changes of weights than the known learning rules in biological neural networks. Since the goal seems to be biologic realism, generally, spiking neurons should be used for the model. This is important as spiking neural networks are much more fragile than artificial ones in terms of stability.

Further remarks:

- In Sec. 3.2.1, noise is added to weight updates. The absolute values of alpha are hard to interpret since it is not clear in what range the weights, activities, and weight updates typically lie.

- In Sec. 3.2.2 it is shown that 10% plastic synapses is enough for reasonable performance. In this case, it should be investigated whether the full network is essential for the memory task at all (especially since later, it is argued that 100 neurons can store up to 100 stimuli).

- For biologic realism, just assuming that the readout value at t = 0 is the target seems a bit too simple. How does this output arise in the first place? At least, an argument for this choice should be presented.


Remarks on writing:

- Fig. 1A is too small to read.

- The caption of Fig. 4C is missing.

- In Fig. 7AB, q_i and q_j are swapped. Also, it is unclear in the figure to which connection the ds and qs belong.

- In 3.6.1, Fig. 7 is referenced, but in the figure the terminology of Eq. 5 is used, which is only introduced in Sec. 3.6.2. This is confusing.

- The beginning of Sec. 3.6 claims that all information is local except d\hat{s}_k / dt, but this is not the case as d_i is not local (which is explained later).

- The details of the ""stimulus presentation"" (i.e. it is not performed explicitly) should be emphasised in 2.1. Also, the description of the target \hat{s} is much clearer in 3.4 than in 2.1 (where it should primarily be explained).

- The title of the citation Cowan (2010) is missing.

- In Fig. 2A, the formulas are too small too read in a printed version.

- In Sec. 3.6.1 some sums are given over k, but k is also the index of a neuron in Fig. 7A (which is referenced there), this can be ambiguous and could be changed. ","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Quadrature-based features for kernel approximation,"['Marina Munkhoeva', 'Yermek Kapushev', 'Evgeny Burnaev', 'Ivan Oseledets']",Reject,2018,"[1, 4, 6, 11]","[6, 8, 11, 16]","[15, 13, 282, 233]","[5, 4, 108, 41]","[8, 6, 143, 117]","[2, 3, 31, 75]","The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules. The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms.

It is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features. Given these previous works, the contribution and novelty of the paper is limited.

[1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017.
[2] Tri Dao, Christopher De Sa, Christopher Ré. Gaussian Quadrature for Kernel Features. NIPS 2017","[4, 6, 7]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer,"['Joseph Suarez', 'Justin Johnson', 'L. Fei-Fei']",Reject,2018,"[2, 4, 16]","[7, 9, 21]","[10, 74, 406]","[4, 28, 226]","[6, 43, 147]","[0, 3, 33]","
Summary: This paper leverages an explicit program format and proposes a stack based RNN to solve question answering. The paper shows state-of-the art performance on the CLEVR dataset.

Clarity:
- The description of the model is vague: I have to looking into appendix on what are the Cell and Controller function.
- The authors should also improve the intro and related work section. Currently there is a sudden jump between deep learning and the problem of interest. Need to expand the related work section to go over more literature on structured RNN.

Pros:
- The model is fairly easy to understand and it achieves state-of-the-art performance on CLEVR.
- The model fuses text and image features in a single model.

Cons:
- This paper doesn’t mention recursive NN (Socher et al., 2011) and Tree RNN (Tai et al., 2015). I think they have fairly similar structure, at least conceptually, the stack RNN can be thought as a tree parser. And since the push/pop operations are static (based on the inputs), it’s no more different than encoding the question structure in the tree edges.
- The IEP (Cells) module (Johnson et al., 2017) seems to do all the heavy-lifting in my opinion. That’s why the proposed method only uses 9M parameters. The task isn’t very challenging to learn because all the stack operations are already given. Table 1 should note clearly which methods use problem specific parsing information to train and which use raw text. Based on my understanding of FiLM at least, they use raw words instead of groundtruth parse trees. So it’s not very surprising that the proposed method can outperform FiLM (by a little bit).
- I don’t fully agree with the title - the stack operations are not differentiable. So whatever network that outputs the stack operation cannot be jointly learned with gradients. This is based on the if-else statements I see in Algorithm 1.

Conclusion:
- Since the novelty is limited and it requires explicit program supervision, and the performance is only on par with the state-of-the-art (FiLM), I am not convinced that this paper brings enough contribution to be accepted. Weak reject.

References:
- Socher, R., Lin, C., Ng, A.Y., Manning, C.D. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. The 28th International Conference on Machine Learning (ICML 2011).
- Tai, K.S., Socher, R., Manning C.D. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. The 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015).","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[2, 2, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Reward Design in Cooperative Multi-agent Reinforcement Learning for Packet Routing,"['Hangyu Mao', 'Zhibo Gong', 'Zhen Xiao']",Reject,2018,"[3, 2, 20]","[8, 4, 25]","[32, 10, 90]","[16, 4, 52]","[13, 5, 11]","[3, 1, 27]","The paper provides an empirical study of different  reward schemes for cooperative multi-agent reinforcement learning.  A number of alternative reward schemes are proposed, partly based on existing literature. These reward schemes are evaluated empirically in a packet routing problem.  

The approach taken by this paper is very ad-hoc. It is not clear to me that this papers offers any general insights or methodologies for reward design for MARL. The only conclusions that can be drawn from this paper is which reward performs best on these specific problem instances(and even this is hard to conclude from the paper).  

In general, it seems strange to propose the packet routing problems as benchmark environments for reward design. From the descriptions in the paper these environments seem relatively complex and make it difficult to study the actual learning dynamics. The results shown provide global performance but do not allow to study specific properties.

The paper is also quite hard to read.  It is littered with non-intuitive abbreviations. The methods and experiments are poorly explained. It claims to study rewards for multi-agent reinforcement learning, but never properly details the learning setting that is considered or how this affects the choice of rewards.  Experiments are mostly described in terms of method A outperforms method B. No effort is made to investigate the cause of these results or to design experiments that would offer deeper insights. The graphs are not properly labelled, poorly described in general and are almost impossible to interpret. The main results are presented simply as a large table of raw performance numbers. This paper does not seem to offer any major fundamental or applied contributions. 
","[2, 5, 5]","[' Strong rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML,"['Xuezhe Ma', 'Pengcheng Yin', 'Jingzhou Liu', 'Graham Neubig', 'Eduard Hovy']",Reject,2018,"[9, 6, 7, 10, 34]","[14, 11, 12, 15, 39]","[79, 61, 20, 641, 517]","[37, 25, 8, 350, 350]","[42, 26, 9, 245, 123]","[0, 10, 3, 46, 44]","The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML. Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary. The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule. (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically. (3) Through one experiment using synthetic data on multi-class classiﬁcation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speciﬁc metrics that is desired to optimize. 

I found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric. Given a reward function, one can define the Bayes decision rule. The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule. The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17). Of course, the moving-out is biased but the replacing is unbiased. 

The second contribution is partially valid, although I doubt how much improvement one can get from SQDML. The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15). In fact, this step can result in biased estimation because the replacement is inside the nonlinear function. When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example. However, when x is not repeated frequently, both RAML and SQDML are biased. Experiment in section 4.1.2 do not validate significant improvement, either.

The numerical results are relatively weak. The synthetic experiment verifies the reward-maximizing property of RAML and SQDML. However, from Figure 2, we can see that the result is quite sensitive to the temperature \tau. Is there any guidelines to choose \tau? For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper. These experiment results show very small improvement compared to the ML baselines (see Table 2,3 and 5).  These results are also lower than the state of the art performance. 

A few questions:
(1). The author may want to check whether (8) can be called a Bayes decision rule. This is a direct result from definition of conditional probability. No Bayesian elements, like prior or likelihood appears here.
(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator. Compared with the n-gram replacement used in the paper, which one is better?
(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14. This will make the comparison much more clear.
(4). What is Theorem 2 trying to convey? Although \tau goes to 0, there is still a gap between Q and Q'. This seems to suggest that for small \tau, Q' is not a good approximation of Q. Are the assumptions in Theorem 2 reasonable? There are several typos in the proof of Theorem 2. 
(5). In section 4.2.2, the authors write ""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. τ than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important"". Could you explain it in more details?

","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,"['Shikhar Sharma', 'Layla El Asri', 'Hannes Schulz', 'Jeremie Zumer']",Reject,2018,"[6, 7, 13, 6]","[9, 11, 17, 6]","[19, 31, 59, 7]","[8, 18, 35, 3]","[10, 13, 13, 4]","[1, 0, 11, 0]","1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation. This paper can be considered as an extension of Liu, et al, 2016 while the later one did an empirical study in non-task-oriented dialogue generation.  

2)My questions are as follows:
i) The author should give the more detailed definition of what is non-task-oriented and task-oriented dialogue system. The third paragraph in the introduction should include one use case about non-task-oriented dialogue system, such as chatbots.
ii) I do not think DSTC2 is good dataset here in the experiments. Maybe the dataset is too simple with limited options or the training/testing are very similar to each other, even the random could achieve very good performance in table 1 and 2. For example, the random solution is only 0.005 (out of 1) worse then d-scLSTM, and it also has a close performance compared with other metrics. Even the random could achieve 0.8 (out of 1) in BLEU, this is a very high performance.
iii) About the scatter plot Figure 3, the authors should include more points with a bad metric score (similar to Figure 1 in Liu 2016). 
iv) About the correlations in figure b, especially for BLEU and METEOR, I do not think they have good correlations with human's judgments. 
v) BLEU usually correlates with human better when 4 or more references are provided. I suggest the authors include some dataset with 4 or more references instead of just 2 references.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Latent Topic Conversational Models,"['Tsung-Hsien Wen', 'Minh-Thang Luong']",Reject,2018,"[9, 9]","[13, 14]","[66, 53]","[34, 25]","[26, 26]","[6, 2]","This paper proposed the combination of topic model and seq2seq conversational model.
The idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.
However, I'm not sure that the empirical evaluation shows the really impressive results.
In particular, the difference between LV-S2S and LTCM seem to be trivial.
There are many configurations in the LSTM-based model.
Can you say that there is no configuration of LV-S2S that outperforms your model?
Moreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.

","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Understanding GANs: the LQG Setting,"['Soheil Feizi', 'Changho Suh', 'Fei Xia', 'David Tse']",Reject,2018,"[11, 17, 27, 26]","[16, 22, 32, 31]","[155, 140, 355, 333]","[61, 71, 222, 112]","[83, 41, 50, 123]","[11, 28, 83, 98]","*Paper summary*

The paper considers GANs from a theoretical point of view. The authors approach GANs from the 2-Wasserstein point of view and provide several insights for a very specific setting. In my point of view, the main novel contribution of the paper is to notice the following fact:

(*) It is well known that the 2-Wasserstein distance W2(PY,QY) between multivariate Gaussian PY and its empirical version QY scales as $n^{-2/d}$, i.e. converges very slow as the dimensionality of the space $d$ increases. In other words, QY is not such a good way to estimate PY in this setting. A somewhat better way is use a Gaussian distribution PZ with covariance matrix S computed as a sample covariance of QY. In this case W2(PY, PZ) scales as $\sqrt{d/n}$.

The paper introduces this observation in a very strange way within the context of GANs. Moreover, I think the final conclusion of the paper (Eq. 19) has a mistake, which makes it hard to see why (*) has any relation to GANs at all.

There are several other results presented in the paper regarding relation between PCA and the 2-Wasserstein minimization for Gaussian distributions (Lemma 1 & Theorem 1). This is indeed an interesting point, however the proof is almost trivial and I am not sure if this provides any significant contribution for the future research.

Overall, I think the paper contains several novel ideas, but its structure requires a *significant* rework and in the current form it is not ready for being published. 

*Detailed comments*

In the first part of the paper (Section 2) the authors propose to use the optimal transport distance Wc(PY, g(PX)) between the data distribution PY (or its empirical version QY) and the model as the objective for GAN optimization. This idea is not novel: WGAN [1] proposed (and successfully implemented) to minimize the particular case of W1 distance by going through the dual form, [2] proposed to approach any Wc using auto-encoder reformulation of the primal (and also shoed that [5] is doing exactly W2 minimization), and [3] proposed the same using Sinkhorn algorithm. So this point does not seem to be novel.

The rest of the paper only considers 2-Wasserstein distance with Gaussian PY and Gaussian g(PX) (which I will abbreviate with R), which looks like an extremely limited scenario (and certainly has almost no connection to the applications of GANs).

Section 3 first establishes a relation between PCA and minimizing 2-Wasserstein distance for Gaussian distributions (Lemma 1, Theorem 1). Then the authors show that if R minimizes W2(PY, R) and QR minimizes W2(QY, QR) then the excess loss W2(PY, QR) - W2(PY, R) approaches zero at the rate $n^{-2/d}$ (both for linear and unconstrained generators). This result basically provides an upper bound showing that GANs need exponentially many samples to minimize W2 distance. I don't find these results novel, as they already appeared in [4] with a matching lower bound for the case of Gaussians (Theorem B.1 in Appendix can be modified easily to show this). As the authors note in the conclusion of Section 3, these results have little to do with GANs, as GANs are known to learn quite quickly (which contradicts the theory of Section 3).

Finally, in Section 4 the authors approach the same W2 problem from its dual form and notice that for the LQG model the optimal discriminator is quadratic. Based on this they reformulate the W2 minimization for LQG as the constrained optimization with respect to p.d. matrix A (Eq 16). The same conclusion does not work unfortunately for W2(QY, R), which is the real training objective of GANs. Theorem 3 shows that nevertheless, if we still constrain discriminator in the dual form of W2(QY, R) to be quadratic, the resulting soliton QR* performs the empirical PCA of Pn. 

This leads to the final conclusion of the paper, which I think contains a mistake. In Eq 19 the first equation, according to the definitions of the authors, reads
\[
W2(PY, QR) = W2(PY, PZ),   (**)
\]
where QR is trained to minimize min_R W2(QY, R) and PZ is as defined in (*) in the beginning of these notes. 
However, PZ is not the solution of min_R W2(QY, R) as the authors notice in the 2nd paragraph of page 8. Thus (**) is not true (at least, it is not proved in the current version of the text). PZ is a solution of min_R W2(QY, R) *where the discriminator is constrained to be quadratic*. This mismatch is especially strange, given the authors emphasize in the introduction that they provide bounds on divergences which are the same as used during the training (see 2nd paragraph on page 2) --- here the bound is on W2, but the empirical GAN actually does a regularized training (with constrained discriminator).

Finally, I don't think the experiments provide any convincing insights, because the authors use W1-minimization to illustrate properties of the W2. Essentially the authors say ""we don't have a way to perform W2 minimization, so we rather do the W1 minimization and assume that these two are kind of similar"".

* Other comments *
(1) Discussion in Section 2.1 seems to never play a role in the paper.
(2) Page 4: in p-Wasserstein distance, ||.|| does not need to be a Euclidean metric. It can be any metric.
(3) Lemma 2 seems to repeat the result from (Canas and Rosasco, 2012) as later cited by authors on page 7?
(4) It is not obvious how does Theorem 2 translate to the excess loss? 
(5) Section 4. I am wondering how exactly the authors are going to compute the conjugate of the discriminator, given the discriminator most likely is a deep neural network?


[1] Arjovsky et al., Wasserstein GAN, 2017
[2] Bousquet et al, From optimal transport to generative modeling: the VEGAN cookbook, 2017
[3] Genevay et al., Learning Generative Models with Sinkhorn Divergences, 2017
[4] Arora et al, Generalization and equilibrium in GANs, 2017
[5] Makhazani et al., Adversarial Autoencoders, 2015","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Interpretable and Pedagogical Examples,"['Smitha Milli', 'Pieter Abbeel', 'Igor Mordatch']",Reject,2018,"[3, 17, 12]","[8, 22, 17]","[26, 608, 109]","[13, 291, 48]","[13, 291, 56]","[0, 26, 5]","This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy. The teacher's strategy should also be  learned from data.  In this case the authors look at finding interpretable teaching strategies.  The authors define the ""good"" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans.  
The suggested method follow an iterative process in which the student and teacher are interchangeably used. At each iteration the teacher generates  examples based on the students current concept. 

I found it very difficult to follow the claims in the paper. Why is it assumed that human intuition is necessarily good?  The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition. There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way. Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned.
How would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing.","[4, 8, 8]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
A Boo(n) for Evaluating Architecture Performance,"['Ondrej Bajgar', 'Rudolf Kadlec', 'and Jan Kleindienst']",Reject,2018,"[3, 12, 23]","[8, 15, 23]","[12, 29, 48]","[4, 19, 38]","[7, 9, 7]","[1, 1, 3]","This manuscript raises an important issue regarding the current lack of standardization regarding methods for evaluating and reporting algorithm performance in deep learning research.  While I believe that raising this issue is important and that the method proposed is a step in the right direction, I have a number of concerns which I will list below.  One risk is that if the proposed solution is not adequate or widely agreeable then we may find a proliferation of solutions from which different groups might pick and choose as it suits their results!

The method of choosing the best model under 'internal' cross-validation to take through to 'external' cross-validation against a second hold-out set should be regarded as one possible stochastic solution to the optimisation problem of hyper-parameter selection.  The authors are right to emphasize that this should be considered part of the cost of the technique, but I would not suggest that one specify a 'benchmark' number of trials (n=5) for comparison.  Rather I would suggest that this is a decision that needs to be explored and understood by the researchers presenting the method in order to understand the cost/benefit ratio for their algorithm provided by attempting to refine their guess of the optimal hyperparameters.  This would then allow for other methods not based on internal cross-validation to be compared on a level footing.

I think that the fundamental issue of stochasticity of concern for repeatability and generalisability of these performance evaluation exercises is not in the stochastic optimisation search but in the use of a single hold-out sample.  Would it not be wise to insist on a mean performance (a mean Boo_n or other) over multiple random partitions of the entire dataset into training and hold-out?  I wonder if in theory both the effect of increasing n and the mean hold-out performance could be learnt efficiently with a clever experimental design. 

Finally, I am concerned with the issue of how to compute the suggested Boo_n score.  Use of a parameteric Gaussian approximation is a strong assumption, while bootstrap methods for order statistics can be rather noisy.  It would be interesting to see a comparison of the results from the parametric and non-parameteric Boo_n versions applied to the test problems.  ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Recurrent Relational Networks for complex relational reasoning,"['Rasmus Berg Palm', 'Ulrich Paquet', 'Ole Winther']",Reject,2018,"[2, 16, 26]","[6, 20, 31]","[27, 40, 140]","[12, 23, 53]","[14, 12, 46]","[1, 5, 41]","This paper introduces recurrent relational networks: a deep neural network for structured prediction (or relational reasoning). The authors use it to achieve state-of-the-art performance on Soduku puzzles and the BaBi task (a text based QA dataset designed as a set of to toy prerequisite tasks for reasoning).

Overall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn't show it has a big impact (could do so by adding more tasks - as they suggest in the discussion). This is why I think the paper is marginally below the acceptance threshold but could be convinced otherwise.

C an the authors give experimental evidences for their claim: ""As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy..."" - 

Pros
- The idea of the paper is clearly presented, the algorithm is easy to follow.
- The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.

Cons
- The recurrent relational networks is basically a complex learned message passing algorithm. As the authors themselves state there are several works from recent years which also tackle this (one missing reference is Deeply Learning the Messages in Message Passing Inference of Lin et al from NIPS 2016). It would been interesting to compare results to these algorithms.
- For the Sudoku the proposed architecture of the network seems a bit to complex, for example why do a 16 embedding is needed for representing a digit between 0-9? Some other choices (batch size of 252) seem very specific.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Balanced and Deterministic Weight-sharing Helps Network Performance,"['Oscar Chang', 'Hod Lipson']",Reject,2018,"[38, 23]","[43, 28]","[23, 197]","[13, 110]","[6, 36]","[4, 51]","The manuscript advocates to study the weight sharing in a more systematic way by proposing ArbNets which defines the weight sharing function as a hash function. In this framework, any existing neural network architectures, including CNN and RNN, could be incorporated into ArbNets.

The manuscript is not well written. There are multiple grammar errors and typos. Content-wise, it is already well known that CNN and RNN can be expressed as general MLP with weight sharing. The introduction of ArbNets does not bring much value or insight to this area. So it seems that most content before experimental section is common sense.

In the experimental section, it is interesting to see how different hash function with different level of entropy can affect the performance of neural nets. However, this single observation cannot enrich the whole manuscript. Two questions:
(1) What is the definition of sparsity here, and how is it controlled?
(2) There seems to be a step change in Figure 3. All the results are either between 10 to 20, or near 50. And the blue line goes up and down. Is this expected?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach,"['Pierre Courtiol', 'Eric W. Tramel', 'Marc Sanselme', 'Gilles Wainrib']",Reject,2018,"[1, 9, 1, 9]","[3, 14, 1, 9]","[2, 40, 2, 17]","[0, 15, 1, 3]","[2, 20, 1, 6]","[0, 5, 0, 8]","The authors approach the task of labeling histology images with just a single global label, with promising results on two different data sets. This is of high relevance given the difficulty in obtaining expert annotated data. At the same time the key elements of the presented approach remain identical to those in a previous study, the main novelty is to replace the final step of the previous architecture (that averages across a vector) with a multiplayer perceptron.  As such I feel that this would be interesting to present if there is interest in the overall application (and results of the 2016 CVPR paper), but not necessarily as a novel contribution to MIL and histology image classification.

Comments to the authors:

* The intro starts from a very high clinical level. A introduction that points out specifics of the technical aspects of this application, the remaining technical challenges, and the contribution of this work might be appreciated by some of your readers.
* There is preprocessing that includes feature extraction, and part of the algorithm that includes the same feature extraction. This is somewhat confusing to me and maybe you want to review the structure of the sections.  You are telling us you are using the first layer (P=1) of the ResNet50 in the method description, and you mention that you are using the pre-final layer in the preprocessing section. I assume you are using the latter, or is P=1 identical to the prefinal layer in your notation?  Tell us. Moreover, not having read Durand 2016, I would appreciate a few more technical details or formal description here and there.  Can you detail about the ranking method in Durand 2016, for example?
* Would it make sense to discuss Durand 2016 in the base line methods section? 
* To some degree this paper evaluates WELDON (Durand 2016) on new data, and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step. Results in table 1 suggest that this leads to some 2-5% performance increase which is a nice result.  I would assume that experimental conditions (training data, preprocessing, optimization, size of ensemble) are kept constant in between those two comparisons? Or is there anything of relevance that also changed (like size of the ensemble, size of training data) because the WELDON results are essentially previously generated results? Please comment in case there are differences. ","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks,"['Alexander L. Gaunt', 'Matthew A. Johnson', 'Alan Lawrence', 'Maik Riechert', 'Daniel Tarlow', 'Ryota Tomioka', 'Dimitrios Vytiniotis', 'Sam Webster']",Reject,2018,"[3, 30, 3, 13, 13, 14, 2]","[7, 35, 6, 18, 18, 18, 6]","[26, 31, 8, 83, 76, 64, 4]","[13, 16, 3, 46, 38, 40, 2]","[13, 4, 2, 35, 24, 12, 2]","[0, 11, 3, 2, 14, 12, 0]","This paper presents AMPNet, that addresses parallel training for dynamic networks. This is accomplished by building a static graph like IR that can serve as a target for compilation for high-level libraries such as tensor flow. In the IR each node of the computation graph is a parallel worker, and synchronization occurs when a sufficient number of gradients have been accumulated. The IR uses constructs such as concat, split, broadcast,.. allowing dynamic, instance dependent control flow decisions. The primary improvement in training performance is from reducing synchronization costs.

Comments for the author:

The paper proposes a solution to an important problem of model parallel training especially over dynamic batching that is increasingly important as we see more complex models where batching is not straightforward. The proposed solution can be effective. However, this is not really evident from the evaluation. Furthermore, the paper can be a little dense read for the ICLR audience. I have the following additional concerns:

1) The paper stresses new hardware throughout the paper. The paper also alludes to “simulator"" of a 1 TFLOPs FPGA in the conclusion. However, your entire evaluation is over CPU. The said simulator is a bunch of sleep() calls (unless some details are skipped). I would encourage the authors to remove these references since these new devices have very different hardware behavior. For example, on a real constrained device, you may not enjoy a large L2 cache which you are benefitting from by doing an entire evaluation over CPUs. Likewise, the vector instruction processing behavior is also very different since these devices have limited power budgets and may not be able to support AVX style instructions. Unless an actual simulator like GEM5 is used, a correct representation of what hardware environment is being used is necessary before making claims that this is ideal for emerging hardware.

2) To continue on the hardware front and the evaluation, I feel for this paper to be accepted or appreciated, a simulated hardware is not necessary. Personally, I found the evaluation with simulated sleep functions more confusing than helpful. An appropriate evaluation for this paper can be just benefits over CPU or GPUs, For example, you have a 7 TFLOPS device (e.g. a GPU or a CPU). Existing algorithms extract X TFLOPs of processing power and using your IR/system one gets Y effective TFLOPs and Y>X. This is all that is required. Currently, looking at your evaluation riddled with hypothetical hardware, it is unclear to me if this is helpful for existing hardware. For example, in Table 1, are Tensorflow numbers only provided over the 1 TFLOPs device (they correspond to the 1 TFLOPs column for all workloads except for MNIST)?  Do you use the parallelism at all in your Tensorflow baseline?  Please clarify.

3) How do you compare for dynamic batching with dynamic IR platforms like pytorch? Furthermore, more details about how dynamic batching is happening in benchmarks mentioned in Table 1 will be nice to have. Finally, an emphasis on the novel contributions of the paper will also be appreciated.

4) Finally, the evaluation appears to be sensitive to the two hyper-parameters introduced. Are they dataset specific? I feel tuning them would be rather cumbersome for every model given how sensitive they are (Figure 5).
","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Avoiding degradation in deep feed-forward networks by phasing out skip-connections,"['Ricardo Pio Monti', 'Sina Tootoonian', 'Robin Cao']",Reject,2018,"[5, 13, 1]","[10, 17, 1]","[27, 4, 1]","[11, 3, 1]","[10, 0, 0]","[6, 1, 0]","UPDATED COMMENT
I've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments. I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice).

ORIGINAL REVIEW BELOW

The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.

It’s a nice, simple idea. However, I don’t think it’s sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren’t that good. 

Comments:

1. Why force the Lag. multipliers to 1 at the end of training? It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I’d be curious as to why.

2.Similarly, it’s not obvious that initializing the multipliers at 0.5 is the best choice. The “looks linear” initialization proposed in “The shattered gradients problem” (Balduzzi et al) implies that alpha=0 may work better. Did the authors try any values besides 0.5? 

3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it’s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it? 

","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neighbor-encoder,"['Chin-Chia Michael Yeh', 'Yan Zhu', 'Evangelos E. Papalexakis', 'Abdullah Mueen', 'Eamonn Keogh']",Reject,2018,"[0, 25, 4, 16]","[5, 30, 9, 21]","[19, 45, 27, 39]","[11, 35, 20, 18]","[6, 6, 6, 6]","[2, 4, 1, 15]","This paper presents a variant of auto-encoder that relaxes the decoder targets to be neighbors of a data point. Different from original auto-encoder, where data point x and the decoder output \hat{x} are forced to be close, the neighbor-encoder encourage the decoder output to be similar to the neighbors of the input data point. By considering the neighbor information, the decoder targets would have smaller intra-class distances, thus larger inter-class distances, which helps to learn better separated latent representation of data in terms of data clusters. The authors conduct experiments on several real but relative small-scale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets. 

The method of neighbor prediction is a simple and small modification of the original auto-encoder, but seems to provide a way to augment the targets such that intra-class distance of decoder targets can be tightened. Improvements in the conducted experiments seem significant compared to the most basic auto-encoder.

Major issues:

There are some unaddressed theoretical questions. The optimal solution to predict the set of neighbor points in mean-squared metric is to predict the average of those points, which is not well justified as the averaged image can easily fall off the data manifold. This may lead to a more blurry reconstruction when k increases, despite the intra-class targets are tight. It can also in turn harm the latent representation when euclidean neighbors are not actually similar (e.g. images in cifar10/imagenet that are not as simple as 10 digits). This seems to be a defect of the neighbor-encoder method and is not discussed in the paper.

The data sets used in the experiments  are relatively small and simple, larger-scale experiments should be conducted. The fluctuations in Figure 9 and 10 suggest the significant variances in the results. Also, more complicated data/images can decrease the actual similarities of euclidean neighbors, thus affecting the results.

The baselines are weak. Only the most basic auto-encoder is compared, no additional variants or other data augmentation techniques are compared. It is possible other variants improve the basic auto-encoder in similar ways. 

Some results are not very well explained. It seems the performance increases monotonically as the number of neighbors increases (Figure 5, 9, 10). Will this continue or when will the performance decrease? I would expect it to decrease as the far away neighbors will be dissimilar. The authors can either attach the nearest neighbors figures or their statistics, and provide explanations on when and why the performance decrease is expected.

Some notations are confusing and need to be improved. For example, X and Y are actually the same set of images, the separation is a bit confusing; y_i \in y in last paragraph of page 4 is incorrect, should use something like y_i in N(y).","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Synthesizing Robust Adversarial Examples,"['Anish Athalye', 'Logan Engstrom', 'Andrew Ilyas', 'Kevin Kwok']",Reject,2018,"[2, 2, 5, 3]","[6, 7, 10, 3]","[16, 37, 48, 3]","[6, 16, 21, 1]","[8, 21, 27, 2]","[2, 0, 0, 0]","Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network.
Rather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.
 

Main comments:
- The idea of building 3D adversarial objects is novel so the study is interesting. However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date. 
See for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.
- The presentation of the results is not very clear. See specific comments below.
- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.


Minor comments:
Fig1 : a bug with color seems to have been fixed
Model section: be consistent with the notations. Bold everywhere or nowhere
Results: The tables are difficult to read and should be clarified:
What does the l2 metric stands for ? 
How about min, max ?
Accuracy -> classification accuracy
Models -> 3D models
Describe each metric (Adversarial, Miss-classified, Correct)
","[5, 8, 6]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On Characterizing the Capacity of Neural Networks Using Algebraic Topology,"['William H. Guss', 'Ruslan Salakhutdinov']",Reject,2018,"[3, 16]","[6, 21]","[17, 419]","[4, 207]","[13, 201]","[0, 11]","Paper Summary:

This paper looks at empirically measuring neural network architecture expressivity by examining performance on a variety of complex datasets, measuring dataset complexity with algebraic topology. The paper first introduces the notion of topological equivalence for datasets -- a desirable measure to use as it is invariant to superficial differences such as rotation, translation and curvature. The definition of homology from algebraic topology can then be used as a robust measure of the ""complexity"" of a dataset. This notion of difficulty focuses roughly on determining the number of holes of dimension n (for varying n) there are in the dataset, with more holes roughly leading to a more complex connectivity pattern to learn. They provide a demonstration of this on two synthetic toy datasets in Figure 1, training two (very small -- 12 and 26 neuron) single hidden layer networks on these two datasets, where the smaller of the two networks is unable to learn the data distribution of the second dataset. These synthetic datasets have a well defined data distribution, and for an empirical sample of N points, a (standard) method of determining connectivity by growing epsilon balls around each datapoint in section 2.3.

The authors give a theoretical result on the importance of homology: if a binary classifier has support homology not equal to the homology of the underlying dataset, then there is at least one point that is misclassified by the classifier. Experiments are then performed with single hidden layer networks on synthetic datasets, and a phase transition is observed: if h_phase is the number of hidden units where the phase transition happens, and h' < h < h_phase, h' has higher error and takes longer to converge than h. Finally, the authors touch on computing homology of real datasets, albeit with a low dimensional projection (e.g. down to 3 dimensions for CIFAR-10).

Main Comments

The motivation to consider algebraic topology and dataset difficulty is interesting, but I think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings. In particular, the majority of experiments and justification of this method comes from use on a low dimensional manifold with either known data distribution, or with a densely sampled manifold. (The authors look at using CIFAR-10, but project this down to 3 dimensions -- as current methods for persistent homology cannot scale -- which somewhat invalidates the goal of testing this out on real data.) This is an important and serious drawback because it seems unlikely that the method described in Figure 3 of determining the connectivity patterns of a dataset are likely to yield insightful results in a high dimensional space with very few datapoints (in comparison to 2^{dimension}), where distance between datapoints is unlikely to have any nice class related correspondence.

Furthermore, while part of the motivation of this paper is to use dataset complexity measured with topology to help select architectures, experiments demonstrating that this might be useful are very rudimentary. All experiments only look at single hidden layers, and the toy task in Figure 1 and in section 3.2.1 and Figure 5 use extremely small networks (hidden size 12-26). It's hard to be convinced that these results necessarily generalize even to other larger hidden layer models. On real datasets, exploring architectures does not seem to be done at all (Section 4).


Minor Comments
Some kind of typo in Thm 1? (for all f repeated twice)
Small typos (missing spaces) in related work and conclusion
How is h_phase determined? Empirically? (Or is there a construction?)

Review Summary:

This paper is not ready to be accepted.","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
STRUCTURED ALIGNMENT NETWORKS,"['Yang Liu', 'Matt Gardner']",Reject,2018,"[4, 10, 20]","[9, 15, 25]","[67, 131, 303]","[29, 68, 158]","[35, 58, 93]","[3, 5, 52]","This paper proposes a model of ""structured alignments"" between sentences as a means of comparing two sentences by matching their latent structures. Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention.

In section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables. but where are the scores for each span? I think it should be c_{ijk} * \delta_{ijk} under the summations instead.

In the same section, the expression for \alpha_{ij} seems to assume that \delta_{ijk} = \dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions). This seems rather limiting, can you comment on that?

In the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing.

The plots in Fig 2 with the marginals on CKY charts are not very enlightening. How do this marginals help solving the NLI task?

Minor comments:
- Sec. 3: ""Language is inherently tree structured"" -- this is debatable...
- page 8: (laf, 2008): bad formatted reference","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Stabilizing GAN Training with Multiple Random Projections,"['Behnam Neyshabur', 'Srinadh Bhojanapalli', 'Ayan Chakrabarti']",Reject,2018,"[6, 6, 12]","[10, 11, 17]","[88, 61, 76]","[39, 24, 35]","[46, 35, 33]","[3, 2, 8]","
- Paper summary

The paper proposes a GAN training method for improving the training stability. The key idea is to let a GAN generator competes with multiple GAN discriminators where each discriminator takes a random low-dimensional projection of an input image for differentiate whether the input image is a real or generated one. Visual generation results from the proposed method with comparison to those generated by the DCGAN were used as the main experimental validation for the merit of the proposed method. Due to poor experimental validation and inconclusive results, the reviewer does not recommend the acceptance of the paper.

- Inconclusive results

The paper fails to compare the proposed method with the GMAN framework [a], which was the first work proposing utilizing multiple discriminators for more stable GAN training. Without comparing to the GMAN work, we do not know whether the benefit is from using multiple discriminators proposed in the GMAN work or from using the random low dimensional projections proposed in this paper. If it is former, then the proposed method has no merits at all.

In addition, the generator loss curve shown in Figure 2 is not making much sense. The generator loss curve will be meaningful if each discriminator update is optimal. However, this is not the case in the proposed method. There is little to conclude from Figure 2.

[a] Durugkar et al. ""Generative multi-adversarial networks."" ICLR 2017

- Poor experimental validation

The paper fails to utilize more established performance metrics such as the inception loss or human evaluation score to evaluate its benefit. It does not compare to other approaches for stabilizing GAN training such as WGAN or LSGAN. The main results shown in the paper are generating 64x64 human face images, which is not impressive.","[3, 8, 5]","[' Clear rejection', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Long-term Forecasting using Tensor-Train RNNs,"['Rose Yu', 'Stephan Zheng', 'Anima Anandkumar', 'Yisong Yue']",Reject,2018,"[8, 3, 13, 12]","[13, 8, 18, 17]","[88, 39, 419, 230]","[35, 10, 154, 101]","[50, 27, 223, 110]","[3, 2, 42, 19]","The paper proposes Tensor-Train RNN and Tensor-Train LSTM (TT-RNN/TLSTM), a RNN/LSTM architecture whose hidden unit at time t h_t is computed from the tensor-vector product between a tensor of weights and a concatenation of hidden units from the previous L time steps. The motivation is to incorporate previous hidden states and high-order correlations among them to better predict long-term temporal dependencies for seq2seq problems. To address the issue of the number of parameters growing exponentially in the rank of the tensor, the model uses a low rank decomposition called the ‘tensor-train decomposition’ to make the number of parameters linear in the rank. Some theoretical analysis on the number of hidden units required for a given estimation error, and experimental results have been provided for synthetic and real sequential data.

First of all, the presentation of the method in section 2.1 is confusing and there seem to be various ambiguities in the notation that harms understanding of the method. The tensor-vector product in equation (6) appears problematic. The notation that I think is standard is as follows: given a tensor W \in R^{n_1 \times … \times n_P} and vectors v_p \in R^{n_p}, the tensor-vector product W \times_{p=1}^P v_p = vec(W) \otimes_{p=1}^P v_p = \sum{i_1,...,i_P} \prod_{p=1}^P v_{p,i_p}. So I’m guessing you want to get rid of the \otimes signs (the kronecker products) in (6) or you want to remove the summation and write W \times_{p=1}^P s_{t-1}. Also \alpha that appears in (6) is never defined. Is it another index? This is confusing because you say W is P-dimensional but have P+1 indices for it including alpha (W_{\alpha i_1 … i_p}). Moreover the dimensionality of W^{hx} x_t in (6) is R^H judging from the notation in page 2, but isn’t the tensor-vector product a scalar? Also am I correct in thinking that s_{t-1} should be [1, h_{t-1}^T, …, h_{t-L}^T], i.e. a vector of length LH+1 rather than a matrix? The notation from page 2 implies that you are using column vectors, so the definition of s_{t-1} makes it appear as an (L+1) by H matrix, which could make the reader interpret s_{t-1;i_1} in (6) as vectors instead of scalars (this is reinforced by the kronecker product between these s_{t-1;i_p}). I had to work this out from the number of parameters (HL+1)^P in section 2.2. The diagram of s_{t-1} in Figure 3 is also confusing, because it isn’t obvious that the unlabelled grey bars are copies of s_{t-1}. Also I notice that the name ‘Tensor Train RNN/LSTM’ has been used in Yang et al, 2017. You probably want to avoid using the same name since the models are different. It would be nice if you could explain in a bit more detail about how they are different in the related work section.

Assuming I have understood the method correctly, the idea of using tensor products to incorporate higher order interactions between the hidden states at different times appears sensible. From the theoretical analysis, you claim that 1) smoother f is easier to approximate, and 2) polynomial interactions are more efficient than linear ones. The first point seems fairly self-explanatory and doesn’t seem to require a proof. The second point isn’t so convincing because you have two additive terms on the right hand side of the inequality in Theorem 3.1 (btw I’m guessing you want the inequality to be the other way round): the first term is independent of p, and the second decreases exponentially with p. Your second point would only hold if this first term is reasonably small, but this doesn’t seem obvious to me.

Regarding the experiments, I’m sceptical as to whether a grid search over hyperparameters for TLSTM vs grid search over the same hyperparameters for (M)LSTM provides a fair comparison. You probably want to compare the models given the same number of parameters, since given the same state size, TLSTM will have many more parameters than (M)LSTM. A plot of x-axis: # parameters, y-axis: average RMSE at convergence would be informative. Moreover for figure 8, you probably want to control the time taken for training instead of just comparing validation loss at the same number of steps. I imagine the best performing TLSTM model will have many more parameters and hence take much longer to train than the best performing LSTM model. 
Moreover, it seems as though the increased prediction accuracy from LSTM is marginal considering you have 3 more hyperparameters to tune (L,S,P - what was the value of P used for the experiments?) and that tuning them is important to prevent overfitting.

I’m also curious as to how TLSTM compares to hierarchical RNN approaches for modelling long-term dependencies. It will be interesting to compare against models like Stacked LSTM (Graves, 2013), Grid LSTM (Kalchbrenner, 2015) and HM LSTM (Chung, 2017). These models have mostly been evaluated on text, but I don’t see any reason they can’t be extended to sequential forecasting on time series data. Also regularisation techniques such as batch-norm for LSTMs (Cooijmans et al, 2016) and layer-norm (Ba et al, 2016) seem to help a lot for increasing prediction accuracy. Did you investigate these techniques to control overfitting?

Other minor comments on presentation:
For figure 6, the legends are inconsistent with the caption. Also you might want to overlay predictions on top of the ground truth for better comparison and also to save space.

Overall, I think there are vast scopes for improvement in presentation and comparisons with other methods, and hence find the paper not yet ready for publication.
","[4, 5, 6]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS,"['Huishuai Zhang', 'Caiming Xiong', 'James Bradbury', 'Richard Socher']",Reject,2018,"[5, 10, 4, 12]","[10, 15, 6, 17]","[68, 383, 18, 229]","[26, 165, 9, 111]","[35, 208, 9, 111]","[7, 10, 0, 7]","In this paper, authors discuss the use of block-diagonal hessian when computing the updates. The block-diagonal hessian makes it easier to solve the ""newton"" directions, as the CG can be run only on smaller blocks (and hence less CG iterations are needed).

The paper is nicely written and all was clear to me. In general, I agree that having larger batch-size is the way to go, for very large datasets and a pure SGD type of methods are having problems to efficiently utilize large clusters.

The only negative thing I find in the paper is the lack of more numerical results. Indeed, the paper is clearly not a theoretical paper, is proposing a new algorithm, hence there should be evidence that it works. For example, I would like to see how the choice of hyper-parameters influences the speed of the algorithm. Was ""CG"" cost included in the ""x""-axis? i.e. if we put ""passes"" over the data as x-axis, then 1 update \approx 30 CG + some more == 32 batch evaluation.
So please try to make the ""x""-axis more fair.

 ","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Semi-Supervised Learning via New Deep Network Inversion,"['Balestriero R.', 'Roger V.', 'Glotin H.', 'Baraniuk R.']",Reject,2018,"[4, 4, 21, 30]","[9, 8, 26, 35]","[87, 6, 148, 495]","[23, 1, 115, 230]","[59, 2, 17, 149]","[5, 3, 16, 116]",This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks. The numerical experiments show a significant improvement in accuracy of the approach.,"[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Phase Conductor on Multi-layered Attentions for Machine Comprehension,"['Rui Liu', 'Wei Wei', 'Weiguang Mao', 'Maria Chikina']",Reject,2018,"[19, 26, 6, 2]","[24, 31, 10, 6]","[365, 604, 6, 6]","[190, 311, 0, 0]","[43, 48, 3, 1]","[132, 245, 3, 5]","Summary: The paper introduces ""Phase Conductor"", which consists of two phases, context-question attention phase and context-context (self) attention phase. Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector. The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention.


Note: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard.


Pros:
- The model achieves SOTA on SQuAD among published papers.
- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel. Visual analysis of the attention map is convincing.
- The paper is overall well-written and clear.

Cons:
- Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers. For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector).
- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt. For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer. While this may be statistically significant, it is a very small gain nonetheless.
- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art. I think this is a valuable engineering contribution, but I feel that it is not well-suited / sufficient for ICLR audience. 


Questions:
- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor?


Errors:
- page 2 last para: ""gives an concrete"" -> ""gives a concrete""
- page 2 last para: ""matching"" -> ""matched""
Figure 1: I think ""passage embedding h"" and ""question embedding v"" boxes should be switched.
- page 7 3.3 first para: ""evidence fully"" -> ""evidence to be fully""


[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015.","[5, 8, 5]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,"['William M. Severa', 'Jerilyn A. Timlin', 'Suraj Kholwadwala', 'Conrad D. James', 'James B. Aimone']",Reject,2018,"[9, 15, 2, 19, 12]","[14, 14, 1, 21, 17]","[36, 3, 1, 33, 65]","[20, 2, 0, 16, 35]","[12, 1, 1, 9, 19]","[4, 0, 0, 8, 11]","Authors propose a greedy scheme to select a subset of (highly correlated) spectral features in a classification task. The selection criterion used is the average magnitude with which this feature contributes to the activation of a next-layer perceptron. Once validation accuracy drops too much, the pruned network is retrained, etc. 

Pro: 
- Method works well on a single data set and solves the problem
- Paper is clearly written 
- Good use of standard tricks 

Con: 
- Little novelty

This paper could be a good fit for an applied conference such as the International Symposium on Biomedical Imaging. 
","[3, 6, 4]","[' Clear rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning,"['Kunkun Pang', 'Mingzhi Dong', 'Timothy Hospedales']",Reject,2018,"[2, 7, 13, 12]","[6, 12, 18, 17]","[5, 36, 96, 305]","[3, 14, 46, 139]","[2, 12, 23, 131]","[0, 10, 27, 35]","The approach solves an important problem as getting labelled data is hard. The focus is on the key aspect, which is generalisation across heteregeneous data. The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.

Pros: 
1. The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on. 
2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets, which isnt very common in most meta-learning papers today, so it’s encouraging that the method works in that regime.

Cons: 
1. The embedding strategy, especially the representative and discriminative histograms, is complicated. It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images. More evidence in the paper for why it would work on harder problems would be great. 
2. The policy network would have to output a probability for each datapoint in the dataset U, which could be fairly large, thus the method is computationally much more expensive than random sampling. A section devoted to showing what practical problems could be potentially solved by this method would be useful.
3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM, or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner.

Significance/Conclusion: The idea of meta-learning or learning to learn is fairly common now. While they do show good performance, it’s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks. 

Comments: There’s lots of typos, please proof read to improve the paper.

Revision: I thank the authors for the updates and addressing some of my concerns. I agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks. I update my review to 6. ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to Compute Word Embeddings On the Fly,"['Dzmitry Bahdanau', 'Tom Bosc', 'Stanisław Jastrzębski', 'Edward Grefenstette', 'Pascal Vincent', 'Yoshua Bengio']",Reject,2018,"[5, 3, 6, 8, 29, 31]","[10, 7, 10, 13, 34, 36]","[55, 9, 53, 104, 128, 975]","[22, 4, 16, 44, 63, 405]","[33, 4, 29, 54, 52, 454]","[0, 1, 8, 6, 13, 116]","This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions. The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings. The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings. The paper is clearly written, and has useful ablation experiments. However, I have a couple of questions/concerns:
- Most of the gains seem to come from using the spelling of the word. As the authors note, this kind of character level modelling has been used in many previous works. 
- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss, but I don’t know the area well enough to make specific suggestions 
- I’m a little skeptical about how often this method would really be useful in practice. It seems to assume that you don’t have much unlabelled text (or you’d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well. All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.
- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81. It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?
","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Efficient Exploration through Bayesian   Deep Q-Networks,"['Kamyar Azizzadenesheli', 'Emma Brunskill', 'Animashree Anandkumar']",Reject,2018,"[3, 18, 13]","[8, 23, 18]","[90, 194, 419]","[26, 120, 154]","[56, 67, 223]","[8, 7, 42]","(Last minute reviewer brought in as a replacement).

This paper proposed ""Bayesian Deep Q-Network"" as an approach for exploration via Thompson sampling in deep RL.
This algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.
The agent then samples from this posterior for an approximate Thompson sampling.
Experimental results show that this outperforms an epsilon-greedy baseline.

There are several things to like about this paper:
- The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways.
- The paper is mostly clear and well written.
- The experimental results are impressive in their outperformance.

However, there are also some issues, many of which have already been raised:
- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example).
- There are some loose and misleading descriptions of the algorithm computing ""the posterior"" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away.
- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.
- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.

Overall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network.
However, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach.
For example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example).

In it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.


===============================================================

Revising my review following the rebuttal period and also the (ongoing) revisions to the paper.

I've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest. Given the ongoing review decisions/issues I'm putting my review slightly below accept.

## Relation to literature on ""randomized value functions""
It's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm). This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm! If you say something like ""they did not estimate a true posterior"" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior. In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN.

## Quality/science of experiments
The experimental results have been updated, and the performance of the baseline now seems much more reasonable. However, the procedure for ""selecting arbitrary number of frames"" to report performance seems really unnecessary... it would be clear that BDQN is outperforming DDQN... you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific. This type of stuff smells like overfitting!","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning,"['Xiangyu Kong', 'Fangchen Liu', 'Bo Xin', 'Yizhou Wang']",Reject,2018,"[13, 7, 2, 19]","[18, 10, 7, 24]","[77, 30, 23, 233]","[21, 17, 9, 129]","[6, 8, 12, 68]","[50, 5, 2, 36]","The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients.
Authors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines.

Overall, I had a very confusing feeling when reading the paper. First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP? How do different agents perceive their time, is it synchronized or not? Do they (partially) share the incentive or may have completely arbitrary rewards?
What is exactly the communication protocol?

I find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks.

Second, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me.
Currently, it looks 

Based on that, I can’t recommend acceptance of the paper.

To make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients).
It would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives.","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Bayesian Hypernetworks,"['David Krueger', 'Chin-Wei Huang', 'Riashat Islam', 'Ryan Turner', 'Alexandre Lacoste', 'Aaron Courville']",Reject,2018,"[9, 13, -2, 11, 11, 6, 7, 3, 30]","[14, 18, 2, 16, 16, 10, 11, 8, 35]","[50, 206, 3, 180, 158, 41, 6, 5, 688]","[33, 86, 2, 98, 89, 19, 4, 2, 398]","[13, 94, 1, 51, 48, 10, 2, 2, 138]","[4, 26, 0, 31, 21, 12, 0, 1, 152]","This paper presents Bayesian Hypernetworks; variational Bayesian neural networks where the variational posterior over the weights is governed by a hyper network that implements a normalizing flow (NF) such as RealNVP and IAF. As directly outputting the weight matrix with a hyper network is computationally expensive the authors instead propose to utilize weight normalisation on the weights and then use the hyper network to output scalar scaling variables for each hidden unit, similarly to what was done at [1]. The main difference with this prior work is that [1] consider these NF scaling variables as auxiliary random variables to a mean field Gaussian distribution over the weights whereas this paper attempts to posit a distribution directly on the weights via the NF. This avoids the nested variational approximation and auxiliary models of [1], which can potentially yield a tighter bound. The proposed method is evaluated on extensive experiments.

This paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at Section 3.3. More specifically, if you generate the parameters \theta according to Eq. 7 and posit a prior over \theta then you will have a problematic variational bound as there will be a KL divergence, KL(q(\theta) || p(\theta)), with distributions of different support (since q(\theta) is defined only along the directions spanned by u), which is infinite. For the KL to be valid you will need to posit a prior distribution over `g`, p(g), and then consider KL(q(g) || p(g)), with q(g) being given by the NF. From the experiment paragraph at page 5 though I deduct that you instead employ “an isotropic standard normal prior over the weights”, i.e. \theta, thus I believe that you indeed have a problematic bound. How do you actually compute logq(\theta) when you employ the parametrisation discussed at 3.3? Did you use that parametrisation in every experiment?

Other than that, I believe that it would be interesting to experiment with a `full` hyper network, i.e. generating directly the entire parameter vector \theta, e.g. at the toy regression experiment where the dimensionality is small. This would then better illustrate the tradeoffs you make when you reduce the flexibility of the hyper-network to just outputting the row scaling variables and the effect this has at the posterior approximation.
 
Typos:
(1) Page 3, 3.1.1 log(\theta) -> logp(\theta).
(2) Eq. 6, it needs to be |det \frac{\partial h(\epsilon)}{\partial \epsilon}|^{-1} or |det \frac{\partial h^{-1}(\theta)}{\partial \theta}| for a valid change of variables formula.

[1] Louizos & Welling, Multiplicative Normalizing Flows for Variational Bayesian Neural Networks.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Adversary A3C for Robust Reinforcement Learning,"['Zhaoyuan Gu', 'Zhenzhong Jia', 'Howie Choset']",Reject,2018,"[0, 8, 26]","[4, 13, 31]","[6, 48, 366]","[1, 32, 227]","[5, 8, 62]","[0, 8, 77]","The authors propose an extension of adversarial reinforcement learning to A3C. The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  

The authors propose extending A3C to produce more robust policies by training a zero-sum game with two agents: a protagonist and an antagonist. The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail. 

The contribution of this work, AR-A3C, is extending adversarial reinforcement learning, namely robust RL (RRL) and robust adversarial RL (RARL), to A3C. In the context of this prior work the novelty is extending the family of adversarial RL methods. However, the proposed method is still within the same family methods as demonstrated by RARL.

The authors state that AR-A3C requires half as many rollouts as compared to RARL. However, no empirical comparison between the two methods is performed. The paper only performs analysis against the A3C and no other adversarial baseline and on only one environment: cartpole.  While they show transfer to the real world cartpole with this technique, there is not sufficient analysis to satisfactorily demonstrate the benefits of the proposed technique. 

The paper reads well. There are a few notational issues in the paper that should be addressed. The authors mislabel the value function V as the  action value, or Q function. The action value function is action dependent where the value function is not.  As a much more minor issue, the authors introduce y as the discount factor, which deviates from the standard notation of \gamma without any obvious reason to do so.

Double blind was likely compromised with the youtube video, which was linked to a real name account instead of an anonymous account.

Overall, the proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.    ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,"['Abhishek Panigrahi', 'Yueru Chen', 'C.-C. Jay Kuo']",Reject,2018,"[2, 3, 32]","[7, 8, 37]","[17, 40, 1008]","[7, 16, 568]","[10, 18, 154]","[0, 6, 286]","This manuscript is fairly well-written, and discusses how the batch normalization step helps to stabilize the scale of the gradients.  Intriguingly, the analysis suggests that using a shallower but wider resnet should provide competitive performance, which is supported by empirical evidence.  This work should help elucidate the structure in the learning, and help to support efforts to improve both learning algorithms and the architecture.

Pros:
Clean, simple analysis
Empirical support suggests that theory captures reasonable effects behind learning

Cons:
The reasonableness of the assumptions used in the analysis needs a more careful analysis.  In particular, the assumption that all weights are independent is valid only at the first random iteration.  Therefore, the utility of this theory during initialization seems reasonable, but during learning the theory seems quite tenuous.  I would encourage the authors to discuss their assumptions, and talk about how the math would change as a result of relaxing the assumptions.
The empirical support does provide evidence that the theory is reasonable.  However, it is limited to a single dataset.  It would be nice to see that the effect happens more generally. Second, it is clear that shallow+wide networks may be better than deep+narrow networks, but it's not clear about how the width is evaluated and supported. I would encourage the authors to do more extensive experiments and evaluate the architecture further.

Revision:
Upon examining the comments of the other reviews, I have agreed with several of their points and it is necessary to increase the explanation of the mathematical points.  I encourage the authors to address these comments and revise their work.","[4, 1, 4]","[' Ok but not good enough - rejection', ' Trivial or wrong', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Lifelong Generative Modeling,"['Jason Ramapuram', 'Magda Gregorova', 'Alexandros Kalousis']",Reject,2018,"[2, 4, 24]","[7, 8, 28]","[25, 18, 104]","[9, 5, 55]","[15, 11, 35]","[1, 2, 14]","The paper proposed a teacher-student framework and a modified objective function to adapt VAE training to streaming data setting. The qualitative experimental result shows that the learned model can generate reasonable-looking samples. I'm not sure about what conclusion to make from the numerical result, as the test negative ELBO actually increased after decreasing initially. Why did it increase?

The modified objective function is a little ad-hoc, and it's unclear how to relate the overall objective function to Bayesian posterior inference (what exactly is the posterior that the encoder tries to approximate?). There is a term in the objective function that is synthetic data specific. Does that imply that the objective function is different depending on if the data is synthetic or real? What is the motivation/justification of choosing KL(Q_student||Q_teacher) as regularisation instead of the other way around? Would that make a difference in the goodness of the learned model? If not, wouldn't KL(Q_teacher||Q_student) result reduction in the variance of gradients and therefore a better choice?

Details on the minimum number of real samples per interval for the model to be able to learn is also missing. Also, how many synthetic samples per real samples are needed? How is the update with respect to synthetic sample scheduled? Given infinite amount of streaming data with a fixed number of classes/underlying distributions and interval length, and sample the class of each interval (uniformly) randomly, will the model/algorithm converge? Is there a minimum number of real examples that the student learner needs to see before it can be turned into a teacher?

Other question: How is the number of latent category J of the latent discrete distribution chosen?

Quality: The numerical experiment doesn't really compare to any other streaming benchmark and is a little unsatisfying. Without a streaming benchmark or a realistic motivating example in which the proposed scheme makes a significant difference, it's difficult to judge the contribution of this work.
Clarity: The manuscript is reasonably well-written. (minor: Paragraph 2, section 5, 'in principle' instead of 'in principal')
Originality: Average. The student-teacher framework by itself isn't novel. The modifications to the objective function appears to be novel as far as I am aware, but it doesn't require much special insights.
Significance: Below average. I think it will be very helpful if the authors can include a realistic motivating example where lifelong unsupervised learning is critical, and demonstrate that the proposed scheme makes a difference in the example.


","[4, 9, 4]","[' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Clustering with Deep Learning: Taxonomy and New Methods,"['Elie Aljalbout', 'Vladimir Golkov', 'Yawar Siddiqui', 'Daniel Cremers']",Reject,2018,"[1, 4, 1, 19]","[5, 9, 6, 24]","[18, 20, 12, 602]","[5, 6, 4, 327]","[11, 13, 8, 186]","[2, 1, 0, 89]","In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods. Finally, the authors propose a new method by using one unexplored combination of taxonomy features.

The paper is well-written and easy to follow. The proposed combination is straightforward, but lack of novelty. From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining. I am not convinced that these changes will lead to a significant difference. The performance of the proposed method and DEPICK are also similar in table 1. 

In addition, the experiments section is not comprehensive enough as well. the author only tested on two datasets. More datasets should be tested for evaluation. In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications. The authors should finish the experiments on comparison methods and fill the entries in Table 1.

In summary, the proposed method is lack of novelty compare to existing methods. The survey part is nice, however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios. Therefore, I think the paper cannot be accepted at this stage.
","[3, 2, 3]","[' Clear rejection', ' Strong rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Exploring the Space of Black-box Attacks on Deep Neural Networks,"['Arjun Nitin Bhagoji', 'Warren He', 'Bo Li', 'Dawn Song']",Reject,2018,"[2, 7, 10, 20]","[7, 11, 15, 25]","[37, 27, 300, 440]","[16, 16, 124, 254]","[20, 9, 148, 154]","[1, 2, 28, 32]","This paper generates adversarial examples using the fast gradient sign (FGS) and iterated fast gradient sign (IFGS) methods, but replacing the gradient computation with finite differences or another gradient approximation method. Since finite differences is expensive in high dimensions, the authors propose using directional derivatives based on random feature groupings or PCA. 

This paper would be much stronger if it surveyed a wider variety of gradient-free optimization methods. Notably, there's two important black-box optimization baselines that were not included: simultaneous perturbation stochastic approximation ( https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation), which avoids computing the gradient explicitly, and evolutionary strategies ( https://blog.openai.com/evolution-strategies/ ), a similar method that uses several random directions to estimate a better descent direction.

The gradient approximation methods proposed in this paper may or may not be better than SPSA or ES. Without a direct comparison, it's hard to know.  Thus, the main contribution of this paper is in demonstrating that gradient approximation methods are sufficient for generating good adversarial attacks and applying those attacks to Clarifai models. That's interesting and useful to know, but is still a relatively small contribution, making this paper borderline. I lean towards rejection, since the paper proposes new methods without comparing to or even mentioning well-known alternatives.

REVISION: Thank you for your response! The additional material does strengthen the paper. There is now some discussion of how Chen et al. differs, and an explicit comparison to SPSA and PSO. I think there are some interesting results here, including attacks on Clarifai. However, the additional evaluations are not thorough. This is understandable (given the limited time frame), but unfortunate. SPSA is only evaluated on MNIST, and while the paper claims its distortion is greater, this is never shown explicitly (or was too difficult for me to find, even when searching through the revised paper). Chen et al. is only compared in terms of time, not on success rate, distortion, or number of queries. These timing results aren't necessarily comparable, since the experiments were done under different conditions. Overall, the new experiments and discussion are a step towards a thorough analysis of zero-order attacks, but they're not there yet. I've increased my rating from 4 to 5, but this is still below the bar for me.","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Generative Dual Memory Network for Continual Learning,"['Nitin Kamra', 'Umang Gupta', 'Yan Liu']",Reject,2018,"[4, 23, 17]","[9, 28, 22]","[16, 31, 219]","[7, 13, 132]","[7, 14, 57]","[2, 4, 30]","This paper introduces a neural network architecture for continual learning. The model is inspired by current knowledge about long term memory consolidation mechanisms in humans. As a consequence, it uses:
-	One temporary memory storage (inspired by hippocampus) and a long term memory
-	A notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks.
Overall, although the result are not very surprising, the approach is well justified and extensively tested. It provides some insights on the challenges and benefits of replay based memory consolidation.

Comments:
	
1-	The results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach. 
2-	It is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume). 
3-	So the main benefit of the approach seems to point towards the direction of what possibly happens in real brains. It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep.
4-	Due to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting
5-	The writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.
","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Dynamic Evaluation of Neural Sequence Models,"['Ben Krause', 'Emmanuel Kahembwe', 'Iain Murray', 'Steve Renals']",Reject,2018,"[13, 2, 15, 32]","[16, 5, 19, 37]","[16, 8, 70, 301]","[5, 1, 36, 209]","[10, 6, 29, 44]","[1, 1, 5, 48]","The authors provide an improved implementation of the idea of dynamic evaluation, where the update of the parameters used in the last time step proposed in (Mikolov et al. 2010) is replaced with a back-propagation through the last few time steps, and uses  RMSprop rather than vanilla SGD. The method is applied to word level and character level language modeling where it yields some gains in perplexity. The algorithm also appears able to perform domain adaptation, in a setting where a character-level language model trained mostly on English manages to quickly adapt to a Spanish test set. 

While the general idea is not novel, the implementation choices matter, and the authors provide one which appears to work well with recently proposed models. The character level experiments on the multiplicative LSTM make the most convincing point, providing a significant improvement over already good results on medium size data sets. Figure 2 also makes a strong case for the method's suitability for applications where domain adaptation is important.

The paper's weakest part is the word level language modeling section. Given the small size of the data sets considered, the results provided are of limited use, especially since the development set is used to fit the RMSprop hyper-parameters. How sensitive are the final results to this choice? Comparing dynamic evaluation to neural cache models is a good idea, given how both depend en medium-term history: (Grave et al. 2017) provide results on the larger text8 and wiki103, it would be useful to see results for dynamic evaluation at least on the former.

An indication of the actual additional evaluation time for word-level, char-level and sparse char-level dynamic evaluation would also be welcome.

Pros:
- Good new implementation of an existing idea
- Significant perplexity gains on character level language modeling
- Good at domain adaptation

Cons:
- Memory requirements of the method
- Word-level language modeling experiments need to be run on larger data sets

(Edit: the authors did respond satisfactorily to the original concern about the size of the word-level data set)","[7, 7, 3]","[' Good paper, accept', ' Good paper, accept', ' Clear rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
ResBinNet: Residual Binary Neural Network,"['Mohammad Ghasemzadeh', 'Mohammad Samragh', 'Farinaz Koushanfar']",Reject,2018,"[2, 2, 19]","[2, 6, 24]","[6, 33, 295]","[4, 13, 162]","[2, 13, 72]","[0, 7, 61]","This paper proposes ResBinNet, with residual binarization, and temperature adjustment. It is a reconfigurable binarization method for neural networks. It improves the convergence rate during training. 

I appreciate a lot that the authors were able to validate their idea by building a prototype of an actual hardware accelerator.

I am wondering what are the values of \gamma’s in the residual binarization after learning? What is its advantage over having only one \gamma, and then the rest are just 1/2*\gamma, 1/4* \gamma, … , etc.? The latter is an important baseline for residual binarization because that corresponds to the widely used fixed point format for real numbers. If you can show some results that residual encoding is better than having {\gamma, 1/2*\gamma, 1/4* \gamma, …, } (which contains only one \gamma), it would validate the need of using this relatively complex binarization scheme. Otherwise, we can just use the l-bit fixed point multiplications, which is off-the-shelf and already highly optimized in many hardwares. 

For the temperature adjustment, modifying the tanh() scale has already had a long history, for example, http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf page 7, which is exactly the same form as in this paper. Adjusting the slope during training has also been explored in some straight-through estimator approaches, such as https://arxiv.org/pdf/1609.01704.pdf. In addition, having this residual binarization and adjustable tanh(), is already adding extra computations for training. Could you provide some data for comparing the computations before and after adding residual binarization and temperature adjustment? 

The authors claimed that ResBinNet converges faster during training, and in table 2 it shows that ResBinNet just needs 1/10 of the training epochs needed by BinaryNet. However, I don’t find it very fair. Given that the accuracy RBN gets is much lower than Binary Net, the readers might suspect that maybe the other two models already reach ResBinNet’s accuracy at an earlier training epochs (like epoch 50), and just take all the remaining epochs to reach a higher accuracy. On the other hand, this comparison is not fair for ResBinNet as well. The model size was much larger in BinaryNet than in ResBinNet. So it makes sense to train a BinaryNet or FINN, in the same size, and then compare the training curves. Lastly, in CIFAR-10 1-level case, it didn’t outperform FINN, which has the same size. Given these experiments, I can’t draw any convincing conclusion.

Apart from that, There is an error in Figure 7 (b), where the baseline has an accuracy of 80.1% but its corresponding bar is lower than RBN1, which has an accuracy of 76%. ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
FAST READING COMPREHENSION WITH CONVNETS,"['Felix Wu', 'Ni Lao', 'John Blitzer', 'Guandao Yang', 'Kilian Weinberger']",Reject,2018,"[25, 16, 16, 2, 15]","[30, 21, 17, 7, 20]","[54, 56, 35, 23, 199]","[22, 27, 28, 12, 106]","[26, 23, 6, 11, 82]","[6, 6, 1, 0, 11]","This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up.

The proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units.

I understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring. However, I think the proposed architecture in this paper is less motivated. Why is GLU chosen? Why is dilation used? According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why?

The architecture search (Table 3 and Figure 4) seems to quite arbitrary. I  would like to see more careful architecture search and ablation studies. Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?

I would like to see more explanations of Figure 4. How important is # of layers and residual connections?

Minor:
- It’d be helpful to add the formulation of gated linear units and residual layers. 
- It is necessary to put Table 5 in the main paper instead of Appendix. These are still the main results of the paper.","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning to Generate Filters for Convolutional Neural Networks,"['Wei Shen', 'Rujie Liu']",Reject,2018,"[24, 18]","[29, 23]","[170, 69]","[81, 46]","[25, 16]","[64, 7]","This paper explores learning dynamic filters for CNNs. The filters are generated by using the features of an autoencoder on the input image, and linearly combining a set of base filters for each layer. This addresses an interesting problem which has been looked at a lot before, but with some small new parts. There is a lot of prior work in this area that should be cited in the area of dynamic filters and steerable filters. There are also parallels to ladder networks that should be highlighted. 

The results indicate improvement over baselines, however baselines are not strong baselines. 
A key question is what happens when this method is combined with VGG11 which the authors train as a baseline? 
What is the effect of the reconstruction loss? Can it be removed? There should be some ablation study here.
Figure 5 is unclear what is being displayed, there are no labels.

Overall I would advise the authors to address these questions and suggest this as a paper suitable for a workshop submission.
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks,"['Xu Sun', 'Bingzhen Wei', 'Xuancheng Ren', 'Shuming Ma']",Reject,2018,"[13, 2, 2, 9]","[18, 4, 6, 14]","[278, 16, 91, 111]","[130, 5, 34, 46]","[132, 9, 50, 62]","[16, 2, 7, 3]","This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks. 
Pros: Good empirical results.
Cons:  There is not much technical contribution. The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved. 

1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2). 

2. Label embedding learning has been investigated in many previous works. The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification. This assumption is not very natural — though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels.  The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works. 

3. The proposed model is not well explained.  
(1) By using the objective in eq.(14), how to learn the embeddings E? 
(2) The authors state “In back propagation, the gradient from z2 is kept from propagating to h”.  This makes the learning process quite arbitrary under the objective in eq.(14). 
(3) The label embeddings are not directly used for the classification (H(y, z’_1)), but rather as auxiliary part of the objective.  How to decide the test labels?
","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
DeepArchitect: Automatically Designing and Training Deep Architectures,"['Renato Negrinho', 'Geoff Gordon']",Reject,2018,"[5, 26]","[8, 30]","[11, 181]","[5, 115]","[6, 49]","[0, 17]","The author present a language for expressing hyperparameters (HP) of a network. This language allows to define a tree structure search space to cover the case where some HP variable exists only if some previous HP variable took some specific value. Using this tool, they explore the depth of the network, when to apply batch-normalization, when to apply dropout and some optimization variables. They compare the search performance of random search, monte carlo tree search and a basic implementation of a Sequential Model Based Search. 

The novelty in this paper is below what is expected for a publication at ICLR. I recommend rejection.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,"['JIANXIONG DONG', 'Jim Huang']",Reject,2018,"[8, 20]","[8, 23]","[2, 14]","[1, 11]","[1, 2]","[0, 1]","The main contributions in this paper are:
1) New variants of a recent LSTM-based model (""ESIM"") are applied to the task of response-selection in dialogue modeling -- ESIM was originally introduced and evaluated for natural language inference. In this new setting, the ESIM model (vanilla and extended) outperform previous models when trained and evaluated on two distinct conversational datasets.

2) A fairly trivial method is proposed to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to these conversational datasets.
The method itself is to combine d1-dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2-dimensional word embeddings that are trained on the task-specific training data (vocabulary T). The enhanced (d1+d2)-dimensional representation for a word is constructed by concatenating its vectors from the two embeddings, setting either the d1- or d2-dimensional subvector to zeros when the word is absent from either S or T, respectively. This method is incorporated as an extension into ESIM and evaluated on the two conversation datasets.

The main results can be characterized as showing that this vocabulary extension method leads to performance gains on two datasets, on top of an ESIM-model extended with character-based word embeddings, which itself outperforms the vanilla ESIM model.

These empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility. 

There is basic novelty in applying the base model to a new task, and the analysis of the role of the special conversational boundary tokens is interesting and can help to inform future modeling choices. The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...

Overall, the presentation switches back and forth between the Douban corpus and the Ubuntu corpus, and between word2vec and Glove embeddings, and this makes it very challenging to understand the details fully.

S3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here, and were not part of the original formulation of ESIM. That statement is currently hidden in the figure caption.

Algorithm 1:
- What set does P denote, and what is the set-theoretic relation between P and T?
- Under one possible interpretation, there may be items in P that are in neither T nor S, yet the algorithm does not define embeddings for those items even though its output is described as ""a dictionary with word embeddings ... for P"". This does not seem consistent? I think the sentence in S4.2 about initializing remaining OOV words as zeros is relevant and wonder if it should form part of the algorithm description?

S4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were ""collected by Lucene retrieval model""?

S4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.

S4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, or does FastText make it unnecessary?

S4.3 - ""It can be seen from table 3 that the original ESIM did not perform well without character embedding."" This is a curious way to describe the result, when, in fact, the ESIM model in table 3 already outperforms all the previous models listed.

S4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, instead of the ambiguous ""word embeddings"".

","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Sequential Coordination of Deep Models for Learning Visual Arithmetic,"['Eric Crawford', 'Guillaume Rabusseau', 'Joelle Pineau']",Reject,2018,"[6, 5, 19]","[8, 10, 24]","[10, 60, 308]","[4, 23, 151]","[4, 33, 119]","[2, 4, 38]","Summary: This work is a variant of previous work (Zaremba et al. 2016) that enables the use of (noisy) operators that invoke pre-trained neural networks and is trained with Actor-Critic. In this regard it lacks a bit of originality. The quality of the experimental evaluation is not great. The clarity of the paper could be improved upon but is otherwise fine. The existence of previous work (Zaremba et al. 2016) renders this work (including its contributions) not very significant. Relations to prior work are missing. But let's wait for the rebuttal phase. 

Pros 
-It is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task

Cons
-Not very novel
-Experimental evaluation is wanting

The focus of this paper is on integrating perception and reasoning in a single system. This is done by specifying an interface that consists of a set of discrete operations (some of which involve perception) and memory slots. A parameterized policy that can make use of these these operations is trained via Actor-Critic to solve some reasoning tasks (arithmetics in this case). 

The proposed system is a variant of previous work (Zaremba et al. 2016) on the concept of interfaces, and similarly learns a policy that utilizes such an interface to perform reasoning tasks, such as arithmetics. In fact, the only innovation proposed in this paper is to incorporate some actions that invoke a pre-trained neural network to “read” the symbol from an image, as opposed to parsing the symbol directly. However, there is no reason to expect that this would not function in previous work (Zaremba et al. 2016), even when the network is suboptimal (in which case the operator becomes noisy and the policy should adapt accordingly). Another notable difference is that the proposed system is trained with Actor-Critic as opposed to Q-learning, but this is not further elaborated on by the authors. 

The proposed system is evaluated on a visual arithmetics task. The input consists of a 2x2 grid of extended MNIST characters. Each location in the grid then corresponds to the 28 x 28 pixel representation of the digit. Actions include shifting the “fovea” to a different entry of the grid, invoking the digit NN or the operator NN which parse the current grid entry, and some symbolic operations that operate on the memory. The fact that the input is divided into a 2x2 grid severely limits the novelty of this approach compared to previous work (Zaremba et al. 2016). Instead it would have been interesting to randomly spawn digits and operators in a 56 x 56 image and maintain 4 coordinates that specify a variable-sized grid that glimpses a part of the image. This would make the task severely more difficult, given fixed pre-trained networks. The addition of the salience network is unclear to me in the context of MNIST digits, since any pixel that is greater than 0 is salient? I presume that the LSTM uses this operator to evaluate whether the current entry contains a digit or an operator. If so, wouldn’t simply returning the glimpse be enough?

In the experiments the proposed system is compared to three CNNs on two different visual arithmetic tasks, one that includes operators as part of the input and one that incorporates operators only in the tasks description. In all cases the proposed method requires fewer samples to achieve the final performance, although given enough samples all of the CNNs will solve the tasks. This is not surprising as this comparison is rather unfair. The proposed system incorporates pre-trained modules, whose training samples are not taken into account. On the other hand the CNNs are trained from scratch and do not start with the capability to recognize digits or operators. Combined with the observation that all CNNs are able to solve the task eventually, there is little insight in the method's performance that can be gained from this comparison. 

Although the visual arithmetics on a 2x2 grid is a toy task it would at least be nice to evaluate some of the policies that are learned by the LSTM (as done by Zaremba) to see if some intuition can be recovered from there. Proper evaluation on a more complex environment (or at least on that does not assume discrete grids) is much desired. When increasing the complexity (even if by just increasing the grid size) it would be good to compare to a recurrent method (Pyramid-LSTM, Pixel-RNN) as opposed to a standard CNN as it lacks memory capabilities and is clearly at a disadvantage compared to the LSTM.

Some detailed comments are:

The introduction invokes evidence from neuroscience to argue that the brain is composed of (discrete) modules, without reviewing any of the counter evidence (there may be a lot, given how bold this claim is).

From the introduction it is unclear why the visual arithmetic task is important.

Several statements including the first sentence lack citations.

The contribution section is not giving any credit to Zaremba et al. (2016) whereas this work is at best a variant of that approach.

In the experiment section the role of the saliency detector is unclear.

Experiment details are lacking and should be included.

The related work section could be more focused on the actual contribution being made.

It strikes me as odd that in the discussion the authors propose to make the entire system differentiable, since this goes against the motivation for this work.


Relation to prior work:

p 1: The authors write: ""We also borrow the notion of an interface as proposed in Zaremba et al. (2016). An interface is a designed, task-specific machine that mediates the learning agent’s interaction with the external world, providing the agent with a representation (observation and action spaces) which is intended to be more conducive to learning than the raw representations. In this work we formalize an interface as a separate POMDP I with its own state, observation and action spaces."" 

This interface terminology for POMDPs was actually introduced in:

J.  Schmidhuber. Reinforcement learning in Markovian and non-Markovian environments. In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3, NIPS'3, pages 500-506. San Mateo, CA: Morgan Kaufmann, 1991.

p 4: authors write: ""For the policy πθ, we employ a Long Short-Term Memory (LSTM)"" 

Do the authors use the (cited) original LSTM of 1997, or do they also use the forget gates (recurrent units with gates) that most people are using now, often called the vanilla LSTM, by Gers et al (2000)?

p 4: authors write: ""One obvious point of comparison to the current work is recent research on deep neural networks designed to learn to carry out algorithms on sequences of discrete symbols. Some of these frameworks, including the Differen-tiable Forth Interpreter (Riedel and Rocktäschel, 2016) and TerpreT (Gaunt et al., 2016b), achieve this by explicitly generating code, while others, including the Neural Turing Machine (NTM; Graves et al., 2014), Neural Random-Access Machine (NRAM; Kurach et al., 2015), Neural Programmer (NP; Neelakan- tan et al., 2015), Neural Programmer-Interpreter (NPI; Reed and De Freitas, 2015) and work in Zaremba et al. (2016) on learning algorithms using reinforcement learning, avoid gen- erating code and generally consist of a controller network that learns to perform actions in a (sometimes differentiable) external computational medium in order to carry out an algorithm.""

Here the original work should be mentioned, on differentiable neural stack machines: 

G.Z. Sun and H.H. Chen and  C.L. Giles and Y.C. Lee and D. Chen. Connectionist Pushdown Automata that Learn Context-Free Grammars. IJCNN-90, Lawrence Erlbaum, Hillsdale, N.J., p 577, 1990.

Mozer, Michael C and Das, Sreerupa. A connectionist symbol manipulator that discovers the structure of context-free languages. Advances in Neural Information Processing Systems (NIPS), p 863-863, 1993.


","[4, 3, 2]","[' Ok but not good enough - rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Neural-Symbolic Approach to Natural Language Tasks,"['Qiuyuan Huang', 'Paul Smolensky', 'Xiaodong He', 'Li Deng', 'Dapeng Wu']",Reject,2018,"[8, 36, 20, 28, 20]","[13, 40, 25, 30, 25]","[58, 64, 316, 329, 458]","[29, 28, 189, 188, 125]","[18, 24, 106, 36, 45]","[11, 12, 21, 105, 288]","The paper claims that ""Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures"", which is false, see ""Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"", ""Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"", ""DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks"" or ""Deep Compositional Question Answering with Neural Module Networks"".

The Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address.

""The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles"" Doesn't refer to any specific kind of layer, or what it make it special.

The idea of using outer product as a layer has already been explored in ""Multimodal compact bilinear pooling for visual question answering and visual grounding""

The following paragraph in page 2 is not clear, very confusing:
The work reported here .... their categories


In page 3 authors claim that the ""vectors are linearly independent"" but didn't specify how they enforce that.

Figure 3 contradicts Figure 1, not clear what are the inputs for module S.

The experiments reported in Table1 are useless, there a tons of previous work with much better results, see 
https://competitions.codalab.org/competitions/3221#results

Even the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. 

There is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. 

The experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset.

The text is missing a lot of references, for example:
 - page 2 GSC
 - page 2 The first approach takes the detected by a CNN ....
 - page 3 previous work where TPRs are hand-crafted","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Large-scale Cloze Test Dataset Designed by Teachers,"['Qizhe Xie', 'Guokun Lai', 'Zihang Dai', 'Eduard Hovy']",Reject,2018,"[4, 5, 3, 34]","[9, 7, 7, 39]","[26, 18, 51, 517]","[10, 8, 25, 350]","[13, 10, 26, 123]","[3, 0, 0, 44]","1) this paper introduces a new cloze dataset, ""CLOTH"", which is designed by teachers. The authors claim that this cloze dataset is a more challenging dataset since CLOTH requires a deeper language understanding and wider attention span. I think this dataset is useful for demonstrating the robustness of current RC models. However, I still have the following questions which lead me to reject this paper.

2) I have the questions as follows:
i) The major flaw of this paper is about the baselines in experiments. I don't think the language model is a robust baseline for this paper.  When a wider span is used for selecting answers, the attention-based model should be a reasonable baseline instead of pure LM. 
ii) the author also should provide the error rates for each kind of questions (grammar questions or long-term reasoning). 
iii) the author claim that this CLOTH dataset requires wider span for getting the correct answer, however, there are only 22.4 of the entire data need long-term reasoning. More importantly, there are 26.5% questions are about grammar. These problems can be easily solved by LM. 
iv) I would not consider 16% percent of accuracy is a ""significant margin"" between human and pure LM-based methods. LM-based methods should not be considered as RC model.
v) what kind accuracy is improved if you use 1-billion corpus trained LM? Are these improvements mostly in grammar? I did not see why larger training corpus for LM could help a lot about reasoning since reasoning is only related to question document.
","[4, 7, 4]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Principled Hybrids of Generative and Discriminative Domain Adaptation,"['Han Zhao', 'Zhenyao Zhu', 'Junjie Hu', 'Adam Coates', 'Geoff Gordon']",Reject,2018,"[6, 6, 7, 15, 26]","[11, 7, 12, 16, 30]","[111, 20, 63, 42, 181]","[54, 9, 31, 29, 115]","[51, 11, 30, 10, 49]","[6, 0, 2, 3, 17]","The authors propose a probabilistic framework for semi-supervised learning and domain adaptation. By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation. A domain confusion loss is added to learn domain-invariant feature representations. The authors compared Dauto with several baseline methods on several datasets and showed improvement. 

The paper is well-organized and easy to follow. The probabilistic framework itself is quite straight-forward. The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme. 

The proposed DAuto is essentially DANN+autoencoder.  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator. It would be interesting to see if the additional auto-encoder part help address the issue. 

The experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017). 

It could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block. DAuto does seem to offer more boost in domain pairs that are less similar. ","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Towards Binary-Valued Gates for Robust LSTM Training ,"['Zhuohan Li', 'Di He', 'Fei Tian', 'Wei Chen', 'Tao Qin', 'Liwei Wang', 'Tie-Yan Liu']",Reject,2018,"[8, 18, 10, 10, 17, 14, 18]","[13, 23, 14, 15, 22, 19, 23]","[29, 230, 69, 122, 485, 217, 565]","[10, 107, 37, 51, 240, 89, 280]","[12, 74, 18, 61, 163, 101, 220]","[7, 49, 14, 10, 82, 27, 65]","This paper aims to push the LSTM gates to be binary. To achieve this, the paper proposes to employ the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution (taking 0 or 1 value). The resulted G2-LSTM is applied for language model and machine translation in the experiments. 

The novelty of this paper is limited. Just directly apply the Gumbel-Softmax trick. 

The motivation is not explained clearly and convincingly. Why need to pursue binary gates? According to the paper, it may give better generalization performance. But there is no theoretical or experimental evidence provided by this paper to support this argument. 

The results of the new G2-LSTM are not significantly better than baselines in the experiments.","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
GENERATIVE LOW-SHOT NETWORK EXPANSION,"['Adi Hayat', 'Mark Kliger', 'Shachar Fleishman', 'Daniel Cohen-Or']",Reject,2018,"[1, 20, 24, 30]","[1, 22, 25, 35]","[2, 23, 23, 486]","[1, 9, 10, 115]","[1, 6, 4, 120]","[0, 8, 9, 251]","The paper proposes a method for adapting a pre-trained network, trained on a fixed number of
classes, to incorporate novel classes for doing classification, especially when the novel classes
only have a few training examples available. They propose to do a `hard' distillation, i.e. they
introduce new nodes and parameters to the network to add the new classes, but only fine-tune the new
networks without modifying the original parameters. This ensures that, in the new expanded and
fine-tuned network, the class confusions will only be between the old and new classes and not
between the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on
the old classes during the fine-tuning process, thus avoiding saving all the original training data.
They show experiments on public benchmarks with three different scenarios, i.e.  base and novel
classes from different domains, base and novel classes from the same domain and novel classes have
similarities among themselves, and base and novel classes from the same domain and each novel class
has similarities with at least one of the base class.                        
                                                                             
- The paper is generally well written and it is clear what is being done     
- The idea is simple and novel; to the best of my knowledge it has not been tested before
- The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation
  (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot
  settings and comparably when large number of training examples of the novel classes are available
- My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12
  subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The
  idea is simple and novel, which is good, but the validation is limited and far from any realistic
  use. Having only O(10) classes is not convincing, especially when the datasets used do have large
  number of classes. I agree that this will not allow or will takes some involved manual effort to
  curate subsets for the settings proposed, but it is necessary for being convincing.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Spectral Graph Wavelets for Structural Role Similarity in Networks,"['Claire Donnat', 'Marinka Zitnik', 'David Hallac', 'Jure Leskovec']",Reject,2018,"[3, 7, 4, 16]","[8, 12, 7, 21]","[14, 100, 22, 460]","[5, 21, 9, 232]","[9, 40, 10, 188]","[0, 39, 3, 40]","The paper derived a way to compare nodes in graph based on wavelet analysis of graph laplacian. The method is correct but it is not clear whether the method can match the performance of state-of-the-art methods such as graph convolution neural network of Duvenaud  et al. and Structure2Vec of Dai et al. in large scale datasets.  
1. Convolutional Networks on Graphs for Learning Molecular Fingerprints. D Duvenaud et al., NIPS 2015. 
2. Discriminative embeddings of latent variable models for structured data. Dai et al. ICML 2016.

","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Censoring Representations with Multiple-Adversaries over Random Subspaces,"['Yusuke Iwasawa', 'Kotaro Nakayama', 'Yutaka Matsuo']",Reject,2018,"[8, 14, 21]","[13, 18, 26]","[56, 44, 252]","[29, 32, 156]","[16, 6, 52]","[11, 6, 44]","The below review addresses the first revision of the paper. The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out.

---

The authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model.

The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.

There are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good?

In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Auxiliary Guided Autoregressive Variational Autoencoders,"['Thomas Lucas', 'Jakob Verbeek']",Reject,2018,"[3, 18]","[7, 23]","[21, 136]","[10, 71]","[10, 41]","[1, 24]","The authors present Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), a hybrid approach that combines the strengths of variational autoencoders (global statistics) and autorregressive models (local statistics) for improved image modeling. This is done by controlling the capacity of the autorregressive component within an auxiliary loss function.

The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model. The authors do not describe how \lambda was selected, which is critical for performance, provided the results in Figure 4. That being said, the contribution from the VAE is likely to be negligible given the performance of PixelCNN++ alone.

- The KL divergence in (3) does more than simply preventing the approximation q() from becoming a point mass distribution.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Coupled Ensembles of Neural Networks,"['Anuvabh Dutt', 'Denis Pellerin', 'Georges Quénot']",Reject,2018,"[2, 25, 33]","[6, 27, 38]","[10, 84, 149]","[6, 60, 122]","[2, 3, 5]","[2, 21, 22]","This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly. Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers). In effect, this paper extends the existing literature suggesting end-to-end branching. While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community.

+ Paper is well written and easy to follow.
+ Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem
+ Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches.
+ [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks.

- Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet. While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation.
- Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful.
- Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent. It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer.
- One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit.

Minor edits: Page 1. 'significantly match and improve' => 'either match or improve'

Additional notes:
- It would interesting to compare this approach with a conditional training pipeline that sequentially adds branches, keeping the previous branches fixed. This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,"['Josh Fromm', 'Matthai Philipose', 'Shwetak Patel']",Reject,2018,"[6, 16, 23]","[11, 21, 26]","[20, 206, 81]","[11, 136, 58]","[6, 36, 6]","[3, 34, 17]","The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights.

1.  The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them.

2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet.

Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures."" So, please show that.

3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?

My concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016].

4. Is trade-off between 1 to 2 bits really important? 

Compared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important?

Reference:
[a]. Trained Ternary Quantization. ICLR 2017
[b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Learning is Robust to Massive Label Noise,"['David Rolnick', 'Andreas Veit', 'Serge Belongie', 'Nir Shavit']",Reject,2018,"[8, 6, 25, 33]","[13, 10, 30, 38]","[63, 47, 314, 176]","[16, 20, 189, 107]","[35, 25, 99, 26]","[12, 2, 26, 43]","The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise. It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named ""structured noise"", and (3) Samples from out-of-dataset classes. The experiments show robustness to these types of noise. 

Review: 
The claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise. This is supported by the literature on ""data cleaning"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.  While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy.

It would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality). The paper did not get a chance to test these differences, and I can only raise a few hypotheses. First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle. This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise. Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general. 

Without such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers. 

Other specific comments: 
-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search. For example, for Conv4, how many channels at each layer? how was the net initialized? which hyper parameters were tuned and with which values? were hyper parameters tuned on a separate validation set? How was the train/val/test split done, etc. These details are useful for judging technical correctness.
-- Section 4, importance of large datasets. The recent paper by Chen et al (2017) would be relevant here.
-- Figure 8 failed to show for me. 
-- Figure 9,10, need to specify which noise model was used.





","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks,"['Nan Rosemary Ke', 'Anirudh Goyal', 'Olexa Bilaniuk', 'Jonathan Binas', 'Laurent Charlin', 'Chris Pal', 'Yoshua Bengio']",Reject,2018,"[8, 5, 5, 7, 13, 22, 31]","[13, 10, 10, 11, 18, 27, 36]","[61, 101, 21, 29, 79, 230, 975]","[22, 40, 10, 12, 35, 98, 405]","[38, 60, 10, 16, 41, 109, 454]","[1, 1, 1, 1, 3, 23, 116]","re. Introduction, page 2: Briefly explain here how SAB is different from regular Attention?

Good paper. There's not that much discussion of the proposed SAB compared to regular Attention, perhaps that could be expanded. Also, I suggest summarizing the experimental findings in the Conclusion.","[8, 5, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Parametrizing filters of a CNN with a GAN,"['Yannic Kilcher', 'Gary Becigneul', 'Thomas Hofmann']",Reject,2018,"[3, 2, 26]","[8, 5, 31]","[20, 24, 205]","[6, 11, 112]","[13, 13, 69]","[1, 0, 24]","Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane. This paper proposes a method to uncover this structure from the filters of a trained ConvNet. The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters. The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter.

The idea of empirically studying the manifold / topological / group structure in the space of filters is interesting. A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well.

My main concerns are:

Controls
To generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced. To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (""x-space"" of the GAN), or varied the magnitude of filters or filter planes. Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines.

Lack of new Insights
The visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than ""something non-trivial is going on in these networks"". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images). In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet.

Interpretation
This is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model. A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers. The presented method does not consider this and hence does not analyze invariances.

Minor issues:
- In the last equation on page 2, the right-hand side is missing a ""min max"".","[4, 4, 2]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Strong rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","['Lukas Balles', 'Philipp Hennig']",Reject,2018,"[3, 9]","[8, 14]","[17, 165]","[6, 66]","[11, 84]","[0, 15]","Stochastic Sign Descent (SSD) and Stochastic Variance Adapted Gradient (SVAG) are inspired by ADAM and studied in this paper, together with momentum terms. 

Analysis showed that SSD should work better than usual SGD when the Hessian of training loss is highly diagonal dominant.  It is intrigued to observe that for MNIST and CIFAR10, SSD with momentum champions with better efficiency than ADAM, SGD and SVAG, while on the other hand, in CIFAR100, momentum-SVAG and SGD beat SSD and ADAM. Does it suggest the Hessians associated with MNIST and CIFAR10 training loss more diagonally dominant? 

There are other adaptive step-sizes such as Barzilai-Borwein (BB) Step Sizes introduced to machine learning by Tan et al. NIPS 2016. Is there any connections between variance adaptation here and BB step size? ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unbiased scalable softmax optimization,"['Francois Fagan', 'Garud Iyengar']",Reject,2018,"[6, 21]","[10, 26]","[11, 74]","[4, 23]","[6, 18]","[1, 33]","The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi-class classification problem (using softmax loss) and with linear model. The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it. At each iteration, SGD samples a subset of training samples and labels. The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach. It seems the implicit SGD approach is better in the experimental comparisons. 

I found the paper quite interesting, but meanwhile I have the following comments and questions: 

- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD. The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch. However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck? Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed. 

- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? Also, I guess we are assuming the obj is strongly convex?

- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data. As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data). 

- All the comparisons are based on ""epochs"", but the competing algorithms are quite different and can have very different running time for each epoch. For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. 

- The claim that ""implicit SGD never overshoots the optimum"" needs more supports. Is it proved in some previous papers? 

- The presentation can be improved. I think it will be helpful to state the algorithms explicitly in the main paper.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Image Segmentation by Iterative Inference from Conditional Score Estimation,"['Adriana Romero', 'Michal Drozdzal', 'Akram Erraqabi', 'Simon Jégou', 'Yoshua Bengio']",Reject,2018,"[8, 9, 3, 3, 31]","[12, 14, 7, 7, 36]","[44, 58, 12, 8, 975]","[20, 21, 6, 2, 405]","[21, 30, 6, 6, 454]","[3, 7, 0, 0, 116]","The paper proposes an image segmentation method which iteratively refines the semantic segmentation mask obtained from a deep net. To this end the authors investigate a denoising auto-encoder (DAE). Its purpose is to provide a semantic segmentation which improves upon its input in terms of the log-likelihood.

More specifically, the authors `propose to condition the autoencoder with an additional input’ (page 1). To this end they use features obtained from the deep net. Instead of training the DAE with ground truth y, the authors found usage of the deep net prediction to yield better results.

The proposed approach is evaluated on the CamVid dataset.

Summary:
——
I think the paper discusses a very interesting topic and presents an elegant approach. A few points are missing which would provide significantly more value to a reader. Specifically, an evaluation on the classical Pascal VOC dataset, details regarding the training protocol of the baseline (which are omitted right now), an assessment regarding stability of the proposed approach (not discussed right now), and a clear focus of the paper on segmentation or conditioning. See comments below for details and other points.

Comments:
——
1. When training the DAE, a combination of squared loss and categorical cross-entropy loss is used. What’s the effect of the squared error loss and would the categorical cross-entropy on its own be sufficient? This question remains open when reading the submission.

2. The proposed approach is evaluated on the CamVid dataset which is used less compared to the standard and larger Pascal VOC dataset. I conjecture that the proposed approach wouldn’t work too well on Pascal VOC. On Pascal VOC, images are distinctly different from each other whereas subsequent frames are similar in CamVid, i.e., the road is always located at the bottom center of the image. The proposed architecture is able to take advantage of this dataset bias, but would fail to do so on Pascal VOC, which has a much more intricate bias. It would be great if the authors could check this hypothesis and report quantitative results similar to Tab. 1 and Fig. 4 for Pascal VOC.

3. The authors mention a grid-search for the stepsize and the number of iterations. What values were selected in the end on the CamVid and hopefully the Pascal VOC dataset?

4. Was the dense CRF applied out of the box, or were its parameters adjusted for good performance on the CamVid validation dataset? While parameters such as the number of iterations and epsilon are tuned for the proposed approach on the CamVid validation set, the submission doesn’t specify whether a similar procedure was performed for the CRF baseline.

5. Fig. 4 seems to indicate that the proposed approach doesn’t converge. Hence an appropriate stepsize and a reasonable number of iterations need to be chosen on a validation set. Choosing those parameters guarantees that the method performs well on average, but individual results could potentially be entirely wrong, particularly if large step sizes are chosen. I suspect this effect to be more pronounced on the Pascal VOC dataset (hence my conjecture in point 2). To further investigate this property, as a reader, I’d be curious to get to know the standard deviation/variance of the accuracy in addition to the mean IoU. Again, it would be great if the authors could check this hypothesis and report those results.

6. I find the experimental section to be slightly disconnected from the initial description. Specifically, the paper `proposes to condition the autoencoder with an additional input’ (page 1). No experiments are conducted to validate this proposal. Hence the main focus of the paper (image segmentation or DAE conditioning) remains vague. If the authors choose to focus on image segmentation, a comparison to state-of-the-art should be provided on classical datasets such as Pascal VOC, if DAE conditioning is the focus, some experiments in this direction should be included in addition to the Pascal VOC results.

Minor comment:
——
- I find it surprising that the authors choose not to cite some related work on combining deep nets with structured prediction.","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS,"['Yoel Zeldes', 'Stavros Theodorakis', 'Efrat Solodnik', 'Aviv Rotman', 'Gil Chamiel', 'Dan Friedman']",Reject,2018,"[6, 10, 2, 2, 11, 29]","[10, 14, 1, 1, 15, 34]","[4, 14, 1, 1, 4, 15]","[1, 8, 0, 0, 2, 6]","[3, 2, 1, 1, 2, 8]","[0, 4, 0, 0, 0, 1]","The paper adresses a very interesting question about the handling of the dynamics of a recommender systems at scale (here for linking to some articles).
The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise. This is equivalent to separate a local estimation error from the noise. 

The idea is interesting but maybe not pushed far enough in the paper:
*At fixed context x, assuming that the error is a function of the average reward u and of the number of displays r of the context could be a constant could be a little bit more supported (this is a variance explanation that could be tested statistically, or the shape of this 2D function f(u,r) could be plot to exhibit its regularity). 
* None of the experiments is done on public data which lead to an impossible to reproduce paper
* The proposed baselines are not really the state of the art (Factorization Machines, GBDT features,...) and the used loss is MSE which is strange in the context of CTR prediction (logistic loss would be a more natural choice)
* I'm not confident with the proposed surrogate metrics. In the paper, the work of Lihong Li &al on offline evaluation on contextual bandits is mentioned and considered as infeasible here because of the renewal of the set of recommendation. Actually this work can be adapted to handle theses situations (possibly requiring to bootstrap if the set is actually regenerating too fast). Also note that Yahoo Research R6a - R6b  datasets where used in ICML'12 Exploration and Exploitation 3 challenge where about pushing some news in a given context and could be reused to support the proposed approach. An other option would be to use some counterfactual estimates (See Leon Bottou &all and Thorsten Joachims &all)
* If the claim is about a better exploration,  I'd like to have an idea of the influence of the tuning parameters and possibly a discussion/comparison over alternatives strategies (including an epsilon-n greedy algorithm)

Besides theses core concerns, the papers suffers of some imprecisions on the notations which should be clarified. 
* As an example using O(1000) and O(1M) in the figure one. Everyone understands what is meant but O notation are made to eliminate constant terms and O(1) = O(1000).
* For eqn (1) it would be better to refer to and ""optimistic strategy"" rather to UCB because the name is already taken by an algorithm which is not this one. Moreover the given strategy would achieve a linear regret if used as described in the paper which is not desirable for bandits algorithms (smallest counter example with two arms following a Bernouilli with different parameters if the best arms generates two zero in a row at the beginning, it is now stuck with a zero mean and zero variance estimate). This is why bandits bounds include a term which increase with the total number of plays. I agree that in practice this effect can be mitigated at that the strategy can be correct in the contextual case (but then I'd like to the dependancies on x to be clear) 
* The papers never mentions whats is a scalar, a vector or a matrix. This creates confusion: as an example eqn (3) can have several different meaning depending if the values are scalars, scalars depending on x or having a diagonal \sigma matrix
* In the paragraph above (2) I unsure of what is a ""binomial noise error distribution"" for epsilon, but a few lines later epsilon becomes a gaussian why not just mention that you assume the presence of a gaussian noise on the parameters of a Bernouilli distribution ? 

 ","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DNN Feature Map Compression using Learned Representation over GF(2),"['Denis A. Gudovskiy', 'Alec Hodgkinson', 'Luca Rigazio']",Reject,2018,"[6, 1, 21]","[11, 3, 25]","[19, 6, 35]","[7, 2, 23]","[10, 4, 8]","[2, 0, 4]","In order to compress DNN intermediate feature maps the authors covert fixed-point activations into vectors over the smallest finite field, the Galois field of two elements (GF(2)) and use nonlinear dimentionality reduction layers.

The paper reads well and the methods and experiments are generally described in sufficient detail.

My main concern with this paper and approach is the performance achieved. According to Table 1 and Table 2 there is a small accuracy benefit from using the proposed approach over the ""quantized"" SqueezeNet baseline. If I am weighing in the need to alter the network for the proposed approach in comparison with the ""quantized"" setting then, from practical point of view, I would prefer the later ""quantized"" approach.
","[5, 7, 4]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"['Balázs Hidasi', 'Alexandros Karatzoglou']",Reject,2018,"[8, 16]","[12, 21]","[28, 90]","[18, 61]","[6, 25]","[4, 4]","This paper presents a few modifications on top of some earlier work (GRU4Rec, Hidasi et al. 2016) for session-based recommendation using RNN. The first one is to include additional negative samples based on popularity raised to some power between 0 and 1. The second one is to mitigate the vanishing gradient problem for pairwise ranking loss, especially with the increased number of negative samples from the first modification. The basic idea is to weight all the negative examples by their “relevance”, since for the irrelevant negatives the gradients are vanishingly small. Experimentally these modifications prove to be effective compared with the original GRU4Rec paper. 

The writing could have been more clear, especially in terms of notations and definitions. I found myself sometimes having to infer the missing bits. For example, in Eq (4) and (5), and many that follow, the index i and j are not defined (I can infer it from the later part), as well as N_s (which I take it as the number of negative examples). This is just one example, but I hope the authors could carefully check the paper and make sure all the notations/terminologies are properly defined or referred with a citation when first introduced (e.g., pointwise, pairwise, and listwise loss functions). I consider myself very familiar with the RecSys literature, and yet sometimes I cannot follow the paper very well, not to mention the general ICLR audience. 

Regarding the two main modifications, I found the negative sampling rather trivial (and I am surprised in Hidasi et al. (2016) the negatives are only from the same batch, which seems a huge computational compromise) with many existing work on related topic: Steck (Item popularity and recommendation accuracy, 2011) used the same “popularity to the power between 0 and 1” strategy (they weighted the positive by the inverse popularity to the power). More closely, the negative sampling distribution in word2vec is in fact a unigram raised to the power of 0.75, which is the same as the proposed strategy here. As for the gradient vanishing problem for pairwise ranking loss, it has been previously observed in Rendle & Freudenthaler (Improving Pairwise Learning for Item Recommendation from Implicit Feedback, 2014) for BPR and they proposed an adaptive negative sampling strategy (trying to sample more relevant negatives while still keeping the computational cost low), which is closely related to the ranking-max loss function proposed in this paper. Overall, I don’t think this paper adds much on top of the previous work, and I think a more RecSys-oriented venue might benefit more from the insights presented in this paper.   

I also have some high-level comments regarding using RNN for session-based recommendation (this was also my initial reaction after reading Hidasi et al. 2016). As mentioned in this paper, when applying RNN on RecSys datasets with longer time-span (which means there can be more temporal dynamics in users’ preference and item popularity), the results are not striking (e.g., Wu et al. 2017) with the proposed methods barely outperforming standard matrix factorization methods. It is puzzling that how RNN can work better for session-based case where a user’s preference can hardly change within such a short period of time. I wonder how a simple matrix factorization approach would work for session-based recommendation (which is an important baseline that is missing): regarding the claim that MF is not suited for session-based because of the absence of the concept of a user, each session can simply be considered as a pseudo-user and approaches like asymmetric matrix factorization (Paterek 2007, Improving regularized singular value decomposition for collaborative filtering) can even eliminate the need for learning user factors. ItemKNN is a pretty weak baseline and I wonder if a scalable version of the SLIM (Ning & Karypis 2011, SLIM: Sparse Linear Methods for Top-N Recommender Systems) would give better results. Finally, my general experience with BPR-type of pairwise ranking loss is that it is good at optimizing AUC, but not very well-suited for head-heavy metrics (MRR, Recall, etc.) I wonder how the propose loss would perform comparing with more competitive baselines. 

Regarding the page limit, given currently the paper is quite long (12 pages excluding references), I suggest the authors cutting down some space. For example, the part about fixing the cross entropy is not very relevant and can totally be put in the appendix. 

Minor comment:

1. Section 3.3.1, “Part of the reasons lies in the rare occurrence…”, should r_j >> r_i be the other way around?
","[4, 8, 6]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Graph Classification with 2D Convolutional Neural Networks,"['Antoine J.-P. Tixier', 'Giannis Nikolentzos', 'Polykarpos Meladianos', 'Michalis Vazirgiannis']",Reject,2018,"[3, 4, 4, 26]","[8, 9, 5, 31]","[30, 55, 19, 316]","[14, 28, 14, 181]","[16, 21, 5, 77]","[0, 6, 0, 58]","The paper introduces a method for learning graph representations (i.e., vector representations for graphs). An existing node embedding method is used to learn vector representations for the nodes. The node embeddings are then projected into a 2-dimensional space by PCA. The 2-dimensional space is binned using an imposed grid structure. The value for a bin is the (normalized) number of nodes falling into the corresponding region. 

The idea is simple and easily explained in a few minutes. That is an advantage. Also, the experimental results look quite promising. It seems that the methods outperforms existing methods for learning graph representations. 

The problem with the approach is that it is very ad-hoc. There are several (existing) ideas of how to combine node representations into a representation for the entire graph. For instance, averaging the node embeddings is something that has shown promising results in previous work. Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing, it is especially important to compare your method more thoroughly to simpler methods. Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. 

The experimental results are also not explained thoroughly enough. For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps. How many times did you run node2vec on each graph? 

","[4, 7, 3]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Clear rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Optimizing the Latent Space of Generative Networks,"['Piotr Bojanowski', 'Armand Joulin', 'David Lopez-Paz', 'Arthur Szlam']",Reject,2018,"[6, 9, 8, 10]","[11, 14, 13, 15]","[75, 130, 60, 138]","[28, 59, 28, 54]","[43, 67, 27, 74]","[4, 4, 5, 10]","In this paper, the authors propose a new architecture for generative neural networks. Rather than the typical adversarial training procedure used to train a generator and a discriminator, the authors train a generator only. To ensure that noise vectors get mapped to images from the target distribution, the generator is trained to map noise vectors to the set of training images as closely as possible. Both the parameters of the generator and the noise vectors themselves are optimized during training. 

Overall, I think this paper is useful. The images generated by the model are not (qualitatively and in my opinion) as high quality as extremely recent work on GANs, but do appear to be better than those produced by DCGANs. More importantly than the images produced, however, is the novel training procedure. For all of their positive attributes, the adversarial training procedure for GANs is well known to be fairly difficult to deal with. As a result, the insight that if a mapping from noise vectors to training images is learned directly, other noise images still result in natural images is interesting.

However, I do have a few questions for the authors, mostly centered around the choice of noise vectors.

In the paper, you mention that you ""initialize the z by either sampling them from a Gaussian distribution or by taking the whitened PCA of the raw image pixels."" What does this mean? Do you sample them from a Gaussian on some tasks, and use PCA on others? Is it fair to assume from this that the initialization of z during training matters? If so, why?

After training, you mention that you fit a full Gaussian to the noise vectors learned during training and sample from this to generate new images. I would be interested in seeing some study of the noise vectors learned during training. Are they multimodal, or is a unimodal distribution indeed sufficient? Does a Gaussian do a good job (in terms of likelihood) of fitting the noise vectors, or would some other model (even something like kernel density estimation) allow for higher probability noise vectors (and therefore potentially higher quality images) to be drawn? Does the choice of distribution even matter, or do you think uniform random vectors from the space would produce acceptable images?","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Automatic Goal Generation for Reinforcement Learning Agents,"['David Held', 'Xinyang Geng', 'Carlos Florensa', 'Pieter Abbeel']",Reject,2018,"[2, 15, 3, 17]","[5, 20, 8, 22]","[18, 116, 20, 608]","[8, 49, 6, 291]","[9, 60, 13, 291]","[1, 7, 1, 26]","This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN.  In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies’ training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below.

1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution.

2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance.

3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is “A policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area”. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method.

4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? 

5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough.

6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough?

7. Minor comments
Achieve tasks -> achieve goals or accomplish/solve tasks
A variation of to -> variation of 
Allows a policy to quickly learn to reach …-> allow an agent to be quickly learn a policy to reach…
…the difficulty of the generated goals -> … the difficulty of reaching
","[6, 4, 8]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Training RNNs as Fast as CNNs,"['Tao Lei', 'Yu Zhang', 'Yoav Artzi']",Reject,2018,"[9, 6, 8]","[14, 11, 13]","[179, 174, 85]","[68, 90, 42]","[15, 81, 39]","[96, 3, 4]","The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. 

In essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.

Pros:
- Fairly well presented
- Wide range of experiments, despite underwhelming absolute results

Cons:
- Quasi-RNNs are almost identical and already have results on small-scale tasks.
- Slightly unfortunate naming that does not account for autoregressive CNNs
- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.
- I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline.

I recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented.

","[4, 8, 7]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neural Networks with Block Diagonal Inner Product Layers,"['Amy Nesky', 'Quentin Stout']",Reject,2018,"[1, 39]","[4, 43]","[4, 84]","[2, 38]","[1, 8]","[1, 38]","This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal. The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning. As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large. However, the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject, which somewhat lessens the strength of their argument.

Quality: this paper is of good quality
Clarity: this paper is clear, but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text.
Originality: this paper introduces block diagonal matrices to structure the weights of a neural network. The idea of structured matrices in this context is not new, but the diagonal block structure appears to be.  
Significance: This paper is somewhat significant.

PROS 
- A new approach to analyzing the behavior of weight matrices during learning
- A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.

CONS
- Some of the figures are hard to read (in particular Fig 1 & 2 left) and would benefit from a better layout.
- It would be valuable to see experiments on bigger datasets than only MNIST and CIFAR-10. 
- I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture.

Main questions:
- Could you briefly comment on the training time in section 4.1? 
- Could you elaborate on the last sentence of section 4.1?
- You state: ""singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations."" Is this a known fact, or something that you observed empirically? In practice, how large must the weight matrix be to observe this behavior?

Nitpicks:
- I believe the term ""fully connected"" is more standard than ""inner product"" and would add clarity to the paper, but I may be mistaken. ","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,"['Xiao Li', 'Yao Ma', 'Calin Belta']",Reject,2018,"[3, 11, 18]","[5, 16, 23]","[11, 96, 341]","[3, 37, 174]","[6, 18, 103]","[2, 41, 64]","This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition.  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho.  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy.  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.

The main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.  The robustness term rho is essential, but it is not defined.  I was also confused by the notation $D_\phi^q$, which was described but not defined.  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals. is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?  In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,"['Stephanie Hyland', 'Cristóbal Esteban', 'Gunnar Rätsch']",Reject,2018,"[4, 4, 22]","[5, 9, 27]","[11, 28, 179]","[5, 10, 82]","[5, 18, 41]","[1, 0, 56]","In this paper, the authors propose a recurrent GAN architecture that generates continuous domain sequences. To accomplish this, they use a generator LSTM that takes in a sequence of random noise as well as a sequence of conditonal information and outputs a sequence. The discriminator LSTM takes a sequence (and conditional information) as input and classifies each element of the sequence as real or synthetic -- the entire sequence is then classified by vote. The authors evaluate on several synthetic tasks, as well as an ICU timeseries data task.

Overall, I thought the paper was clearly written and extremely easy to follow. To the best of my knowledge, the method proposed by the authors is novel, and differs from traditional sentence generation (as an example) models because it is intended to produce continuous domain outputs. Furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion.

My most important piece of feedback is that I think it would be useful to include a few examples of the eICU time series data, both real and synthetic. This might give a better sense of: (1) how difficult the task is, (2) how much variation there is in the real data from patient to patient, and (3) how much variation we see in the synthetic time series. Are the synthetic time series clearly multimodal, or do they display some of the mode collapse behavior occasionally seen in GANs?

I would additionally like to see a few examples of the time series data at both the 5 minute granularity and the 15 minute granularity. You claim that downsampling the data to 15 minute time steps still captures the relevant dynamics of the data -- is it obvious from the data that variations in the measured variables are not significant over a 5 minute interval? As it stands, this is somewhat an unknown, and should be easy enough to demonstrate.","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Discrete Autoencoders for Sequence Models,"['Lukasz Kaiser', 'Samy Bengio']",Reject,2018,"[14, 32]","[18, 37]","[84, 261]","[44, 155]","[32, 60]","[8, 46]","This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder. 
However, the experiments are too weak to demonstrate the effectiveness of using discrete representations.
The design of the experiments on language model is problematic.
There are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper. 


Generally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations. 
However, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart. 
The proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people. 
How is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model?
The qualitative evaluation on 'Deciperhing the Latent Code' is not enough either. 
In addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively.
One suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case.

Overall, this paper is more suitable for the workshop track. It also needs a lot of more studies on related work.","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 1, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation,"['Yi-Lin Tuan', 'Hung-yi Lee']",Reject,2018,"[1, 10]","[5, 15]","[29, 383]","[11, 166]","[17, 195]","[1, 22]","Quality: The paper proposes a direct improvement over SeqGAN by Yu. et. al. (2017). My assessment is partially determined by comparing this paper to Yu et. al (2017). In my opinion, this paper is lacking in quality in comparison to Yu et. al (2017). In particular, Yu et. al. (2017) provides detailed derivation of the policy gradient accompanied by a pseudo-code (algorithm) on how one can implement SeqGAN. On the contrary, this paper does not provide such details. Perhaps, all of the details of SeqGAN follows immediately, but the paper should not assume that all readers will be familiar with SeqGAN. 

Clarity: 

1. The paper provides a review of related methods on conditional sequence generation in Section 3. However, it is very brief and as a non-expert in this field, I needed to refer to the original papers anyways. Perhaps, the review of the related methods can go to the Appendix and this space can be better utilized to expand on the original contributions made by the paper. 
2. MCMC (Markov chain Monte Carlo) is mentioned in 4.1 but it is not explained. 
3. Figure 1 is not sufficiently explained; neither in text nor in the figure caption. It would help greatly to describe the details of the network architecture shown in this figure.

Originality: The paper proposes a generalization of SeqGAN; however, in my opinion, the methodological contribution appears to be only incremental on SeqGAN. 

Significance: The paper's significance may be evaluated in terms of its impact on applications as it proposes an improvement over the previous work of SeqGAN. However, the extent to which the evaluation is carried out is somewhat unsatisfactory with only one real application. Also, the applications considered in the experiments are primarily on dialogue generation. My initial impression is that the methodology lacks generality and may perhaps cater better to domain specific publication venues. 
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Maintaining cooperation in complex social dilemmas using deep reinforcement learning,"['Alexander Peysakhovich', 'Adam Lerer']",Reject,2018,"[10, 5]","[14, 9]","[55, 49]","[26, 21]","[27, 23]","[2, 5]","This paper addresses multiagent learning problems in which there is a social dilemma: settings where there are no 'cooperative polices' that form an equilibrium. The paper proposes a way of dealing with these problems via amTFT, a variation of the well-known tit-for-that strategy, and presents some empirical results.

My main problem with this paper is clarity and I am afraid that not everything might be technically correct. Let me just list my main concerns in the below.

The definition of social dilemma, is unclear:
""A social dilemma is a game where there are no cooperative policies which form equilibria. In other
words, if one player commits to play a cooperative policy at every state, there is a way for their
partner to exploit them and earn higher rewards at their expense.""
does this mean to say ""there are no cooperative *Markov* policies"" ? It seems to me that the paper precisely intents to show that by resorting to history-dependent policies (such as both using amTFT), there is a cooperative equilibrium. 

I don't understand:
""Note that in a social dilemma there may be policies which achieve the payoffs of cooperative policies because they cooperate on the trajectory of play and prevent exploitation by threatening non-cooperation on states which are never reached by the trajectory. If such policies exist, we call the social dilemma solvable.""
is this now talking about non-Markov policies? If not, there seems to be a contradiction?

The work focuses on TFT-like policies, motivated by 
""if one can commit to them, create incentives for a partner to behave cooperatively""
however it seems that, as made clear below definition 4, we can only create such incentives for sufficiently powerful agents, that remember and learn from their failures to cooperate in the past?

Why is the method called ""approximate Markov""? As soon as one introduces history dependence, the Markov property stops to hold?

On page 4, I have problems following the text due to inconsistent use of notation: subscripts and superscripts seem random, it is not clear which symbols denote strategy profiles (rather than individual strategies), there seems mix-ups between 'i' and '1' / '2', there is sudden use of \hat{}, and other undefined symbols (Q_CC?).

For all practical purposes, it seems that the made assumptions imply uniqueness of the cooperative joint strategy. I fully appreciate that the coordination question is difficult and important, so if the proposed method is not compatible with dealing with that important question, that strikes me as a large drawback.

I have problems understanding how it is possible to guarantee ""If they start in a D phase, they eventually return to a C phase."" without making more assumptions on the domain. The clear example being the typical 'heaven or hell' type of problems: what if after one defect, we are trapped in the 'hell' state where no cooperation is even possible? 

""If policies converge with this training then πˆ is a Markov equilibrium (up to function approximation)."" There are two problems here:
1) A problem is that very typically things will not converge... E.g., 
Wunder, Michael, Michael L. Littman, and Monica Babes. ""Classes of multiagent q-learning dynamics with epsilon-greedy exploration."" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.
2) ""Up to function approximation"" could be arbitrary large?


Another significant problem seems to be with this statement:
""while in the cooperative reward schedule the standard RL convergence guarantees apply. The latter is because cooperative training is equivalent to one super-agent controlling both players and trying to optimize for a single scalar reward."" The training of individual learners is quite different from ""joint action learners"" [Claus & Boutilier 98], and this in turn is different from a 'super-agent' which would also control the exploration. In absence of the super-agent, I believe that the only guarantee is that one will, in the limit, converge to a Nash equilibrum, which might be arbitrary far from the optimal joint policy. And this only holds for the tabular case. See the discussion in 
A concise introduction to multiagent systems and distributed artificial intelligence. N Vlassis. Synthesis Lectures on Artificial Intelligence and Machine Learning 1 (1), 1-71

Also, the approach used in the experiments ""Cooperative (self play with both agents receiving sum of rewards) training for both games"", would be insufficient for many settings where a cooperative joint policy would be asymmetric.

The entire approach hinges on using rollouts (the commented lines in Algo. 1). However, it is completely not clear to me how this works. The one paragraph is insufficient to get across these crucial parts of the proposed approach.

It is not clear why the tables in Figure 1 are not symmetric; this strikes me as extremely problematic. It is not clear what the colors encode either.

It also seems that ""grim"" is better against all, except against amTFT, why should we not use that? In general, the explanation of this closely related paper by De Cote & Littman (which was published at UAI'08), is insufficient. It is not quite clear to me what the proposed approach offers over the previous method.









","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks,"['Yau-Shian Wang', 'Hung-Yi Lee']",Reject,2018,"[2, 10]","[7, 15]","[18, 383]","[6, 166]","[12, 195]","[0, 22]","This paper proposes a model for generating long text strings given shorter text strings, and for inferring suitable short text strings given longer strings. Intuitively, the inference step acts as a sort of abstractive summarization. The general gist of this paper is to take the idea from ""Language as a Latent Variable"" by Miao et al., and then change it from a VAE to an adversarial autoencoder. The authors should cite ""Adversarial Autoencoders"" by Makzhani et al. (ICLR 2016).

The experiment details are a bit murky, and seem to involve many ad-hoc decisions regarding preprocessing and dataset management. The vocabulary is surprisingly small. The reconstruction cost is not precisely explained, though I assume it's a teacher-forced conditional log-likelihood (conditioned on the ""summary"" sequence). The description of baselines for REINFORCE is a bit strange -- e.g., annealing a constant in the baseline may affect variance of the gradient estimator, but the estimator is still unbiased and shouldn't significantly impact exploration. Similar issues are present in the ""Self-critical..."" paper by Rennie et al. though, so this point isn't a big deal.

The results look decent, but I would be more impressed if the authors could show some benefit relative to the supervised model, e.g. in a reasonable semisupervised setting. Overall, the paper covers an interesting topic but could use extra editing to clarify details of the model and training procedure, and could use some redesign of the experiments to minimize the number of arbitrary (or arbitrary-seeming) decisions.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learnability of Learned Neural Networks,"['Rahul Anand Sharma', 'Navin Goyal', 'Monojit Choudhury', 'Praneeth Netrapalli']",Reject,2018,"[14, 6, 34, 3, 14]","[19, 11, 39, 6, 18]","[53, 25, 332, 4, 16]","[20, 8, 150, 1, 6]","[5, 4, 8, 1, 2]","[28, 13, 174, 2, 8]","Summary:
This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of ""learnability"" --- the learnability of a model (N1) is defined as the ""expected agreement"" between the output of N1, and the output of another model N2 which has been trained to match N1 (on a dataset of size n).  The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is --- furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model.

The paper presents a number of interesting results:
1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler).
2) Networks trained with random data are significantly less learnable than networks trained on real data.
3) Networks trained on small mini-batches (larger variance SGD updates) are more learnable than those trained on large minibatches.

These results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; these results at first seem to run counter to the ideas from statistical learning theory that models with high capacity (VC dimension, radamacher complexity, etc.) have much weaker generalization guarantees than lower capacity models.  These results suggest that models that have high capacity (by one definition) are also capable of being simple (by another definition).  These results nicely complement the work which studies the ""sharpness/curvature"" of the local minima found by neural networks, which argue that the minima which generalize better are those with lower curvature.

Review:
Quality:  I found this to be high quality work. The paper presents many results across a variety of network architectures.  One area for improvement is presenting results on larger datasets (currently all experiments are on CIFAR-10), and/or on non-convolutional architectures.  Additionally, a discussion of why learnabiblity might imply low generalization error would have been interesting (the more formal, the better), though it is unclear how difficult this would be.

Clarity:  The paper is written clearly.  A small point: Step 2 in section 3.1 should specify that argmax of N1(D2) is used to generate labels for the training of the second network.  Also, what dataset D_i is used for tables 3-6? Please specify.

Originality: The specific questions tackled in this paper are original (learnability on random vs. real data, large vs. small networks, and large vs. small mini-batch training).  But it is unclear to me exactly how original this use of ""learnability"" is in evaluating how simple a model is.  It seems to me that this particular use of ""learnability"" is original, even though PAC learnability was defined a while ago.

Significance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize.  I believe it is important to find new ways of formally defining the ""simplicity/capacity"" of a model, such that ""simpler"" models can be proven to have smaller generalization gap (between train and test error) relative to more ""complicated"" models. It is clear that VC dimension and radamacher complexity alone are not enough to explain the generalization performance of neural networks, and that neural networks with high capacity by these definitions are likely ""simple"" by other definitions (as we have seen in this paper).  This paper makes an important contribution to this conversation, and could perhaps provide a starting point for theoreticians to better explain why deep networks generalize well.

Pros
- nice experiments, with very interesting results.
- Helps explain one way in which large networks are in fact ""simple""

Cons
- The paper does not attempt to relate the notion of learnability to that of generalization performance.  All it says is that these two metrics appear to be well correlated.","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
One-shot and few-shot learning of word embeddings,"['Andrew Kyle Lampinen', 'James Lloyd McClelland']",Reject,2018,"[2, 34]","[7, 38]","[36, 74]","[12, 35]","[23, 15]","[1, 24]","I am highly sympathetic to the goals of this paper, and the authors do a good job of contrasting human learning with current deep learning systems, arguing that the lack of a mechanism for few-shot learning in such systems is a barrier to applying them in realistic scenarios. However, the main evaluation only considers four words - ""bonuses"", ""explained"", ""marketers"", ""strategist"" - with no explanation of how these words were chosen. Can I really draw any meaningful conclusions from such an experimental setup? Even the authors acknowledge, in footnote 1, that, for one of the tests, getting lower perplexity in three out of the four casess ""may just be chance variation, of course"". I wonder why we can't arrive at a similar conclusion for the other results in the paper. At the very least I need convincing that this is a reasonable experimental paradigm.

I don't understand the first method for initializing the word embeddings. How can we use the ""current"" embedding for a word if it's never been seen before? What does ""current"" mean in this context?

I also didn't understand the Latin square setup. Training on ten different permutations of the ten sentences suggests that all ten sentences are being used, so I don't see how this can lead to a few-shot or one-shot scenario.

","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unseen Class Discovery in Open-world Classification,"['Lei Shu', 'Hu Xu', 'Bing Liu']",Reject,2018,"[6, 8, 31]","[11, 13, 36]","[53, 67, 350]","[24, 29, 235]","[29, 38, 67]","[0, 0, 48]","This paper concerns open-world classification.  The open-world related tasks have been defined in many previous works. This paper had made a good survey. 
The only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes.  Unfortunately, this paper is lack of novelty. 

Firstly, the problem context and setting is kinda synthesized. I cannot quite imagine in what kind of applications we can get “a set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class”.

Secondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models. I do not really see enough novelty here.

Thirdly, the experiments are only on the MNIST and EMNIST; still not quite sure any real-world problems/datasets can be used to validate this approach.
I also cannot see the promising performance. The clustering results of rejected
examples are still far from the ground truth, and comparing the result with
a total unsupervised K-means is a kind of unreasonable.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Asymmetric Multi-task Feature Learning,"['Hae Beom Lee', 'Eunho Yang', 'Sung Ju Hwang']",Reject,2018,"[2, 13, 9]","[6, 18, 14]","[26, 122, 207]","[11, 68, 95]","[15, 49, 108]","[0, 5, 4]","Summary: The paper proposes a multi-task feature learning framework with a focus on avoiding negative transfer. The objective has two kinds of terms to minimise: (1) The reweighed per-task loss, and (2) Regularisation. The new contribution is an asymmetric reconstruction error in the regularisation term, and one parameter matrix in the regulariser influences the reweighing of the pre-task loss. 

Strength: 
The method has some contribution in dealing with negative transfer. The experimental results are positive.
Weakness:
Several issues in terms of concept, methodology, experiments and analysis.

Details:
1. Overall conceptual issues.
1.1. Unclear motivation re prior work. The proposed approach is motivated by the claim that GO-MTL style models assumes symmetric transfer where bad tasks can hurt good tasks. This assertion seems flawed. The point of grouping/overlap in “GO”-MTL is that a “noisy”, “hard”, or “unrelated"" task can just take its own latent predictor that is disjoint from the pool of predictors shared by the good/related tasks. 
Correspondingly, Fig 2 seems over-contrived. A good GO-MTL solution would assign the noisy task $w_3$ its own latent basis, and let the two good tasks share the other two latent bases. 

1.2  Very unclear intuition of the algorithm. In the AMTFL, task asymmetry is driven by the per-task loss. The paper claims this is because transfer must go from easy=>hard to avoid negative transfer. But this logic relies on several questionable assumptions surrounding conflating the distinct issues of difficulty and relatedness: (i) There could be several easy tasks that are totally un-related. One could construct synthetic examples with data that are trivially separable (easy) but require unrelated or orthogonal classifiers. (ii) A task could appear to be “easy"" just by severe overfitting, and therefore still be detrimental to transfer despite low loss. (iii) A task could be very ""difficult"" in the sense of high loss, but it could still be perfectly learned in the sense of finding the ideal ""ground-truth” classifier, but for a dataset that is highly non-separable in the provided feature-space. Such a perfectly learned classifier may still be useful to transfer despite high loss. (iv) Analogous to point (i), there could be several “difficult” tasks that are indeed related and should share knowledge. (Since difficult/high loss != badly learned as mentioned before). Overall there are lots of holes in the intuitive justification of the algorithm.

2. Somewhat incremental method. 
3.1 It’s a combination of AMTL (Lee 2016) and vanilla auto encoder. 

3. Methodology issues: 
3.1 Most of the explanation (Sec 3-3.1) is given re: Matrix B in Eq.(4) (AMTL method’s objective function). However the final proposed model uses matrix A in Eq.(6) for the same purpose of measuring the amount of outgoing transfers from task $t$ to all other tasks. However in the reconstruction loss, they work in very different ways: matrix B is for the reconstruction of model parameters, while matrix A is for the reconstruction of latent features. This is a big change of paradigm without adequate explanation. Why is it still a valid approach?
3.2 Matrix B in the original paper of AMTL (Eq.(1) of Lee et al., 2016) has a constraint $B \geq 0$, should matrix A have the same constraint? If not, why?
3.3 Question Re: the |W-WB| type assumption for task relatedness. A bad task could learn an all-zero vector of outgoing related ness $b^0_t$ so it doesn’t directly influence other tasks in feed-forward sense. But hat about during training? Does training one task’s weights endup influencing other tasks’s weights via backprop? If a bad task is defined in terms of incoming relatedness from good tasks, then tuning the bad task with backprop will eventually also update the good tasks? (presumably detrimentally).

4. Experimental Results not very strong.
4.1 Tab 1: Neural Network NN and MT-NN beat the conventional shallow MTL approaches decisively for AWA and MNIST.  The difference between MT-NN and AMTFL is not significant. The performance boost is more likely due to using NNs rather than the proposed MTL module. For School, there is not significant difference between the methods. For ImageNet-Room AMTL and AMTFL have overlapping errors. Also, a variant of AMTL (AMTL-imbalance) was reported in Lee’2016, but not here where the number is $40\pm1.71$. 
4.2 Tab 2: The “real” experiments are missing state of the art competitors. Besides a deep GO-MTL alternative, which should be a minimum,  there are lots of deep MTL state of the art: Misra CVPR’16 , Yang ICLR’17, Long arXiv/NIPS’17 Multilinear Relationship Nets,  Ruder arXiv’17 Sluice Nets, etc.

5. Analysis
5.1 The proposed method revolves around the notion of “noisy”/“unrelated”/“difficult” tasks. Although the paper conflates them, it may still be a useful algorithm in practice. But it in this case it should devise much better analysis to provide insight and convince us that this is not a fatal oversimplification: What is the discovered relatedness matrix in some benchmarks? Does the discovered relatedness reflect expert knowledge where this is available? Is there a statistically significant correlation between relatedness and task difficulty in practice? Or between relatedness and degree of benefit from transfer, etc? But this is hard to do cleanly as even if the results show a correlation between difficulty and relatedness, it may just be because that’s how relatedness is defined in the proposed algorithm.
","[3, 5, 6]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to Infer Graphics Programs from Hand-Drawn Images,"['Kevin Ellis', 'Daniel Ritchie', 'Armando Solar-Lezama', 'Joshua B. Tenenbaum']",Reject,2018,"[5, 10, 14, 25]","[10, 15, 19, 30]","[39, 65, 157, 610]","[20, 22, 81, 353]","[14, 28, 49, 226]","[5, 15, 27, 31]","I think the idea of inferring programmatic descriptions of handwritten diagrams is really cool, and that the combination of SMC-based inference with constraint-based synthesis is nice. I also think the application is clearly useful – one could imagine that this type of technology would eventually become part of drawing / note-taking applications.

That said, based on the current state of the manuscript, I find it difficult to recommend acceptance. I understand that the ICLR does not strictly have a page limit, but I think submitting a manuscript of over 11 pages is taking things a bit too far. The manuscript would greatly benefit from a thorough editing pass and some judicious reconsideration of space allocated to figures. Moreover, despite its relative verbosity, or perhaps because of it, I found it surprisingly difficult to extract simple implementation details from the text (for example I had to dig up the size of the synthetic training corpus from the 44-page appendix). 

Presentation issues aside, I think this is great work. There is a lot here, and I am sympathetic to the challenges of explaining everything clearly in a single (short) paper. That said, I do think that the authors need to take another stab at this to get the manuscript to a point where it can be impactful. 

Minor Comments 

- I don't understand what the ""hypothesis"" is in the trace hypothesis. Breaking down the problem into an AIR-style sequential detection task and a program induction is certainly a reasonable thing to do. However, the word ""hypothesis"" is generally used to refer to a testable explanation of a phenomenon, which is not really applicable here. 

- How is the edit distance defined? In particular, are we treating the drawing commands as a set or a sequence when we calculate ""the number of drawing commands by which two trace sets differ""?

- I took me a while to understand that the authors first consider the case of SMC for synthetic images with a pixel-based likelihood, and then move on to SMC with and edit-distance based surrogate likelihood for hand-drawn pictures. The text seems to suggest that only 100 of such hand drawn images were actually used, is that correct?
 
- What does the (+) operator do in Figure 3?

- I am not sure that ""correcting errors made by the neural network"" is the most accurate way to describe a reranking of the top-k samples returned by the SMC sweep.

- Table 3 is very nice, but does not need to be a full page. 

- I would recommend that the authors consolidate wrap-around figures into full-width figures. 
","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Continuous-Time Flows for Efficient Inference and Density Estimation,"['Changyou Chen', 'Chunyuan Li', 'Liqun Chen', 'Wenlin Wang', 'Yunchen Pu', 'Lawrence Carin']",Reject,2018,"[11, 8, 15, 16, 7, 22]","[16, 13, 20, 20, 10, 27]","[176, 167, 48, 64, 43, 602]","[87, 71, 16, 33, 26, 306]","[79, 82, 2, 24, 17, 173]","[10, 14, 30, 7, 0, 123]","The authors try to use continuous time generalizations of normalizing flows for improving upon VAE-like models or for standard density estimation problems.

Clarity: the text is mathematically very sloppy / hand-wavy.

1. I do not understand proposition (1). I do not think that the proof is correct (e.g. the generator L needs to be applied to a function -- the notation L(x) does not make too much sense): indeed, in the case when the volatility is zero (or very small), this proposition would imply that any vector field induces a volume preserving transformation, which is indeed false.

2. I do not really see how the sequence of minimization Eq(5) helps in practice. The Wasserstein term is difficult to hand.

3. in Equation (6), I do not really understand what $\log(\bar{\rho})$ is if $\bar{\rho}$ is an empirical distribution. One really needs $\bar{\rho}$ to be a probability density to make sense of that.","[3, 6, 6]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
LatentPoison -- Adversarial Attacks On The Latent Space,"['Antonia Creswell', 'Biswa Sengupta', 'Anil A. Bharath']",Reject,2018,"[3, 21, 14]","[7, 26, 18]","[31, 110, 44]","[6, 48, 6]","[21, 32, 22]","[4, 30, 16]","This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense. 

But let’s assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless. 

The other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.

","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hierarchical Adversarially Learned Inference,"['Mohamed Ishmael Belghazi', 'Sai Rajeswar', 'Olivier Mastropietro', 'Negar Rostamzadeh', 'Jovana Mitrovic', 'Aaron Courville']",Reject,2018,"[4, 5, 3, 7, 3, 18]","[7, 9, 7, 12, 8, 23]","[10, 24, 9, 51, 18, 309]","[4, 11, 2, 22, 6, 135]","[6, 12, 7, 29, 12, 160]","[0, 1, 0, 0, 0, 14]","******
Please note the adjusted review score after revisions and clarifications of the authors. 
The paper was improved significantly but still lacks novelty. For context, multi-layer VAEs also were not published unmodified as follow-up papers since the objective is identical. Also, I would suggest the authors study the modified prior with marginal statistics and other means to understand not just 'that' their model performs better with the extra degree of freedom but also 'how' exactly it does it. The only evaluation is sampling from z1 and z2 for reconstruction which shows that some structure is learned in z2 and the attribute classification task. However, more statistical understanding of the distributions of the extra layers/capacity of the model would be interesting.
******

The authors propose a hierarchical GAN setup, called HALI, where they can learn multiple sets of latent variables.
They utilize this in a deep generative model for image generation and manage to generate good-looking images, faithful reconstructions and good inpainting results.

At the heart of the technique lies the stacking of GANS and the authors claim to be proposing a novel model here.
First, Emily Denton et. al proposed a stacked version of GANs in ""Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks"", which goes uncited here and should be discussed as it was the first work stacking GANs, even if it did so with layer-wise pretraining.
Furthermore, the differences to another very similar work to that of the authors (StackGan by Huan et al) are unclear and not well motivated.
And third, the authors fail to cite 'Adversarial Message Passing' by Karaletsos 2016, which has first introduced joint training of generative models with structure by hierarchical GANs and generalizes the theory to a particular form of inference for structured models with GANs in the loop. 
This cannot be called concurrent work as it has been around for a year and has been seen and discussed at length in the community, but the authors fail to acknowledge that their basic idea of a joint generative model and inference procedure is subsumed there. In addition, the authors also do not offer any novel technical insights compared to that paper and actually fall short in positioning their paper in the broader context of approximate inference for generative models.

Given these failings, this paper has very little novelty and does not perform accurate attribution of credit to the community.
Also, the authors propose particular one-off models and do not generalize this technique to an inference principle that could be reusable.

As to its merits, the authors manage to get a particularly simple instance of a 'deep gan' working for image generation and show the empirical benefits in terms of image generation tasks. 
In addition, they test their method on a semi-supervised task and show good performance, but with a lack of details.

In conclusion, this paper needs to flesh out its contributions on the empirical side and position its exact contributions accordingly and improve the attribution.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,"['Ivo Popov', 'Nicolas Heess', 'Timothy P. Lillicrap', 'Roland Hafner', 'Gabriel Barth-Maron', 'Matej Vecerik', 'Thomas Lampe', 'Tom Erez', 'Yuval Tassa', 'Martin Riedmiller']",Reject,2018,"[8, 10, 11, 16, 5, 3, 6, 12, 14, 26]","[7, 15, 16, 21, 9, 3, 10, 17, 18, 31]","[2, 200, 127, 39, 17, 7, 30, 51, 39, 168]","[1, 81, 47, 15, 5, 2, 14, 23, 22, 100]","[1, 111, 70, 19, 10, 5, 16, 24, 14, 47]","[0, 8, 10, 5, 2, 0, 0, 4, 3, 21]","The authors propose to learn to pick up a block and put it on another block using DDPG. A few tricks are described, which I believe already appear in prior work. The discussion of results presented in prior work also has a number of issues. The claim of ""data efficient"" learning is not really accurate, since even with demonstrations, the method requires substantially more experience than prior methods. Overall, it's hard to discern a clear contribution, either experimentally or conceptually, and the excessive claims in the paper are very off-putting. This would perhaps make a reasonable robotics paper if it had a real-world evaluation and if the claims were scoped more realistically, but as-is, I don't think this work is ready for publication.

More detailed comments:

The two main contributions -- parallel training and asynchrony -- already appear in the Gu et al. paper. In fact, that paper demonstrates learning entirely in the real world, and substantially more efficiently than described in this paper. The authors don't discuss this at all, except a passing mention of Gu et al.

The title is not appropriate for this paper. The method is data-efficient compared to what? The results don't look very data efficient: the reported result is something on the order of 160 robot-hours, and 16 robot-hours with demonstration. That's actually dramatically less efficient than prior methods.

""our results on data efficiency hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots"": Prior work already shows successful stacking policies on real robots, as well as successful pick-and-place policies and a variety of other skills. The funny thing is that many of these papers are actually cited by the authors, but they simply pretend that those works don't exist when discussing the results.

""We assess the feasibility of performing analogous experiments on real robotics hardware"": I assume this is a typo, but the paper does not actually contain any real robotics hardware experiments.

""To our knowledge our results provide the first demonstration of end-to-end learning for a complex manipulation problem involving multiple freely moving objects"": This was demonstrated by Finn et al. in ""Deep Spatial Autoencoders for Visuomotor Learning,"" with training times that are a tiny fraction of those reported in this paper, and using raw images and real hardware.

""both rely on access to a well defined and fully observed state space"": This is not true of the Finn et al. paper mentioned above.","[2, 4, 3]","[' Strong rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Regularization for Deep Learning: A Taxonomy,"['Jan Kukačka', 'Vladimir Golkov', 'Daniel Cremers']",Reject,2018,"[2, 4, 19]","[5, 9, 24]","[7, 20, 602]","[2, 6, 327]","[4, 13, 186]","[1, 1, 89]","The main aim of ICLR conference, at least as it is written on its website, is to provide new results on theories, methods and algorithms, supporting further breakthroughs in AI and DL.

In this respect the authors of the paper claim that their “systematic approach enables the discovery of new, improved regularization methods by combining the best properties of the existing ones.”

However, the authors did not provide any discoveries concerning new approaches to regularisation supporting this claim. Thus, the main contribution of the paper is that the authors made a review and performed classification of available regularisation methods. So, the paper is in fact a survey paper, which is more appropriate for full-scale journals. The work, developed by the authors, is really big. However, I am not sure it will bring a lot of benefits for readers except those who need review for some reports, introductions in PhD thesis, etc.

Although the authors mentioned some approaches to combine different regularisations, they did not performed any experiments supporting their ideas.

Thus, I think that
- the paper is well written in general,
- it can be improved (by taking into account several important comments from the Reviewer 2) and served as a review paper in some appropriate journal,
- the paper is not suited for ICLR proceedings due to reasons, mentioned above.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Realtime query completion via deep language models,"['Po-Wei Wang', 'J. Zico Kolter', 'Vijai Mohan', 'Inderjit S. Dhillon']",Reject,2018,"[10, 3, 16, 24, 16]","[14, 8, 17, 29, 21]","[20, 102, 16, 277, 234]","[12, 51, 9, 160, 118]","[5, 50, 7, 71, 108]","[3, 1, 0, 46, 8]","The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance. The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors. Details are also given on how the authors are able to achieve realtime completion.

Overall, it’s nice a nice study of the query completion application. The paper is well explained, and it’s also nice that the runtime is shown for each of the algorithm blocks. Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks. The final dataset is also a good size (36M search queries).

My major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments. Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas. But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates. So the paper feels directed to an audience with less background in neural net LMs.

Secondly, the experiments could have more thorough/stronger baselines. I don’t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data. The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers. More regularization required?","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Inducing Grammars with and for Neural Machine Translation,"['Ke Tran', 'Yonatan Bisk']",Reject,2018,"[9, 5]","[14, 7]","[105, 16]","[50, 8]","[53, 8]","[2, 0]","This paper induces latent dependency syntax in the source side for NMT. Experiments are made in En-De and En-Ru.

The idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017). In light of this, I see very little novelty in this paper. The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word. Seems thin for a ICLR paper.

Caption of Fig 1: ""subject/object"" are syntactic functions, not semantic roles.

I don't see how the German verb ""orders"" inflects with gender... Can you post the gold German sentence?

Sec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.
 
Expressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.

Why is hard attention (sec 3.3) necessary? It's not differentiable and requires sampling for training. This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).

Experimentally, the gains are quite small compared to flat attention, which is disappiointing.

In table 3, it would be very helpful to display the English source.

Table 4 is confusing. The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?

Comparison with predicted parses by Spacy are by no means ""gold"" parses...

Minor comments:
- Sec 1: ""... optimization techniques like Adam, Attention, ..."" -> Attention is not an optimization technique, but part of a model
- Sec 1: ""abilities not its representation"" -> comma before ""not""
","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Byte-Level Recursive Convolutional Auto-Encoder for Text,"['Xiang Zhang', 'Yann LeCun']",Reject,2018,"[5, 31]","[6, 36]","[10, 315]","[4, 162]","[6, 113]","[0, 40]","The authors propose autoencoding text using a byte-level encoding and a convolutional network with shared filters such that the encoder and decoder should exhibit recursive structure. They show that the model can handle various languages and run various experiments testing the ability of the autoencoder to reconstruct the text with varying lengths, perturbations, depths, etc.

The writing is fairly clear, though many of the charts and tables are hard to decipher without labels (and in Figure 8, training errors are not visible -- maybe they overlap completely?).

Main concern would be the lack of experiments showing that the network learns meaningful representations in the hidden layer. E.g. through semi-supervised learning experiments or experiments on learning semantic relatedness of sentences. Obvious citations such as https://arxiv.org/pdf/1511.06349.pdf and https://arxiv.org/pdf/1503.00075.pdf are missing, along with associated baselines. Although the experiment with randomly permuting the samples is nice, would hesitate to draw any conclusions without results on downstream tasks and a clearer survey of the literature.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning,"['Wenbin Li', 'Jeannette Bohg', 'Mario Fritz']",Reject,2018,"[7, 10, 17]","[8, 15, 22]","[14, 157, 256]","[8, 59, 128]","[6, 72, 114]","[0, 26, 14]","The authors use a variant of deep RL to solve a  simplified 2d physical stacking task. To accommodate different goal stacking states the authors extend the state representation of DQN. The input to the network is the current state of the environment as represented by the 2d projection of the objects in the simulated grid world and a representation of the goal state in the same projection space. The reward function in its basic form rewards only the correctly finished model. A number of heuristics are used to augment this reward function so as to provide shaping rewards along the way and speed up learning. The learnt policy is evaluated on the successful assembly of the target stack and on a distance measure between the stack specified as goal and the actual stack. 

Currently, I don’t understand from the manuscript, how DQN is actually trained. Are all different tasks used on a single network? If so, is it surprising that the network performs worse than when augmenting the state representation with the goal? Or are separate DQNs trained for multiple tasks?

The definition of value function at the bottom of page 4 uses the definition for continual tasks but in the current setting the tasks are naturally episodic. This should be reflected by the definition.

It would be good if the authors could comment on any classic research in RL augmenting the state representation with the goal state and any recent related developments, e.g. multi-task RL or the likes of Dosovitskiy & Koltun “Learning to act by predicting the future”.

It would be helpful do obtain more information about the navigation task, especially a plot of sorts would be helpful. Currently, it is particularly difficult to judge exactly what the authors did. 

How physically “rich” is this environment compared to some of the cited work, e.g. Yildirim et al. or Battaglia et al:?

Overall it feels as if this is an interesting project but that it is not yet ready for publication. ","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
ShakeDrop regularization,"['Yoshihiro Yamada', 'Masakazu Iwamura', 'Koichi Kise']",Reject,2018,"[26, 19, 31]","[29, 24, 36]","[10, 103, 220]","[4, 74, 179]","[4, 11, 11]","[2, 18, 30]","The paper proposes a new form of regularization that is an extension of ""Shake-Shake"" regularization (Gastaldi, 2017). The original ""shake-shake"" proposes using two residual paths adding to the same output (so x + F_1(x) + F_2(x)), and during training, considering different randomly selected convex combinations of the two paths (while using an equally weighted combination at test time). However, this paper contends that this requires additional memory, and attempt to achieve similar regularization with a single path. To do so, they train a network with a single residual path, where the residual is included without attenuation in some cases with some fixed probability, and  attenuated randomly (or even inverted) in others. The paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1). Experiments show improved generalization on CIFAR-10 and CIFAR-100.

I don't think the paper contains sufficiently novel elements to be accepted as a conference track paper at ICLR. While it is interesting that this works well (especially the ""negative"" weight on the residual), the proposed method is fundamentally a combination of prior work: dropout and ""shake-shake"" regularization. Moreover, the evaluation is somewhat limited---essentially, I feel there isn't conclusive proof that ""shake-drop"" is a generically useful regularization technique. For one, the method is evaluated only on small toy-datasets: CIFAR-10 and CIFAR-100. I think at the very least, evaluation on Imagenet is necessary. The proposed regularization is applied only to the ""PyramidNet"" architecture---which begs the question of whether the proposed regularization is useful only for this specific network architecture. It would have been more useful to see results with and without ""shake-drop"" on different architectures (the point being to show a consistent improvement with this regularization, rather than achieving 'state of the art' on CIFAR-10). Moreover, it would be interesting to see if the hyperparameter comparison shown in Tables 1 and 2 remained consistent across architectures.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
INTERPRETATION OF NEURAL NETWORK IS FRAGILE,"['Amirata Ghorbani', 'Abubakar Abid', 'James Zou']",Reject,2018,"[2, 4, 9]","[7, 9, 14]","[36, 30, 208]","[14, 10, 78]","[16, 16, 109]","[6, 4, 21]","The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments: 

- What's the limitation of this attack method?

- How reliable are the interpretations? 

- The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. 

- Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting. 
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 4, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
A Semantic Loss Function for Deep Learning with Symbolic Knowledge,"['Jingyi Xu', 'Zilu Zhang', 'Tal Friedman', 'Yitao Liang', 'Guy Van den Broeck']",Reject,2018,"[15, 10, 2, 4, 10]","[20, 14, 6, 9, 15]","[50, 9, 13, 37, 186]","[23, 4, 7, 15, 101]","[15, 1, 5, 20, 70]","[12, 4, 1, 2, 15]","The authors propose a new loss function that is directed to take into account Boolean constraints involving the variables of a classification problem. This is a nice idea, and certainly relevant. The authors clearly describe their problem, and overall the paper is well presented. The contributions are a loss function derived from a set of axioms, and experiments indicating that this loss function captures some valuable elements of the input. This is a valid contribution, and the paper certainly has some significant strengths.

Concerning the loss function, I find the whole derivation a bit distracting and unnecessary. Here we have some axioms, that are not simple when taken together, and that collectively imply a loss function that makes intuitive sense by itself. Well, why not just open the paper with Definition 1, and try to justify this definition on the basis of its properties. The discussion of axioms is just something that will create debate over questionable assumptions. Also it is frustrating to see some axioms in the main text, and some axioms in the appendix (why this division?). 

After presenting the loss function, the authors consider some applications. They are nicely presented; overall the gains are promising but not that great when compared to the state of the art --- they suggest that the proposed semantic loss makes sense. However I find that the proposal is still in search of a ""killer app"". Overall, I find that the whole proposal seems a bit premature and in need of more work on applications (the work on axiomatics is fine as long as it has something to add).

Concerning the text, a few questions/suggestions:
- Before Lemma 3, ""this allows..."" is the ""this including the other axioms in the appendix?
- In Section 4, line 3: I suppose that the constraint is just creating a problem with a class containing several labels, not really a multi-label classification problem (?).
- The beginning of Section 4.1 is not very clear. By reading it, I feel that the best way to handle the unlabeled data would be to add a direct penalty term forcing the unlabeled points to receive a label. Is this fair?
- Page 6: ""a mor methodological""... should it be ""a more methodical""?
- There are problems with capitalization in the references. Also some references miss page numbers and some do not even indicate what they are (journal papers, conference papers, arxiv, etc).
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generating Differentially Private Datasets Using GANs,"['Aleksei Triastcyn', 'Boi Faltings']",Reject,2018,"[7, 34]","[11, 39]","[15, 382]","[5, 262]","[8, 61]","[2, 59]","The paper proposes a technique for differentially privately generating synthetic data using GAN, and experimentally showed that their method achieves both high utility and good privacy.
The idea of building a differentially private GAN and generating differentially private synthetic data is very interesting. However, my main concern is the privacy aspect of the technique, as it is not explained clearly enough in the paper. There is also room for improvement in the presentation and clarity of the paper.

More details:
- About the differential privacy aspect:
  The author didn't provide detailed privacy analysis of the Gaussian noise layer, and I don't find the values of the sensitivity (C = 1) provided in the answer to a public comment easy to see. Also, the paper mentioned that the batch size is 32 and the author mentioned in the comment that the std of the Gaussian noise is 0.7, and the number of epoch is 50 or 150. I think these values would lead to epsilon much larger than 8 (as in Table 1). However, in Section 5.2, it is said that ""Privacy bounds were evaluated using the moments accountant and the privacy amplification theorem (Abadi et al., 2016), and therefore, are data-dependent and are tighter than using normal composition theorems."" I don't see clearly why privacy amplification is needed here, and why using moments accountant and privacy amplification can lead to data-dependent privacy loss.
  In general, I don't find the privacy analysis of this paper clear and detailed enough to convince me about the correctness of the privacy results. However, I am very happy to change my opinion if there are convincing details in the rebuttal.

- About the presentation:
  As a paper proposing a differentially private algorithm, detailed and formal analysis of the privacy guarantees is essential to convince the readers. For example, I think it would be much better if there is a formal theorem showing the sensitivity of the Gaussian noise layer. And it would be better to restate (in Appendix 7.4) not only the definition of moments accountant, but the composition and tail bound, as well as the moments accountant for the Gaussian mechanism, since they are all used in the privacy analysis of this paper.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Global Convergence of Policy Gradient Methods for Linearized  Control Problems,"['Maryam Fazel', 'Rong Ge', 'Sham M. Kakade', 'Mehran Mesbahi']",Reject,2018,"[18, 11, 20, 22]","[23, 16, 25, 27]","[121, 155, 297, 187]","[63, 63, 144, 111]","[38, 78, 126, 30]","[20, 14, 27, 46]","The paper studies the global convergence for policy gradient methods for linear control problems. 
(1) The topic of this paper seems to have minimal connection with ICRL. It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully. 

(2) I am not convinced if the main results are novel. The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming. The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. I am not sure if it is a good idea to examine the convergence purely from an optimization perspective.

(3) The main results of this paper seem technical sound. However, the results seem a bit limited because it does not apply to neural-network function approximator. It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted. I might have missed something here. I strongly suggest that these results be submitted to a more suitable venue.

","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Siamese Survival Analysis with Competing Risks,"['Anton Nemchenko', 'Kartik Ahuja', 'Mihaela Van Der Schaar']",Reject,2018,"[1, 1, 19]","[1, 5, 24]","[3, 16, 829]","[2, 6, 344]","[1, 8, 230]","[0, 2, 255]","The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. The author follow the reasoning that '... these ideas were not explored in the context of survival analysis', thereby disregarding the significant published literature based on the Concordance Index (CI). 

Besides this deficit, the paper does not present a proper statistical setup (e.g. 'Is censoring assumed to be at random? ...) , and numerical results are only referring to some standard implementations, thereby again neglecting the state-of-the-art solution. That being said, this particular use of deep learning in this context might be novel.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
An inference-based policy gradient method for learning options,"['Matthew J. A. Smith', 'Herke van Hoof', 'Joelle Pineau']",Reject,2018,"[2, 7, 19]","[6, 12, 24]","[3, 79, 308]","[3, 41, 151]","[0, 30, 119]","[0, 8, 38]","This paper proposes what is essentially an off-policy method for learning options in complex continuous problems.  The idea is to use policy gradient style algorithms to update a suite of options using relatively 

On the positive side, I like the core idea of this paper.  The idea of updating multiple options at once is a good one.  I think the authors should definitely continue to investigate this line of work.  I also appreciated that the authors took the time to try and visualize what was learned.  The paper is generally well-written and easy to read.

On the negative side: ultimately, the algorithm doesn't seem to work all that well.  Empirically, the method doesn't seem to perform substantially better than other algorithms, although there seems to be some slight advantage.  A clearly missing comparison would be something like TRPO or DDPG.

Figure 1 was helpful in understanding marginalization and the forward algorithm.  Thanks.

Was there really only 4 options that were learned?  How would this scale to more?
","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Generative Adversarial Networks using Adaptive Convolution,"['Nhat M. Nguyen', 'Nilanjan Ray']",Reject,2018,"[1, 18]","[5, 23]","[12, 141]","[5, 74]","[7, 33]","[0, 34]","The paper proposes to use Adaptive Convolution (Niklaus 2017) in the context of GANs. A simple paper with: idea, motivation, experiments

Idea:
It proposes a block called AdaConvBlock that replaces a regular Convolution with two steps:
step 1: regress convolution weights per pixel location conditioned on the input
step 2: do the convolution using these regressed weights
Since local convolutions are generally expensive ops, it provides a few modifications to the size and shape of convolutions to make it efficient (like using depthwise)

Motivation:
- AdaConvBlock gives more local context per kernel weight, so that it can generate locally flexible objects / pixels in images

Motivation is hand-wavy, the claim would need good experiments.

Experiments:
- Experiments are very limited, only overfit to inception score.
- The experiments are not constructed to support the motivation / claim, but just to show that model performance improves.

Inception score experiments as the only experiments of a paper are woefully inadequate. The inception score is computed using a pre-trained imagenet model. It is not hard to overfit to.
The experiments need to support the motivation / claim better.
Ideally the experiments need to show:
- inception score improvements
- actual samples showing that this local context helped produced better local regions / shapes
- some kind of human evaluation supporting claims

The paper's novelty is also quite limited.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Tensor Contraction & Regression Networks,"['Jean Kossaifi', 'Zack Chase Lipton', 'Aran Khanna', 'Tommaso Furlanello', 'Anima Anandkumar']",Reject,2018,"[4, -3, 3, 4, 16]","[9, 1, 7, 9, 21]","[36, 1, 18, 25, 542]","[10, 1, 7, 4, 158]","[1, 0, 1, 1, 23]","[25, 0, 10, 20, 361]","In this paper, new layer architectures of neural networks using a low-rank representation of tensors are proposed. The main idea is assuming Tucker-type low-rank assumption for both a weight and an input. The performance is evaluated with toy data and Imagenet.

[Clarity]
The paper is well written and easy to follow.

[Originality]
I mainly concern about the originality. Applying low-rank tensor decomposition in a network architecture has a lot of past studies and I feel this paper fails to clarify what is really distinguished from the other studies. For example, I found at least two papers [1,2] that are relevant. ([2] appears at the reference but it is not referred to.) How is the proposed method different from them?

Also, the ""end-to-end"" feature is repeatedly emphasized in the paper, but I don't understand its benefit. 

[1] Tai, Cheng, et al. ""Convolutional neural networks with low-rank regularization."" arXiv preprint arXiv:1511.06067 (2015).
[2] Lebedev, Vadim, et al. ""Speeding-up convolutional neural networks using fine-tuned cp-decomposition."" arXiv preprint arXiv:1412.6553 (2014).

[Significance]
In the experiments, the proposed method is compared with the vanilla model (i.e., the model having no low-rank structure) but with no other baseline using different compression techniques such as Novikov et al., 2015. So I cannot judge whether this method is better in terms of compression-accuracy tradeoff.


Pros:
- The proposed model (layer architecture) is simple and easy to implement

Cons:
- The novelty is low
- No competitive baseline in experiments
","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Distributed non-parametric deep and wide networks,"['Biswa Sengupta', 'Yu Qian']",Reject,2018,"[14, 16]","[18, 21]","[44, 119]","[6, 56]","[22, 12]","[16, 51]","- The paper is fairly written and it is clear what is being done
- There is not much novelty in the paper; it combines known techniques and is a systems paper, so I 
  would judge the contributions mainly in terms of the empirical results and messsage conveyed (see
  third point)
- The paper builds on a  previous paper (ICCV Workshops, https://arxiv.org/pdf/1707.06923.pdf),
  however, there is non-trivial overlap between the two papers, e.g. Fig. 1 seems to be almost the
  same figure from that paper, Sec 2.1 from the previous paper is largely copied     
- The message from the empirical validation is also not novel, in the ICCVW paper it was shown that
  the combination of different modalities etc. using a multiple kernel learning framework improved
  results (73.3 on HMDB51), while in the current paper the same message comes across with another
  kind of (known) method for combining different classifiers and modality (without iDT their best
  results are 73.6 for CNN+GP-PoE) ","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
An Out-of-the-box Full-network Embedding for Convolutional Neural Networks,"['Dario Garcia-Gasulla', 'Armand Vilalta', 'Ferran Parés', 'Jonatan Moreno', 'Eduard Ayguadé', 'Jesús Labarta', 'Ulises Cortés', 'Toyotaro Suzumura']",Reject,2018,"[9, 2, 2, 30, 36, 33, 19]","[13, 6, 6, 34, 40, 37, 24]","[64, 19, 19, 419, 369, 175, 136]","[28, 8, 7, 290, 279, 89, 84]","[23, 8, 8, 26, 18, 17, 36]","[13, 3, 4, 103, 72, 69, 16]","The paper addresses the scenario when using a pretrained deep network as learnt feature representation for another (small) task where retraining is not an option or not desired. In this situation it proposes to use all layers of the network to extract feature from, instead of only one layer. 
Then it proposes to standardize different dimensions of the features based on their response on the original task. Finally, it discretize each dimension into {-1, 0, 1} to compress the final concatenated feature representation. 
Doing this, it shows improvements over using a single layer for 9 target image classification datasets including object, scene, texture, material, and animals.

The reviewer does not find the paper suitable for publication at ICLR due to the following reasons:
- The paper is incremental with limited novelty.
- the results are not encouraging
- the pipeline of standardization, discretization is relatively costly, the final feature vector still large. 
- combining different layers, as the only contribution of the paper, has been done in the literature before,  for instance:
“The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling
for Image Classification” CVPR 2016
","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Bounding and Counting Linear Regions of Deep Neural Networks,"['Thiago Serra', 'Christian Tjandraatmadja', 'Srikumar Ramalingam']",Reject,2018,"[7, 11, 16]","[12, 15, 21]","[32, 20, 104]","[13, 7, 63]","[14, 7, 31]","[5, 6, 10]","This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. 

The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. 

The improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. 

The improved upper bound for maxout networks follows a similar intuition but appears to be novel. 

The paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. 

Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. 



","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,"['Soham Parikh', 'Ananya Sai', 'Preksha Nema', 'Mitesh M Khapra']",Reject,2018,"[1, 1, 2, 10]","[6, 6, 7, 15]","[6, 12, 27, 144]","[2, 3, 13, 71]","[4, 7, 14, 64]","[0, 2, 0, 9]","This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations.

It is a well-written paper, however, I am not very convinced by its motivation, the proposed model and the experimental results. 

First of all, the improvement is rather limited. It is only 0.4 improvement overall on the RACE dataset; although it outperforms GAR on 7 out of 13 categories; but why is it worse on the other 6 categories? I don’t see any convincing explanations here.

Secondly, in terms of the development of reading comprehension models, I don’t see why we need to care about eliminating the irrelevant options. It is hard to generalize to any other RC/QA tasks. If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to. The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option. Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined. 

Some visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning to search with MCTSnets,"['Arthur Guez', 'Theophane Weber', 'Ioannis Antonoglou', 'Karen Simonyan', 'Oriol Vinyals', 'Daan Wierstra', 'Remi Munos', 'David Silver']",Reject,2018,"[11, 11, 6, 11, 12, 15, 23, 19]","[15, 16, 10, 15, 17, 16, 28, 23]","[48, 70, 26, 102, 209, 70, 266, 174]","[21, 26, 10, 44, 101, 35, 143, 88]","[22, 38, 12, 51, 98, 27, 96, 58]","[5, 6, 4, 7, 10, 8, 27, 28]","The authors introduce an approach for adding learning to search capability to Monte Carlo tree search. The proposed method incorporates simulation-based search inside a neural network by expanding, evaluating and backing-up a vector-embedding. The key is to represent the internal state of the search by a memory vector at each node. The computation of the network proceeds just like a simulation of MCTS, but using a simulation policy based on the memory vector to initialize the memory vector at the leaf. The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. The paper is thorough and well-explained. My only complaint is the evaluation is only done on one domain, Sokoban. More evaluations on diverse domains are called for.  ","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Sensor Transformation Attention Networks,"['Stefan Braun', 'Daniel Neil', 'Enea Ceolini', 'Jithendar Anumula', 'Shih-Chii Liu']",Reject,2018,"[3, 5, 3, 2, 30]","[4, 7, 8, 4, 34]","[9, 32, 25, 12, 188]","[7, 20, 16, 8, 112]","[2, 11, 5, 3, 33]","[0, 1, 4, 1, 43]","Summary: 

The authors consider the use of attention for sensor, or channel, selection. The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels. Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features. The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data.

Review:

The paper reads well, but as a standard application of attention lacks novelty. The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017). Furthermore, while their approach is sold as a general sensor fusion technique, most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase. Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better! The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better! http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed…

Overall Assessment: 

In summary, the paper lacks novelty wrt technique, and as an “application-of-attention” paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed. As such, I recommend that the paper be rejected.


Additional comments: 

-	The experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised? Were the baselines with concatenated features optimized independently? Why is there no multi-channel baseline for the GRID results? 
-	Issue with noise bursts plot (Input 1+2 attention does not sum to 1)
-	A concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)…
","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Autoregressive Convolutional Neural Networks for Asynchronous Time Series,"['Mikolaj Binkowski', 'Gautier Marti', 'Philippe Donnat']",Reject,2018,"[2, 4, 4]","[6, 7, 4]","[18, 19, 16]","[9, 10, 8]","[9, 8, 7]","[0, 1, 1]","The author proposed:
1. A data augmentation technique for asynchronous time series data.
2. A convolutional 'Significance' weighting neural network that assigns normalised weights to the outputs of a fully-connected autoregressive 'Offset' neural network, such that the output is a weighted average of the 'Offset' neural net.
3. An 'auxiliary' loss function.

The experiments showed that:
1. The proposed method beat VAR/CNN/ResNet/LSTM 2 synthetic asynchronous data sets, 1 real electricity meter data set and 1 real financial bid/ask data set. It's not immediately clear how hyper-parameters for the benchmark models were chosen.
2. The author observed from the experiments that the depth of the offset network has negligible effect, and concluded that the 'Significance' network has crucial impact. (I don't see how this conclusion can be made.)
3. The proposed auxiliary loss is not useful.
4. The proposed architecture is more robust to noise in the synthetic data set compared to the benchmarks, and together with LSTM, are least prone to overfitting.

Pros
- Proposed a useful way of augmenting asynchronous multivariate time series for fitting autoregressive models
- The convolutional Significance/weighting networks appears to reduce test errors (not entirely clear)

Cons
- The novelties aren't very well-justified. The 'Significance' network was described as critical to the performance, but there is no experimental result to show the sensitivity of the model's performance with respect to the architecture of the 'Significance' network. At the very least, I'd like to see what happens if the weighting was forced to be uniform while keeping the 'Offset' network and loss unchanged.
- It's entirely unclear how the train and test data was split. This may be quite important in the case of the financial data set.
- It's also unclear if model training was done on a rolling basis, which is common for time series forecasting.
- The auxiliary loss function does not appear to be very helpful, but was described as a key component in the paper.

Quality: The quality of the paper was okay. More details of the experiments should be included in the main text to help interpret the significance of the experimental results. The experiment also did not really probe the significance of the 'Significance' network even though it's claimed to be important.
Clarity: Above average. 
Originality: Mediocre. Nothing really shines. Weighted average-type architecture has been proposed many times in neural networks (e.g., attention mechanisms). 
Significance: Low. It's unclear how useful the architecture really is.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior,"['Xiang Zhang', 'Nishant Vishwamitra', 'Hongxin Hu', 'Feng Luo']",Reject,2018,"[37, 3, 12, 17]","[42, 7, 17, 21]","[543, 14, 165, 65]","[261, 11, 105, 33]","[71, 2, 17, 8]","[211, 1, 43, 24]","This paper proposes a new convolutional network architecture, which is tested on three image classification tasks.

Pros:
The network is very clean and easy to implement, and the results are OK.

Cons:
The idea is rather incremental compared to FractalNet. The results seem to be worse than existing networks, e.g., DenseNet (Note that SVHN is no longer a good benchmark dataset for evaluating state-of-the-art CNNs). Not much insights were given.

One additional question: Skip connections have been shown to be very useful in ConvNets. Why not adopt it in CrescendoNet? What's the point of designing a network without skip connections?
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,"['Sahil Sharma', 'Girish Raguvir J *', 'Srivatsan Ramesh *', 'Balaraman Ravindran']",Reject,2018,"[6, 2, 2, 21]","[11, 4, 6, 26]","[36, 5, 2, 253]","[12, 1, 1, 142]","[12, 3, 1, 86]","[12, 1, 0, 25]","The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state. The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.

However, the method by which these confidence estimates are refined could be better described. The authors describe these confidences variously as: ""some notion of confidence that the agent has in the value function estimate"" and ""weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015)"". But the exact method is difficult to piece together from what is written. I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters. This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5. If so, this should be stated explicitly.

There is another theoretical point that could be clearer. The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor. Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return. In my opinion, the quality of the paper would be much improved by a brief discussion of this, and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captured.

Nonetheless, I believe that the paper represents an interesting and worthy submission to the conference. I would strongly urge the authors to improve the method description in the camera read version though. A few additional comments are as follows:

  • The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches. However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better. It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting.
  • On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with.
  • On page 3, where delta_t is defined (the j step return TD error, I think the middle term should be $gamma^j V(S_{t+j})$
  • On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help.","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems.,"['Yiping Song', 'Rui Yan', 'Cheng-Te Li', 'Jian-Yun Nie', 'Ming Zhang', 'Dongyan Zhao']",Reject,2018,"[4, 12, 31, 17, 11, 7]","[9, 17, 36, 22, 16, 12]","[36, 158, 291, 217, 334, 196]","[21, 104, 190, 146, 202, 123]","[11, 19, 43, 51, 96, 64]","[4, 35, 58, 20, 36, 9]","The approach involves multiple steps.
On a high level the query is first used to retrieve k best matching response candidates. Then a concatenation of the query and the candidates are fed into a generative model to generate an additional artificial candidate.
In a final step, the k+1 candidates are re-ranked to report the final response.
Each of these steps involves careful engineering and for each there are some minor novel components.
Yet, not all of the steps are presented in complete technical detail.
Also, training corpora and human labeling of the test data do not seem to be publicly available.
Consequently, it would be hard to exactly reproduce the results of the paper.
Experimental validation also is relatively thin.
While the paper report both BLEU metrics and Fleiss kappa from a small-scale human test, the results are based on a single split of a single corpus into training, validation and test data.
While the results for the ensemble are reported to be higher than for the various components for almost all metrics, measures of spread/variance would allow the reader to better judge the degree and significance of improvement.

Minor:
The paper should be read by a native speaker, as it involves a number of minor grammar issues and typos.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Loss Functions for Multiset Prediction,"['Sean Welleck', 'Zixin Yao', 'Yu Gai', 'Jialin Mao', 'Zheng Zhang', 'Kyunghyun Cho']",Reject,2018,"[2, 2, 2, 2, 25, 9]","[7, 2, 7, 7, 30, 14]","[55, 2, 11, 15, 137, 396]","[25, 1, 4, 8, 86, 154]","[30, 1, 7, 6, 38, 215]","[0, 0, 0, 1, 13, 27]","Summary: 
The paper considers the prediction problem where labels are given as multisets. The authors give a definition of a loss function for multisets and show experimental results. The results show that the proposed methods optimizing the loss function perform better than other alternatives.

Comments: 
The problem of predicting multisets looks challenging and interesting. The experimental results look nice. On the other hand, I have several concerns about writing and technical discussions. 

First of all, the goal of the problem is not exactly stated. After I read the experimental section, I realized that the goal is to optimize the exact match score (EM) or F1 measure w.r.t. the ground truth multisets. This goal should be explicitly stated in the paper. Now then, the approach of the paper is to design surrogate loss functions to optimize these criteria. 

The technical discussions for defining the proposed loss function seems not reliable for the reasons below. Therefore, I do not understand the rationale of the definition of the proposed loss function.:  
- An exact definition of the term multiset is not given. If I understand it correctly, a multiset is a “set” of instances allowing duplicated ones. 
- There is no definition of Prec or Rec (which look like Precision and Recall) in Remark 1. The definitions appear in Appendix, which might not be well-defined. For example, let y, Y be mutisets , y=[a, a, a] and Y = [a, b]. Then, by definition, Prec(y,Y)=3/3 =1. Is this what you meant? (Maybe, the ill-definedness  comes from the lack of definition of inclusion in a mutiset.) 
- I cannot follow the proof of Remark 1 since it does not seem to take account of the randomness by the distribution \pi^*. 
- I do not understand the definition of the oracle policy exactly. It seems to me that, the oracle policy knows the correct label (multi-set) \calY for each instance x and use it to construct \calY_t. But, this implicit assumption is not explicitly mentioned. 
- In (1), (2) and Definition 3, what defines \calY_t? If \calY_t is determined by some “optimal” oracle, you cannot define the loss function in Def. 3 since it is not known a priori. Or, if the learner determines \calY_t, I don’t understand why the oracle policy is optimal since it depends on the learner’s choices. 

Also, I expect an investigation of theoretical properties of the proposed loss function, e.g., relationship to EM or F1 or other loss functions. Without understanding the theoretical properties and the rationale, I cannot judge the goodness of the experimental results (look good though). In other words, I cannot judge the paper in a qualitative perspective, not in a quantitative view. 

As a summary, I think the technical contribution of the paper is marginal because of the lack of reliable mathematical discussion or investigation.
","[4, 5, 7]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection,"['Ludovic Trottier', 'Philippe Giguère', 'Brahim Chaib-draa']",Reject,2018,"[5, 14, 31]","[7, 19, 36]","[13, 99, 187]","[8, 58, 125]","[4, 28, 16]","[1, 13, 46]","The collaborative block that authors propose is a generalized module that can be inserted in deep architectures for better multi-task learning. The problem is relevant as we are pushing deep networks to learn representation for multiple tasks. The proposed method while simple is novel. The few places where the paper needs improvement are:

1. The authors should test their collaborative block on multiple tasks where the tasks are less related. Ex: Scene and object classification. The current datasets where the model is evaluated is limited to Faces which is a constrained setting. It would be great if Authors provide more experiments beyond Faces to test the universality of the proposed approach.
2. The Face datasets are rather small. I wonder if the accuracy improvements hold on larger datasets and if authors can comment on any large scale experiments they have done using the proposed architecture. 

In it's current form I would say the experiment section and large scale experiments are two places where the paper falls short.  ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Reinforcement and Imitation Learning for Diverse Visuomotor Skills,"['Yuke Zhu', 'Ziyu Wang', 'Josh Merel', 'Andrei Rusu', 'Tom Erez', 'Serkan Cabi', 'Saran Tunyasuvunakool', 'János Kramár', 'Raia Hadsell', 'Nando de Freitas', 'Nicolas Heess']",Reject,2018,"[7, 7, 7, 9, 14, 2, 1, 9, 16, 19, 10]","[12, 10, 11, 13, 18, 7, 6, 14, 21, 24, 15]","[146, 53, 49, 28, 39, 19, 20, 21, 101, 199, 200]","[66, 20, 19, 13, 22, 7, 5, 10, 47, 101, 81]","[74, 29, 25, 14, 14, 12, 11, 11, 44, 84, 111]","[6, 4, 5, 1, 3, 0, 4, 0, 10, 14, 8]","This paper claims to present a ""general deep reinforcement learning"" method that addresses the issues of real-world robotics: data constraints, safety, and lack of state information, and exploration by using demonstrations. However, this paper actually addresses these problems by training in a simulator, and only transferring 2 of the 6 tasks to the real world. The real world results are  lackluster. However, the simulated results are nice.

The method in the paper is as follows: the environment reward is augmented by a reward function learned from human demonstrations using GAIL on full state (except for the arm). Then, an actor-critic method is used where the critic gets full state information, while the actor needs to learn from an image. However, the actor's convolutional layers are additionally trained to detect the object positions. 

Strengths:
+ The simulated tasks are novel and difficult (sorting, clearing a table)
+ Resetting to demonstration states is a nice way to provide curriculum

Limitations:
+ The results make me worries that the simulation environments have been hyper-tailored to the method, as the real environments looks very similar, and should transfer. 
+ Each part of the method is not particularly novel. Combining IRL and RL has been done before (as the authors point out in the related work), side-training perception module to predict full state has been done before (""End-to-end visuomotor learning""), diversity of training conditions has been done before (Domain randomization).
+ Requiring hand-specified clusters of states for both selecting starting states and defining a reward functions requires domain knowledge. Why can't they be clustered using a clustering method?
+ Because the method needs simulation to learn a policy, it is limited to tasks that can be simulated somewhat accurately (e.g. ones with simple dynamics). As shown by the poor transfer of the stacking task, block stacking with foam blocks is not a such task.


Questions:
+ How many demonstrations do you use per task?
+ What are the ""relative"" positions included in the ""object-centric"" state input? 

Misleading parts of the paper:
+ The introduction of the paper primes the reader to expect a method that can work on a real system. However, this method only gets 64% accuracy on a simple block lifting task, 35% on a stacking task.
+ ""Appendix C. ""We define functions on the underlying physical state to determine the stage of a state…The definition of stages also gives rise to a convenient way of specifying the reward functions without hand-engineering a shaping reward. ""-> You are literally hand engineering a shaping reward. The main text misleadingly refers to ""sparse reward"", which usually refers to a single reward upon task completion.

In conclusion, I find that the work lacks significance because the results are dependent on a list of hacks that are only possible in simulation.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Composable Planning with Attributes,"['Amy Zhang', 'Adam Lerer', 'Sainbayar Sukhbaatar', 'Rob Fergus', 'Arthur Szlam']",Reject,2018,"[7, 8, 10, 10, 16]","[12, 13, 14, 15, 21]","[54, 43, 55, 138, 131]","[17, 17, 26, 54, 72]","[33, 26, 27, 74, 49]","[4, 0, 2, 10, 10]","- This paper proposes a framework where the agent has access to a set of user defined attributes parametrizing features of interest. The agent learns a policy for transitioning between similar sets of attributes and given a test task, it can repurpose its attributes to reactively plan a policy to achieve the task. A grid world and tele-kinetically operated block stacking task is used to demonstrate the idea

- This framework is exactly the same as semi-MDPs (Precup, Sutton) and its several generalizations to function approximators as cited in the paper. The authors claim that the novelty is in using the framework for test generalization. 

- So the main burden lies on experiments. I do not believe that the experiments alone demonstrate anything substantially new about semi-MDPs even within the deep RL setup. There is a lot of new vocabulary (e.g. sets of attributes) that is introduced, but it dosen't really add a new dimension to the setup. But I do believe in the general setup and I think its an important research direction. However the demonstrations are not strong enough yet and need further development. For instance automatically discovering attributes is the next big open question and authors allude to it.

- I want to encourage the authors to scale up their stacking setup in the most realistic way possible to develop this idea further. I am sure this will greatly improve the paper and open new directions of researchers. 

","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Attention-based Graph Neural Network for Semi-supervised Learning,"['Kiran K. Thekumparampil', 'Sewoong Oh', 'Chong Wang', 'Li-Jia Li']",Reject,2018,"[5, 15, 11, 12]","[9, 20, 16, 16]","[24, 80, 196, 66]","[8, 41, 80, 38]","[14, 30, 89, 22]","[2, 9, 27, 6]","The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined. The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect. Further the proposed method also provides class relation based on the edge-wise relevance.

The paper is easy to follow and the idea would be reasonable. 

Importance of the propagation layer than the non-linear layer is interesting, and I think it is worth showing.

Variance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Is there any good explanation of this low variance of AGNN?

Interpretation of Figure 2 is not clear. All colored nodes except for the thick circle are labeled node? I couldn't judge those predictions are appropriate or not.","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
State Space LSTM Models with Particle MCMC Inference,"['Xun Zheng', 'Manzil Zaheer', 'Amr Ahmed', 'Yuan Wang', 'Eric P. Xing', 'Alex Smola']",Reject,2018,"[8, 8, 18, 23, 18, 23]","[11, 13, 22, 28, 23, 28]","[27, 143, 120, 605, 625, 341]","[16, 69, 82, 278, 339, 216]","[10, 73, 21, 48, 218, 83]","[1, 1, 17, 279, 68, 42]","This article presents an approach for learning and inference in nonlinear state-space models (SSM) based on LSTMs. Learning is done using a stochastic EM where Particle PMCM is used to sample state trajectories.

The model is presented assuming that SSMs are linear. This is not necessarily the case since nonlinear SSMs have been used for a long time (see for example Ljung, 1999, ""System Identification, Theory for the User""). The presented model is a nonlinear SSM with a particular structure that uses LSTMs.

The model described in the paper is Markovian: if one defines the variable sz_t = {s_t, z_t} there exists a Markov chain for the latent state sz:

sz_t -> sz_{t+1} -> sz_{t+2} -> ...

Marginalizing the latent variables s_t leads to a structure that, in general, is not Markovian. The authors claim that this marginalization ""allows the SSL to have non-Markovian state transition"". The word ""allows"" may mislead the reader in thinking that the model has gained some appealing property whereas the model is still essentially Markovian as evidenced by the Markov chain in sz. Any general algorithm for inference in nonlinear Markovian models could be used for inference of sz.

The algorithm used for inference and learning is stochastic EM with PMCMC but the authors do not cite important prior work such as: Lindsten (2013) ""An efficient stochastic approximation EM algorithm using conditional particle filters""


Pros:

The model is sound.

The overall structure of the paper is good.


Cons:

The authors formulate the problem in such a way that they are forced to use an algorithm for non-Markovian models when they could have conserved the Markovian structure by choosing the appropriate parameterization.

The presentation of state-space models, filtering and smoothing shows some lack of familiarity with the literature. The control theory literature has dealt with nonlinear SSMs for decades and there is recent work in the machine learning community on nonlinear SSMs, e.g. Gaussian Process SSMs. 

I would advise against the use of non-English expressions unless they are used precisely:

   - sine qua non: LSTMs are not literally an indispensable model for sequence modeling nowadays. If the use of Latin was unavoidable, ""de facto standard"" would have been slightly more accurate.

   - bona fide: I am not sure what the authors wanted to say.

   - naívely: the correct spelling would be naïvely or naively.","[3, 5, 7]","[' Clear rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Theoretical properties of the global optimizer of two-layer Neural Network,"['Digvijay Boob', 'Guanghui Lan']",Reject,2018,"[2, 13]","[7, 18]","[17, 92]","[4, 9]","[10, 39]","[3, 44]","I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity.

The paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al). However, the paper does not properly relate their results, assumptions in the context of the existing literature. Much explanation is needed in the author reply in order to clear these questions.

The work should not be evaluated from a practical perspective as it is of a theoretical nature.

I agree with most of the criticism raised by other reviewers. However, I also believe the authors managed to clear essentially of the criticism in they reply. The paper lacks in clarity as currently written. 

The results are interesting, but more explanation is needed for the main message to be conveyed more clearly. I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission.
","[7, 7, 4]","[' Good paper, accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Overcoming the vanishing gradient problem in plain recurrent networks,"['Yuhuang Hu', 'Adrian Huber', 'Shih-Chii Liu']",Reject,2018,"[3, 1, 2, 30]","[7, 3, 4, 34]","[23, 7, 12, 188]","[12, 3, 8, 112]","[10, 1, 3, 33]","[1, 3, 1, 43]","Summary: 
The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. This identity connection acts as a “surrogate memory” component, preserving hidden activations over time steps. 
The experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames. It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI.
The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs.

Review: 
I very much like the paper. The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!
I have a few comments and questions:
1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise? If bit-wise, can you elaborate why? I might have missed something.
2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c? Also some curves in the appendix stop abruptly without visible explosions. Were these experiments run until completion? If so, would it be possible to plot the complete curves?
3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task. Optimal hyperparameters are usually model-specific. Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript.
Le et al. (2015) for instance perform a coarse grid search for each model.
4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up. There is no (multiplicative) gating as in Highway Networks.
5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating. The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions.
6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful. Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations. Would RINs readily learn to reset parts of the hidden state?
7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.

Overall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). After a revision I would consider to increase the score.

References:
Henaff, Mikael, Arthur Szlam, and Yann LeCun. ""Recurrent orthogonal networks and long-memory tasks."" In International Conference on Machine Learning, pp. 2034-2042. 2016.
Le, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. ""A simple way to initialize recurrent networks of rectified linear units."" arXiv preprint arXiv:1504.00941 (2015).","[7, 2, 4]","[' Good paper, accept', ' Strong rejection', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Discrete Sequential Prediction of Continuous Actions for Deep RL,"['Luke Metz', 'Julian Ibarz', 'Navdeep Jaitly', 'James Davidson']",Reject,2018,"[3, 8, 12, 43]","[8, 13, 16, 45]","[48, 45, 78, 33]","[19, 17, 31, 14]","[29, 24, 38, 14]","[0, 4, 9, 5]","Originality
--------------
When the action space is N-dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1-D actions. 

Clarity
---------
1) Explicitly writing down DDPG will be helpful
2) The number of actions in each of the domains will also be useful

Quality
----------
1) The paper reports experimental results on order of actions as well as binning, and the results confirm with what one would expect from intuition. 
2) It will be important to talk about the case when the action dimension N is very large, what happens in that case? Does the proposed method would work in such a scenario? A discussion is needed.
3) Given that the ordering of actions does not matter, what is the real take away of looking at them as 'sequence' (which has not temporal structure because action order could be arbitrary)?


Significance
----------------
While the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i=1,..N) for Q^L, which is a limitation. Further, since the actions could be arbitrary, it is unclear where 'sequence' approach helps. These limit the understand and hence significance.
","[5, 7, 4]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[1, 5, 5]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Convolutional Normalizing Flows,"['Guoqing Zheng', 'Yiming Yang', 'Jaime Carbonell']",Reject,2018,"[17, 15, 18, 31]","[22, 20, 23, 36]","[161, 93, 160, 210]","[68, 57, 95, 77]","[8, 12, 58, 72]","[85, 24, 7, 61]","In this paper, the authors propose a type of Normalizing Flows (Rezende and Mohamed, 2015) for Variational Autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) they call Convolutional Normalizing Flows.
More particularly, it aims at extending on the Planar Flow scheme proposed in Rezende and Mohamed (2015). The authors notice an improvement through their method over Normalizing Flows, IWAE with diagonal gaussian approximation, and standard Variational Autoencoders. 
As noted by AnonReviewer3, several baselines are missing. But the authors partly address that issue in the comment section for the MNIST dataset.
The requirement of h being bijective seems wrong. For example, if h was a rectifier nonlinearity in the zero-derivative regime, the Jacobian determinant of the ConvFlow would be 1. 
More importantly, the main issue is that this paper might need to highlight the fundamental difference between their proposed method and Inverse Autoregressive Flow (Kingma et al., 2016). The proposed connectivity pattern proposed for the convolution in order to make the Jacobian determinant computation is exactly the same as Inverse Autoregressive Flow and the authors seems to be aware of the order dependence of their architecture which is every similar to autoregressive models. This presentation of the paper can be misleading concerning the true innovation in the model trained. Proposing ConvFlow as a type of Inverse Autoregressive Flow would be more accurate and would allow to highlight better the innovation of the work.
Since this work does not offer additional significant insight over Inverse Autoregressive Flow, its value should be on demonstrating the efficiency of the proposed method. MNIST and Omniglot seems insufficient for that purpose given currently published work.
In the current state, I can't recommend the paper for acceptance. 


Danilo Jimenez Rezende, Shakir Mohamed: Variational Inference with Normalizing Flows. ICML 2015
Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra: Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models. ICML 2014
Diederik P. Kingma, Max Welling: Auto-Encoding Variational Bayes. ICLR 2014
Diederik P. Kingma, Tim Salimans, Rafal Józefowicz, Xi Chen, Ilya Sutskever, Max Welling: Improving Variational Autoencoders with Inverse Autoregressive Flow. NIPS 2016","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,"['Gabriel Synnaeve', 'Zeming Lin', 'Jonas Gehring', 'Vasil Khalidov', 'Nicolas Carion', 'Nicolas Usunier']",Reject,2018,"[9, 3, 6, 1, 1, 11, 3, 14]","[14, 7, 11, 1, 5, 16, 7, 19]","[132, 22, 23, 3, 12, 23, 12, 125]","[59, 11, 15, 1, 3, 14, 4, 72]","[67, 9, 7, 2, 7, 7, 8, 44]","[6, 2, 1, 0, 2, 2, 0, 9]","# Summary
This paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state. This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map. The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft.

[Pros]
- Interesting problem

[Cons]
- The proposed method is not much novel.
- The evaluation is a bit limited to two specific downstream prediction tasks.

# Novelty and Significance
- The problem considered in this paper is interesting.
- The proposed method is not much novel. 
- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks. It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks. Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible.

# Quality
- The experimental result is not much comprehensive. The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks. It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks).

# Clarity
- I did not fully understand the learning objective. Does the model try to reconstruct the state of the current time-step or the future? The learning objective is not clearly defined. In Section 4.1, the target x and y have time steps from t1 to t2. What is the range of t1 and t2? If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results.","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Parametric Manifold Learning Via Sparse Multidimensional Scaling,"['Gautam Pai', 'Ronen Talmon', 'Ron Kimmel']",Reject,2018,"[4, 10, 26]","[9, 15, 31]","[20, 100, 268]","[8, 33, 116]","[9, 26, 58]","[3, 41, 94]","The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension. They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure. The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework.

The paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional).

The abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc.

In the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities. This is in general not true. Even if the number of parameters is small, learning them might require complex computations on the whole data set. On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N).

In section 2.1, the authors claim ""Spectral techniques are non-parametric in nature""; this is wrong again. E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words.

In section 2.2, it says ""observation that the double centering..."". Can you provide a citation for this?

In section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity. It is not quite clear from the text what the resulting complexity would be. With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2). Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is.

Figure 3, contrary to text, does not provide a visualisation to the sampling mechanism.

In the experiments section, can you provide a citation for ADAM and explain how the parameters were selected? Also, it is not meaningful to measure the quality of a visualisation via the MDS fit. There are more useful approaches to this task, such as the quality framework [*].

In figure 4a, x-axis should be ""number of landmarks"".

It is not clear why the equation 6 holds. Citation?
It is also not clear how exactly the equation 7 is evaluated. It says ""By varying the number of layers and the number of nodes..."", but the nodes and layer are not a part of the equation.

The notation for equation 8 is not explained.

Figure 6a shows visualisations by different techniques and is evaluated ""by looking at it"". Again, use [*].

[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.
","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Independent Causal Mechanisms,"['Giambattista Parascandolo', 'Mateo Rojas Carulla', 'Niki Kilbertus', 'Bernhard Schoelkopf']",Reject,2018,"[11, -1, 1, 20, 24]","[16, 4, 6, 25, 29]","[11, 5, 28, 252, 121]","[6, 1, 9, 153, 73]","[4, 3, 17, 69, 33]","[1, 1, 2, 30, 15]","This paper describes a setting in which a system learns collections of inverse-mapping functions that transform altered inputs to their unaltered ""canonical"" counterparts, while only needing unassociated and separate sets of examples of each at training time.  Each inverse map is an ""expert"" E akin to a MoE expert, but instead of using a feed-forward gating on the input, an expert is selected (for training or inference) based on the value of a distribution-modeling function c applied to the output of all experts:  The expert with maximum value c(E(x)) is selected.  When c is an adversarially trained discriminator network, the experts learn to model the different transformations that map altered images back to unaltered ones.  This is demonstrated using MNIST with a small set of synthetic translations and noise.

The fact that these different inverse maps arise under these conditions is interesting --- and Figure 5 is quite convincing in showing how each expert generalizes.  However, I think the experimental conditions are very limited:  Only one collection of transformations is studied, and on MNIST digits only.  In particular, I found the fact that only one of ten transformations can be applied at a time (as opposed to a series of multiple transforms) to be restrictive.  This is touched on in the conclusion, but to me it seems fundamental, as any real-world new example will undergo significantly more complex processes with many different variables all applied at once.

Another direction I think would be interesting, is how few examples are needed in the canonical distribution?  For example, in MNIST, could the canonical distribution P be limited to just one example per digit (or just one example per mode / style of digit, e.g. ""2"" with loop, and without loop)?  The different handwriters of the digits, and sampling and scanning process, may themselves constitute in-the-wild transformations that might be inverted to single (or few) canonical examples --- Is this possible with this mechanism?

Overall, it is nice to see the different inverse maps arise naturally in this setting.  But I find the single setting limiting, and think the investigation could be pushed further into less restricted settings, a couple of which I mention above.



Other comments:

- c is first described to be any distribution model, e.g. the autoencoder described on p.5.  But it seems that using such a fixed, predefined c like the autoencoder may lead to collapse:  What is preventing an expert from learning a single constant mode that has high c value?  The adversarially trained c doesn't suffer from this, because presumably the discriminator will be able to learn the difference between a single constant mode output and the distribution P.  But if this is the case, it seems a critical part of the system, not a simple implementation choice as the text seems to say.

- The single-net baseline is good, but I'd like to get a clearer picture of its results.  p.8 says this didn't manage to ""learn more than one inverse mechanism"" --- Does that mean it learns to invert a single mechanism (that is always translates up, for example, when presented an image)?  Or that it learned some mix of transforms that didn't seem to generalize as well?  Or does it have some other behavior?  Also, I'm not entirely clear on how it was trained wrt c --- is argmax(c(E(x)) always just the single expert?  Is c also trained adversarially?  And if so, is the approximate identity initialization used?
","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Weighted Transformer Network for Machine Translation,"['Karim Ahmed', 'Nitish Shirish Keskar', 'Richard Socher']",Reject,2018,"[3, 4, 12]","[7, 8, 17]","[12, 46, 229]","[6, 16, 111]","[5, 27, 111]","[1, 3, 7]","TL;DR of paper: they modify the Transformer architecture of Vaswani et al. (2017) to used branched attention with learned weights instead of concatenated attention, and achieve improved results on machine translation.

Using branches instead of a single path has become a hot architecture choice recently, and this paper applies the branching concept to multi-head attention. Weirdly, they propose using two different sets of weights for each branch: (a) kappa, which premultiplies the head before fully connected layers, and (b) alpha, which are the weights of the sum of the heads after the fully connected layers. Both weights have simplex constraints. A couple of questions about this:

* What is the performance of only using kappa? Only alpha? Neither? What happens if I train only of them?
* What happens if you remove the simplex constraints (i.e., don't have to sum to one, or can be negative)?
* Why learn a global set of weights for the branch combiners? What happens if the weights are predicted for each input example? This is the MoE experiment, but where k = M (i.e., no discrete choices made).
* Are the FFN layer parameters shared across the different heads?
* At the top of page 4, it is said ""all bounds are respected during each training step by projection"". What does this mean? Is projected gradient descent used, or is a softmax used? If the former, why not use a softmax?
* In Figure 3, it looks like the kappa and alpha values are still changing significantly before they are frozen. What happens if you let them train longer? On the same note, the claim is that Transformer takes longer to train. What is the performance of Transformer if using the same number of steps as the weighted Transformer?
* What are the Transformer variants A, B, and C?

While the results are an improvement over the baseline Transformer, my main concern with this paper is that the improved results are because of extensive hyperparameter tuning. Design choices like having a separate learning rate schedule for the alpha and kappa parameters, and needing to freeze them at the end of training stoke this concern. I'm happy to change my score if the authors can provide empirical evidence for each design choice","[6, 9, 4]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,"['Kenny J. Young', 'Shuo Yang', 'Richard S. Sutton']",Reject,2018,"[3, 36, 14]","[7, 41, 19]","[18, 168, 226]","[6, 83, 94]","[12, 63, 39]","[0, 22, 93]","The paper proposes a modified approach to RL, where an additional ""episodic memory"" is kept by the agent. What this means is that the agent has a reservoir of n ""states"" in which states encountered in the past can be stored. There are then of course two main questions to address (i) which states should be stored and how (ii) how to make use of the episodic memory when deciding what action to take. 

For the latter question, the authors propose using a ""query network"" that based on the current state, pulls out one state from the memory according to certain probability distribution. This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory. Intuitively, one can see why this may be advantageous as one gets some information from the past. (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)

The first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights. The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose. However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past). There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable. 

There is also a toy example created to show that this approach works well compared to the RNN based approaches.

Positives:

- An interesting new idea that has potential to be useful in RL
- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks)

Negatives:
- The math is fudged around quite a bit with approximations that are not always justified
- While overall the writing is clear, in some places I feel it could be improved. I had a very hard time understanding the set-up of the problem in Figure 2. [In general, I also recommend against using figure captions to describe the setup.]
- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Variational Bi-LSTMs,"['Samira Shabanian', 'Devansh Arpit', 'Adam Trischler', 'Yoshua Bengio']",Reject,2018,"[4, 8, 3, 31]","[9, 13, 8, 36]","[11, 45, 82, 975]","[3, 20, 39, 405]","[8, 25, 43, 454]","[0, 0, 0, 116]","This paper proposes a particular form of variational RNN that uses a forward likelihood and a backwards posterior.  Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.

My first concern with this paper is that the derivation in Eq. 1 does not seem to be correct.  There is a p(z_1:T) term that should appear in the integrand.

It is not clear to me why h_t should depend on \tilde{b}_t.  All paths from input to output through \tilde{b}_t also pass through z_t so I don't see how this could be adding information.  It may add capacity to the decoder in the form of extra weights, but the same could be achieved by making z_t larger. Why not treat \tilde{b}_t symmetrically to \tilde{h}_t, and use it only as a regularizer?  

In the no reconstruction loss experiments do you still sample \tilde{b}_t in the generative part?  Baselines where the \tilde{b}_t -> h_t edge is removed would be very nice.

It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.

I don't understand the description of the ""Skip Gradient"" trick.  Exactly which gradients are you skipping at random?

Do you have any intuition for why it is sometimes necessary to set beta=0?
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Understanding Deep Learning Generalization by Maximum Entropy,"['Guanhua Zheng', 'Jitao Sang', 'Changsheng Xu']",Reject,2018,"[2, 11, 20]","[4, 16, 25]","[4, 121, 571]","[0, 52, 307]","[4, 31, 55]","[0, 38, 209]","Summary:

This paper presents a derivation which links a DNN to recursive application of
maximum entropy model fitting. The mathematical notation is unclear, and in
one cases the lemmas are circular (i.e. two lemmas each assume the other is
correct for their proof). Additionally the main theorem requires complete
independence, but the second theorem provides pairwise independence, and the
two are not the same.

Major comments:

- The second condition of the maximum entropy equivalence theorem requires
  that all T are conditionally independent of Y. This statement is unclear, as
it could mean pairwise independence, or it could mean jointly independent
(i.e. for all pairs of non-overlapping subsets A & B of T I(T_A;T_B|Y) = 0).
This is the same as saying the mapping X->T is making each dimension of T
orthogonal, as otherwise it would introduce correlations. The proof of the
theorem assumes that pairwise independence induces joint independence and this
is not correct.

- Section 4.1 makes an analogy to EM, but gradient descent is not like this
  process as all the parameters are updated at once, and only optimised by a
single (noisy) step. The optimisation with respect to a single layer is
conditional on all the other layers remaining fixed, but the gradient
information is stale (as it knows about the previous step of the parameters in
the layer above). This means that gradient descent does all 1..L steps in
parallel, and this is different to the definition given.

- The proofs in Appendix C which are used for the statement I(T_i;T_j) >=
  I(T_i;T_j|Y) are incomplete, and in generate this statement is not true, so
requires proof.

- Lemma 1 appears to assume Lemma 2, and Lemma 2 appears to assume Lemma 1.
  Either these lemmas are circular or the derivations of both of them are
unclear.

- In Lemma 3 what is the minimum taken over for the left hand side? Elsewhere
  the minimum is taken over T, but T does not appear on the left hand side.
Explicit minimums help the reader to follow the logic, and implicit ones
should only be used when it is obvious what the minimum is over.

- In Lemma 5, what does ""T is only related to X"" mean? The proof states that
  Y -> T -> X forms a Markov chain, but this implies that T is a function of
Y, not X.

Minor comments:

- I assume that the E_{P(X,Y)} notation is the expectation of that probability
  distribution, but this notation is uncommon, and should be replaced with a
more explicit one.

- Markov is usually romanized with a ""k"" not a ""c"".

- The paper is missing numerous prepositions and articles, and contains
  multiple spelling mistakes & typos.","[2, 3, 6]","[' Strong rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
PACT: Parameterized Clipping Activation for Quantized Neural Networks,"['Jungwook Choi', 'Zhuo Wang', 'Swagath Venkataramani', 'Pierce I-Jen Chuang', 'Vijayalakshmi Srinivasan', 'Kailash Gopalakrishnan']",Reject,2018,"[13, 15, 7, 10, 22, 11]","[18, 20, 11, 15, 26, 15]","[95, 205, 83, 39, 68, 47]","[53, 99, 59, 23, 50, 29]","[17, 8, 11, 10, 4, 12]","[25, 98, 13, 6, 14, 6]","This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. 

The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too.

Overall the paper is a descent one, but with limited novelty. I am a weak reject","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Lifelong Word Embedding via Meta-Learning,"['Hu Xu', 'Bing Liu', 'Lei Shu', 'Philip S. Yu']",Reject,2018,"[8, 31, 6, 39]","[13, 36, 11, 44]","[67, 350, 53, 1879]","[29, 235, 24, 991]","[38, 67, 29, 389]","[0, 48, 0, 499]","This paper presents a lifelong learning method for learning word embeddings.  Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings.

The general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.  However, this submission needs to be improved in terms of clarity and its experiments.

In terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why.  When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful.  What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?  The paper states the purpose of the meta learning is ""to learn a general word context similarity from the first m domains"", but I was never sure what this meant.  Further, some of the paper's pseudocode includes unexplained steps like ""invert by domain index"" and ""scanco-occurrence"".  

In terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works.  First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered.  Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy, see :
""Think Globally, Embed Locally—Locally Linear Meta-embedding of Words"", Bollegala et al., 2017
https://arxiv.org/pdf/1709.06671.pdf

That paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission.

Beyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach.  What if we just selected the one past domain that was most similar to the new domain, by some measure?  And how does the performance of the technique depend on the setting of m?  Investigating some of these questions would help us understand how well the approach works and in which settings.

Minor:

Second paragraph, GloVec should be GloVe

""given many domains with uncertain noise for the new domain"" -- not clear what ""uncertain noise"" means, perhaps ""uncertain relevance"" would be more clear

The text refers to a Figure 3 which does not exist, probably means Figure 2.  I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2

When m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary.

""from the first m domain corpus"" -> ""from the first m domains""?

""may not helpful"" -> ""may not be helpful""

""vocabularie"" -> ""vocabulary""

""system first retrieval"" -> ""system first retrieves""

COMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines.  The concatenation does fairly comparably to LL in Tables 3&4.  LL wins by a bit more in Table 2.  Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation.","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,"['Taesung Lee', 'Youngja Park']",Reject,2018,"[8, 18]","[12, 22]","[27, 59]","[17, 38]","[5, 7]","[5, 14]","This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence. Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related ""sentences"" could be encoded, including document title, section title, footnotes, hyperlinked sentences. This is a valid good idea and indeed improves results. The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful. The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution.

On the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text. There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable. The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference. And the coreference experiment isn't that clearly described nor necessarily that meaningful. Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting.

Other comments:
 - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a ""topic sentence"" at the beginning.
 - p.5. Notation seemed a bit non-standard. I thought most people use \sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid....
 - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common.
 - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them. A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work. The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015). More generally, a now quite common way to handle this problem is to use ""pointing"" or ""copying"", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too. 
 - p.7. Why such an old Wikipedia dump? Most people use a more recent one!
 - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model.
 - p.8. For various reasons, the coreference results seem less useful than they could have been, but they do show some value for the technique in the area of domain-specific coreference.

","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
CyCADA: Cycle-Consistent Adversarial Domain Adaptation,"['Judy Hoffman', 'Eric Tzeng', 'Taesung Park', 'Jun-Yan Zhu', 'Phillip Isola', 'Kate Saenko', 'Alyosha Efros', 'Trevor Darrell']",Reject,2018,"[8, 6, 16, 7, 8, 15, 25, 32]","[13, 8, 21, 12, 13, 20, 30, 37]","[112, 30, 156, 121, 126, 321, 249, 653]","[51, 14, 57, 47, 59, 158, 128, 376]","[57, 15, 12, 59, 62, 147, 82, 230]","[4, 1, 87, 15, 5, 16, 39, 47]","This paper proposes  a natural extension of the CycleGAN approach. This is achieved by leveraging the feature and semantic losses to achieve a more realistic image reconstruction. The experiments show that including these additional losses is critical for improving the models performance.  The paper is very well written and technical details are well described and motivated. It would be good to identify the cases where the model fails and comment on those. For instance, what if the source data cannot be well reconstructed from adapted target data? What are the bounds of the domain discrepancy in this case? ","[9, 5, 5]","[' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning Graph Convolution Filters from Data Manifold,"['Guokun Lai', 'Hanxiao Liu', 'Yiming Yang']",Reject,2018,"[5, 4, 35]","[7, 9, 40]","[18, 74, 305]","[8, 35, 173]","[10, 33, 88]","[0, 6, 44]","The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs. The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel. The weights within a depth channel is shared thus maintaining the stationary requirement. The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception. 

The paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016). 

The paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification. The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters. For image classification, the performance of proposed method is below its predecessor Xception network. 

Pros:
i) Detailed review of the existing work and comparison with the proposed work.
ii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings.
iii) Superior performance with fewer number of parameters compared to other methods. 
Cons:
i) The architecture of the 2 layer MLP used to learn weights for a particular depth channel is not provided.
ii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,"['Andrew Hallam', 'Edward Grant', 'Vid Stojevic', 'Simone Severini', 'Andrew G. Green']",Reject,2018,"[2, 29, 2, 16, 2]","[2, 33, 3, 19, 2]","[2, 75, 3, 74, 2]","[1, 54, 1, 14, 1]","[1, 3, 2, 23, 1]","[0, 18, 0, 37, 0]","The authors study compressing feed forward layers using low rank tensor decompositions. For instance a feed forward layer of 4096 x 4096 would first be reshaped into a rank-12 tensor with each index having dimension 2, and then a tensor decomposition would be applied to reduce the number of parameters. 

Previous work used tensor trains which decompose the tensor as a chain. Here the authors explore a tree like decomposition. The authors only describe their model using pictures and do not provide any rigorous description of how their decomposition works.

The results are mediocre. While the author's approach does seem to reduce the feed forward net parameters by 30% compared to the tensor train decomposition for similar accuracy, the total number of parameters for both MERA (authors' approach) and Tensor Train is similar since in this regime the CNN parameters dominate (and the authors' approach does not work to compress those).

","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Self-Training Method for Semi-Supervised GANs,"['Alan Do-Omri', 'Dalei Wu', 'Xiaohua Liu']",Reject,2018,"[2, 16, 18]","[7, 21, 23]","[8, 111, 113]","[3, 57, 67]","[5, 3, 6]","[0, 51, 40]","The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.

1.	The experiment is weak. Firstly, only one data set is employed for evaluation, which is hard to justify the applicability of the proposed approach. Secondly, the compared methods are too few and do not include many state-of-the-art SSL methods like graph-based approaches. Thirdly, in these cases, the results in table 1 contain evident redundancy. Fourthly, the performance improvement over compared method is not significant and the result is based on 3 splits of data set, which is obviously not convincing and involves large variance. 
2.	The paper claims that ‘when paired with deep, semi-supervised learning has had a few success’. I do not agree with such a claim. There are many success SSL deep learning studies on embedding. They are not included in the discussions. 
3.	The layout of the paper could be improved. For example, there are too many empty spaces in the paper. 
4.	Overall technically the proposed approach is a bit straightforward and does not bring too much novelty.
5.	The format of references is not consistent. For example, some conference has short name, while some does not have. ","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings,"['Amelie Royer', 'Konstantinos Bousmalis', 'Stephan Gouws', 'Fred Bertsch', 'Inbar Mosseri', 'Forrester Cole', 'Kevin Murphy']",Reject,2018,"[4, 15, 9, 2, 6, 13, 11]","[9, 19, 11, 6, 11, 17, 16]","[14, 36, 16, 9, 24, 49, 31]","[8, 17, 9, 3, 10, 23, 15]","[6, 14, 7, 6, 12, 16, 14]","[0, 5, 0, 0, 2, 10, 2]","This paper proposed an X-shaped GAN for the so called semantic style transfer task, in which the goal is to transfer the style of an image from one domain to another without altering the semantic content of the image. Here, a domain is collectively defined by the images of the same style, e.g., cartoon faces. 

The cost function used to train the network consists of five terms of which four are pretty standard: a reconstruction loss, two regular GAN-type losses, and an imitation loss. The fifth term, called the semantic consistency loss, is one of the main contributions of this paper. This loss ensures that the translated images should be encoded into about the same location as the embedding of the original image, albeit by different encoders. 

Strengths:
1. The new CartoonSet dataset is carefully designed and compiled. It could facilitate the future research on style transfer. 
2. The paper is very well written. I enjoyed reading the paper. The text is concise and also clear enough and the figures are illustrative.
3. The semantic consistency loss is reasonable, but I do not think this is significantly novel. 

Weaknesses:
1. Although “the key aim of XGAN is to learn a joint meaningful and semantically consistent embedding”, the experiments are actually devoted to the qualitative style transfer only. A possible experiment design for evaluating “the key aim of XGAN” may be the facial attribute prediction. The CartoonSet contains attribute labels but the authors may need collect such labels for the VGG-face set.
2. Only one baseline is considered in the style transfer experiments. Both CycleGAN and UNIT are very competitive methods and would be better be included in the comparison. 
3. The “many-to-many” is ambiguous. Style transfer in general is not a one-to-one or many-to-one mapping. It is not necessary to stress the many-to-many property of the proposed new task, i.e., semantic style transfer. 

The CartoonSet dataset and the new task, which is called semantic style transfer between two domains, are nice contributions of this paper. In terms of technical contributions, it is not significant to have the X-shaped GAN or the straightforward semantic consistency loss. The experiments are somehow mismatched with the claimed aim of the paper. ","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
The Cramer Distance as a Solution to Biased Wasserstein Gradients,"['Marc G. Bellemare', 'Ivo Danihelka', 'Will Dabney', 'Shakir Mohamed', 'Balaji Lakshminarayanan', 'Stephan Hoyer', 'Remi Munos']",Reject,2018,"[12, 10, 2, 13, 10, 2, 23]","[17, 14, 7, 17, 15, 6, 28]","[102, 28, 67, 66, 90, 16, 266]","[45, 12, 30, 27, 28, 2, 143]","[51, 14, 36, 33, 56, 12, 96]","[6, 2, 1, 6, 6, 2, 27]","The manuscript proposes to use the Cramer distance as a measure between distributions (acting as a loss) when optimizing
an objective function using stochastic gradient descent (SGD). Cramer distance is a Bregman divergence and is a member of the Lp family of divergences.  Here a ""distance"" means a symmetric divergence measure that satisfies the relaxed triangle inequality. The motivation for using the Cramer distance is that it has unbiased sample gradients while still enjoying some other properties such as scale sensitivity and sum invariant. The authors also proof that for the Bernoulli distribution, there is a lower bound independent of the sample size for the deviation between the gradient of the Cramer distance, and the expectation of the estimated gradient of the Cramer distance. Then, the multivariate case of the Cramer distance, called the energy distance, is also briefly presented. The paper closes with some experiments on ordinal regression using neural networks and training GANs using the Cramer distance. 

In general, the manuscript is well written and the ideas are smoothly presented. While the manuscript gives some interesting insights, I find that the contribution could have been explained in a more broader sense, with a stronger compelling message.

Some remarks and questions:

1.	The KL divergence considered here is sum invariant but not scale sensitive, and has unbiased sample gradients. The 
	authors are considering here the standard (asymmetric) KL divergence (sec. 2.1). Is it the case that losing scale
	sensitivity make the KL divergence insensitive to the geometry of the outcomes? or is it due to the fact the KL 
	divergence is not symmetric? or ?

2.	The main argument for the paper is that the simple sample-based estimate for the gradient using the Wasserstein 
	metric is a biased estimate for the true gradient of the Wasserstein distance, and hence it is not favored with
	SGD-type algorithms. Are there any other estimators in the literature for the gradient of the Wasserstein distance?
	Was this issue overlooked in the literature?

3.	I am not sure if a biased estimate for the gradient will lead to a ``wrong minimum'' in an energy space that has 
	infinitely many local minima.  Of course one should use an unbiased estimate for the gradient whenever this is possible.
	However, even when this is possible, there is no guarantee that this will consistently lead to deeper and ``better''
	minima, and there is no guarantee as well that these deep local minima reflect meaningful results.

4.	To what extent can one generalize theorem 1 to other probability distributions (continuous and discrete) and to the 
	multivariate cases as well?

5.	I also don't think that the example given in sec. 4.2 and depicted in Fig. 1 is the best and simplest way to illustrate
	the benefit of Cramer distance over Wasserstein. Similarly, the experiments for the multivariate case using GANs and
	Neural Networks do not really deliver tangible, concrete and conclusive results. Partly, these results are very  
       qualitative, which can be understood within the context of GANs. However, the authors could have used other       
       models/algorithms where they can obtain concrete quantitative results (for this type of contribution). In addition, 
	such sophisticated models (with various hyper-parameters) can mask the true benefit for the Cramer distance, and can 
	also mask the extent of how good/poor is the sample estimate for the Wasserstein gradient.","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 5, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,"['Charles H. Martin', 'Michael W. Mahoney']",Reject,2018,"[2, 16]","[6, 21]","[11, 364]","[3, 128]","[7, 178]","[1, 58]","I find myself having a very hard time making a review of this paper,  because I mostly agree with the intro and discussion, and certainly agree that the ""typical"" versus ""worse case"" analysis is certainly an important point.  The authors are making a strong case for the use of these models to understand overfitting and generalization in deep leaning.

The problem is however that, except from advocating the use of these ""spin glass"" models studied back in the days by Seung, Sompolinksy, Opper and others, there are little new results presented in the paper. The arguments using the Very Simple Deep Learning (VSDL) are essentially a review of old known results --which I agree should maybe be revisited-- and the motivation to their application to deep learning stems from the reasoning  that, since this is the behavior observed in all these model, well then deep learning should behave just the same as well. This might very well be, but this is precisly the point: is it ? 

After reading the paper,  I agree with many points and enjoyed reading the discussion. I found interesting ideas discussed and many papers reviewed, and ended up discovering interesting papers on arxiv as a concequence.

This is all nice, interesting, and well written, but at the end of the day, the paper is not doing too much beyond being a nice review of all ideas. While this has indeed some values, and might trigger a renewal of interested for these approaches, I will let the comity decide if this is the material they want in ICLR.

A minor comment: The generalization result of [9,11] obtained with heuristic tools (the replica method of statistical mechanics) and plotted in Fig.1 (a) has been proven recently with rigorous mathematical methods in arxiv:1708.03395 

Another remark:  if deep learning is indeed well described by these models, then again so are many other simpler problems, such as compressed sensing, matrix and tensor factorization, error corrections, etc etc... with similar phase diagram as in fig. 1.  For instance gaussian mixtures are discussed in http://iopscience.iop.org/article/10.1088/0305-4470/27/6/016/and  SVM (which the authors argue should behave quite differently) methods have been treated by statistical mechanics tools in https://arxiv.org/pdf/cond-mat/9811421.pdf with similar phase diagrams. I am a bit confused what would be so special about deep learning then?
","[7, 6, 3]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks,"['Benjamin J. Meyer', 'Ben Harwood', 'Tom Drummond']",Reject,2018,"[2, 3, 20]","[7, 6, 25]","[9, 14, 200]","[4, 7, 121]","[4, 6, 55]","[1, 1, 24]","(Summary)
This paper proposes weighted RBF distance based loss function where embeddings for cluster centroids and data are learned and used for class probabilities (eqn 3). The authors experiment on CUB200-2011, Cars106, Oxford 102 Flowers datasets.

(Pros)
The citations and related works cover fairly comprehensive and up-to-date literatures on deep metric learning.

(Cons)
The proposed method is unlikely to scale with respect to the number of classes. ""..our approach is also free to create multiple clusters for each class.."" This makes it unfair to deep metric learning baselines in figures 2 and 3 because DMP baselines has memory footprint constant in the number of classes. In contrast, the proposed method have linear memory footprint in the number of classes. Furthermore, the authors ommit how many centroids are used in each experiments.

(Assessment)
Marginally below acceptance threshold. The method is unlikely to scale and the important details on how many centroids the authors used in each experiments is omitted.","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Prototype Matching Networks for Large-Scale Multi-label  Genomic Sequence Classification,"['Jack Lanchantin', 'Arshdeep Sekhon', 'Ritambhara Singh', 'Yanjun Qi']",Reject,2018,"[3, 2, 3, 18]","[8, 7, 7, 23]","[27, 26, 34, 124]","[10, 11, 10, 65]","[15, 14, 13, 47]","[2, 1, 11, 12]","The authors of this manuscript proposed a model called PMN based on previous works for the classification of transcription factor binding. Overall, this manuscript is not well written. Clarification is needed in the method and data sections. The model itself is an incremental work, but the application is novel. My specific concerns are given below.

1. It is unclear how the prototype of a TF is learned. Detailed explanation is necessary. 

2. Why did the authors only allow a TF to have only one prototype? A TF can have multiple distinct motifs.

3. Why peaks with p-value>=1 were defined as positive? Were negative classes considered in the computational experiments?

4. What's the relationship between the LSTM component in the proposed method and sparse coding?

5. The manuscript contains lots of low-end issues, such as:
5.1. Inconsistency in the format when referring to equations (eq. equation, Equation, attention LSTM, attentionLSTM, t and T etc);
5.2. Some ""0""s are missing in Table 3;
5.3. L2 should be L_2 norm; 
5.4. euclidean -> Euclidean; pvalue-> p-value;
5.5. Some author name and year citations in the manuscript should be put in brackets;
5.6. The ENCODE paper should be cited properly, (""Consortium et al., 2012"" is weird!) ;
5.7. The references should be carefully reformatted, for example, some words in the references should be in uppercase (e.g. DNA, JASPER, CNN etc.), some items are duplicated, ...

Comments for the revised manuscript: I decide to keep my decision as it is. My major and minor concerns are not fully well addressed in the revised paper. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
When and where do feed-forward neural networks learn localist representations?,"['Ella M. Gale', 'Nicolas Martin', 'Jeffrey Bowers']",Reject,2018,"[8, 23, 10]","[13, 27, 15]","[51, 37, 32]","[14, 31, 12]","[29, 1, 12]","[8, 5, 8]","Quality and Clarity
The neural networks and neural codes are studied  in a concise way, most of the paper is clear. The section on data design, p3, could use some additional clarification wrt to how the data input is encoded (right now, it is hard to understand exactly what happens). 

Originality
I am not aware of other studies on this topic, the proposed approach seems original. 

Significance
The biggest problem I have is with the significance: I don't see at all how finding somewhat localized responses in the hidden layer of an MLP with just one hidden layer has any bearing on deeper networks structured as CNNs: compared to MLPs, neurons in CNNs have much smaller receptive fields, and are known to be sensitive to selective and distinct  features.  

Overall the results seem rather trivial without greater implications for modern deep neural networks: ie, of course adding dropout improves the degree of localist coding (sec 3.4). Similarly, for a larger network, you will find fewer localist codes (though this is hard to judge, as an exact definition of selectivity is missing). 

Minor issues: the ""selectivity"" p3 is not properly defined.  On p3, a figure is undefined. 
Typo: p2: ""could as be"". 
Many of the references are ugly : p3,  ""in kerasChollet (2015)"", this needs fixing. ","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Pixel Deconvolutional Networks,"['Hongyang Gao', 'Hao Yuan', 'Zhengyang Wang', 'Shuiwang Ji']",Reject,2018,"[2, 17, 7, 12]","[6, 22, 12, 17]","[37, 155, 58, 224]","[14, 76, 15, 85]","[18, 21, 21, 76]","[5, 58, 22, 63]","This paper is well written and easy to follow. The authors propose pixel deconvolutional layers for convolutional neural networks. The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers. 
The method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer. PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer. The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset). 

The authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower). In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation). Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm). More comments and further exploration on this results should be done. Why no performance boost? Is it because of the residual connection? Or other component of DeepLab? Is the proposed layer really useful once a powerful model is used?

I also think the experiments on VAE are not conclusive. The authors simply show set of generated images. First, it is difficult to see the different of the image generated using deconv and PixelDCL. Second, a set of 20 qualitative images does not (and cannot) validate any research idea.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unbiasing Truncated Backpropagation Through Time,"['Corentin Tallec', 'Yann Ollivier']",Reject,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","This is an interesting paper.

It is well known that TBPTT is biased because of a fixed truncation length. The authors propose to make it  unbiased by sampling different truncation lengths and hence changing  the optimization procedure which corresponds to adding noise in the gradient estimates which leads to  unbiased gradients. 

Pros:

- Its a well written and easy to follow paper.
- If I understand correctly, they are changing the optimization procedure so that the proposed approach is able to find a local minima, which was not possible by using truncated backpropagation through time.  
- Its interesting to see in there PTB results that they get better validation score as compared to truncated BPTT.

Cons: 

- Though the approach is interesting, the results are quite preliminary. And given the fact there results are worse than the LSTM baseline (1.40 v/s 1.38). The authors note that it might be because of they are applying without sub-sequence shuffling. 

- I'm not convinced of the approach yet. The authors could do some large scale experiments on datasets like Text8 or speech modelling. 


Few points

- If I'm correct that the proposed approach indeed changes the optimization procedure, than I'd like to know what the authors think about exposure bias issue. Its a well known[1, 2] that we can't sample from RNN's for more number of steps, than what we used for trained (difference b/w teacher forcing and free running RNN). I'd like to know how does there method perform in such a regime (where you sample for more number of steps than you have trained for)

- Another thing, I'd like to see is the results of this model as compared to truncated backpropagation when you increase the sequence length. For example, Lets say you are doing language modelling on PTB, how the result varies when you change the length of the input sequence. I'd like to see a graph where on X axis is the length of the input sequence and on the Y axis is the bpc score (for PTB) and how does it compare to truncated backpropagation through time. 

-  PTB dataset has still not very long term dependencies, so I'm curious what the authors think about using there method for something like speech modelling or some large scale experiments.

- I'd expect the proposed approach to be more computationally expensive as compared to Truncated Back-propagation through time. I dont think the authors mentioned this somewhere in the paper. How much time does a single update takes as compared to Truncated Back-propagation through time ?

- Does the proposed approach help in flow of gradients?  

- In practice for training RNN's people use gradient clipping which also makes the gradient biased. Can the proposed method be used for training longer sequences?  

[1] Scheduled Sampling For Sequence Prediction with RNN's https://arxiv.org/abs/1506.03099
[2] Professor Forcing  https://arxiv.org/abs/1610.09038


Overall, Its an interesting paper which requires some more analysis to be published in this conference. I'd be very happy to increase my score if the authors can provide me results what I have asked for. ","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Post-training for Deep Learning,"['Thomas Moreau', 'Julien Audiffren']",Reject,2018,"[-1, 9]","[1, 14]","[1, 61]","[1, 24]","[0, 24]","[0, 13]","This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. However, the results do not convince this reviewer to switch to using 'post-training'.

Learning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago. However, freezing the layers and continue to train the last layer is of a minor novelty. The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \lambda term which is added for post-training but not added for the baseline. c.f. Eq 3 and Eq 4.
Therefore, it's unclear whether the gain in generalization is due to an additional \lambda term or from the post-training training itself.

A way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.

Other notes, 

Remark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments.

For table 1, please use decimal points instead of commas.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Contextual Explanation Networks,"['Maruan Al-Shedivat', 'Avinava Dubey', 'Eric P. Xing']",Reject,2018,"[5, 10, 18]","[9, 15, 23]","[42, 49, 625]","[19, 27, 339]","[20, 19, 218]","[3, 3, 68]","The article ""Contextual Explanation Networks"" introduces the class of models which learn the intermediate explanations in order to make final predictions. The contexts can be learned by, in principle, any model including neural networks, while the final predictions are supposed to be made by some simple models like linear ones. The probabilistic model allows for the simultaneous training of explanation and prediction parts as opposed to some recent post-hoc methods.

The experimental part of the paper considers variety of experiments, including classification on MNIST, CIFAR-10, IMDB and also some experiments on survival analysis. I should note, that the quality of the algorithm is in general similar to other methods considered (as expected). However, while in some cases the CEN algorithm is slightly better, in other cases it appears to sufficiently loose, see for example left part of Figure 3(b) for MNIST data set. It would be interesting to know the explanation. Also, it would be interesting to have more examples of qualitative analysis to see, that the learned explanations are really useful. I am a bit worried, that while we have interpretability with respect to intermediate features, these features theirselves might be very hard to interpret.

To sum up, I think that the general idea looks very natural and the results are quite supportive. However, I don't feel myself confident enough in this area of research to make strong conclusion on the quality of the paper.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 5, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs,"['Jean Maillard', 'Stephen Clark', 'Dani Yogatama']",Reject,2018,"[4, 20, 10]","[9, 25, 14]","[22, 139, 74]","[10, 94, 33]","[9, 28, 34]","[3, 17, 7]","This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017). Differently from (Yogatama et al, 2017), this paper doesn’t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way. 

Overall, I think it is really an interesting direction and the proposed method sounds reasonable. However, I am concerned about the following points:  

- The improvements are really limited on both the SNLI and the Reverse Dictionary tasks. (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it’d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold. Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures.

- The computational complexity of this model shouldn’t be neglected. If I understand it correctly, the model needs to compute O(N^3) LSTM compositions. This should be at least discussed in the paper. And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).

- I am wondering about the effects of the temperature parameter t. Is that important for training?

Minor:
- What is the difference between LSTM and left-branching LSTM?
- I am not sure if the attention overt chart is a highlight of the paper or not. If so, better move that part to the models section instead of mention it briefly in the experiments section. Also, if any visualization (over the chart) can be provided, that’d be helpful to understand what is going on. 
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Stochastic Training of Graph Convolutional Networks,"['Jianfei Chen', 'Jun Zhu']",Reject,2018,"[6, 14, 15]","[11, 19, 20]","[40, 468, 343]","[16, 204, 185]","[19, 211, 122]","[5, 53, 36]","The paper proposes a method to speed up the training of graph convolutional networks, which are quite slow for large graphs. The key insight is to improve the estimates of the average neighbor activations (via neighbor sampling) so that we can either sample less neighbors or have higher accuracy for the same number of sampled neighbors. The idea is quite simple: estimate the current average neighbor activations as a delta over the minibatch running average. I was hoping the method would also include importance sampling, but it doesn’t. The assumption that activations in a graph convolution are independent Gaussians is quite odd (and unproven). 

Quality: Statistically, the paper seems sound. There are some odd assumptions (independent Gaussian activations in a graph convolution embedding?!?) but otherwise the proposed methodology is rather straightforward. 

Clarity: It is well written and the reader is able to follow most of the details. I wish the authors had spent more time discussing the independent Gaussian assumption, rather than just arguing that a graph convolution (where units are not interacting through a simple grid like in a CNN) is equivalent to the setting of Wang and Manning (I don’t see the equivalence). Wang and Manning are looking at MLPs, not even CNNs, which clearly have more independent activations than a CNN or a graph convolution. 

Significance: Not very significant. The problem of computing better averages for a specific problem (neighbor embedding average) seems a bit too narrow. The solution is straightforward, while some of the approximations make some odd simplifying assumptions (independent activations in a convolution, infinitesimal learning rates). 

Theorem 2 is not too useful, unfortunately: Showing that the estimated gradient is asymptotically unbiased with learning rates approaching zero over Lipchitz functions does not seem like an useful statement. Learning rates will never be close enough to zero (specially for large batch sizes). And if the running activation average converges to the true value, the training is probably over. The method should show it helps when the values are oscillating in the early stages of the training, not when the training is done near the local optimum.


","[4, 3, 7]","[' Ok but not good enough - rejection', ' Clear rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Characterizing Sparse Connectivity Patterns in Neural Networks,"['Sourya Dey', 'Kuan-Wen Huang', 'Peter A. Beerel', 'Keith M. Chugg']",Reject,2018,"[3, 4, 28, 23]","[7, 5, 33, 27]","[21, 11, 205, 88]","[7, 5, 118, 44]","[11, 4, 45, 14]","[3, 2, 42, 30]","The paper seems to claims that
1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,
2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices.
I have two problems with these claims:
1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers.
2) The authors reject the technique of 'Deep compression' as being impractical. I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network.

p3. What does 'normalized' mean? Batch-norm?
p3. Are you using an L2 weight penalty? If not, your fully-connected baseline may be unnecessarily overfitting the training data.
p3. Table 1. Where do the choice of CL Junction densities come from? Did you do a grid search to find the optimal level of sparsity at each level?
p7-8. I had trouble following the left/right & front/back notation.
p8. Figure 7. How did you decide which data points to include in the plots?","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Document Embeddings With CNNs,"['Shunan Zhao', 'Chundi Lui', 'Maksims Volkovs']",Reject,2018,"[2, 5, 12]","[3, 10, 17]","[4, 12, 43]","[2, 7, 27]","[2, 1, 14]","[0, 4, 2]","This paper uses CNNs to build document embeddings.  The main advantage over other methods is that CNNs are very fast.

First and foremost I think this: ""The code with the full model architecture will be released … and we thus omit going into further details here.""  is not acceptable.  Releasing code is commendable, but it is not a substitute for actually explaining what you have done.  This is especially true when the main contribution of the work is a network architecture.  If you're going to propose a specific architecture I expect you to actually tell me what it is.

I'm a bit confused by section 3.1 on language modelling.  I think the claim that it is showing ""a direct connection to language modelling"" and that ""we explore this relationship in detail"" are both very much overstated.  I think it would be more accurate to say this paper takes some tricks that people have used for language modelling and applies them to learning document embeddings.

This paper proposed both a model and a training objective, and I would have liked to see some attempt to disentangle their effect.  If there is indeed a direct connection between embedding models and language models then I would have also expected to see some feedback effect from document embedding to language modeling.  Does the embedding objective proposed here also lead to better language models?

Overall I do not see a substantial contribution from this paper. The main claims seem to be that CNNs are fast, and can be used for NLP, neither of which are new.
","[4, 6, 2]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Strong rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Fixing Weight Decay Regularization in Adam,"['Ilya Loshchilov', 'Frank Hutter']",Reject,2018,"[9, 17]","[10, 22]","[41, 252]","[23, 103]","[16, 120]","[2, 29]","At the heart of the paper, there is a single idea: to decouple the weight decay from the number of steps taken by the optimization process (the paragraph at the end of page 2 is the key to the paper). This is an important and largely overlooked area of implementation and most off-the-shelf optimization algorithms, unfortunately, miss this point, too. I think that the proposed implementation should be taken seriously, especially in conjunction with the discussion that has been carried out with the work of Wilson et al., 2017 (https://arxiv.org/abs/1705.08292).

The introduction does a decent job explaining why it is necessary to pay attention to the norm of the weights as the training progresses within its scope. However, I would like to add a couple more points to the discussion: 
- ""Optimal weight decay is a function (among other things) of the total number of epochs / batch passes."" in principle, it is a function of weight updates. Clearly, it depends on the way the decay process is scheduled. However, there is a bad habit in DL where time is scaled by the number of epochs rather than the number of weight updates which sometimes lead to misleading plots (for instance, when comparing two algorithms with different batch sizes).
- Another ICLR 2018 submission has an interesting take on the norm of the weights and the algorithm (https://openreview.net/forum?id=HkmaTz-0W&noteId=HkmaTz-0W). Figure 3 shows the histograms of SGD/ADAM with and without WD (the *un-fixed* version), and it clearly shows how the landscape appear misleadingly different when one doesn't pay attention to the weight distribution in visualizations. 
- In figure 2, it appears that the training process has three phases, an initial decay, a steady progress, and a final decay that is more pronounced in AdamW. This final decay also correlates with the better test error of the proposed method. This third part also seems to correspond to the difference between Adam and AdamW through the way they branch out after following similar curves. One wonders what causes this branching and whether the key the desired effects are observed at the bottom of the landscape.
- The paper concludes with ""Advani & Saxe (2017) analytically showed that in the limited data regime of deep networks the presence of eigenvalues that are zero forms a frozen subspace in which no learning occurs and thus smaller (e.g., zero) initial weight norms should be used to achieve best generalization results."" Related to this there is another ICLR 2018 submission (https://openreview.net/forum?id=rJrTwxbCb), figure 1 shows that the eigenvalues of the Hessian of the loss have zero forms at the bottom of the landscape, not at the beginning. Back to the previous point, maybe that discussion should focus on the second and third phases of the training, not the beginning. 
- Finally, it would also be interesting to discuss the relation of the behavior of the weights at the last parts of the training and its connection to pruning. 

I'm aware that one can easily go beyond the scope of the paper by adding more material. Therefore, it is not completely reasonable to expect all such possible discussions to take place at once. The paper as it stands is reasonably self-contained and to the point. Just a minor last point that is irrelevant to the content of the work: The slash punctuation mark that is used to indicate 'or' should be used without spaces as in 'epochs/batch'.

Edit: Thanks very much for the updates and refinements. I stand by my original score and would like to indicate my support for this style of empirical work in scientific conferences.","[7, 8, 4]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Transfer Learning to Learn with Multitask Neural Model Search,"['Catherine Wong', 'Andrea Gesmundo']",Reject,2018,"[3, 15, 5, 1, 33, 19]","[8, 20, 10, 2, 38, 23]","[39, 116, 70, 5, 481, 101]","[15, 58, 31, 3, 318, 44]","[21, 8, 27, 2, 76, 5]","[3, 50, 12, 0, 87, 52]","In this paper authors are summarizing their work on building a framework for automated neural network (NN) construction across multiple tasks simultaneously. 

They present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller. The idea behind building such a framework is motivated by the successes of recently proposed reinforcement based approaches for finding the best NN architecture across the space of all possible architectures. Authors cite the Neural Architecture Search (NAS) framework as an example of such a framework that yields better results compared to NN architectures configured by humans. 

Overall I think that the idea is interesting and the work presented in this paper is very promising. Given the depth of the empirical analysis presented the work still feels that it’s in its early stages. In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.

Authors use two text classification tasks from the NLP domain to showcase the benefits of their proposed architecture. It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs. This is especially the case for the transfer learning task. 

In the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler. 

Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures. In addition, on the transfer learning evaluation approach they showcase the benefit of using the proposed framework in terms of the initially retrieved architecture and the number of iterations required to obtain the best performing one. 
For better clarity figures 3 and 5 should be made bigger. 
What is LSS in figure 4?","[4, 5, 7]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,"['Meng Li', 'Liangzhen Lai', 'Naveen Suda', 'Vikas Chandra', 'David Z. Pan']",Reject,2018,"[4, 7, 6, 17, 22]","[9, 11, 10, 22, 27]","[73, 36, 25, 121, 421]","[37, 16, 10, 71, 268]","[24, 15, 11, 39, 46]","[12, 5, 4, 11, 107]","1. This is an interesting paper - introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm
2. From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta-learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level
3. However, the main issue is that the meta-learning mechanism is a bit ad-hoc and empirical - therefore not sure how seamless and user-friendly it will be in general, it seems it needs empirical studies for every new application - this basically involves generation of a pareto front and then choose pareto-optimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Distribution Regression Network,"['Connie Kou', 'Hwee Kuan Lee', 'Teck Khim Ng']",Reject,2018,"[1, 15, 21, 5, -2]","[5, 20, 26, 10, 2]","[20, 189, 263, 21, 6]","[11, 121, 109, 2, 2]","[8, 40, 39, 4, 2]","[1, 28, 115, 15, 2]","The paper considers distribution to distribution regression with MLPs.  The authors use an energy function based approach.  They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters.

This seems to be a nice treatment of distribution to distribution regression with neural networks. The approach is methodological similar to using expected likelihood kernels.  While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters.  That’s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation.  In the discussion, it says 
“For future work, a possible study is to investigate what classes of problems DRN can solve.”  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.  Its practical utility is questionable.  It’s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.  In the introduction, it would also improve the paper to outline clear points of methodological novelty.  
","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Neural Compositional Denotational Semantics for Question Answering,"['Nitish Gupta', 'Mike Lewis']",Reject,2018,"[8, 12]","[13, 17]","[42, 116]","[20, 50]","[21, 61]","[1, 5]","This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like ""red"" and also compositional operators like ""not"".

I think this model is elegant, beautiful and timely. The authors do a good job of explaining it clearly. I like the modules of composition that seem to make a very intuitive sense for the ""algebra"" that is required and the parsing algorithm is clean. 

However, I think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.

I have 2.5 major issues with the paper and a few minor comments: 

Parsing:

* The authors don't really say what is the base case for \Psi that scores tokens (unless I missed it and if indeed it is missing it really needs to be added) and only provide the recursive case. From that I understand that the only features that they use are whether a certain word makes sense in a certain position of the rule application in the context of the question. While these features are based on Durrett et al.'s neural syntactic parser it seems like a pretty weak signal to learn from. This makes me wonder, how does the parser learn whether one parse is better than the other? Only based on this signal? It makes me suspicious that the distribution of language is not very ambiguous and that as long as you can construct a tree in some context you can do it in almost any other context. This is probably due to the fact that the CLEVR dataset was generated mostly using templates and is not really natural utterances produced by people. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced.

* Related to that is that the decision for a score of a certain type t for a span (i,j) is the sum for all possible rule applications, rather than a max, which again means that there is no competition between different parse trees that result with the same type of a single span. Can the authors say something about what the parser learns? Does it learn to extract from the noise clear parse trees? What is the distribution of rules in those sums? is there some rule that is more preferred than others usually? It seems like there is loss of information in the sum and it is unclear what is the effect of that in the paper.

Evaluation:

* Related to that is indeed the fact that they use CLEVR only. There  is now the Cornell NLVR dataset that is more challenging from a language perspective and it would be great to have an evaluation there as well. Also the authors only compare to 3 baselines where 2 don't even see the entire KB, so the only ""real"" baseline is relation net. The authors indeed state that it is state-of-the-art on clevr. 

* It is worth noting that relation net is reported to get 95.5 accuracy while the authors have 89.4. They use a subset so this might be the reason, but I am not sure how they compared to relation net exactly. Did they re-tune parameters once you have the new dataset? This could make a difference in the final accuracy and cause an unfair advantage.

* I would really appreciate more analysis on the trees that one gets. Are sub-trees interpretable? Can one trace the process of composition? This could have been really nice if one could do that. The authors have a figure of a purported tree, but where does this tree come from? From the mode? Form the authors?

Scalability:
* How much of a problem would it be to scale this? Will this work in larger domains? It seems they compute an attention score over every entity and also over a matrix that is squared in the number of entities. So it seems if the number of entities is large that could be very problematic. Once one moves to larger KBs it might become hard to maintain full differentiability which is one of the main selling points of the paper. 

Minor comments:
* I think the phrase ""attention"" is a bit confusing - I thought of a distribution over entities at first. 
* The feature function is not super clearly written I think - perhaps clarify in text a bit more what it does.
* I did not get how the denotation that is based on a specific rule applycation t_1 + t_2 --> t works. Is it by looking at the grounding that is the result of that rule application?
* Authors say that the neural enquirer and neural symbolic machines produce flat programs - that is not really true, the programs are just a linearized form of a tree, so there is nothing very flat about it in my opinion.

Overall, I really enjoyed reading the paper, but I was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the KB is large.


","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deep Continuous Clustering,"['Sohil Atul Shah', 'Vladlen Koltun']",Reject,2018,"[14, 1, 1]","[19, 6, 5]","[144, 15, 4]","[57, 6, 3]","[60, 7, 1]","[27, 2, 0]","The authors proposed a new clustering algorithm named deep continuous clustering (DCC) that integrates autoencoder into continuous clustering. As a variant of  continuous clustering (RCC), DCC formed a global continuous objective for joint nonlinear dimensionality reduction and clustering. The objective can be directly optimized using SGD like method. Extensive experiments on image and document datasets show the effectiveness of DCC. However, part of experiments are not comprehensive enough. 

The idea of integrating autoencoder with continuous clustering is novel, and the optimization part is quite different. The trick used in the paper (sampling edges but not samples) looks interesting and seems to be effective. 

In the following, there are some detailed comments:
1. The paper is well written and easy to follow, except the definition of Geman-McClure function is missing. It is difficult to follow Eq. (6) and (7).
2. Compare DCC to RCC, the pros and cons are obvious. DCC does improve the performance of clustering with the cost of losing robustness. DCC is more sensitive to the hyper-parameters, especially embedding dimensionality d. With a wrong d DCC performs worse than RCC on MNIST and similar on Reuters. Since clustering is one unsupervised learning task. The author should consider heuristics to determine the hyper-parameters. This will increase the usability of the proposed method.
3. However, the comparison to the DL based partners are not comprehensive enough, especially JULE and DEPICT on image clustering. Firstly, the authors only reported AMI and ACC, but not NMI that is reported in JULE. For a fair comparison, NMI results should be included. Secondly, the reported results do not agree with the one in original publication. For example, JULE reported ACC of 0.964 and 0.684 on MNIST and YTF. However, in the appendix the numbers are 0.800 and 0.342 respectively. Compared to the reported number in JULE paper, DCC is not significantly better.

In general, the paper is interesting and proposed method seems to be promising. I would vote for accept if my concerns can be addressed.

The author's respond address part of my concerns, so I have adjusted my rating.","[7, 6, 3]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,"['Thilo Strauss', 'Markus Hanselmann', 'Andrej Junginger', 'Holger Ulmer']",Reject,2018,"[4, 2, 2, 16]","[9, 4, 6, 18]","[13, 5, 4, 19]","[3, 0, 1, 13]","[6, 4, 3, 5]","[4, 1, 0, 1]","This paper describes the use of ensemble methods to improve the robustness of neural networks to adversarial examples. Adversarial examples are images that have been slightly modified (e.g. by adding some small perturbation) so that the neural network will predict a wrong class label.

Ensemble methods have been used by the machine learning community since long time ago to provide more robust and accurate predictions.

In this paper the authors explore their use to increase the robustness of neural networks to adversarial examples.

Different ensembles of 10 neural networks are considered. These include techniques such as bagging or injecting noise in the 
training data. 

The results obtained show that ensemble methods can sometimes significantly improve the robustness against adversarial examples. However,
the performance of the ensemble is also highly deteriorated by these examples, although not as much as the one of a single neural network.

The paper is clearly written.

I think that this is an interesting paper for the deep learning community showing the benefits of ensemble methods against adversarial
examples. My main concern with this paper is the lack of comparison with alternate techniques to increase the robustness against adversarial examples. The authors should have compared with the methods described in:

(Goodfellow et al., 2014; Papernot et al., 2016c), 
(Papernot et al., 2016d) 
(Gu & Rigazio, 2014)

Furthermore, the ensemble approach has the main disadvantage of increasing the prediction time by a lot. For example, with 10 elements in the ensemble, predictions are 10 times more expensive.
------------------------------
I have read the updated version of the paper. I think the authors have done a good job comparing with related techniques. Therefore, I have slightly increased my score.
","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Log-DenseNet: How to Sparsify a DenseNet,"['Hanzhang Hu', 'Debadeepta Dey', 'Allie Del Giorno', 'Martial Hebert', 'J. Andrew Bagnell']",Reject,2018,"[6, 10, 3, 36, 18]","[10, 15, 7, 41, 23]","[22, 66, 4, 409, 195]","[5, 31, 1, 282, 117]","[5, 31, 3, 64, 58]","[12, 4, 0, 63, 20]","The paper proposes a nice idea of sparsification of skip connections in DenseNets. The authors decide to use a principle for sparsification that would minimize the distance among layers during the backpropagation. 

The presentation of the paper could be improved. The paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow. E. g., Fig 1 d is discussed in Appendix and not in the main body of the paper, thus, it could be moved to Appendix section.

Table 1 and 3 presents the results only for LogDenseNet V1, would it be possible to add results for V2 that have different MBD. Also, the budget for the skip connections is defined as log(i) in Table 1 and Table 2 has the budget of log(i/2), would it be possible to add the total number of skip connections to the tables? It would be interesting to compare the total number of skip connections in Jegou et. al. to LogDenseNet V1 in Table 3.

Other issues:
- Table 3, has an accuracy of nan. What does it mean? Not available or not a number? 
- L is used as the depth, however, in table 1 it appears as short for Log-DenseNetV1. Would it be possible to use another letter here?
- “…, we make x_i also take the input from x_{i/4}, x_{i/8}, x_{i/16}…”. Shouldn’t x_{1/2} be used too?
- I’m not sure I understand the reasons behind blurred image in Fig 2 at ½. It is mentioned that “it and its feature are at low resolution”. Could the authors comment on that?
- Abstract: “… Log-DenseNets are easier than DenseNet to implement and to scale.” It is not clear why would LogDenseNets be easier to implement. ","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Incremental Learning through Deep Adaptation,"['Amir Rosenfeld', 'John K. Tsotsos']",Reject,2018,"[12, 43]","[14, 48]","[24, 230]","[9, 119]","[13, 51]","[2, 60]","----------------- Summary -----------------
The paper tackles the problem of task-incremental learning using deep networks. It devises an architecture and a training procedure aiming for some desirable properties; a) it does not require retraining using previous tasks’ data, b) the number of network parameters grows only sublinearly c) it preserves the output of the previous tasks intact.

----------------- Overall -----------------
The paper tackles an important problem, aims for important characteristics, and does extensive and various experiments. While the broadness of the experiments are encouraging, the main task which is to propose an effective task-incremental learning procedure is not conclusively tested, mainly due to the lack of thorough ablation studies (for instance when convolutional layers are fixed) and the architecture seems to change from one baseline (method) to another.

----------------- Details -----------------
- in the abstract it says: ""Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.""
The linear-combination constraint in the proposed approach is a strong one and can learn a sub-optimal solution for the newly introduced tasks.

- Page 3: R^C → R^{C_o}

- The notation is (probably unnecessarily) too complicated, perhaps it’s better to formulate it without being faithful to the actual implementation but for higher clarity and ease of understanding. For instance, one could start from denoting feature maps and applying the controller/transform matrix W on that, circumventing the clutter of convolutional kernels.

- What is the DAN architecture? 

- In table 1 a better comparison is when using same architecture (instead of VGG) to train it from scratch or fine-tune from ImageNet (the first two rows)

- What is the architecture used for random-weights baseline?

- An experiment is needed where no controller is attached but just the additional fully-connected layers to see the isolated improvements gained by the linear transform of convolutional layers.

- Multiple Base Networks: The assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance. So, finding the best base network using the validation set is not relevant.

- The same concern as above applies to the transferability and dataset decider experiments
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Understanding Grounded Language Learning Agents,"['Felix Hill', 'Karl Moritz Hermann', 'Phil Blunsom', 'Stephen Clark']",Reject,2018,"[7, 7, 15, 20]","[12, 9, 20, 25]","[84, 44, 178, 139]","[35, 22, 91, 94]","[43, 20, 70, 28]","[6, 2, 17, 17]","This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Nonetheless the paper takes on an ambitious goal of relating questions in machine learning in cognitive science and does a reasonably good job of analyzing the results.

Comments:

1. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail:

http://www.iub.edu/~cogdev/labwork/kinds.pdf
http://www.iub.edu/~cogdev/labwork/Ontology2003.pdf

2. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases? It seems like there is a strong basic color bias. This seems like one of the most novel findings in the paper and is worth highlighting.

This figure and the corresponding analysis could be made more systematic by mapping out the degree of shape versus color bias as a function of the number of shape and color terms in a 2D plot. The resulting plot would show the degree of bias towards color.

3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more"" hypothesis in language development by Jeff Elman and Elissa Newport:

https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf
http://www.sciencedirect.com/science/article/pii/0388000188900101

4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later. I found these human results hard to reconcile with the results from the models. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper.

5. The section on layerwise attention claims to give a “computational level” explanation, but this is a misleading term to use — it is not a computational level explanation in the sense introduced by David Marr which is the standard use of this term in cognitive science. The explanation of layerwise attention could be clearer.

Minor:

“analagous” -> “analogous”

The paper runs longer than eight pages, and it is not obvious that the extra space is warranted.
","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Massively Parallel Hyperparameter Tuning,"['Lisha Li', 'Kevin Jamieson', 'Afshin Rostamizadeh', 'Katya Gonina', 'Moritz Hardt', 'Benjamin Recht', 'Ameet Talwalkar']",Reject,2018,"[1, 10, 12, 10, -1, 13, 17, 11]","[6, 15, 17, 12, 1, 18, 22, 16]","[14, 99, 70, 15, 1, 130, 212, 126]","[6, 49, 37, 10, 1, 61, 90, 54]","[8, 47, 30, 3, 0, 62, 91, 61]","[0, 3, 3, 2, 0, 7, 31, 11]","This paper adapts the sequential halving algorithm that underpins Hyperband to run across multiple workers in a compute cluster. This represents a very practical scenario where a user of this algorithm would like to trade off computational efficiency for a reduction in wall time. The paper's empirical results confirm that indeed significant reductions in wall time come with modest increases in overall computation, it's a practical improvement.

The paper is crisply written, the extension is a natural one, the experiment protocols and choice of baselines are appropriate.

The left panel of figure 3 is blurry, compared with the right one.","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
UCB EXPLORATION VIA Q-ENSEMBLES,"['Richard Y. Chen', 'Szymon Sidor', 'Pieter Abbeel', 'John Schulman']",Reject,2018,"[13, 4, 17, 8]","[13, 8, 22, 13]","[9, 14, 608, 66]","[3, 4, 291, 27]","[6, 9, 291, 37]","[0, 1, 26, 2]","The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.

There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I’m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn’t convinced by the link to Bayesian RL in this paper.

I liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results:
-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? 
-How does the distribution of Q values look like during different phases of learning?
-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.
-What’s different between Alg 1 and bootstrapped DQN (other than the action selection)?

Minor things:
-Missing propto in Eq 7?
-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere…
-it looks more a Bellman residual update as written in (11).
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning to diagnose from scratch by exploiting dependencies among labels,"['Li Yao', 'Eric Poblenz', 'Dmitry Dagunts', 'Ben Covington', 'Devon Bernard', 'Kevin Lyman']",Reject,2018,"[15, 4, 2, 2, 2, 17]","[20, 4, 1, 3, 1, 18]","[130, 3, 1, 5, 1, 6]","[64, 0, 0, 0, 0, 0]","[15, 3, 1, 5, 1, 5]","[51, 0, 0, 0, 0, 1]","This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images. 
Authors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification. Experiments are clearly described and results are significantly better compared to state of the art.

The only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain. Considering that their paper is titled as a work to use ""dependencies"" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation. 

On the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently. To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well. 

That said, the paper has several positive aspects in all areas:

Originality - the paper presents first combination of DenseNets with LSTM-based output factorization,
Writing clarity - the paper is very well written and clear.
Quality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done,
Significance - Apart from the issue of intractable inference which is arguably a large limitation of this work, the application in medical field is significant. 

","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,"['Yichi Zhang', 'Zhijian Ou']",Reject,2018,"[10, 18]","[15, 23]","[174, 101]","[71, 52]","[52, 42]","[51, 7]","The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. 

I have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. 

The following reference also needs to be discussed in the context of using SG-MCMC in RNN.
- ""Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling"", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"

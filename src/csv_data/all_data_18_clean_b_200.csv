title,Authors,decision,year,academic_age,current_age,total_num_pub,total_num_conference,total_num_informal,total_num_journal,review,rating_score,rating_text,confidence_score,confidence_text
Block-Sparse Recurrent Neural Networks,"['Sharan Narang', 'Eric Undersander', 'Gregory Diamos']",Reject,2018,"[4, 2, 11]","[9, 7, 15]","[33, 11, 49]","[8, 4, 26]","[23, 6, 20]","[2, 1, 3]","Thanks to the authors for their response.

Though the paper presents an interesting approach, but it relies heavily on heuristics (such as those mentioned in the initial review) without a thorough investigation of scenarios in which this might not work. Also, it might be helpful to investigate if there ways to better group the variables for group  lasso regularization. The paper therefore needs further improvements towards following a more principled approach.

=====================================
This paper presents methods for inducing sparsity in terms of blocks of weights in neural networks which aims to combine benefits of sparsity and faster access based on computing architectures. This is achieved by pruning blocks of weights and group lasso regularization. It is demonstrated empirically that model size can be reduced by upto 10 times with some loss in prediction accuracy.

Though the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be :

- The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved. Also, the algorithm for computing the pruning threshold is based on a new set of hyper-parameters. It is not clear under what conditions the above settings will (not) work.

 - For the group lasso method, since there are many ways to group the variable, it is not clear how the variables are grouped. Is there a reasoning behind a particular grouping of the variables. Individually, group lasso does not seem to work, and gives much worse results. The reasons for worse performance could be investigated. It is possible that important weights are in different groups, and group sparsity is forcing some of them to be zero, and hence leading to worse results. It would be insightful to explain the kind of solver used for group lasso regularization, and if that works for large-scale problems.

 - The results for various kinds of sparsity are unclear in the sense that it is not clear how to set the block size a-priori for having minimum reduction in accuracy and still significant sparsity without having to repeat the process for various choices.

Overall, the paper does not seem to present novel ideas, and is mainly focused on evaluating the impact of block-based sparsity instead of weight pruning by Han etal. As mentioned in Section 2, regularization has been used earlier to achieve sparsity in deep networks. In this view the significance over existing work is relatively narrow, and no explicit comparison with existing methods is provided. It is possible that an existing method leads to pruning method such as by Han etal. leads to 8x decrease in model size while retaining the accuracy, while the proposed method leads to 10x decrease while also decreasing the accuracy by 10%. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper.","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Topology Adaptive Graph Convolutional  Networks,"['Jian Du', 'Shanghang Zhang', 'Guanhang Wu', 'José M. F. Moura', 'Soummya Kar']",Reject,2018,"[9, 7, 3, 37, 13]","[9, 12, 7, 42, 18]","[21, 108, 17, 502, 319]","[7, 45, 10, 260, 130]","[9, 54, 7, 99, 106]","[5, 9, 0, 143, 83]","The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step.

The proposed idea is not exceptional original, but the paper has several strong points:

* The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one.
* The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level.
* The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach).
* The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets.

In summary, I think the paper can be accepted for ICLR.
----------- EDIT -----------
After reading the publications mentioned by the other reviewers as well as the following related contributions

* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)
* Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)

I agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Normalized Direction-preserving Adam,"['Zijun Zhang', 'Lin Ma', 'Zongpeng Li', 'Chuan Wu']",Reject,2018,"[12, 22, 16, 17]","[17, 27, 21, 22]","[64, 260, 268, 212]","[11, 119, 136, 113]","[13, 32, 25, 29]","[40, 109, 107, 70]","The paper extended the Adam optimization algorithm to preserve the update direction. Instead of using the un-centered variance of individual weights, the proposed method adapts the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector. The authors empirically demonstrated the method works well on CIFAR-10/100 tasks.

Comments:

- I found the paper very hard to follow. The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest. The authors also combined the proposed method with a few existing deep learning tricks in the paper. All those tricks that, ie. section 3.3 and 4, should go into the background section.

- Overall, the only contribution of the paper seems to be the ad-hoc modification to Adam in Eq. (9). Why is this a reasonable modification? Do we expect this modification to fail in any circumstances? The experiments on CIFAR dataset and one CNN architecture do not provide enough evidence to show the proposed method work well in general.

","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Depth separation and weight-width trade-offs for sigmoidal neural networks,"['Amit Deshpande', 'Navin Goyal', 'Sushrut Karmalkar']",Reject,2018,"[17, 16, 2]","[22, 21, 6]","[52, 92, 31]","[27, 41, 16]","[21, 37, 15]","[4, 14, 0]","This paper proves a new separation results from 3-layer neural networks to 2-layer neural networks. The core of the analysis is a proof that any 2-layer neural networks can be well approximated by a polynomial function with reasonably low degrees. Then the authors constructs a highly non-smooth function can be represented by a 3-layer network, but impossible to approximate by any polynomial-degree polynomial function.

Similar results about polynomial approximation can be found in [1] (Theorem 4). To me, the result proved in [1] is spiritually very similar to propositions 3-4. The authors need to justify the difference.

The main strength of the new separation result is that it holds for a larger class of input distributions. Comparing to Daniely (2017) which requires the input distribution to be spherically uniform, the new result only needs the distribution to be lower bounded by 1/poly(d) in a small ball of radius 1/poly(d). Conceptually I don't think this is a much weaker condition. For a ""truly"" non-uniform distribution, one should allow its density function to be very close to zero at certain regions of the ball. Nevertheless, the result is a step forward from Daniely (2017) and the paper is well written.

I am still in doubt of the practical value of such kind of separation results. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. This function has a super large Lipschitz constant, which we don't expect to see in practice. Consider the function f(x)=cos(Nx). When N is chosen large enough, the function f can not be well approximated by any 2-layer network with polynomial size. Does it imply that the family of cosine functions is rich enough so that it is a better family to learn than 2-layer neural networks? I guess the answer would be negative. In addition, the paper doesn't show that any 2-layer network can be well approximated by a 3-layer network, which is a missing piece in justifying the richness of 3-layer nets.

Finally, the constructed ""hard"" function has order d^5 Lipschitz constant, but Theorem 7 assumes that the 2-layer networks' weight must be bounded by O(d^2). This assumption is crucial to the proof but not well justified (especially considering the d^5 factor in the function definition).

[1] On the Computational Efficiency of Training Neural Networks, Livni et al., NIPS'14","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
A Classification-Based Perspective on GAN Distributions,"['Shibani Santurkar', 'Ludwig Schmidt', 'Aleksander Madry']",Reject,2018,"[5, 5, 14]","[10, 10, 19]","[41, 104, 127]","[17, 50, 55]","[24, 50, 64]","[0, 4, 8]","This paper propose to evaluate the distributions learned by GAN using classification-based methods. As two examples, the authors evaluates the mode collapse effect and measure the diversity for GAN distributions. The proposed approaches are experimental but does not require human inspection. The main idea is to fit a classifier on the training data and also learn a GAN model using the training data. Then generate simulated data using GAN and use the classifier to predict the labels of the simulated data. The distribution of predicted labels  and the labels of the true data can be easily compared.

Detailed comments:

1. The proposed method is purely experimental. It  would be better to gain some theoretical insights of this methodology. Moreover, in terms of experiments, it would be nice to consider more examples except for mode collapse and diversity, since these problems are well-known for GAN.

2. Since mode collapse is a well-known phenomenon, the novelty of this paper is not sufficient.

3. There are other measures for the quality of GAN. For example, the inception scores and mode scores (Salimans et al. 2016, Che et al. 2017). It would be nice to compare the method here with other related work.

References:
1. Improved Techniques for Training GANs https://arxiv.org/abs/1606.03498

2. Mode Regularized Generative Adversarial Networks https://arxiv.org/abs/1612.02136

","[3, 6, 5]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Influence-Directed Explanations for Deep Convolutional Networks,"['Anupam Datta', 'Matt Fredrikson', 'Klas Leino', 'Linyi Li', 'Shayak Sen']",Reject,2018,"[1, 5, 18, 11, 12]","[6, 8, 22, 16, 17]","[24, 20, 140, 89, 64]","[11, 9, 82, 47, 25]","[12, 11, 49, 35, 23]","[1, 0, 9, 7, 16]","SUMMARY 
========
This paper proposes to measure the ""influence"" of single neurons w.r.t. to a quantity of interest represented by another neuron, typically w.r.t. to an output neuron for a class of interest, by simply taking the gradient of the corresponding output neuron w.r.t to the considered neuron. This gradient is used as is, given a single input instance, or else, gradients are averaged over several input instances. 
In the latter case the averaging is described by an ad-hoc distribution of interest P which is introduced in the definition of the influence measure, however in the present work only two types of averages are practically used: either the average is performed over all instances belonging to one class, or over all input instances.

In other words, standard gradient backpropagation values (or average of them) are used as a proxy to quantify the importance of neurons (these neurons being within hidden layers or at the input layer), and are intended to better explain the classification, or sometimes even misclassification, performed by the network.

The proposed importance measure is theoretically justified by stating a few properties (called axioms) an importance measure should generally verify, and then showing the proposed measure fullfills these requirements.

Empirically the proposed measure is used to inspect the classification of a few input instances, to extract ""class-expert"" neurons, and to find a preprocessing bug in one model. The only comparison to a related work method is done qualitatively on one image visualization, where the proposed method is compared to Integrated Gradients [Sundararajan et al. 2017].

WEAKNESSES
==========
The similarity and differences between the proposed method and related work is not made clear. For example, in the case of a single input instance, and when the quantity of interest is one output neuron corresponding to one class, the proposed measure is identical to the image-specific class saliency of [Simonyan et al. 2014].
The difference to Integrated Gradients [Sundararajan et al. 2017] at the end of Section 1.1 is also not clearly formulated: why is the constraint on distribution marginality weaker here ?
An important class of explanation methods, namely decomposition-based methods (e.g. LRP, Excitation Backprop, Deep Taylor Decomposition), are not mentioned. Recent work (Montavon et al., Digital Signal Processing, 2017), discusses the advantages of decomposition-based methods over gradient-based approaches. Thus, the authors should clearly state the advantages/disadvantes of the proposed gradient-based method over decomposition-based techniques.

Concerning the theoretical justification:
It is not clear how Axiom 2 ensures that the proposed measure only depends on points within the input data manifold. This is indeed an important issue, since otherwise the gradients in equation (1) might be averaged completely outside the data manifold and thus the influence measure be unrelated to the data and problem the neural network was trained on. Also the notation used in Axiom 5 is very confusing. Moreover it seems this axiom is even not used in the proof of Theorem 2.

Concerning the experiments:
The experimental setup, especially in Section 3.3.1, is not well defined: on which layer of the network is the mask applied? What is the ""quantity of interest"": shouldn't it be an output neuron value rather than h|i (as stated at the begin of the fourth paragraph of Section 3.3.1)?
The proposed method should to be quantitatively compared with other explanation techniques (e.g. by iteratively perturbing most relevant pixels and tracking the performance drop, see Samek et al., IEEE TNNLS, 2017).
The last example of explaining the bug is not very convincing, since the observation that class 2 distinctive features are very small in the image space, and thus might have been erased through gaussian blur, is not directly related to the influence measure and could have been made aso independently from it.

CONCLUSION
==========
Overall this work does not introduce any new importance measure for neurons, it merely formalizes the use of standard backpropagation gradients as influence measure.
Using gradients as importance measure was already done in previous work (e.g. [Simonyan et al. 2014]). Though taking the average of gradients over several input instances is new, this information might not be of great help for practical applications.
Recent work also showed that raw gradients are less informative than decomposition-based quantities to explain the classification decisions made by a neural network.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Federated Learning: Strategies for Improving Communication Efficiency,"['Jakub Konečný', 'H. Brendan McMahan', 'Felix X. Yu', 'Ananda Theertha Suresh', 'Dave Bacon', 'Peter Richtárik']",Reject,2018,"[6, 16, 8, 9, 9, 14]","[10, 21, 12, 14, 14, 16]","[41, 98, 73, 296, 124, 19]","[7, 41, 36, 82, 59, 3]","[27, 52, 34, 168, 59, 7]","[7, 5, 3, 46, 6, 9]","This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance.  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server.  Experiments are based on the federated averaging algorithm.  The work is valuable, but has room for improvement.

The paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms.  This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used.  For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?  How is the structured update ""learned""?  It would be very helpful to include algorithms.

It seems like a good strategy is to subsample, perform Hadamard rotation, then quantise.    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data.  It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo.  

If convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:
VARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING

pros:

- good use of intuition to guide algorithm choices
- good compression with little loss of accuracy on best strategy
- good problem for FA algorithm / well motivated
- 

cons:

- some experiment choices do not appear well motivated / inclusion is not best choice
- explanations of algos / lack of 'algorithms' adds to confusion

a useful reference:

Strom, Nikko. ""Scalable distributed dnn training using commodity gpu cloud computing."" Sixteenth Annual Conference of the International Speech Communication Association. 2015.

","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning Non-Metric Visual Similarity for Image Retrieval,"['Noa Garcia', 'George Vogiatzis']",Reject,2018,"[2, 16]","[7, 21]","[48, 58]","[21, 38]","[22, 8]","[5, 12]","This paper presents a simple image retrieval method. Paper claims it is a deep learning method, however it is not an end-to-end network. The main issue of the paper is lack of technical contributions.

Paper assumes that image retrieval task can be reformulated at a supervised similarity learning task. That is fine, however image retrieval is traditionally an unsupervised task. 

Even after using supervised method and deep learning technique, still this method is not able to obtain better results than hand crafted methods. Why is that? See - paper from CVPR2012 -  Arandjelović, Relja, and Andrew Zisserman. ""Three things everyone should know to improve object retrieval."" Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012.

Paper make use of external signal to obtain y_{i,j}. It is not clear to me how does this generalize to large datasets?

If features are L2 normalized, why you need to normalize the features again in equation 5?

In equation 5, why not simply use a max margin deep similarity metric learning method with slack variables to generalizability?

The performance of entire network really rely on the accuracy of y_{i,j} and it is not clear the obtained performance is simply due to this supervision.

Paper does not argue well why we need this supervision.

Technically, there is nothing new here.
","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Anomaly Detection with Generative Adversarial Networks,"['Lucas Deecke', 'Robert Vandermeulen', 'Lukas Ruff', 'Stephan Mandt', 'Marius Kloft']",Reject,2018,"[-3, -3, -3, -3, -3, 3]","[1, 1, 1, 1, 1, 7]","[2, 1, 3, 1, 1, 13]","[2, 1, 2, 1, 1, 6]","[0, 0, 1, 0, 0, 2]","[0, 0, 0, 0, 0, 5]","In the paper, the authors proposed using GAN for anomaly detection.
In the method, we first train generator g_\theta from a dataset consisting of only healthy data points.
For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \approx g_\theta(z).
If such a representation z could be found, x is deemed to be healthy, and anomalous otherwise.
For searching z, the authors proposed a gradient-descent based method that iteratively update z.
Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.
The authors claimed that this parameter update is one of the novelty of their method, making it different from the method of Schlegl et al. (2017).
In the experiments, the authors showed that the proposed method attained the best AUC on MNIST and CIFAR-10.

In my first reading of the paper, I felt that the baselines in the experiments are too primitive.
Specifically, for KDE and OC-SVM, a naive PCA is used to reduce the data dimension.
Nowadays, there are several publicly available CNNs that are trained on large image datasets such as ImageNet.
Then, one can use such CNNs as feature extractor, that will give better low dimensional expression of the data than the naive PCA.
I believe that the performances of KDE and OC-SVM can be improved by using such feature extractors.

Additionally, I found that some well-known anomaly detection methods are excluded from the comparison.
In Emmott et al. (2013), which the authors referred as a related work, it was reported that Isolation Forest and Ensemble of GMMs performed well on several datasets (better than KDE and OC-SVM).
It would be essential to add these methods as baselines to be compared with the proposed method.

Overall, I think the experimental results are far from satisfactory.


### Response to Revision ###
It is interesting to see that the features extracted from AlexNet are not helpful for anomaly detection.
It would be interesting to see whether features extracted from middle layers are helpful or they are still useless.
I greatly appreciate the authors for their extensive experiments as a response to my comments.
However, I have decided to keep my score unchanged, as the additional experiments have shown that the performance of the proposed method is not significantly better than the other methods.
In particular, in MNIST, GMM performed better.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
MINE: Mutual Information Neural Estimation,"['Mohamed Ishmael Belghazi', 'Sai Rajeswar', 'Aristide Baratin', 'Devon Hjelm', 'Aaron Courville']",Reject,2018,"[4, 5, 1, 5, 18]","[7, 9, 5, 9, 23]","[10, 24, 14, 71, 309]","[4, 11, 3, 31, 135]","[6, 12, 10, 37, 160]","[0, 1, 1, 3, 14]","This paper presents a new method for estimation of mutual information (MI) based on the Donsker-Varhan (DV) representation of KL-divergence. This representation requires the calculation of a supremum over a set of functions and a lower bound is achieved when a neural network is used for the maximisation of it. Computing the DV representation also requires evaluating expectations wrt to the distributions of interest, the proposed method uses Monte-Carlo estimates based on the empirical distributions.

The experiments evaluating the quality of the OMIE estimator for mutual information should be more thorough to make a point that OMIE beats competing estimators. The bivariate Gaussian case presented in Figure 1 is not a very relevant test case as estimating MI is especially difficult in higher dimensions. It would also be interesting to know the number of samples used as the ratio nbr dimensions/samples matters for estimation quality. The caption for Figure 2 mentions “bivariate Gaussians of dimension 50”, do the author mean two Gaussians of dimension 50 each?

The results of the proposed method on the swiss-roll dataset look good, however the authors only provide a comparison to a classic GAN where it seems more natural to compare with the other works on mode-dropping for GAN cited in the related works section. A comparison with InfoGAN and Dai et al. would be especially relevant to evaluate the effectiveness of OMIE. 

On the application of OMIE to the Information Bottleneck (IB) problem:
How was the optimization of the objective exactly performed? How are gradients calculated? Is the reparametrisation trick used? More details should be provided on the results presented in table 3. Are the results obtained on the test set? What was the value of beta and to which values of I(X,Z) and I(Z,Y) does it correspond? Was the misclassification rate averaged over multiple runs?

The generalization to f-divergences is interesting but seems rather straightforward. 

The second line of equation (20) does not make sense to me, it is not equivalent to the first line.

The methods proposed in Alemi et al. and Chalk et al. differ also in the way the bounds are estimated, not only in the choice of the marginal distribution.

The authors mention that strong consistency and convergence properties (page 3) are proven in the appendix, however I could not find them.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
End-to-End Abnormality Detection in Medical Imaging,"['Dufan Wu', 'Kyungsang Kim', 'Bin Dong', 'Quanzheng Li']",Reject,2018,"[6, 7, 13, 15]","[10, 12, 18, 20]","[20, 36, 76, 156]","[4, 13, 22, 66]","[11, 12, 32, 53]","[5, 11, 22, 37]","This paper proposes to jointly model computed tomography reconstruction and lesion detection in the lung, training the mapping from raw sinogram to detection outputs in an end-to-end manner. In practice, such a mapping is computed separately, without regard to the task for wich the data is to be used. Because such a mapping loses information, optimizing such a mapping jointly with the task should preserve more information that is relevant to the task. Thus, using raw medical image data should be useful for lesion detection in CT as well as most other medical image analysis tasks.


Style considerations:

The work is adequately motivated and the writing is generally clear. However, some phrases are awkward and unclear and there are occasional minor grammar errors. It would be useful to ask a native English speaker to polish these up, if possible. Also, there are numerous typos that could nonetheless be easily remedied with some final proofreading. Generally, the work is well articulated with sound structure but needs polish.

A few other minor style points to address:
- ""g"" is used throughout the paper for two different networks and also to define gradients - if would be more clear if you would choose other letters.
- S3.3, p. 7 : reusing term ""iteration""; clarify
- fig 10: label the columns in the figure, not in the description
- fig 11: label the columns in the figure with iterations
- fig 8 not referenced in text


Questions:

1. Before fine-tuning, were the reconstruction and detection networks trained end-to-end (with both L2 loss and cross-entropy loss) or were they trained separately and then joined during fine-tuning?
(If it is the former and not the latter, please make that more clear in the text. I expect that it was indeed the former; in case that it was not, I would expect fully end-to-end training in the revision.)

2. Please confirm: during the fine-tuning phase of training, did you use only the cross-entropy loss and not the L2 loss?

3a. From equation 3 to equation 4 (on an iteration of reconstruction), the network g() was dropped. It appears to replace the diagonal of a Hessian (of R) which is probably a conditioning term. Have you tried training a g() network? Please discuss the ramifications of removing this term.

3b. Have you tracked the condition number of the Jacobian of f() across iterations? This should be like tracking the condition number of the Hessian of R(x).

4. Please discuss: is it better to replace operations on R() with neural networks rather than to replace R()? Why?

5. On page 5, you write ""masks for lung regions were pre-calculated"". Were these masks manual segmentations or created with an automated method?

6. Why was detection only targetted on ""non-small nodules""? Have you tried detecting small nodules?

7. On page 11, you state: ""The tissues in lung had much better contrast in the end-to-end network compared to that in the two-step network"". I don't see evidence to support that claim. Could you demonstrate that?

8. On page 12, relating to figure 11, you state:

""Whereas both methods kept similar structural component, the end-to-end method had more focus on the edges and tissues inside lung compared to the two-step method. As observed in figure 11(b), the structures of the lung tissue were much more clearer in the end-to-end networks. This observation indicated that sharper edge and structures were of more importance for the detection network than the noise level in the reconstructed images, which is in accordance with human perceptions when radiologists perform the same task.""

However, while these claims appear intuitive and such results may be expected, they are not backed up by figure 11. Looking at the feature map samples in this figure, I could not identify whether they came from different populations. I do not see the evidence for ""more focus on the edges and tissues inside lung"" for the end-to-end method in fig 11. It is also not obvious whether indeed ""the structures of the lung tissue were much more clearer"" for the end-to-end method, in fig 11. Can you clarify the evidence in support of these claims? 


Other points to address:

1. Please report statistical significance for your results (eg. in fig 5b, in the text, etc.). Also, please include confidence intervals in table 2.

2. Although cross-entropy values, detection metrics were not (except for the ROC curve with false positives and false negatives). Please compute: accuracy, precision, and recall to more clearly evaluate detection performance.

3a. ""Abnormality detection"" implies the detection of anything that is unusual in the data. The method you present targets a very specific abnormality (lesions). I would suggest changing ""abnormality detection"" to ""lesion detection"".

3b. The title should also be updated accordingly. Considering also that the presented work is on a single task (lesion detection) and a single medical imaging modality (CT), the current title appears overly broad. I would suggest changing it from ""End-to-End Abnormality Detection in Medical Imaging"" -- possibly to something like ""End-to-End Computed Tomography for Lesion Detection"".


Conclusion:

The motivation of this work is valid and deserves attention. The implementation details for modeling reconstruction are also valuable. It is interesting to see improvement in lesion detection when training end-to-end from raw sinogram data.  However, while lung lesion detection is the only task on which the utility of this method is evaluated, detection improvement appears modest. This work would benefit from additional experimental results or improved analysis and discussion.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Transformation Autoregressive Networks,"['Junier Oliva', 'Avinava Dubey', 'Barnabás Póczos', 'Eric P. Xing', 'Jeff Schneider']",Reject,2018,"[6, 10, 8, 17, 16, 18, 26]","[11, 15, 13, 22, 21, 23, 31]","[65, 49, 143, 239, 419, 625, 177]","[29, 27, 69, 120, 207, 339, 118]","[35, 19, 73, 105, 201, 218, 52]","[1, 3, 1, 14, 11, 68, 7]","This paper offers an extension to density estimation networks that makes them better able to learn dependencies between covariates of a distribution.

This work does not seem particularly original as applying transformations to input is done in most AR estimators.

Unfortunately, it's not clear if the work is better than the state-of-the-art. Most results in the paper are comparisons of toy conditional models. The paper does not compare to work for example from Papamakarios et al. on the same datasets. The one Table that lists other work showed LAM and RAM to be comparable. Many of the experiments are on synthetic results, and the paper would have benefited from concentrating on more real-world datasets.","[5, 5, 8]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unsupervised Hierarchical Video Prediction,"['Nevan Wichers', 'Dumitru Erhan', 'Honglak Lee']",Reject,2018,"[7, 3, 0, 22]","[11, 8, 4, 27]","[3, 20, 14, 294]","[3, 11, 8, 171]","[0, 0, 6, 106]","[0, 9, 0, 17]","The paper presents a method for predicting future video frames. The method is based on Villegas et al. (2017), with the main difference being that no ground truth pose is needed to train the network.

The novelty of the method is limited. It seems that there is very little innovation in terms of network architecture compared to Villegas et al. The difference is mainly on how the network is trained. But it is straightforward to train the architecture of Villegas et al. without pose -- just use any standard choice of loss that compares the predicted frame versus the ground truth frame. I don't see what is non-trivial or difficult about not using pose ground truth in training.

Overall I think the contribution is not significant enough. 
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Sparse-Complementary Convolution for Efficient Model Utilization on CNNs,"['Chun-Fu (Richard) Chen', 'Jinwook Oh', 'Quanfu Fan', 'Marco Pistoia', 'Gwo Giun (Chris) Lee']",Reject,2018,"[8, 10, 14, 20]","[12, 15, 19, 25]","[73, 39, 77, 83]","[40, 26, 50, 53]","[23, 0, 21, 16]","[10, 13, 6, 14]","This paper introduces a new design of kernels in convolutional neural networks. The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels. Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels.

Strengths:
- The complementary kernels come at no loss compare to standard ones
- The resulting wider networks can achieve better accuracies than the original ones

Weaknesses:
- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions
- The improvement over the baseline is not very impressive
- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)

Detailed comments:
- The separation into + and x patterns is quite clear for 3x3 kernels. However, two such patterns would not be sufficient for 5x5 or 7x7 kernels. This idea would have more impact if it generalized to arbitrary kernel dimensions.

- The improvement over the original models are of the order of less than 1 percent. I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches. It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment.

- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper. To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors). What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS.

- In Section 2, the authors review ideas of so-called random kernel sparsity. Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead. They also do not require pre-training and re-training, but just a single training procedure. Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model.

- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks.

- I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt. Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this?

- Is there a particular reason why the central points appears in both complementary kernels (+ and x)?

- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation? Did the baseline (original model) reported here also use 50k? What would the results be with 45k?

- Fig. 5 is not entirely clear to me. What was the width of each layer? The original one or the modified one?

- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model.

- In Table 4, I find it surprising that there is an actual speedup for the model with larger width. I would have expected the same runtime. How do the authors explain this? 
","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
An empirical study on evaluation metrics of generative adversarial networks,"['Gao Huang', 'Yang Yuan', 'Qiantong Xu', 'Chuan Guo', 'Yu Sun', 'Felix Wu', 'Kilian Weinberger']",Reject,2018,"[2, 7, 16, 5, 4, 25, 15]","[7, 12, 21, 10, 4, 30, 20]","[43, 207, 104, 78, 10, 54, 199]","[20, 71, 47, 27, 5, 22, 106]","[23, 85, 34, 42, 4, 26, 82]","[0, 51, 23, 9, 1, 6, 11]","This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation. This paper has some interesting insights and a few ideas of how to validate an evaluation method. The topic is an important one and a very difficult one. However, the work has some problems in rigor and justification and the conclusions are overstated in my view.

Pros
-Several interesting ideas for evaluating evaluation metrics are proposed
-The authors tackle a very challenging subject

Cons
-It is not clear why GANs are the only generative model considered
-Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper.
-The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification
-The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for “discriminativeness” and seems like something that can be gamed. 
- The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported.

Several references I suggest:
https://arxiv.org/abs/1706.08500 (FID score)
https://arxiv.org/abs/1511.04581 (MMD as evaluation)
","[5, 5, 7, 8]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 5, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Discovering Order in Unordered Datasets: Generative Markov Networks,"['Yao-Hung Hubert Tsai', 'Han Zhao', 'Nebojsa Jojic', 'Ruslan Salakhutdinov']",Reject,2018,"[5, 6, 16, 23]","[9, 11, 21, 28]","[68, 111, 419, 155]","[31, 54, 207, 102]","[33, 51, 201, 33]","[4, 6, 11, 20]","
The authors deal with the problem of implicit ordering in a dataset and the challenge of recovering it, i.e. when given a random dataset with no explicit ordering in the samples, the model is able to recover an ordering. They propose to learn a distance-metric-free model that assumes a Markov chain as the generative mechanism of the data and learns not only the transition matrix but also the optimal ordering of the observations.


> Abstract
“Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically. ”
I am not sure what automatically refers here to. Do the authors mean that the GMN model does not explicitly assume any ordering in the observed dataset? This needs to be better stated here. 
“Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically; given an unordered dataset, it outputs the best -most possible- ordering.”

Most of the models assume an explicit ordering in the dataset and use it as an integral modelling assumption. Contrary to that they propose a model where no ordering assumption is made explicitly, but the model itself will recover it if any.

> Introduction
The introduction is fairly well structured and the example of the joint locations in different days helps the reader.  

In the last paragraph of page 1, “we argue that … a temporal model can generate it.”, the authors present very good examples where ordered observations (ballerina poses, video frames) can be shuffled and then the proposed model can recover a temporal ordering out of them. What I would like to think also here is about an example where the recovered ordering will also be useful as such. An example where the recovered ordering will increase the importance of the inferred solution would be more interesting..



2. Related work
This whole section is not clear how it relates to the proposed model GMN. Rewriting is strongly suggested. 
The authors mention Deep Generative models and One-shot learning methods as related work but the way this section is constructed makes it hard for the reader to see the relation. It is important that first the authors discuss the characteristics of GMN that makes it similar to Deep generative models and the one-shot learning models. They should briefly explain the characteristics of DGN and one-shot learning so that the readers see the relationship. 
Also, the authors never mention that the architecture they propose is deep.
 
Regarding the last paragraph of page 2, “Our approach can be categorised … can be computed efficiently.”:
Not sure why the authors assume that the samples can be sampled from an unmixed chain. An unmixed chain can also result in observing data that do not exhibit the real underlying relationships. Also the authors mention couple of characteristics of the GMN but without really explaining them.  What are the explicit and implicit models [1] … this needs more details. 

[1] P. J. Diggle and R. J. Gratton. Monte Carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society. Series B (Methodological), pages 193–227, 1984. 

“Second, prior approaches were proposed based on the notion of denoising models. In other words, their goal was generating high-quality images; on the other hand, we aim at discovering orders in datasets.” —>this bit is confusing. Do the authors mean that prior approaches were considering the observed ordering as part of the model assumptions and were just focusing on the denoising? 

3. Generative Markov models
First, I would like to draw the attention of the authors on the terminology they use. The states here are not the latent states usually referred in the literature of Markov chains. The states here are observed and should not be confused with the emissions also usually stated in the corresponding literature. There are as many states as the number of observations and not differentiation is made for ties. All these are based on my understanding of the model.

In  the Equation just before equation (1),  on the left hand side, shouldn’t \pi be after the `;’. It’s an average over the possible \pi.  We cannot  consider the average over \pi when we also want to find the optimal \pi.  The sum doesn’t need to be there. Shouldn’t it just be  max_{\theta, \pi} log P({s_i}^{n}_{i=1}; \pi, \theta) ?
Equation (1), same. The summation over the possible \pi is confusing. It’s an optimisation problem…

page 4, section 3.1: The discussion about the use of Neural Net for the construction of the transition matrix needs expansion. It is unclear how the matrix is constructed. Please add more details. E.g. use of soft-max non-linear transformation so that the output of the Neural Net can be interpreted as the probabilities of jumping to one of the possible states. In this fashion, we map the input (current state) and transform it to the probability gf occupying states at the next time step.

Why this needs expansion: The construction of the transition matrix is the one that actually plays the role of the distance metric in the related models. More specifically, the choice of the non-linear function that outputs the transition probability is crucial; e.g. a smooth function will output comparable transition probabilities to similar inputs (i.e. similar states). 

section 3.2: 
My concern about averaging over \pi applies on the equations here too. 

“However, without further assumption on the structure of the transitional operator..”—> I think the choice of the nonlinear function in the output node of the NN is actually related to the transition matrix and defines the probabilities. It is a confusing statement to make and authors need to discuss more about it. After all, what is the driving force of the inference? This is a problem/task where the observations are considered in a number of different permutations. As such, the ordering is not fixed and the main driving force regarding the best choice of ordering should come from the architecture of the transition matrix; what kind of transitions does the Neural Net architecture favour? Distance free metric but still assumptions are made that favour specific transitions over others. 

“At first, Alg. 1 enumerates all the possible states appearing in the first time step. For each of the following steps, it finds the next state by maximizing the transition probability at the current step, i.e., a local search to find the next state. ” —>  local search in the sense that the algorithm chooses as the next state the state with the biggest transition probability (to it) as defined in the Neural Net (transition operator) output? This is a deterministic step, right? 

4.1 DISCOVERING ORDERS IN DATASETS 
Nice description of the datasets. In the <MSR_SenseCam> the choice of one of the classes needs to be supported.  Why? What do the authors expect to happen if a number of instances from different classes are chosen? 

4.1.1 IMPLICIT ORDERS IN DATASETS 
The explanation of the inferred orderings for the GMN and Nearest Neighbour model is not clear. In figure 2, what forces the GMN to make distinguishable transitions as opposed to the Nearest neighbour approach that prefers to get stuck to similar states? Is it the transition matrix architecture as defined by the neural network? 

>> Figure 10: why use of X here? Why not keep being consistent by using s?

*** DO the authors test the model performance on a ordered dataset (after shuffling it…) ?  Is the model able of recovering the order? **
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,"['Brendan Maginnis', 'Pierre Richemond']",Reject,2018,"[2, 2]","[3, 7]","[4, 18]","[0, 3]","[3, 15]","[1, 0]","Summary:
This paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention. The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.

Pros:
- Addresses an issue of RWAs.

Cons:
-The paper addresses a problem with an issue with RWAs. But it is not clear to me why would that be an important contribution.
-The writing needs more work.
-The experiments are lacking and the results are not good enough.

General Comments:

This paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address. I would still like to have a better justification on why should we care about RWA and fixing that model. 

The writing of this paper seriously needs more work.  The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c.

The experiments are only on toyish and small scale tasks. According to the results the model doesn't really do better than a simple LSTM or GRU.","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unleashing the Potential of CNNs for Interpretable Few-Shot Learning,"['Boyang Deng', 'Qing Liu', 'Siyuan Qiao', 'Alan Yuille']",Reject,2018,"[2, 2, 10, 36]","[7, 6, 14, 41]","[24, 22, 49, 702]","[7, 10, 21, 337]","[15, 11, 26, 263]","[2, 1, 2, 102]","The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015). This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. The clustered representations are the visual concepts. This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images.

 A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class.

The results a are convincing, even if they are not state of the art in all the trials. 
The paper is very easy to follows, and the results are explained in a very simple way.


Few comments:
The authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general.","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Stochastic Hyperparameter Optimization through Hypernetworks,"['Jonathan Lorraine', 'David Duvenaud']",Reject,2018,"[1, 9]","[5, 13]","[16, 103]","[6, 52]","[10, 49]","[0, 2]","*Summary*

The paper proposes to use hyper-networks [Ha et al. 2016] for the tuning of hyper-parameters, along the lines of [Brock et al. 2017]. The core idea is to have a side neural network sufficiently expressive to learn the (large-scale, matrix-valued) mapping from a given configuration of hyper-parameters to the weights of the model we wish to tune.
The paper gives a theoretical justification of its approach, and then describes several variants of its core algorithm which mix the training of the hyper-networks together with the optimization of the hyper-parameters themselves. Finally, experiments based on MNIST illustrate the properties of the proposed approach.

While the core idea may appear as appealing, the paper suffers from several flaws (as further detailed afterwards):
-Insufficient related work
-Correctness/rigor of Theorem 2.1
-Clarity of the paper (e.g., Sec. 2.4)
-Experiments look somewhat artificial
-How scalable is the proposed approach in the perspective of tuning models way larger/more complex than those treated in the experiments?

*Detailed comments*

-""...and training the model to completion."" and ""This is wasteful, since it trains the model from scratch each time..."" (and similar statement in Sec. 2.1): Those statements are quite debatable. There are lines of work, e.g., in Bayesian optimization, to model early stopping/learning curves (e.g., Domhan2014, Klein2017 and references therein) and where training procedures are explicitly resumed (e.g., Swersky2014, Li2016). The paper should reformulate its statements in the light of this literature.

-""Uncertainty could conceivably be incorporated into the hypernet..."". This seems indeed an important point, but it does not appear as clear how to proceed (e.g., uncertainty on w_phi(lambda) which later needs to propagated to L_val); could the authors perhaps further elaborate?

-I am concerned about the rigor/correctness of Theorem 2.1; for instance, how is the continuity of the best-response exploited? Also, throughout the paper, the argmin is defined as if it was a singleton while in practice it is rather a set-valued mapping (except if there is a unique minimizer for L_train(., lambda), which is unlikely to be the case given the nature of the considered neural-net model). In the same vein, Jensen's inequality states that Expectation[g(X)] >= g(Expectation[X]) for some convex function g and random variable X; how does it precisely translate into the paper's setting (convexity, which function g, etc.)? 

-Specify in Alg. 1 that ""hyperopt"" refers to a generic hyper-parameter procedure.

-More details should be provided to better understand Sec. 2.4. At the moment, it is difficult to figure out (and potentially reproduce) the model which is proposed.

-The training procedure in Sec. 4.2 seems quite ad hoc; how sensitive was the overall performance with respect to the optimization strategy? For instance, in 4.2 and 4.3, different optimization parameters are chosen.

-typo: ""weight decay is applied the..."" --> ""weight decay is applied to the...""

-""a standard Bayesian optimization implementation from sklearn"": Could more details be provided? (there does not seem to be implementation there http://scikit-learn.org/stable/model_selection.html to the best of my knowledge)

-The experimental set up looks a bit far-fetched and unrealistic: first scalar, than diagonal and finally matrix-weighted regularization schemes. While the first two may be used in practice, the third scheme is not used in practice to the best of my knowledge.

-typo: ""fit a hypernet same dataset."" --> ""fit a hypernet on the same dataset.""

-(Franceschi2017) could be added to the related work section.

*References*

(Domhan2014) Domhan, T.; Springenberg, T. & Hutter, F. Extrapolating learning curves of deep neural networks ICML 2014 AutoML Workshop, 2014

(Franceschi2017) Franceschi, L.; Donini, M.; Frasconi, P. & Pontil, M. Forward and Reverse Gradient-Based Hyperparameter Optimization preprint arXiv:1703.01785, 2017

(Klein2017) Klein, A.; Falkner, S.; Springenberg, J. T. & Hutter, F. Learning curve prediction with Bayesian neural networks International Conference on Learning Representations (ICLR), 2017, 17

(Li2016) Li, L.; Jamieson, K.; DeSalvo, G.; Rostamizadeh, A. & Talwalkar, A. Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization preprint arXiv:1603.06560, 2016

(Swersky2014) Swersky, K.; Snoek, J. & Adams, R. P. Freeze-Thaw Bayesian Optimization preprint arXiv:1406.3896, 2014

*********
Update post rebuttal
*********

I acknowledge the fact that I read the rebuttal of the authors, whom I thank for their detailed answers.

My minor concerns have been clarified. Regarding the correctness of the proof, I am still unsure about the applicability of Jensen inequality; provided it is true, then it is important to see that the results seem to hold only for particular hyperparameters, namely regularization parameters (as explained in the new updated proof). This limitation should be exposed transparently upfront in the paper/abstract. 
Together with the new experiments and comparisons, I have therefore updated my rating from 5 to 6.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 1, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']"
Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,"['Kyle Helfrich', 'Devin Willmott', 'Qiang Ye']",Reject,2018,"[2, 8, 23]","[6, 12, 28]","[8, 12, 34]","[3, 3, 3]","[4, 6, 5]","[1, 3, 26]","The paper is clearly written, with a good coverage of previous relevant literature. 
The contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced.
Therefore, the paper must show that this new method performs better in some way compared with previous methods. They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.

Pros:
1. New, relatively simple method for learning orthogonal weight matrices for RNN

2. Clearly written

3. Quite good results on several relevant tasks.

Cons:
1. Technical novelty is somewhat limited

2. Experiments do not evaluate run time, memory use, computational complexity, or stability. Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Clipping Free Attacks Against Neural Networks,['Boussad ADDAD'],Reject,2018,"[11, 24, 22]","[11, 24, 26]","[9, 7, 34]","[3, 2, 15]","[1, 1, 1]","[5, 4, 18]","This paper presents a reparametrization of the perturbation applied to features in adversarial examples based attacks. It tests this attack variation on against Inception-family classifiers on ImageNet. It shows some experimental robustness to JPEG encoding defense.

Specifically about the method: Instead of perturbating a feature x_i by delta_i, as in other attacks, with delta_i in range [-Delta_i, Delta_i], they propose to perturbate x_i^*, which is recentered in the domain of x_i through a heuristic ((x_i ± Delta_i + domain boundary that would be clipped)/2), and have a similar heuristic for computing a Delta_i^*. Instead of perturbating x_i^* directly by delta_i, they compute the perturbed x by x_i^* + Delta_i^* * g(r_i), so they follow the gradient of loss to misclassify w.r.t. r (instead of delta). 

+/-:
+ The presentation of the method is clear.
+ ImageNet is a good dataset to benchmark on.
- (!) The (ensemble) white-box attack is effective but the results are not compared to anything else, e.g. it could be compared to (vanilla) FGSM nor C&W.
- The other attack demonstrated is actually a grey-box attack, as 4 out of the 5 classifiers are known, they are attacking the 5th, but in particular all the 5 classifiers are Inception-family models.
- The experimental section is a bit sloppy at times (e.g. enumerating more than what is actually done, starting at 3.1.1.).
- The results on their JPEG approximation scheme seem too explorative (early in their development) to be properly compared.

I think that the paper need some more work, in particular to make more convincing experiments that the benefit lies in CIA (baselines comparison), and that it really is robust across these defenses shown in the paper.","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Novelty Detection with GAN,"['Mark Kliger', 'Shachar Fleishman']",Reject,2018,"[23, 13, 17]","[28, 18, 22]","[218, 72, 318]","[113, 32, 146]","[19, 13, 27]","[86, 27, 145]","
The paper proposes a GAN for novelty detection (predicting novel versus nominal data), using a mixture generator with feature matching loss.  The key difference between this paper and previous is the different definition of mixture generator.  Here the authors enforce p_other to have some significant mass in the tails of p_data (Def 1), forcing the 'other' data to be on or around the true data, creating a tight boundary around the nominal data.

The paper is well written, derives cleanly from previous work, and has solid experiments.  The experiments are weak 1) in the sense that they are not compared against simple baselines like p(x) (from, say, just thresholding a vae, or using a more powerful p(x) model -- there are lots out there), 2) other than KNNs, only compared with class-prediction based novelty detection (entropy, thresholds), and 3) in my view perform consistently, but not significantly better, than simply using the entropy of the class predictions.  How would entropy improve if it was a small ensemble instead of a single classifier?

The authors may be interested in [1], a principled approach for learning a well-calibrated uncertainty estimate on predictions.    Considering how well entropy works, I would be surprised in the model in [1] does not perform even better.

pros:
- good application of GAN models
- good writing and clarity
- solid experiments and explanations

cons:
 - results weak relative to naive baseline (entropy)
 - weak comparisons
 - lack of comparison to density models 


[1] Louizos, Christos, and Max Welling. ""Multiplicative Normalizing Flows for Variational Bayesian Neural Networks."" arXiv preprint arXiv:1703.01961 (2017).","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The (Un)reliability of saliency methods,"['Pieter-Jan Kindermans', 'Sara Hooker', 'Julius Adebayo', 'Kristof T. Schütt', 'Maximilian Alber', 'Sven Dähne', 'Dumitru Erhan', 'Been Kim']",Reject,2018,"[7, 2, 5, 3, 7, 9, 13, 9]","[11, 7, 9, 8, 11, 10, 17, 14]","[39, 30, 17, 13, 22, 26, 57, 74]","[17, 5, 6, 3, 6, 9, 26, 29]","[16, 23, 9, 9, 14, 4, 26, 41]","[6, 2, 2, 1, 2, 13, 5, 4]","The authors explore how different methods of visualizing network decisions (saliency methods) react to mean shifts of the input data by comparing them on two networks that are build to compensate for this mean shift. With the emergence of more and more saliency methods, the authors contribute an interesting idea to a very important and relevant discussion.

However, I'm missing a more general and principled discussion. The question that the authors address is how different saliency methods react to transformations of the input data. Since the authors make sure that their two models compensate for these transformation, the difference in saliency can be only due to underlying assumptions about the input data made by the saliency methods and therefore the discussion boils down to which invariance properties are justified for which kind of input -- it is not by chance that the attribution methods that work are exactly those that extract statistics from the input data and therefore compensate for the input transformation: IG with black reference point and Pattern Attribution.
The mean shift explored by the authors assumes that there is no special point in the input space (especially that zero is not a special point).
However, since images usally are considered bounded by 0 and 1 (or 255), there are in fact two special points (as a side note, in Figure 2 left column, the two inputs look very different which might be due to the fact that it is not at all obvious how to visualize ""image"" input that does not adhere to the common image input structure).
Would the authors argue that scaling the input with a positive factor should also lead to invariant saliency methods?
What about scaling with a negative factor?
I would argue that if the input has a certain structure, then it should be allowed for the saliency method to make use of this structure.

Minor points:

Understanding the two models in section 3 is a bit hard since the main point (both networks share the weights and biases except for the bias of the first layer) is only said in 2.1
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Three factors influencing minima in SGD,"['Stanisław Jastrzębski', 'Zac Kenton', 'Devansh Arpit', 'Nicolas Ballas', 'Asja Fischer', 'Amos Storkey', 'Yoshua Bengio']",Reject,2018,"[6, 2, 8, 7, 9, 31, 23]","[10, 6, 13, 12, 14, 36, 28]","[53, 14, 45, 75, 79, 975, 135]","[16, 4, 20, 39, 30, 405, 62]","[29, 10, 25, 35, 36, 454, 54]","[8, 0, 0, 1, 13, 116, 19]","The paper investigates how the learning rate and mini-batch size in SGD impacts the optima that the SGD algorithm finds.
Empirically, the authors argue that it was observed that larger learning rates converge to minima which are more wide,
and that smaller learning rates more often lead to convergence to minima which are narrower, i.e. where the Hessian has large Eigenvalues. In this paper, the authors derive an analytical theory that aims at explaining this phenomenon.

Point of departure is an analytical theory proposed by Mandt et al., where SGD is analyzed in a continuous-time stochastic
formalism. In more detail, a stochastic differential equation is derived which mimicks the behavior of SGD. The advantage of
this theory is that under specific assumptions, analytic stationary distributions can be derived. While Mandt et al. focused
on the vicinity of a local optima, the authors of the present paper assumed white diagonal gradient noise, which allows to
derive an analytic, *global* stationary distribution (this is similar as in Langevin dynamics).

Then, the authors focus again on individual local optima and ""integrate out"" the stationary distribution around a local optimum, using again a Gaussian assumption. As a result, the authors obtain un-normalized probabilities of getting trapped in a given local optimum. This un-normalized probability depends on the strength of the value of the loss function in the vicinity of the optimum, the gradient noise, and the width of the optima. In the end, these un-normalized probabilities are taken as
probabilities that the SGD algorithm will be trapped around the given optimum in finite time.


Overall assessment:
I find the analytical results of the paper very original and interesting. The experimental part has some weaknesses. The paper could be drastically improved when focusing on the experimental part.

Detailed comments:

Regarding the analytical part, I think this is all very nice and original. However, I have some comments/requests:

1. Since the authors focus around Gaussian regions around the local minima, perhaps the diagonal white noise assumption could be weakened. This is again the multivariate Ornstein-Uhlenbeck setup examined in Mandt et al., and probably possesses an analytical solution for the un-normalized probabilities (even if the noise is multivariate Gaussian). Would the authors to consider generalizing the proof for the camera-ready version perhaps?

2. It would be nice to sketch the proof of theorem 2 in the main paper, rather than to just refer to the appendix. In my opinion, the theorem results from a beautiful and instructive calculation that should provide the reader with some intuition.

3. Would the authors comment on the underlying theoretical assumptions a bit more? In particular, the stationary distribution predicted by the Ornstein-Uhlenbeck formalism is never reached in practice. When using SGD in practice, one is in the initial mode-seeking phase. So, why is it a reasonable assumption to still use results obtained from the stationary (equilibrated) distribution which is never reached?


Regarding the experiments: here I see a few problems. First, the writing style drops in quality. Second, figures 2 and 3 are cryptic. Why do the authors focus on two manually selected optima? In which sense is this statistically significant? How often were the experiments repeated? The figures are furthermore hard to read. I would recommend overhauling the entire experiments section.

Details:

- Typo in Figure 2: ”with different with different”.
- “the endpoint of SGD with a learning rate schedule η → η/a, for some a > 0, and a constant batch size S, should be the same
  as the endpoint of SGD with a constant learning rate and a batch size schedule S → aS.” This is clearly wrong as there are many local minima, and running teh algorithm twice results in different local optima.  Maybe add something that this only true on average, like “the characteristics of these minima ... should be the same”.","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,"['Jiong Zhang', 'Qi Lei', 'Inderjit S. Dhillon']",Reject,2018,"[20, 8, 24]","[25, 13, 29]","[109, 92, 277]","[50, 40, 160]","[18, 35, 71]","[41, 17, 46]","This paper suggests a reparametrization of the transition matrix. The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.

The paper is well-written and authors explain related work adequately. The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary. The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact. 

I have two comments on the experiment section:

- Choice of experiments. Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.

- Stopping condition. The plots suggest that the optimization has stopped earlier for some models. Is this because of some stopping condition or because of gradient explosion? Is there a way to avoid this?

- Quality of figures. Figures are very hard to read because of small font. Also, the captions need to describe more details about the figures.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
IVE-GAN: Invariant Encoding Generative Adversarial Networks,"['Robin Winter', 'Djork-Arnè Clevert']",Reject,2018,"[18, 13]","[22, 18]","[9, 27]","[2, 7]","[4, 11]","[3, 9]","The paper proposes a modified GAN objective, summarized in Eq.(3). It consists of two parts:
(A) Classic GAN term: \E_{x ~ P_{data} } \log D'(x) + \E_{z ~ P_{Z}, z' ~ P_{Z'}  } \log D'( G(z',E(x))   )
(B) Invariant Encoding term: \E_{x ~ P_{data} }  [ \log D(T(x),x) + \E_{z' ~ P_{Z'}  } \log D( G(z',E(x)), x   ) ]

Term (A) is standard, except the latent space of original GAN is decomposed into (1) the feature, which should be invariant between x and T(x), and (2) the noise, which is for the diversity of generated x.

Term (B) is the proposed invariant-encoding scheme. It is essentially a conditional GAN, where the the generated sample G(z',E(x)) is conditioned on input sample x, which guarantees that the generated sample is T(x) of x. 
In fact, this could be theoretically justified. Suggestion: the authors might want to follow the proofs of Proposition 1 or 2 in [*] to show similar conclusion, making the paper stronger.

[*] ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching, NIPS 2017

The definition of feature-invariant is E(T(x))=E(x)=E(G(z',E(x))), while the objective of the paper achieves T(x)=G(z',E(x)). Applying E() to both side yields the invariant features.  It might be better to make this point clear.

Overall Comments:

Originality: the proposed IVE-GAN algorithm is quite novel.
Quality: The paper could be stronger, if the theoretical justification has been provided. 
Clarity: Overall clear, while important details are missing. Please see some points in Detailed Comments.
Significance: The idea is interesting, it would be better if the quantitative evidence has been provided to demonstrate the use of the learned invariant feature. For example, some classification task to demonstrate the learned rotation-invariant feature shows higher accuracy.

Detailed Comments:

-- In Figure 1, please explains that ""||"" is the concatenation operator for better illustration.

-- When summarizing the contributions of the paper, it is mentioned that ""our GANs ... without mode collapsing issues"". This is a strong point to claim. While precisely identifying the ""mode collapsing issue"" itself is difficult, the authors only show that samples in all modes are generated on the toy datasets. Please consider to rephrase. 

-- In Section 2, y is first used to indicate true/false of x in Eq.(1), then y is used to indicate the associated information (e.g., class label) of x in Eq.(2). Please consider to avoid overloading notations.

-- In Eq.(3), the expectation \E_{z ~ P_Z} in the 3rd term is NOT clear, as z is not involved in the evaluation. I guess it may be implemented as z=E(x), where x ~ P_{data}. From the supplement tables, It seems that the novel sample G(z', E(x)) is implemented as G evaluated on the concatenation of noise sample z' ~ P_{Z'} and encoded feature z=E(x). 
I am wondering how to generate novel samples? Related to this,  Please clarify how to implement: ""To generate novel samples, we can draw samples z ~ P_Z as latent space"".

-- Section 5, ""include a encoding unit"" ---> ""an""

-- In Supplement, please revise G(z'E(x)) to G(z', E(x)) in every table.
","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
An information-theoretic analysis of deep latent-variable models,"['Alex Alemi', 'Ben Poole', 'Ian Fischer', 'Josh Dillon', 'Rif A. Saurus', 'Kevin Murphy']",Reject,2018,"[4, 8, 13, 12, 3, 24]","[8, 13, 17, 17, 8, 29]","[46, 61, 45, 36, 43, 164]","[15, 24, 19, 12, 18, 91]","[30, 36, 23, 21, 24, 57]","[1, 1, 3, 3, 1, 16]","Summary:

This paper optimizes the beta-VAE objective and analyzes the resulting models in terms of the two components of the VAE loss: the reconstruction error (which the authors refer to as distortion, “D”) and the KL divergence term (which the authors refer to as rate, “R”). Various VAEs using either PixelCNN++ or a simpler model for the encoder, decoder, or marginal distribution of a VAE are trained on MNIST (with some additional results on OMNIGLOT) and analyzed in terms of samples, reconstructions, and their rate-distortion trade-off.

Review:

I find it difficult to point my finger to novel conceptual or theoretical insights in this paper. The idea of maximizing information for unsupervised learning of representations has of course been explored a lot (e.g., Bell & Sejnowski, 1995). Deeper connections between variational inference and rate-distortion have been made before (e.g., Balle et al., 2017; Theis et al., 2017), while this paper merely seems to rename the reconstruction and KL terms of the ELBO. Variational lower and upper bounds on mutual information have been used before as well (e.g., Barber & Agakov, 2003; Alemi et al., 2017), although they are introduced like new results in this paper. The derived “sandwich equation” only seems to be used to show that H - D - R <= 0, which also follows directly from Gibbs’ inequality (since the left-hand side is a negative KL divergence). The main contributions therefore seem to be the proposed analysis of models in the R-D plane, and the empirical contribution of analyzing beta-VAEs.

Based on the R-D plots, the authors identify a potential problem of generative models, namely that none of the trained models appear to get close to the “auto-encoding limit” where the distortion is close zero. Wouldn’t this gap easily be closed by a model with identity encoder, identity decoder, and PixelCNN++ for the marginal distribution? Given that autoregressive models generally perform better than VAEs in terms of log-likelihood, the model’s performance would probably be closer to the true entropy than the ELBO plotted in Figure 3a). What about increasing the capacity of the used in this paper? This makes me wonder what exactly the R-D plot can teach us about building better generative models.

The toy example in Figure 2 is interesting. What does it tell us about how to build our generative models? Should we be using powerful decoders but a lower beta?

The authors write: “we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable”. Yet in Figure 3b) it appears that changing the rate of a model can influence the generative performance (ELBO) quite a bit?","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Character Level Based Detection of DGA Domain Names,"['Bin Yu', 'Jie Pan', 'Jiaming Hu', 'Anderson Nascimento', 'Martine De Cock']",Reject,2018,"[29, 27, 3, 18, 20]","[34, 32, 7, 23, 25]","[330, 121, 19, 121, 189]","[141, 46, 10, 48, 109]","[13, 7, 3, 45, 29]","[176, 68, 6, 28, 51]","
SUMMARY

This paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection. A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points. DGA detection concerns the (automatic) distinction of actual and artificially generated domain names. In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters. A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task. A domain name data set with two million instances is used for the experiments. The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced.

COMMENTS

The introduction is well-written, clear, and concise. It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem. The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution.

The methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment. In the beginning of the section, it is not clarified why, if a 75 character string is encoded as a 128 byte ASCII sequence, the content has to be stored in a 75 x 128 matrix instead of a vector of size 128. This is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach.

Section 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the “baseline” Random Forest classifier. Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures? The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%). How many times is this procedure repeated? (That is, how many experimental runs were averaged or was the experiment run once?).

In summary, this is an interesting and well-written paper on a timely topic. The main conclusion is intuitive. Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures. Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion.

MINOR COMMENTS

Abstract: “Little studies” -> “Few studies”

Table 1: “approach” -> “approaches”

Figure 1: Use the same y-axis scale for all subplots (if possible) to simplify comparison. Also, try to move Figure 1 so that it appears closer to its inline reference in the text.

Section 3: “based their on popularity” -> “based on their popularity”

","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,"['Shuai Tang', 'Hailin Jin', 'Chen Fang', 'Zhaowen Wang', 'Virginia R. de Sa']",Reject,2018,"[7, 19, 11, 12, 27]","[12, 24, 16, 17, 31]","[51, 162, 140, 132, 62]","[14, 95, 65, 64, 35]","[19, 46, 34, 48, 11]","[18, 21, 41, 20, 16]","Update:

I'm going to change my review to a 6 to acknowledge the substantial improvements you've made—I no longer fear that there are major errors in the paper, but this paper is still solidly borderline, and I'm not completely convinced that any new claim is true. The evidence presented for the main claim—that you can get by without an autoregressive decoder when pretraining encoders—is somewhat persuasive, but isn't as unequivocal as I'd hope, and even if the claim is true, it is arguably too limited a claim for an ICLR main conference paper. As R1 says, a *ACL short paper would be more appropriate.  The writing is also still unclear in places.

----

This paper presents a new RNN encoder–CNN decoder hybrid design for use in pretraining reusable sentence encoders on Kiros's SkipThought objective. The task is interesting and important, and the results are generally good: The new model outperforms SkipThought, and all other prior models for training sentence encoders on unlabeled data. However, some of the design choices seem a bit odd, and I have a large number of minor concerns about the paper. I'd like to see the authors' replies and the other reviews before I can confidently endorse this paper as correct.


Non-autoregressive decoding with a CNN strikes me as a somewhat ill-posed problem, even for in this case where you don't actually use the decoder in the final application of your model. At each position, you're training your model to predict a distribution over all words that could appear at the beginning/tenth position/twentieth position in sentences on some topic. I'd appreciate some more discussion of why this should or shouldn't hurt performance. I'd be less concerned about this if the results supporting the use of the CNN decoder were a bit more conclusive: while they are better on average across your smaller experiments, your largest experiment (2400D) shows them roughly tied.

Your paper opens with the line ""Context information plays an important role in human language understanding."" This sounds like it's making an empirical claim that your paper doesn't support, but it's so vague that it's hard to tell exactly what that claim is. Please clarify this or remove it.

This sentence is quite inaccurate: ""The idea of learning from the context information was first successfully applied to vector representation learning for words in Mikolov et al. (2013b) and learning from the occurrence of words also succeeded in Pennington et al. (2014)."" Turney and Pantel 2010 ( https://www.jair.org/media/2934/live-2934-4846-jair.pdf ) offer a survey of the substantial prior work that existed at that time.

The ""Neighborhood Hypothesis"" is given quite a lot of importance, given that it's a fairly small empirical effect without any corresponding theory. The fact that it's emphasized so heavily makes me suspect that I can guess the author of the paper. I'd tone down that part of your framing.

I would appreciate some more analysis of which of the non-central tricks that you describe in section 3 help. For example, max pooling seems reasonable, but you report yourself that mean pooling generally works much better in prior work. Without an explicit experiment, it's not clear why you'd add a mean pooling component.

It seems misleading to claim that your CNN is modeled after AdaSent, as that model uses a number of layers that varies with the length of the sentence (and differs from yours in a few other less-important ways). Please correct or clarify.

The use of “†” in table to denote models that predict the next sentence in a sequence doesn't make sense. It should apply to all of your models if I understand correctly. Please clarify.

You could do a better job at table placement and formatting. Table 3 is in the wrong section, for example.

You write that: ""Our proposed RNN-CNN model gets similar result on SNLI as Skip-thought, but with much less training time."" This seems to be based on a comparison between your model run on your hardware and their model run on their (presumably older) hardware, and possibly also with their older version of CuDNN. If that's right, you should tone down this claim or offer some more evidence.","[6, 7, 3]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Gaussian Process Neurons,"['Sebastian Urban', 'Patrick van der Smagt']",Reject,2018,"[21, 24]","[24, 29]","[7, 189]","[4, 88]","[0, 12]","[3, 89]","The paper addresses the problem of learning the form of the activation functions in neural networks.  The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net. This  somehow allows to non-parametrically infer from the data the ""shape"" of the activation functions needed for a specific problem.  The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference.  The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper).  Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations.   
       
In general I like the idea and I believe that it can lead to a very useful model. However, I have found the current paper quite preliminary and incomplete.  The authors need to address the following:  

First (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc). At the moment in the last section you mention ""We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication. In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters."" ===>  Well all this needs to be included in the same paper.  

Secondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013). Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights).  

Thirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1. This is the technical part of your paper which is a non-standard approximation. I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.  
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,"['Jihyung Moon', 'Hyochang Yang', 'Sungzoon Cho']",Reject,2018,"[1, 2, 30]","[6, 2, 34]","[12, 2, 135]","[6, 0, 38]","[6, 2, 7]","[0, 0, 90]","This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks, but I can't recommend this paper's acceptance in its current form.

other questions / comments:
- ""we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question."" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?
- writing could be cleaned up for spelling / grammar (e.g., ""last 70 stories"" instead of ""last 70 sentences""), currently the paper is very hard to read and it took me a while to understand the model","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning To Generate Reviews and Discovering Sentiment,"['Alec Radford', 'Rafal Jozefowicz', 'Ilya Sutskever']",Reject,2018,"[3, 6, 12]","[7, 8, 17]","[28, 18, 99]","[9, 6, 49]","[19, 11, 45]","[0, 1, 5]","This paper shows that an LSTM language model trained on a large corpus of Amazon product reviews can learn representations that are useful for sentiment analysis. 
Given representations from the language model, a logistic regression classifier is trained with supervised data from the task of interest to produce the final model.
The authors evaluated their approach on six sentiment analysis datasets (MR, CR, SUBJ, MPQA, SST, and IMDB), and found that the proposed method is competitive with existing supervised methods. 
The results are mixed, and they understandably are better for test datasets from similar domains to the Amazon product reviews dataset used to train the language model.
An interesting finding is that one of the neurons captures sentiment property and can be used to predict sentiment as a single unit.

I think the main result of the paper is not surprising and does not show much beyond we can do pretraining on unlabeled datasets from a similar domain to the domain of interest. 
This semi-supervised approach has been known to improve in the low data regime, and pretraining an expressive neural network model with a lot of unlabeled data has also been shown to help in the past.
There are a few unanswered questions in the paper:
- What are the performance of the sentiment unit on other datasets (e.g., SST, MR, CR)? Is it also competitive with the full model?
- How does this method compare to an approach that first pretrains a language model on the training set of each corpus without using the labels, and then trains a logistic regression while fixing the language model? Is the large amount of unlabeled data important to obtain good performance here? Or is similarity to the corpus of interest more important?
- I assume that the reason to use byte LSTM is because it is cheaper than a word level LSTM. Is this correct or was there any performance issue with using the word directly?
- More analysis on why the proposed method does well on the binary classification task of SST, but performs poorly on the fine-grained classification would be useful. If the model is capturing sentiment as is claimed by the authors, why does it only capture binary sentiment instead of a spectrum of sentiment level?

The paper is also poorly written. There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.","[4, 2, 4]","[' Ok but not good enough - rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
The Principle of Logit Separation,"['Gil Keren', 'Sivan Sabato', 'Björn Schuller']",Reject,2018,"[3, 12, 18]","[8, 17, 23]","[34, 78, 964]","[15, 32, 573]","[15, 32, 167]","[4, 14, 224]","The paper is well-written which makes it easy to understand its main
thrust - choosing loss functions so that at test time one can
accurately (and speedily) determine whether an example is in a given
class, ie loss functions which are aligned with the ""Principle of Logit
Separation (PoLS)"". 

When the ""Principle of logit separation"" was first given (second page)
I found it confusing and difficult to parse (too many ""any""s, I could
not work out how the quantification worked). However, the formal
definition (Definition 2.1) was fine. Why not just use this - and drop
the vague, wordy definition?

The paper is fairly 'gentle'. For example, we are taken through
examples of loss functions which satisfy ""PoLS"" and those which don't.
No 'deep' mathematical reasoning is required - but I don't see this as
a deficiency.

The experiments are reasonably chosen and, as expected, show the
benefits of using PoLS-aligned loss functions.

My criticism of the paper is that I don't think there is enough
motivation. We have that normal classification is linear in the number
of classes. This modest computational burden (ie just linear),
apparently, is too slow for certain applications.  I would like more
evidence for this, including some examples of this problem including
in the paper. This is lacking from the current version.


typos, etc

max-maring -> max-margin
the seconds term -> the second term

","[6, 4, 3]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Less-Overlapping Representations,"['Hongbao Zhang', 'Pengtao Xie', 'Eric Xing']",Reject,2018,"[6, 5, 18]","[11, 8, 23]","[113, 9, 625]","[41, 4, 339]","[65, 4, 218]","[7, 1, 68]","The paper studies a regularization method to promote sparsity and reduce the overlap among the supports of the weight vectors in the learned representations. The motivation of using this regularization is to enhance the interpretability of the learned representation and avoid overfitting of complex models. 

To reduce the overlap among the supports of the weight vectors, an existing method (Xie et al, 2017b) encouraging orthogonality is adopted to let the Gram matrix of the weight vectors to be close to the identity matrix (so that each weight vector is with unit norm and any pair of vectors are approximately orthogonal).

Neural network and sparse coding are considered as two case studies. The alternating algorithm for solving the regularized sparse coding formulation is common and less attracted. I think the major point is to see how much benefit that the regularization can afford for learning deep neural networks. To avoid overfitting, some off-the-shelf methods, e.g., dropout which can be viewed as a kind of regularization, are commonly used for deep neural networks. Are there any connections between the adopted regularization terms and the existing methods? Will these less overlapped parameters control the activation of different neurons? I think these are some straightforward questions while there are not much explanations on those aspects.

For training neural networks, a simple sub-gradient method is used because of the non-smoothness of the regularization terms. When training with large neural networks, will the sub-gradient method affect the efficiency a lot compared without using the regularizer? For example, in the image classification problem with ResNet.

It is better not to use dropout in the experiments (language modeling and image classification), because one of the motivation of using the proposed regularizer is to avoid overfitting while dropout does the same work and may affect the evaluation of effectiveness of the regularization.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Style Memory: Making a Classifier Network Generative,"['Rey Wiyatno', 'Jeff Orchard']",Reject,2018,"[1, 18]","[5, 23]","[8, 60]","[2, 35]","[5, 6]","[1, 19]","The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called ""style memory"", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally ""style"" and ""content"", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.

The use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see ""Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure"" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)","[3, 3, 4]","[' Clear rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"['Mattias Teye', 'Hossein Azizpour', 'Kevin Smith']",Reject,2018,"[1, 7, 15]","[5, 12, 20]","[2, 51, 34]","[1, 19, 19]","[0, 27, 11]","[1, 5, 4]","This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization.

There is a lot going on in this paper. Although the overall presentation is clean, there are few key shortfalls (see below). Overall, the reported functionality is nice, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive).

Some open questions that I find crucial:

* How exactly is the “stochastic forward-pass” performed that gives rise to the moment estimates? This step is the real meat of the paper, yet I struggle to find a concrete definition in the text. Is this really just an average over a few recent weights during optimization? If so, how is this method specific to batch normalization? Maybe I’m showing my own lack of understanding here, but it’s worrying that the actual sampling technique is not explained anywhere. This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here? In Bayesian Dropout, there is an explicit variational objective. Here, this is replaced by an implicit regularizer. The argument in Section 3.3 seems rather weak to me. To paraphrase it: If the prior vanishes, so does the regularizer. Fine. But what's the regularizer that's vanishing? The sentence that ""the influence of the prior diminishes as the size of the training data increases"" is debatable for something as over-parametrized as a DNN. I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.

* I’m confused about the statements made about the “constant uncertainty” baseline. First off, how is this (constant) width of the predictive region chosen? Did I miss this, or is it not explained anywhere? Unless I misunderstand the definition of CRPS and PLL, that width should matter, no? Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is “quite a reasonable baseline”. That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?). 
On a related point: What would Figure 2 look like for the constand uncertainty setting? Just a horizontal line in blue and red? But at which level?

I like this paper. It is presented well (modulo the above problems), and it makes some strong points. But I’m worried about the empirical evaluation, and the omission of crucial algorithmic details. They may hide serious problems.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
YellowFin and the Art of Momentum Tuning,"['Jian Zhang', 'Ioannis Mitliagkas', 'Christopher Re']",Reject,2018,"[6, 9]","[9, 14]","[26, 78]","[12, 30]","[13, 46]","[1, 2]","[Apologies for short review, I got called in late. Marking my review as ""educated guess"" since i didn't have time for a detailed review]

The paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[1, 3, 5]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification,"['Kshiteesh Hegde', 'Malik Magdon-Ismail', 'Ram Ramanathan', 'Bishal Thapa']",Reject,2018,"[3, 22, 23, 12]","[4, 27, 27, 16]","[7, 211, 86, 19]","[4, 112, 53, 14]","[3, 49, 8, 2]","[0, 50, 25, 3]","The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems. The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix. The experimental results supports the claim.

It is very positive that the figures are very helpful for delivering the information.

The work seems to be a little bit incremental. The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix. A novelty of this work seems to be transforming a graph into an image. By the proposed representation, the authors are able to apply image classification methods (supervised or unsupervised) to subgraph classification. 

It will be better if the authors could provide more details in the methodology or framework section.

The experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods. It seem to be promising when using transfer learning.

The last two process figures in 1.1 can be improved. No caption or figure number is provided.

It will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearby.
For example:
""the test example is correctly classified if and only if its ground truth matches C.""(P5)
""We carry out this exercise 4 times and set n to 8, 16, 32 and 64 respectively.""(P6)

Some minor issues:
""Zhu et al.(2011) discuss heterogeneous transfer learning where in they use...""(P3)
""Each label vector (a tuple of label, label-probability pairs)."" (incomplete sentence?P5)","[6, 6, 3]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,"['Su Young Lee', 'Sungik Choi', 'Sae-Young Chung']",Reject,2018,"[11, 3, 21]","[14, 7, 25]","[5, 7, 151]","[2, 2, 57]","[2, 3, 43]","[1, 2, 51]","This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.

The paper is interesting, but it lacks the proper comparisons to previously published techniques.

The results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA.

In the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.

There has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.

In the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum,"['Omer Levy', 'Kenton Lee', 'Nicholas FitzGerald', 'Luke Zettlemoyer']",Reject,2018,"[7, 7, 8, 21]","[12, 12, 13, 26]","[122, 66, 25, 346]","[55, 27, 13, 169]","[62, 36, 10, 167]","[5, 3, 2, 10]","This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate. It shows comparable results with standard LSTM.

I believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results. 

However, the formulation is very similar to ""[1] Semi-supervised Question Retrieval with Gated Convolutions"" 2016 by Lei, and ""Deriving Neural Architectures from Sequence and Graph Kernels"" which give theoretical view from string kernel about why this type of networks works. Both of the two paper don't have output gate and non-linearity of ""Wx_t"" and results on PTB also stronger than this paper. It also have some visualization about how the model decay the weights. Other AnonReviewer also point out some similar work. I won't repeat it here. In the paper, the author argued ""we propose and evaluate the minimal changes..."" but I think the these type of analysis also been covered by [1], Figure 5. 

On the experimental side, to draw the conclusion, ""weighted sum"" is enough for LSTM. I think at least Machine Translation and other classification results should be added. I'm not very familiar with SQuAD dataset, but the results seems worse than ""Reading Wikipedia to answer open-domain questions"" Table 4 which seems use a vanilla LSTM setup. 

Update: the revised version of the paper addresses all my concerns about experiments. So I increased my score. 
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Temporal Clustering: Fully unsupervised learning of time-domain features,"['Naveen Sai Madiraju', 'Seid M. Sadat', 'Dimitry Fisher', 'Homa Karimabadi']",Reject,2018,"[1, 9, 3, 15]","[3, 9, 3, 17]","[3, 2, 3, 19]","[0, 0, 0, 10]","[3, 1, 3, 2]","[0, 1, 0, 7]","The authors proposed an algorithm named Deep Temporal Clustering (DTC) that integrates autoencoder with time-series data clustering. Compared to existing methods, DTC used a network structure (CNN + BiLSTM) that suits time-series data. In addition, a new clustering loss with different similarity measures are adopted to DTC. Experiments on different time-series data show the effectiveness of DTC compared to complete link. 

Although the idea of applying deep learning for temporal clustering is novel and interesting, the optimization problem is not clearly stated and experiments section is not comprehensive enough.

Here are the detailed comments.
The methods are described in a higher level language. The formula of overall loss function and its optimization should be written down to avoid unclearness.
The framework adopt the K-medoid clustering idea. But complete-link is used for initialization and comparison. Is that a difference? In addition, how to generate K centroids from complete-link clustering is not described at all.
The author Dynamic Time Warping is too expensive to integrate into DTC. However, most of the evaluated dataset are with small time points. Even for the longer ones, DTC does dimensionality reduction to make the time-series shorter. I do not see why quadratic computation is a problem here. DTW is most effective similarity measure for time-series data clustering. There is no excuse to skip it.
Is DTC robust to hyperparameters? If not, are there any guidelines to tune the hyperparameters, which is very important for unsupervised clustering. 

In summary, the method need to be described clearer, state-of-the-arts need to be compared and the usability of the method needs to be discussed. Therefore, at the current stage the paper cannot be accepted in my opinion. 
","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases,"['Mengying Sun', 'Inci M. Baytas', 'Zhangyang Wang', 'Jiayu Zhou']",Reject,2018,"[2, 5, 10, 7, 12]","[7, 10, 15, 12, 17]","[32, 18, 84, 518, 189]","[16, 8, 55, 219, 106]","[5, 6, 14, 251, 48]","[11, 4, 15, 48, 35]","This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. 
Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables.

The idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. 
Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. 
The proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …). 

Overall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?

Other remarks. 

- In section 2.2 and 4 there is some confusion between iteration indices and samples indices “i”. 

- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.  

-  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. 

- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. 

- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting.

- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing?
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning,"['Xiaojun Xu', 'Chang Liu', 'Dawn Song']",Reject,2018,"[11, 11, 20]","[16, 14, 25]","[78, 66, 440]","[30, 40, 254]","[19, 22, 154]","[29, 4, 32]","This submission proposes a new seq2sel solution by adopting two new techniques, a sequence-to-set model and column attention mechanism. They show performance improve over existing studies on WikiSQL dataset.

While the paper is written clearly, the contributions of the work heavily depends on the WikiSQL dataset. It is not sure if the approach is generally applicable to other sequence-to-sql workloads. Detailed comments are listed below:

1. WikiSQL dataset contains only a small class of SQL queries, with aggregation over single table and various filtering conditions. It does not involve any complex operator in relational database system, e.g., join and groupby. Due to its simple structure, the problem of sequence-to-sql translation over WikiSQL is actually simplified as a parameter selection problem for a fixed template. This greatly limits the generalization of approaches only applicable to WikiSQL. The authors are encouraged to explore other datasets available in the literature.

2. The ""order-matters"" motivation is not very convincing. It is straightforward to employ a global ordering approach to rank the columns and filtering conditions based on certain rules, e.g., alphabetical order. That could ensure the orders in the SQL results are always consistent.

3. The experiments do not fully verify how the approaches bring performance improvements. In the current version, the authors only report superficial accuracy results on final outcomes, without any deep investigation into why and how their approach works. For instance, they could verify how much accuracy improvement is due to the insensitivity to order in filtering expressions.

4. They do not compare against state-of-the-art solution on column and expression selection. While their attention mechanism over the columns could bring performance improvement, they should have included experiments over existing solutions designed for similar purpose. In (Yin, et al., IJCAI 2016), for example, representations over the columns are learned to generate better column selection.

As a conclusion, I find the submission contains certain interesting ideas but lacks serious research investigations. The quality of the paper could be much enhanced, if the authors deepen their studies on this direction.","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Online Hyper-Parameter Optimization,"['Damien Vincent', 'Sylvain Gelly', 'Nicolas Le Roux', 'Olivier Bousquet']",Reject,2018,"[7, 16]","[7, 20]","[7, 116]","[6, 84]","[1, 25]","[0, 7]","
# Summary of paper
The paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. 

# Summary of review
I find the contribution to be incremental, and the validation weak. Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution. None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject. 

# Detailed comments

1. The distinction between parameters and hyperparameters (section 3) should be revised. First, the definition of parameters should not include the word parameters. Second, it is not clear what ""parameters of the regularization"" means. Typically, the regularization depends on both hyperparameters and parameters. The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets.

2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct. This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015).

3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to. theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used).

4. The hypothesis are not correctly specified. Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated.

5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this. While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm.

# Minor

missing . after ""hypergradient exactly"".

""we could optimization the hyperparam-"" (typo)

References:
 Justin  Domke.    Generic  methods  for  optimization-based modeling.  In
International Conference on Artificial Intelligence and Statistics, 2012.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,"['Leslie N. Smith', 'Nicholay Topin']",Reject,2018,"[6, 5]","[11, 9]","[21, 29]","[3, 12]","[15, 17]","[3, 0]","This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.

The paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The ""super-convergence"" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.

The explanation of the cause of ""super-convergence"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Connectivity Learning in Multi-Branch Networks,"['Karim Ahmed', 'Lorenzo Torresani']",Reject,2018,"[3, 22]","[7, 27]","[12, 154]","[6, 83]","[5, 61]","[1, 10]","The paper is clear and well written.
It is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.

This paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the ""gates"" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)).

The main contribution appears to be the optimisation procedure for the binary masking tensor g. But this procedure is not justified: does each step minimise the loss? This seems unlikely due to the sampling. Can the authors show that the procedure will always converge? It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).
","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Data augmentation instead of explicit regularization,"['Alex Hernández-García', 'Peter König']",Reject,2018,"[2, 29]","[7, 34]","[22, 96]","[5, 30]","[17, 9]","[0, 57]","The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation.

I think it's a great idea to investigate the effects of data augmentation more thoroughly. While it is a technique that is often used in literature, there hasn't really been any work that provides rigorous comparisons with alternative approaches and insights into its inner workings. Unfortunately I feel that this paper falls short of achieving this.

Experiments are conducted on two fairly similar tasks (image classification on CIFAR-10 and CIFAR-100), with two different network architectures. This is a bit meager to be able to draw general conclusions about the properties of data augmentation. Given that this work tries to provide insight into an existing common practice, I think it is fair to expect a much stronger experimental section. In section 2.1.1 it is stated that this was a conscious choice because simplicity would lead to clearer conclusions, but I think the conclusions would be much more valuable if variety was the objective instead of simplicity, and if larger-scale tasks were also considered.

Another concern is that the narrative of the paper pits augmentation against all other regularisation techniques, whereas more typically these will be used in conjunction. It is however very interesting that some of the results show that augmentation alone can sometimes be enough.

I think extending the analysis to larger datasets such as ImageNet, as is suggested at the end of section 3, and probably also to different problems than image classification, is going to be essential to ensure that the conclusions drawn hold weight.



Comments:

- The distinction between ""explicit"" and ""implicit"" regularisation is never clearly enunciated. A bunch of examples are given for both, but I found it tricky to understand the difference from those. Initially I thought it reflected the intention behind the use of a given technique; i.e. weight decay is explicit because clearly regularisation is its primary purpose -- whereas batch normalisation is implicit because its regularisation properties are actually a side effect. However, the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques, so I guess this is not the intended meaning. Please clarify this, as the terms crop up quite often throughout the paper. I suspect that the distinction is somewhat arbitrary and not that meaningful.

- In the abstract, it is already implied that data augmentation is superior to certain other regularisation techniques because it doesn't actually reduce the capacity of the model. But this ignores the fact that some of the model's excess capacity will be used to model out-of-distribution data (w.r.t. the original training distribution) instead. Data augmentation always modifies the distribution of the training data. I don't think it makes sense to imply that this is always preferable over reducing model capacity explicitly. This claim is referred to a few times throughout the work.

- It could be more clearly stated that the reason for the regularising effect of batch normalisation is the noise in the batch estimates for mean and variance.

- Some parts of the introduction could be removed because they are obvious, at least to an ICLR audience (like ""the model would not be regularised if alpha (the regularisation parameter) equals 0"").

- The experiments with smaller dataset sizes would be more interesting if smaller percentages were used. 50% / 80% / 100% are all on the same order of magnitude and this setting is not very realistic. In practice, when a dataset is ""too small"" to be able to train a network that solves a problem reliably, it will generally be one or more orders of magnitude too small, not 2x too small.

- The choices of hyperparameters for ""light"" and ""heavy"" motivation seem somewhat arbitrary and are not well motivated. Some parameters which are sampled uniformly at random should be probably be sampled log-uniformly instead, because they represent scale factors. It should also be noted that much more extreme augmentation strategies have been used for this particular task in literature, in combination with padding (for example by Graham). It would be interesting to include this setting in the experiments as well.

- On page 7 it is stated that ""when combined with explicit regularization, the results are much worse than without it"", but these results are omitted from the table. This is unfortunate because it is a very interesting observation, that runs counter to the common practice of combining all these regularisation techniques together (e.g. L2 + dropout + data augmentation is a common combination). Delving deeper into this could make the paper a lot stronger.

- It is not entirely true that augmentation parameters depend only on the training data and not the architecture (last paragraph of section 2.4). Clearly more elaborate architectures benefit more from data augmentation, and might need heavier augmentation to perform optimally because they are more prone to overfitting (this is in fact stated earlier on in the paper as well). It is of course true that these hyperparameters tend to be much more robust to architecture changes than those of other regularisation techniques such as dropout and weight decay. This increased robustness is definitely useful and I think this is also adequately demonstrated in the experiments.

- Phrases like ""implicit regularization operates more effectively at capturing reality"" are too vague to be meaningful.

- Note that weight decay has also been found to have side effects related to optimization (e.g. in ""Imagenet classification with deep convolutional neural networks"", Krizhevsky et al.)

REVISION: I applaud the effort the authors have put in to address many of my and the other reviewers' comments. I think they have done so adequately for the most part, so I've decided to raise the rating from 3 to 5, for what it's worth.

The reason I have decided not to raise it beyond that, is that I still feel that for a paper like this, which studies an existing technique in detail, the experimental side needs to be significantly stronger. While ImageNet experiments may be a lot of work, some other (smaller) additional datasets would also have provided more interesting evidence. CIFAR-10 and CIFAR-100 are so similar that they may as well be considered variants of the same dataset, at least in the setting where they are used here.

I do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but I think for real-world relevance, variety in problem settings (i.e. datasets) is simply much more important. I think it would be fine if additional experiments on other datasets were not varied along all these other axes, to cut down on the amount of work this would involve. But not including them at all unfortunately makes the results much less impactful.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs),"['Brad Carlile', 'Guy Delamarter', 'Paul Kinney', 'Akiko Marti', 'Brian Whitney']",Reject,2018,"[23, 13, 18, 2, 17]","[22, 12, 17, 1, 16]","[3, 3, 2, 2, 11]","[0, 1, 1, 0, 7]","[2, 2, 1, 2, 1]","[1, 0, 0, 0, 3]","
Summary:
- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function.

Contributions:
- The paper proposes a cheaper activation and validates it with an MNIST experiment. The paper also shows major speedup compared to ELU and TANH (unit-wise speedup).

Pros:
- The proposed function has similar behavior as ELU but 4x cheaper.
- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future.
- The paper is clearly written and key contributions are well present.

Cons:
- Clearly, the proposed function is not faster than ReLU. In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN). But the authors also need to justify that ISRLU (or ELU) doesn’t need BN. In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion. To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks. At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU. The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU? I couldn’t find it based on the results presented here.
- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted.
- The speedup is only measured on CPU. For practical usage, especially in computer vision, GPU speedup is needed to show an impact.

Conclusion:
- Based on the comments above, I recommend weak reject.

References:
- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit. In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16).","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,"['Zhao Chen', 'Vijay Badrinarayanan', 'Chen-Yu Lee', 'Andrew Rabinovich']",Reject,2018,"[2, 12, 24, 16]","[3, 15, 29, 18]","[6, 43, 74, 40]","[2, 18, 39, 18]","[4, 21, 26, 21]","[0, 4, 9, 1]","The paper addresses an important problem in multitask learning. But its current form has several serious issues. 

Although I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible. There are many things unclear. For example:

-  it starts with talking about multiple tasks, and then immediately talks about a ""filter F"", without defining what the kind of network is being addressed. 

- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks. It is not clear how it is used. In particular, it is not clear how it is used to ""update the task weights""

- Equation 2 seems sloppy. “j” appears as a free index on the right side, but it doesn’t appear on the left side. 

As a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality.

The toy experiment is not convincing. 

- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task. This is different from the sum of the original losses, which seems to be the one used to train the “equal weight” baseline. A more fair baseline is to directly use the evaluation metric as the training loss. 
- the curves seem to have not converged.

The experiments on NYUv2 involves non-standard settings, without a good justification. So it is not clear if the proposed method can make a real difference on state of the art systems. 

And the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth). However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting — it can in fact magnify gradients of certain tasks and cause over-training and over-fitting. So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. ","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Improving image generative models with human interactions,"['Andrew Kyle Lampinen', 'David So', 'Douglas Eck', 'Fred Bertsch']",Reject,2018,"[2, 2, 20, 2]","[7, 1, 24, 6]","[36, 1, 89, 9]","[12, 0, 59, 3]","[23, 1, 22, 6]","[1, 0, 8, 0]","Summary:
This paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction. However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture. 

Pros:
+ The paper is well-written and has just a few typos: 2.1: “an Gan”.
+ The idea is very interesting. 

Cons:

- Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing.

- Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent. The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed.  

- Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$.

- Page 3- 2.2.It is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all. 

- Page 4- sec 3 "".. it should be seen as a success""; the claim is not supported well.

- Page 4- sec 3.2- last paragraph.
This claim lacks scientific support, otherwise please cite proper references. The claim seems like a subjective understanding of conscious perception and unconscious perception of affective stimuli is totally disregarded.
The experimental setup is not convincing.

- Page 4. 3.3) ""Note that.. outdoor images"" this is implicitly adding the designers' bias to the results. The statement lacks scientific support.

- Page 4. 3.3) the importance of texture and shape is disregarded. “In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al”
The architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)

- Page 6-Sec 4.2) "" It had more difficulty optimizing for the three-color result"" why? please discuss it.

- The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$","['Qixuan Huang', 'Anshumali Shrivastava', 'Yiqiu Wang']",Reject,2018,"[1, 8, 2]","[2, 13, 6]","[3, 164, 18]","[1, 74, 7]","[2, 86, 9]","[0, 4, 2]","Thanks to the authors for their feedback.
==============================
The paper presents a method for classification scheme for problems involving large number of classes in multi-class setting. This is related to the theme of extreme classification but the setting is restricted to that of multi-class classification instead of multi-label classification. The training process involves data transformation using R hash functions, and then learning R classifiers. During prediction the probability of a test instance belonging to a class is given by the sum of the probabilities assigned by the R meta-classifiers to the meta-class in the which the given class label falls. The paper demonstrates better results on ODP and Imagenet-21K datasets compared to LOMTree, RecallTree and OAA.

There are following concerns regarding the paper which don't seem to be adequately addressed :
 
 - The paper seems to propose a method in which two-step trees are being constructed based on random binning of labels, such that the first level has B nodes. It is not intuitively clear why such a method could be better in terms of prediction accuracy than OAA. The authors mention algorithms for training and prediction, and go on to mention that the method performs better than OAA. Also, please refer to point 2 below.

 - The paper repeatedly mentions that OAA has O(Kd) storage and prediction complexity. This is however not entirely true due to sparsity of training data, and the model. These statements seem quite misleading especially in the context of text datasets such as ODP. The authors are requested to check the papers [1] and [2], in which it is shown that OAA can perform surprisingly well. Also, exploiting the sparsity in the data/models, actual model sizes for WikiLSHTC-325K from [3] can be reduced from around 900GB to less than 10GB with weight pruning, and sparsity inducing regularizers. It is not clear if the 160GB model size reported for ODP took the above suggestions into considerations, and which kind of regularization was used. Was the solver used from vowpal wabbit or packages such as Liblinear were used for reporting OAA results.

 - Lack of empirical comparison - The paper lacks empirical comparisons especially on large-scale multi-class LSHTC-1/2/3 datasets [4] on which many approaches have been proposed. For a fair comparison, the proposed method must be compared against these datasets. It would be important to clarify if the method can be used on multi-label datasets or not, if so, it needs to be evaluated on the XML datasets [3].

[1] PPDSparse - http://www.kdd.org/kdd2017/papers/view/a-parallel-and-primal-dual-sparse-method-for-extreme-classification
[2] DiSMEC - https://arxiv.org/abs/1609.02521
[3] http://manikvarma.org/downloads/XC/XMLRepository.html
[4] http://lshtc.iit.demokritos.gr/LSHTC2_CFP","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Deterministic Policy Imitation Gradient Algorithm,"['Fumihiro Sasaki', 'Atsuo Kawaguchi']",Reject,2018,"[0, 32]","[5, 35]","[7, 7]","[4, 5]","[0, 0]","[3, 2]","The paper lists 5 previous very recent papers that combine IRL, adversarial learning, and stochastic policies. The goal of this paper is to do the same thing but with deterministic policies as a way of decreasing the sample complexity. The approach is related to that used in the deterministic policy gradient work. Imitation learning results on the standard control problems appear very encouraging.

Detailed comments:

""s with environment"" -> ""s with the environment""?

""that IL algorithm"" -> ""that IL algorithms"".

""e to the real-world environments"" -> ""e to real-world environments"".

"" two folds"" -> "" two fold"".

""adopting deterministic policy"" -> ""adopting a deterministic policy"".

""those appeared on the expert’s demonstrations"" -> ""those appearing in the expert’s demonstrations"".

""t tens of times less interactions"" -> ""t tens of times fewer interactions"".

Ok, I can't flag all of the examples of disfluency. The examples above come from just the abstract. The text of the paper seems even less well edited. I'd highly recommend getting some help proof reading the work.

""Thus, the noisy policy updates could frequently be performed in IL and make the learner’s policy poor. From this observation, we assume that preventing the noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations benefits to the imitation."": The justification for filtering is pretty weak. What is the statistical basis for doing so? Is it a form of a standard variance reduction approach? Is it a novel variance reduction approach? If so, is it more generally applicable?

Unfortunately, the text in Figure 1 is too small. The smallest font size you should use is that of a footnote in the text. As such, it is very difficult to assess the results.

As best I can tell, the empirical results seem impressive and interesting.
","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,"['Tianchan Guan', 'Xiaoyang Zeng', 'Mingoo Seok']",Reject,2018,"[6, 14, 12]","[10, 19, 17]","[18, 337, 155]","[10, 209, 99]","[4, 11, 8]","[4, 117, 48]","The idea of this work is fairly simple. Two main problems exist in end devices for deep learning: power and memory. There have been a series of works showing how to discretisize neural networks. This work, discretisize a NN incrementally. It does so in the following way: First, we train the network with the memory we have. Once we train and achieve a network with best performance under this constraint, we take the sign of each weight (and leave them intact), and use the remaining n-1 bits of each weight in order to add some new connections to the network. Now, we do not change the sign weights, only the new n-1 bits. We continue with this process (recursively) until we don't get any improvement in performance. 

Based on experiments done by the authors, on MNIST, having this procedure gives the same performance with 3-4 times less memory or increase in performance of 1% for the same memory as regular network. 

I like the idea, and I think it is indeed a good idea for IoT and end devices. The main problem with this method that there is undiscussed payment with current hardware architectures. I think there is a problem with optimizing the memory after each stage was trained. Also, current architectures do not support a single bit manipulations, but is much more efficient on large bits registers. So, in theory this might be a good idea, but I think this idea is not out-of-the-box method for implementation.

Also, as the authors say, more experiments are needed in order to understand the regime in which this method is efficient. To summarize, I like this idea, but more experiments are needed in order to understand this method merits. ","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling,"['Shengjia Zhao', 'Jiaming Song', 'Stefano Ermon']",Reject,2018,"[4, 4, 10]","[8, 9, 15]","[54, 120, 406]","[28, 55, 199]","[26, 60, 200]","[0, 5, 7]","Update after rebuttal
==========
Thanks for your response on my questions. The stated usefulness of the method unfortunately do not answer my worry about the significance. It remains unclear to me how much ""real"" difference the presented results would make to advance the existing work on generative models. Also, the authors did not promised any major changes in the final version in this direction, which is why I have reduced my score.

I do believe that this work could be useful and should be resubmitted. There are two main things to improve. First, the paper need more work on improving the clarity. Second, more work needs to be added to show that the paper will make a real difference to advance/improve existing methods.

==========
Before rebuttal
==========
This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. Using this framework, the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions. 

The paper has interesting elements and the results are original. The main issue is that the significance is unclear. The writing in Section 3 is unclear for me, which further made it challenging to understand the consequences of the theorems presented in that section. 

Here is a big-picture question that I would like to know answer for. Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches? Clarification on this will help me evaluate the significance of the paper.

I have three main clarification points. First, what is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems? Second, is the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems?

Third, the objective of section 3 is to show that ""only some choices of lambda lead to a dual with a tractable equivalent form"". Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper.

Some small comments:
- Eq. 4. It might help to define MI to remind readers.
- After Eq. 7, please add a proof (may be in the Appendix). It is not that straightforward to see this. Also, I suppose you are saying Eq. 3 but with f from Eq. 4.
- Line after Eq. 8, D_i is ""one"" of the following... Is it always the same D_i for all i or it could be different? Make this more clear to avoid confusion.
- Last line in Para after Eq. 15, ""This neutrality corresponds to the observations made in.."" It might be useful to add a line explaining that particular ""observation""
- Def. 7, the names did not make much sense to me. You can add a line explaining why this name is chosen.
- Def. 8, the last equation is unclear. Does the first equivalence impy the next one? 
- Writing in Sec. 3.3 can be improved. e.g., ""all linear operations on log prob."" is very unclear, ""stated computational constraints"" which constraints?
","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
CNNs as Inverse Problem Solvers and Double Network Superresolution,"['Cem TARHAN', 'Gözde BOZDAĞI AKAR']",Reject,2018,"[1, 27]","[2, 32]","[2, 125]","[0, 89]","[1, 11]","[1, 25]","The method proposes a new architecture for solving image super-resolution task. They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems.

The writing of the paper needs improvement. I was not able to understand the proposed connection, as notation is inconsistent and it is difficult to figure out what the authors are stating. I am willing to reconsider my evaluation if the authors provide clarifications.

The paper does not refer to recent advances in the problem, which are (as far as I know), the state of the art in the problem in terms of quality of the solutions. This references should be added and the authors should put their work into context.

1) Arguably, the state of the art in super resolution are techniques that go beyond L2 fitting. Specifically, methods using perceptual losses such as:

Johnson, J. et al ""Perceptual losses for real-time style transfer and super-resolution."" European Conference on Computer Vision. Springer International Publishing, 2016.

Ledig, Christian, et al. ""Photo-realistic single image super-resolution using a generative adversarial network."" arXiv preprint arXiv:1609.04802 (2016).

PSNR is known to not be directly related to image quality, as it favors blurred solutions. This should be discussed.

2) The overall notation of the paper should be improved. For instance, in (1), g represents the observation (the LR image), whereas later in the text, g is the HR image. 

3) The description of Section 2.1 is quite confusing in my view. In equation (1), y is the signal to be recovered and K is just the downsampling plus blurring. So assuming an L1 regularization in this equation assumes that the signal itself is sparse. Equation (2) changes notation referring y as f. 

4) Equation (2) seems wrong. The term multiplying K^T is not the norm (should be parenthesis).

5) The first statement of Section 2.2. seems wrong. DL methods do state the super resolution problem as an inverse problem. Instead of using a pre-defined basis function they learn an over-complete dictionary from the data, assuming that natural images can be sparsely represented. Also, this section does not explain how DL is used for super resolution. The cited work by Yang et al learns a two coupled dictionaries (one for LR and HL), such that for a given patch, the same sparse coefficients can reconstruct both HR and LR patches. The authors just state the sparse coding problem.

6) Equation (10) should not contain the \leq \epsilon.

7) In the second paragraph of Section 3, the authors mention that the LR image has to be larger than the HR image to prevent border effects. This makes sense. However, with the size of the network (20 layers), the change in size seems to be quite large. Could you please provide the sizes? When measuring PSNR, is this taken into account? 

8) It would be very helpful to include an image explaining the procedure described in the second paragraph of Section 3.

9) I find the description in Section 3 quite confusing. The authors relate the training of a single filter (or neuron) to equation (7), but they define D, that is not used in all of Section 2.1. And K does not show in any of the analysis given in the last paragraph of page 4. However, D and K seem two different things (it is not just one for the other), see bellow.

10) I cannot understand the derivation that the authors do in the last paragraph of page 4 (and beginning of page 5). What is phi_l here? K in equation (7) seems to match to D here, but D here is a collection of patches and in (7) is a blurring and downsampling operator. I cannot review this section. I will wait for the author's response clarifications.

11) The authors describe a change in roles between the representations and atoms in the training and testing phase respectively. I do not understand this. If I understand correctly, the final algorithm, the authors train a CNN mapping LR to HR images. The network is used in the same way at training and testing.

12) It would be useful to provide more details about the training of the network. Please describe the training set used by Kim et al. Are the two networks trained independently? One could think of fine-tuning them jointly (including the aggregation).

13) The authors show the advantage of separating networks on a single image, Barbara. It would be good to quantify this better (maybe in terms of PSNR?). This observation might be true only because the training loss, say than the works cited above. Please comment on this.

14) In figures 3 and 4, the learned filters are those on the top (above the yellow arrow). It is not obvious to me that the reflect the predominant structure in the data. (maybe due to the low resolution).

15) This work is related to (though clearly different)  that of LISTA (Learned ISTA) type of networks, proposed in:

Gregor, K., & LeCun, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML) 

Which connect the network architecture with the optimization algorithm used for solving the sparse coding problem. Follow up works have used these ideas for solving inverse problems as well.
","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,"['Dino S. Ratcliffe', 'Luca Citi', 'Sam Devlin', 'Udo Kruschwitz']",Reject,2018,"[2, 13, 10, 23]","[3, 17, 15, 28]","[2, 62, 91, 142]","[2, 48, 45, 107]","[0, 1, 26, 11]","[0, 13, 20, 24]","- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics. The key idea is that the agent learns a shared representations for tasks with different visual statistics

- A lot of important references  touching on very similar ideas are missing. For e.g. ""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks"", ""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping"", ""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics"". 

- This paper has a lot of orthogonal details. For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context. 

- Only single runs for the results are shown in plots. How statistically valid are the results?

- In the last section authors mention the intent to do future work on atari and other env. Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Parsimonious Deep Feed-forward Networks,"['Zhourong Chen', 'Xiaopeng Li', 'Nevin L. Zhang']",Reject,2018,"[4, 20, 27]","[9, 25, 32]","[26, 123, 151]","[12, 48, 69]","[13, 16, 55]","[1, 59, 27]","The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs).  The proposed method is focused on discovering sparse neural networks.  The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks.

The main weakness of the paper is lack of cohesion in contributions and difficulty in delineating the scope of their proposed approach.

Below are some suggestions for improving the paper:

Can you enumerate the paper’s contributions and specify the scope of this work?  Where is this method most applicable and where is it not applicable?

Why is the paper focused on these specific contributions?  What problem does this particular set of contributions solve that is not solvable by the baselines?  There needs to be a cohesive story that puts the elements together.  For example, you explain how the algorithm for creating the backbone can use unsupervised data.  On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems.

The motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network.  However, it does not adequately motivate the skip-path connections or applications of the method to supervised tasks.

Is this work extending the applicability of baselines to new types of problems?  Or is this work focused on improving the performance of existing methods?  Answers to these questions can automatically determine suitable experiments to run as well.  It's not clear if Pruned FNNs are the most suitable baseline for evaluating the results.  Can your work be compared experimentally with any of the constructive methods from the related work section?  If not, why?

When contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on?  It would be more informative to explain how the baselines contribute to the solution instead of just citing them and highlighting their differences.

Regarding the experimental results, is there any insight on why the dense networks are falling short?  For example, if it is due to overfitting, is there a correlation between performance and size of FNNs?  Do you observe a similar performance vs FNNs in existing methods?  Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution.
","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[2, 5, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Latent forward model for Real-time Strategy game planning with incomplete information,"['Yuandong Tian', 'Qucheng Gong']",Reject,2018,"[13, 2]","[18, 4]","[150, 13]","[69, 5]","[73, 7]","[8, 1]","Summary:

This paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games. The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.

Forecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.

The paper:
1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.
2. Evaluates how informative the latent states are via state reconstruction.
3. trains variatns of a forward model f on the hidden states of the various learned agents.
4. evaluates different f within MCTS for MiniRTS.

Pro:
- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods.
- The experimental setting is very non-trivial and novel.

Con:
- The manuscript is unclear in many parts -- this should be greatly improved.
1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model-free agent?
2. How is the forward model / value function used in MCTS? I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.

- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X. Yet it is unclear how this informs us about the quality of the learned forward models f. It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix.

- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models. Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8.

- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent. Is this because of the MCTS approach? Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. 

Overall:
I think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis.

Detailed:
- What are the right prediction tasks that ensure the latent space captures enough of the forward model?
- What is the error of the raw h-predictions? Only the state-reconstruction error is shown now.
- Figure 6 / sect 4.2: which model-free agent is used? Also fig 6 misses captions.
- Figure 8: scrambled caption.
- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?
","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Weighted Representations for Generalization Across Designs,"['Fredrik D. Johansson', 'Nathan Kallus', 'Uri Shalit', 'David Sontag']",Reject,2018,"[7, 5, 10, 14]","[12, 10, 15, 19]","[51, 126, 60, 171]","[22, 44, 27, 88]","[24, 69, 26, 75]","[5, 13, 7, 8]","This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function. Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs.
Overall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs. 

The main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift. However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section.","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,"['Yanqi Zhou', 'Wei Ping', 'Sercan Arik', 'Kainan Peng', 'Greg Diamos']",Reject,2018,"[5, 9, 8, 2, 11]","[10, 14, 13, 4, 15]","[58, 56, 76, 15, 49]","[24, 26, 26, 7, 26]","[24, 30, 41, 8, 20]","[10, 0, 9, 0, 3]","By generating multiple samples at once with the LSTM, the model is introducing some independence assumptions between samples that are from neighbouring windows and are not conditionally independent given the context produced by Wavenet. This reduces significantly the generality of the proposed technique.

Pros:
- Attempting to solve the important problem of speeding up autoregressive generation.
- Clarity of the write-up is OK, although it could use some polishing in some parts.
- The work is in the right direction, but the paucity of results and lack of thoroughness reduces somewhat the work's overall significance.

Cons:
- The proposed technique is not particularly novel and it is not clear whether the technique can be used to get speed-ups beyond 2x - something that is important for real-world deployment of Wavenet.
- The amount of innovation is on the low side, as it involves mostly just fairly minor architectural changes.
- The absolute results are not that great (MOS ~3.8 is not close to the SOTA of 4.4 - 4.5)


","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Key Protected Classification for GAN Attack Resilient Collaborative Learning,"['Mert Bülent Sarıyıldız', 'Ramazan Gökberk Cinbiş', 'Erman Ayday']",Reject,2018,"[0, 13, 13]","[4, 18, 18]","[11, 61, 113]","[4, 28, 52]","[6, 22, 33]","[1, 11, 28]","Collaborative learning has been proposed as a way to learn over federated data while preserving privacy. However collaborative learning has been shown to be suscepti
ble to active attacks in which one of the participants uses a GAN to reveal information about another participant.

This paper proposes a collaborative learning framework (CLF) that mitigates the GAN attack. The framework involves using the neural net to learn a mapping of the inp
ut to a high-dimensional vector and computing the inner product of this vector to a random class-specific key (the final class prediction is the argmax of this inner product). The class-specific key can be chosen randomly by each participant. By choosing sufficiently long random keys, the probability of an attacker guessing the key can be reduced. Experiments on two datasets show that this scheme successfully avoids the GAN attack.
 
1. Some of the details of key sharing are not clear and would appear to be important for the scheme to work. For example, if participants have instances associated with the same class, then they would need to share the key. This would require a central key distribution scheme which would then allow the attacker to also get access to the key.

2. I would have  liked to see how the method works with an increasing fraction of adversarial participants (I could only see experiments with one adversary). Similarly, I would have liked to see experiments with and without the fixed dense layer to see its contribution to effective learning. ","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Statestream: A toolbox to explore layerwise-parallel deep neural networks,['Volker Fischer'],Reject,2018,['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"In this paper, the authors present an open-source toolbox to explore layerwise-parallel deep neural networks. They offer an interesting and detailed comparison of the temporal progression of layerwise-parallel and layerwise-sequential networks, and differences that can emerge in the results of these two computation strategies.

While the open-source toolbox introduced in this paper can be an excellent resource for the community interested in exploring these networks, the present submission offers relatively few results actually using these networks in practice. In order to make a more compelling case for these networks, the present submission could include more detailed investigations, perhaps demonstrating that they learn differently or better than other implementations on standard training sets.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Automatic Parameter Tying in Neural Networks,"['Yibo Yang', 'Nicholas Ruozzi', 'Vibhav Gogate']",Reject,2018,"[10, 12, 15]","[15, 17, 20]","[97, 49, 94]","[37, 31, 68]","[48, 13, 18]","[12, 5, 8]","Approach is interesting however my main reservation is with the data set used for experiments and making general (!) conclusions. MNIST, CIFAR-10 are too simple tasks perhaps suitable for debugging but not for a comprehensive validation of quantization/compression techniques. Looking at the results, I see a horrific degradation of 25-43% relative to DC baseline despite being told about only a minimal loss in accuracy. A number of general statements is made based on MNIST data, such as on page 3 when comparing GMM and k-means priors, on page 7 and 8 when claiming that parameter tying and sparsity do not act strongly to improve generalization. In addition, by making a list of all hyper parameters you tuned I am not confident that your claim that this approach requires less tuning. 

Additional comments:

(a) you did not mention student-teacher training
(b) reference to previously not introduced K-means prior at the end of section 1
(c) what is that special version of 1-D K-means?
(d) Beginning of section 4.1 is hard to follow as you are referring to some experiments not shown in the paper.
(e) Where is 8th cluster hiding in Figure 1b?
(f) Any comparison to a classic compression technique would be beneficial.
(g) You are referring to a sparsity at the end of page 8 without formally defining it. 
(h) Can you label each subfigure in Figure 3 so I do not need to refer to the caption? Can you discuss this diagram in the main text, otherwise what is the point of dumping it in the appendix?
(i) I do not understand Figure 4 without explanation. ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Adversarial reading networks for machine comprehension,"['Quentin Grail', 'Julien Perez']",Reject,2018,"[1, 11]","[4, 14]","[2, 36]","[1, 20]","[1, 9]","[0, 7]","Summary:

This paper proposes an adversarial learning framework for machine comprehension task. Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task. Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.


My Comments:

This paper is a direct application of adversarial learning to the task of reading comprehension. It is a reasonable idea and authors indeed show that it works.

1. The paper needs a lot of editing. Please check the minor comments.

2. Why is the adversary called narrator network? It is bit confusing because the job of that network is to obfuscate the passage.

3. Why do you motivate the learning method using self-play? This is just using the idea of adversarial learning (like GAN) and it is not related to self-play.

4. In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting. How is this happening? Can you elaborate more?

5. The learning framework is not explained in a precise way. What do you mean by re-initializing and retraining the narrator? Isn’t it costly to reinitialize the network and retrain it for every turn? How many such epochs are done? You say that test set also contains obfuscated documents. Is it only for the validation set? Can you please explain if you use obfuscation when you report the final test performance too? It would be more clear if you can provide a complete pseudo-code of the learning procedure.

6. How does the narrator choose which word to obfuscate? Do you run the narrator model with all possible obfuscations and pick the best choice?

7. Why don’t you treat number of hops as a hyper-parameter and choose it based on validation set? I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.

8. In figure 2, how are rounds constructed? Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement? This will be clear if you provide the pseudo-code for learning.

9. I do not understand author's’ justification for figure-3. Is it the case that the model learns to attend to last sentences for all the questions? Or where it attends varies across examples?

10. Are you willing to release the code for reproducing the results?

Minor comments:

Page 1, “exploit his own decision” should be “exploit its own decision”
In page 2, section 2.1, sentence starting with “Indeed, a too low percentage …” needs to be fixed.
Page 3, “forgetting is compensate” should be “forgetting is compensated”.
Page 4, “for one sentences” needs to be fixed.
Page 4, “unknow” should be “unknown”.
Page 4, “??” needs to be fixed.
Page 5, “for the two first datasets” needs to be fixed.
Table 1, “GMenN2N” should be “GMemN2N”. In caption, is it mean accuracy or maximum accuracy?
Page 6, “dataset was achieves” needs to be fixed.
Page 7, “document by obfuscated this word” needs to be fixed.
Page 7, “overall aspect of the two first readers” needs to be fixed.
Page 8, last para, references needs to be fixed.
Page 9, first sentence, please check grammar.
Section 6.2, last sentence is irrelevant.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DNN Model Compression Under Accuracy Constraints,"['Soroosh Khoram', 'Jing Li']",Reject,2018,"[2, 12]","[7, 17]","[10, 50]","[6, 34]","[2, 4]","[2, 12]","1. Summary

This paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change.


2. High level paper

- I believe the writing is a bit sloppy. For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below). This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models.


3. High level technical

- I'm confused at the first and second lines of equation (19). In the first line, shouldn't the first term not contain \Delta W ? In the second line, shouldn't the first term be \tilde{\mathcal{L}}(W_0 + \Delta W) ?
- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data. My worry is if you're compressing these networks with your method are the weights not treated as binary anymore? Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright. But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply.
- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354). What is your compression ratio for 0 accuracy loss? I think this is a key experiment that should be run as this result would be much easier to compare with the other methods.
- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers?
- The first paper to propose weight sharing was not Han et al., 2015, it was actually:
Chen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. ""Compressing Neural Networks with the Hashing Trick"" ICML 2015
Although they did not learn the weight sharing function, but use random hash functions.


4. Low level technical

- The end of Section 2 has an extra 'p' character
- Section 3.1: ""Here, X and y define a set of samples and ideal output distributions we use for training"" this sentence is a bit confusing. Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3.
- Section 3.1: ""W is the learnt model...\hat{W} is the final, trained model"" This is unclear: W and \hat{W} seem to describe the same thing. I would just remove ""is the learnt model and""


5. Review summary

While the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read. This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Autonomous Vehicle Fleet Coordination With Deep Reinforcement Learning,['Cane Punma'],Reject,2018,['no_match'],['no_match'],['no match'],['no match'],['no match'],['no match'],"

This paper proposes to use deep reinforcement learning to solve a multiagent coordination task. In particular, the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi companies. 

The paper does not really introduce new methods, and as such, this paper should be seen more as an application paper. I think that such a paper could have merits if it would really push the boundary of the feasible, but I do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines). As such, I do not really see any real grounds for acceptance.

Finally, there are also many other weaknesses. The paper is quite poorly written in places, has poor formatting (citations are incorrect and half a bibtex entry is inlined), and is highly inadequate in its treatment of related work. For instance, there are many related papers on:

-taxi fleet management (e.g., work by Pradeep Varakantham)
 
-coordination in multi-robot systems for spatially distributed tasks (e.g., Gerkey and much work since)

-scaling up multiagent reinforcement learning and multiagent MDPs (Guestrin et al 2002, Kok & Vlassis 2006, etc.)

-dealing with partial observability (work on decentralized POMDPs by Peshkin et al, 2000, Bernstein, Amato, etc.)

-multiagent deep RL has been very active last 1-2 years. E.g., see other papers by Foerster, Sukhbataar, Omidshafiei


Overall, I see this as a paper which with improvements could make a nice workshop contribution, but not as a paper to be published at a top-tier venue.

","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Deep Learning Inferences with Hybrid Homomorphic Encryption,"['Anthony Meehan', 'Ryan K L Ko', 'Geoff Holmes']",Reject,2018,"['no_match', 10, 'no_match']","['no_match', 15, 'no_match']","['no match', 85, 'no match']","['no match', 55, 'no match']","['no match', 11, 'no match']","['no match', 19, 'no match']","This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm. 
The paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it. It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear.

The results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper. For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6).

Result interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used.
Overall, fine work, well organized, decomposed, and its rationale clearly explained. The good results obtained support the design decisions made.
Our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims.

Nota: 
     - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0?
     - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2=5?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Neural Clustering By Predicting And Copying Noise,"['Sam Coope', 'Andrej Zukov-Gregoric', 'Yoram Bachrach']",Reject,2018,"[2, 3, 14]","[5, 5, 19]","[14, 8, 150]","[8, 6, 98]","[6, 2, 31]","[0, 0, 21]","This paper presents an algorithm for clustering using DNNs. The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs. The second step also shrinks the number of targets over time to achieve clustering. Intuitively, the randomness in target may achieve certain regularization effect.

My concerns:
1. There is no analysis on what the regularization effect is. What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot?
2. The delete-and-copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout. Without a consistent objective function, the algorithm seems somewhat heuristic.
3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it. If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case.
4. Does the  Hungarian algorithm used for matching scales to much larger datasets?
5. While the algorithm empirically improve over k-means, I believe at this point combinations of DNN with classical clustering algorithms already exist and comparisons with such stronger baselines are missing. The authors have listed a few related algorithms in the last paragraph on page 1. I think the following one is also relevant:
-- Law et al. Deep spectral clustering learning. ICML 2015.

","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Generative Discovery of Relational Medical Entity Pairs,"['Chenwei Zhang', 'Yaliang Li', 'Nan Du', 'Wei Fan', 'Philip S. Yu']",Reject,2018,"[6, 8, 13, 20, 39]","[11, 13, 18, 24, 44]","[104, 178, 176, 236, 1879]","[45, 100, 108, 161, 991]","[38, 55, 38, 33, 389]","[21, 23, 30, 42, 499]","In the medical context, this paper describes the classic problem of ""knowledge base completion"" from structured data only (no text).  The authors argue for the advantages of a generative VAE approach (but without being convincing).  They do not cite the extensive literature on KB completion.  They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods.

The authors seem unaware of a large literature on ""knowledge base completion.""  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015], 

The paper claims that operating on pre-structured data only (without using text) is an advantage.  I don't find the argument convincing.  There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. ""universal schema"" [Riedel et al, 2014].

The paper claims that ""discriminative approaches"" need to iterate over all possible entity pairs to make predictions.  In their generative approach they say they find outputs by ""nearest neighbor search.""  But the same efficient search is possible in many of the classic ""discriminatively-trained"" KB completion models also.

It is admirable that the authors use an interesting (and to my knowledge novel) data set.  But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995.  The method is evaluated only against their own VAE-based alternatives.  It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.
","[2, 4, 4]","[' Strong rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Data Augmentation by Pairing Samples for Images Classification,['Hiroshi Inoue'],Reject,2018,[22],[26],[95],[40],[4],[51],"The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target.  Since this scheme is asymmetric and uses quite unrealistic input images, a training scheme is used where the technique is only enabled in the middle of training (not very beginning or end), and in an alternating on-off fashion.  This improves classification performance nicely on a variety of datasets.

This is a simple technique, and the paper is concise and to the point.  However, I would have liked to see a few additional comparisons.

First, this augmentation technique seems to have two components:  One is the mixing of inputs, but another is the effective dropping of labels from one of the two images in the pair.  Which of these are more important, and can they be separated?  What if some of the images' labels are changed at random, for half the images in a minibatch, for example?  This would have the effect of random label changes, but without the input mixing.  Likewise, what if both labels in the pair are used as targets (with 0.5 assigned to each in the softmax target)?  This would mix the images, but keep targets intact.

Second, the bottom of p.3 says that multiple training procedures were evaluated, but I'd be interested to see the results of some of these.  In particular, is it important to alternate enabling and disabling SamplePairing, or does it also work to mix samples with and without it in each minibatch (e.g. 3/4 of the minibatch with pairing augmentation, and 1/4 without it)?

I liked the experiment mixing images from within a restricted training set composed of a subset of the CIFAR images, compared to mixing these images with CIFAR training set images outside the restricted sample (p.5 and Fig 5).  This suggests to me, however, that it's possible the label manipulations may play an important role.  Or, is an explanation why this performs not as well that the network will train these mixing images to random targets (that of the training image in the pair), and never see this example again, whereas by using the training set alone, the mixing image is likely to be repeated with its correct label?  Some more discussion on this would be nice.

Overall, I think this is an interesting technique that appears to achieve nice results.  It could be investigated deeper at some key points.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning what to learn in a neural program,"['Richard Shin', 'Dawn Song']",Reject,2018,"[9, 20]","[13, 25]","[38, 440]","[21, 254]","[16, 154]","[1, 32]","In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle. In particular, they aim at generating a complete set that fully specifies the behavior of the oracle. The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation. The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle. The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class. Their experiments show that this improve technique can produce complete training sets for three programs.

It is nice to see the application of ideas from different areas for learning-related questions. However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all? Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data. If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter. One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter. During research, we have multiple executable oracles and need to produce good training data from them. The authors' technique may let us do this data-generation easily. But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.

 ","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Learning Independent Features with Adversarial Nets for Non-linear ICA,"['Philemon Brakel', 'Yoshua Bengio']",Reject,2018,"[8, 31]","[12, 36]","[32, 975]","[15, 405]","[14, 454]","[3, 116]","The focus of the paper is independent component analysis (ICA) and its nonlinear variants such as the post non-linear (PNL) ICA model. Motivated by the fact that estimating mutual information and similar dependency measures require density estimates and hard to optimize, the authors propose a Wasserstein GAN (generative adversarial network) based solution to tackle the problem, with illustrations on 6 (synthetic) and 3-dimemensional (audio) examples. The primary idea of the paper is to use the Wasserstein distance as an independence measure of the estimated source coordinates, and optimize it in a neural network (NN) framework.

Although finding novel GAN applications is an exciting topic, I am not really convinced that ICA with the proposed Wasserstein GAN based technique fulfills this goal.
 
Below I detail my reasons:

1)The ICA problem can be formulated as the minimization of pairwise mutual information [1] or one-dimensional entropy [2]. In other words, estimating the joint dependence of the source coordinates is not necessary; it is worthwhile to avoid it.

2)The PNL ICA task can be efficiently tackled by first 'removing' the nonlinearity followed by classical linear ICA; see for example [3].

3)Estimating information theoretic (IT) measures (mutual information, divergence) is a quite mature field with off-the-self techniques, see for example [4,5,6,8]. These methods do not estimate the underlying densities; it would be superfluous (and hard).

4)Optimizing non-differentiable IT measures can computationally quite efficiently carried out in the ICA context by e.g., Givens rotations [7]; differentiable ICA cost functions can be robustly handled by Stiefel manifold methods; see for example [8,9].

5)Section 3.1: This section is devoted to generating samples from the product of the marginals, even using separate generator networks. I do not see the necessity of these solutions; the subtask can be solved by independently shuffling all the coordinates of the sample.

6)Experiments (Section 6): 
i) It seems to me that the proposed NN-based technique has some quite serious divergence issues: 'After discarding diverged models, ...' or 'Unfortunately, the model selection procedure also didn't identify good settings for the Anica-g model...'.
ii) The proposed method gives pretty comparable results to the chosen baselines (fastICA, PNLMISEP) on the selected small-dimensional tasks. In fact, [7,8,9] are likely to provide more accurate (fastICA is a simple kurtosis based method, which is 
a somewhat crude 'estimate' of entropy) and faster estimates; see also 2).

References:
[1] Pierre Comon. Independent component analysis, a new concept? Signal Processing, 36:287-314, 1994.
[2] Aapo Hyvarinen and Erkki Oja. Independent Component Analysis: Algorithms and Applications. Neural Networks, 13(4-5):411-30, 2000. 
[3] Andreas Ziehe, Motoaki Kawanabe, Stefan Harmeling, and Klaus-Robert Muller. Blind separation of postnonlinear mixtures using linearizing transformations and temporal decorrelation. Journal of Machine Learning Research, 4:1319-1338, 2003.
[4] Barnabas Poczos, Liang Xiong, and Jeff Schneider. Nonparametric divergence: Estimation with applications to machine learning on distributions. In Conference on Uncertainty in Artificial Intelligence, pages 599-608, 2011.
[5] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, Alexander Smola. A Kernel Two-Sample Test. Journal of Machine Learning Research, 13:723-773, 2012.
[6] Alan Wisler, Visar Berisha, Andreas Spanias, Alfred O. Hero. A data-driven basis for direct estimation of functionals of distributions. TR, 2017. (https://arxiv.org/abs/1702.06516) 
[7] Erik G. Learned-Miller, John W. Fisher III. ICA using spacings estimates of entropy. Journal of Machine Learning Research, 4:1271-1295, 2003.
[8] Francis R. Bach. Michael I. Jordan. Kernel Independent Component Analysis. Journal of Machine Learning Research 3: 1-48, 2002.
[9] Hao Shen, Stefanie Jegelka and Arthur Gretton. Fast Kernel-Based Independent Component Analysis, IEEE Transactions on Signal Processing, 57:3498-3511, 2009.
","[3, 5, 6]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,"['Michael O. Vertolli', 'Jim Davies']",Reject,2018,"[5, 31]","[5, 35]","[7, 113]","[5, 78]","[1, 2]","[1, 33]","Summary: 
The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic. In particular, the authors propose to change the energy function associated with the auto-encoder, from an L2 norm (a single number) to an energy function with multiple components. Their energy function is inspired by the structured similarity index (SSIM), and the three components they use are the L1 score, the gradient magnitude similarity score, and the chromium score. Using this energy function, the authors hypothesize, that it will force the generator to generate realistic images. They test their hypothesis on a single dataset, namely, the CelebA dataset. 

Review: 
While the idea proposed in the paper is somewhat novel and there is nothing obviously wrong about the proposed approach, I thought the paper is somewhat incremental. As a result I kind of question the impact of this result. My suspicion is reinforced by the fact that the experimental section is extremely weak. In particular the authors test their model on a single relatively straightforward dataset. Any reason why the authors did not try on other datasets involving natural images? As a result I feel that the title and the claims in the paper are somewhat misleading and premature: that the proposed techniques improves the training and evaluation of energy based gans. 

Over all the paper is clearly written and easy to understand. 

Based on its incremental nature and weak experiments, I'm on the margin with regards to its acceptance. Happy to change my opinion if other reviewers strongly think otherwise with good reason and are convinced about its impact. ","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Contextual memory bandit for pro-active dialog engagement,"['julien perez', 'Tomi Silander']",Reject,2018,"[11, 29]","[14, 31]","[36, 42]","[20, 27]","[9, 6]","[7, 9]","The paper ""CONTEXTUAL MEMORY BANDIT FOR PRO-ACTIVE DIALOG ENGAGEMENT"" proposes to address the problem of pro-active dialog engagement by the mean of a bandit framework that selects dialog situations w.r.t. to the context of the system. Authors define a neural archiecture managing memory with the mean of a contextual attention mechanism.

My main concern about this paper is that the proposal is not enough well described. A very large amount of technical details are missing for allowing the reader to understand the model (and reproduce the experiments). The most important ones are about the exploration policies which are not described at all, while it is a very central point of the paper. The only discussion given w.r.t. the exploration policy is a very general overview about Thompson Sampling. But nothing is said about how it is implemented in the case of the proposed model. How is estimated p(\Theta|D) ? Ok  authors give p(\Theta|D) as a product between prior and likelihood. But it is not sufficient to get p(\Theta|D), the evidence should also been considered (for instance by using variational inference). Also, what is the prior of the parameters ? How is distributed r given a,x and \Theta ? 

Also, not enough justification is given about the general idea of the model. Authors should give more intuitions about the mechanism they propose. Figure 2 should be able to help, but no reference to this figure is given in the text, so it is very difficult to extract any information from it. Authors only (roughly) describe  the architecture without justifying their choices.

At last, the experiments really fail at demonstrating the relevance of the approach, as only questionable artificial data is used. On the first hand it appears mandatory to me to consider some (even minimal) experiments on real data for such proposal. On the other one, the simulated data used there cannot correspond to cues to validate the approach since they appear very far from real scenarios: the trajectories do not depend on what is recommended. Ok only the recommended places reveal some reward but it appears not as a sufficiently realistic scenario to me. Also, very too few baselines are considered: only different versions of the proposal and a random baseline are considered. A classical contextual bandit instance (such as LinUCB) would have been a minimum.

Other remarks:
      - the definition of q is not given
      - user is part of the context x  in the bandit section but not after where it is denoted as u.
      - the notion of time window should be more formally described
      - How is built the context is not clear in the experiments section

","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus,"['Vuong Ho Quan', 'Yiming Zhang', 'Kenny Song', 'Xiao-Yue Gong', 'Keith W. Ross']",Reject,2018,"[1, 1, 1, 33]","[5, 1, 5, 37]","[9, 2, 6, 180]","[4, 1, 1, 102]","[5, 1, 3, 17]","[0, 0, 2, 61]","The authors present two autoregressive models for sampling action probabilities from a factorized discrete action space. On a multi-agent gridworld task and a multi-agent multi-armed bandit task, the proposed method seems to benefit from their lower-variance entropy estimator for exploration bonus. A few key citations were missing - notably the LSTM model they propose is a clear instance of an autoregressive density estimator, as in PixelCNN, WaveNet and other recently popular deep architectures. In that context, this work can be viewed as applying deep autoregressive density estimators to policy gradient methods. At least one of those papers ought to be cited. It also seems like a simple, obvious baseline is missing from their experiments - simply independently outputting D independent softmaxes from the policy network. Without that baseline it's not clear that any actual benefit is gained by modeling the joint distribution between actions, especially since the optimal policy for an MDP is provably deterministic anyway. The method could even be made to capture dependencies between different actions by adding a latent probabilistic layer in the middle of the policy network, inducing marginal dependencies between different actions. A direct comparison against one of the related methods in the discussion section would help better contextualize the paper as well. A final point on clarity of presentation - in keeping with the convention in the field, the readability of the tables could be improved by putting the top-performing models in bold, and Table 2 should almost certainly be replaced by a boxplot.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
TRL: Discriminative Hints for Scalable Reverse Curriculum Learning,"['Chen Wang', 'Xiangyu Chen', 'Zelin Ye', 'Jialu Wang', 'Ziruo Cai', 'Shixiang Gu', 'Cewu Lu']",Reject,2018,"['no_match', 10, 0, 3, 0, 7, 8]","['no_match', 15, 5, 8, 5, 12, 13]","['no match', 131, 11, 28, 3, 89, 252]","['no match', 65, 4, 13, 1, 38, 107]","['no match', 39, 5, 13, 2, 49, 116]","['no match', 27, 2, 2, 0, 2, 29]","This paper proposes a new method for reverse curriculum generation by gradually reseting the environment in phases and classifying states that tend to lead to success. It additionally proposes a mechanism for learning from human-provided ""key states"".

The ideas in this paper are quite nice, but the paper has significant issues with regard to clarity and applicability to real-world problems:
First, it is unclear is the proposed method requires access only high-dimensional observations (e.g. images) during training or if it additionally requires low-dimensional states (e.g. sufficient information to reset the environment). In most compelling problems settings where a low-dimensional representation that sufficiently explains the current state of the world is available during training, then it is also likely that one can write down a nicely shaped reward function using that state information during training, in which case, it makes sense to use such a reward function. This paper seems to require access to low-dimensional states, and specifically considers the sparse-reward setting, which seems contrived.
Second, the paper states that the assumption ""when resetting, the agent can be reset to any state"" can be satisfied in problems such as real-world robotic manipulation. This is not correct. If the robot could autonomously reset to any state, then we would have largely solved robotic manipulation. Further, it is not always realistic to assume access to low-dimensional state information during training on a real robotic system (e.g. knowing the poses of all of the objects in the world).
Third, the experiments section lacks crucial information needed to understand the experiments. What is the state, observation, and action space for each problem setting? What is the reward function for each problem setting? What reinforcement learning algorithm is used in combination with the curriculum and tendency rewards? Are the states and actions continuous or discrete? Without this information, it is difficult to judge the merit of the experimental setting.
Fourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc. Could each of the components be motivated further through more discussion and/or ablative studies?
Finally, the main text of the paper is substantially longer than the recommended page limit. It should be shortened by making the writing more concise.

Beyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technical content, experiments, and related work:
I'm wondering -- can the reward shaping in Equation 2 be made to satisfy the property of not affecting the final policy? (see Ng et al. '09) If so, such a reward shaping would make the method even more appealing.
How do the experiments in section 5.4 compare to prior methods and ablations? Without such a comparison, it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks. At the very least, the paper should compare the performance of the proposed method to the performance a random policy.

The paper is missing some highly relevant references. First, how does the proposed method compare to hindsight experience replay? [1] Second, learning from keyframes (rather than demonstrations) has been explored in the past [1]. It would be preferable to use the standard terminology of ""keyframe"".

[1] Andrychowicz et al. Hindsight Experience Replay. 2017
[2] Akgun et al. Keyframe-based Learning from Demonstration. 2012

In summary, I think this paper has a number of promising ideas and experimental results, but given the significant issues in clarity and significance to real world problems, I don't think that the current version of this paper is suitable for publication in ICLR.

More minor feedback on clarity and correctness:
- Abstract: ""Deep RL algorithms have proven successful in a vast variety of domains"" -- This is an overstatement.
- The introduction should be more clear with regard to the assumptions. In particular, it would be helpful to see discussion of requiring human-provided keyframes. As is, it is unclear what is meant by ""checkpoint scheme"", which is not commonly used terminology.
- ""This kind of spare reward, goal-oriented tasks are considered the most difficult challenges"" -- This is also an overstatement. Long-horizon tasks and high-dimensional observations are also very difficult. Also, the sentence is not grammatically correct.
- ""That is, environment"" -> ""That is, the environment""
- In the last paragraph of the intro, it would be helpful to more clearly state what the experiments can accomplish. Can they handle raw pixel inputs?
- ""diverse domains"" -> ""diverse simulated domains""
- ""a robotic grasping task"" -> ""a simulated robotic grasping task""
- There are a number of issues and errors in citations, e.g. missing the year, including the first name, incorrect reference
- Assumption 1: \mathcal{P} has not yet been defined.
- The last two paragraphs of section 3.2 are very difficult to understand without reading the method yet
- ""conventional RL solver tend"" -> ""conventional RL tend"", also should mention sparse reward in this sentence.
- Algorithm 1 and Figure 1 are not referenced in the text anywhere, and should be
- The text in Figure 1 and Figure 3 is extremely small
- The text in Figure 3 is extremely small


","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,"['Xia Xiao', 'Sanguthevar Rajasekaran']",Reject,2018,[32],[37],[318],[181],[19],[118],"Summary:
This paper proposes parallel GANs (PGANs). This is a new architecture which composes the generator based on a mixture of weak generators with the main intended purpose that each unique generator may suffer mode collapse, but as long as each generator collapses to a distinct mode, the combination of generators will cover the whole image distribution. The paper proposes a number of technical details to 1) ensure that each sub generator offers distinct information (adjustment component, C) and 2) to efficiently train the generators in parallel while accumulating information to update both the discriminator and the adjustment component. 
Results are shown on a synthetic dataset of gaussian mixtures, demonstrating that the model does indeed find all modes within the data, and on two small real image datasets: MNIST and CIFAR-10. Overall the parallel generator model results in ~x2 speedup in training time compared with a single complex generator model.

Strengths:
Mode collapse in GANs is a timely and unsolved problem. While most work aims to construct auxiliary loss function to prevent this collapse, this paper instead chooses to accept the collapse and instead encourage multiple models which collapse to unique modes. Though this does present a new problem in chooses the number of modes to estimate within a data source, the paper also presents a solution to systematically combine redundant modes over time, making the model more robust to the choice of number of generators overall. 

Weaknesses:
Organization - The paper is quite difficult to read. Some concepts are presented out of order. For example, the notion of an adjustment component is very natural but not introduced until after it is mentioned a few times. Similarly, G_{-k} is mentioned many times but not clearly defined.  I would suggest to the authors to reorder the subsections in the method part to first outline the main idea: (parallel generators to capture different parts of overall distribution), mention the need to prevent redundancy between the generators (C), and mention some technical overhead in determining how to process all generated images by D. All of this may be discussed within the context of Fig 1. Also Fig 1a-b may be combined and may aid in explanation. 

Experiments - Comparison is limited to single generator models. Many other generator approaches exist beyond a single generator/discriminator GAN. In particular, different loss functions for training the generator (LS-GAN etc). Missing some relevant details like why use HogWild or what it is. 

Minimal understanding - I would like to know what exactly each generator contributes in the real world datasets. Can you show some generations from each mode? Is there a human perceivable difference?

Figure 4: why does the inception score for the single generator models vary with the #generators?

Last paragraph before 4.2.1: Please clarify this sentence - “we designed a relatively strong discriminator with a high learning rate, since the gradient vanish problem is not observed in reverse KL GAN.” 

Typo: last line page 7: “we the use” → “we use the”","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
The Mutual Autoencoder: Controlling Information in Latent Code Representations,"['Mary Phuong', 'Max Welling', 'Nate Kushman', 'Ryota Tomioka', 'Sebastian Nowozin']",Reject,2018,"[0, 19, 12, 13, 12]","[5, 24, 16, 18, 17]","[8, 390, 33, 76, 121]","[4, 190, 19, 38, 69]","[4, 166, 13, 24, 42]","[0, 34, 1, 14, 10]","This paper presents mutual autoencoders (MAE). MAE aims to address the limitation of regular variational autoencoders (VAE) for latent representation learning — VAE sometimes simply ignores the latent code z, especially with a powerful decoding distribution. The idea of MAE is to optimize the VAE objective subject to a constraint on the mutual information between the data x and latent code z: setting the mutual information constraints larger will force the latent code z to learn a meaningful representation of the data. An approximation strategy is employed to approximate the intractable mutual information. Experimental results on both synthetic data and movie review data demonstrate the effectiveness of the MAEs.  

Overall, the paper is well-written. The problem that VAEs fail to learn a meaningful representation is a well-known issue. This paper presents a simple, yet principled modification to the VAE objective to address this problem. I do, however, have two major concerns about the paper:

1. The proposed idea to add a mutual information constraint between the data x and latent code z is a very natural fix to the failure of regular VAEs. However, mutual information itself is not a quantity that is easy to comprehend and specify. This is not like, e.g., l2 regularization parameter, for which there exists a relatively clear way to specify and tune. For mutual information, at least it is not clear to me, how much mutual information is “enough” and I am pretty sure it is model/data-dependent. To make it worse, there exist no metrics in representation learning for us to easily tune this mutual information constraint. It seems the only way to select the mutual information constraint is to qualitative inspect the model fits. This makes the method less practical. 

2. The approximation to the mutual information seems rather loose. If I understand correctly, the optimization of MAE is similar to that of a regular VAE, with an additional parametric model r_w(z|x) which is used to approximate the infomax bound. (And this also adds an additional term to the gradient wrt \theta). r_w(z|x) is updated at the same time as \theta, which means r_w(z|x) is quite far from being an optimal r* as it is intended, especially early during the optimization. Further more, all the derivation following Eq (12-13) are based on r* being optimal, while in reality, it is probably not even close. This makes the whole approximation quite hand-waving. 

Related to 2, the discussion in Section 6 deserves more elaboration. It seems that having a flexible encoder is quite important, yet the authors only mention lightly that they use the approximate posterior from Cremer et al. (2017). Will MAE not work without this? How will VAE (without the mutual information constraint) work with this? A lot of the details seem to be glossed over. 

Furthermore, this work is also related to the deep variational information bottleneck of Alemi et al. 2017 (especially in the appendix they derived the VAE objective using information bottleneck principle). My intuition is that using a larger mutual information constraint in MAE is somewhat similar to setting the regularization \beta to be smaller than 1 — both are making the approximating posterior more concentrated. I wonder if the authors have explored this idea. 
 

Minor comments:

1. It would be more informative to include the running time in the presented results. 

2. Since the goal of r_w(z | x) is to approximate the posterior p(z | x), what about directly using q(z | x) to approximate it? 

3. In Algorithm 1, should line 14 and 15 be swapped? It seems samples are required in line 14 as well. 

4. Nitpicking: technically the model in Eq (1) is not a hierarchical model. 

","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,"['Mikhail Yurochkin', 'Dung Thai', 'Hung Hai Bui', 'XuanLong Nguyen']",Reject,2018,"[3, 2, 23, 19]","[8, 7, 27, 24]","[70, 14, 82, 76]","[31, 7, 55, 33]","[38, 7, 13, 28]","[1, 0, 14, 15]","Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. A projected sub-gradient descent algorithm is used. The UPS optimizer by itself is not new.

Graph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter. First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. There are some concerns about this network that need to be clarified:
1. sigma is never clarified in the main context or experiments
2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random.
3. another explanation about the weights as the rescaling to matrix A needs to further clarified. As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction. What is the disagreement between L1 penalty and prediction quality? Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?
4. Authors stated that the last step is to build a mapping from the GPS features into the response Y. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper. In the experiments, authors only stated that “we fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph”, and then the graph is used to train existing models as the input of the graph. Which architecture is used for building the mapping ?

In the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified. For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on. More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm. 
","[6, 4, 3]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
"Learning to Select: Problem, Solution, and Applications","['Heechang Ryu', 'Donghyun Kim', 'Hayong Shin']",Reject,2018,"[3, 'no_match', 26]","[7, 'no_match', 31]","[9, 'no match', 50]","[4, 'no match', 20]","[5, 'no match', 5]","[0, 'no match', 25]","The paper proposed a new framework called `Learning to select’, in which a best candidate needs to be identified in the decision making process such as job dispatching. A CNN architecture is designed, called `Variable-Length CNN’, to solve this problem.

My major concern is on the definition of the proposed concept of `learning-to-select’. Essentially, I’ve not seen its key difference from the classification problem. While `even in the case of completely identical candidates, the label can be 1 in some situations, and in some other situations the label can be 0’, why not including such `situations’ into your feature vector (i.e., x)? Once you do it, the gap between learning to select and classification will vanish. If this is not doable, you should better make more discussions, especially on what the so-called `situations’ are.  Furthermore, the application scope of the proposed framework is not very well discussed. If it is restricted to job dispatching scenarios, why do we need a new concept “learning to select”?

The proposed model looks quite straightforward. Standard CNN is able to capture the variable length input as is done in many NLP tasks. Dynamic computational graph is not new either. In this sense, the technical novelty of this work is somehow limited.

The experiments are weak in that the data are simulated and the baselines are not strong. I’ve not gained enough insights on why the proposed model could outperform the alternative approaches. More discussions and case studies are sorely needed.
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the Construction and Evaluation of Color Invariant Networks,['Konrad Groh'],Reject,2018,[9],[12],[8],[2],[0],[6],"The authors investigate a modified input layer that results in color invariant networks. The proposed methods are evaluated on two car datasets. It is shown that certain color invariant ""input"" layers can improve accuracy for test-images from a different color distribution than the training images.


The proposed assumptions are not well motivated and seem arbitrary. Why is using a permutation of each pixels' color a good idea?

The paper is very hard to read. The message is unclear and the experiments to prove it are of very limited scope, i.e. one small dataset with the only experiment purportedly showing generalization to red cars.

Some examples of specific issues:
- the abstract is almost incomprehensible and it is not clear what the contributions are
- Some references to Figures are missing the figure number, eg. 3.2 first paragraph, 
- It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters?
- are the training and testing sets all disjoint (sec 4.3)?
- at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg ""tested on nored-test"")
- Sec 4.6: The explanation for why the accuracy drops for all models is not clear. Is it because the total number of training images drops? If that's the case the whole experimental setup seems flawed.
- Sec 4.6: the authors refer to the ""order net"" beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline. In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set. Is that Fig 8 @0%? The baseline seems to be best performing on ""all cars"" and ""non-red cars""

In order to be at an appropriate level for any publication the experiments need to be much more general in scope.
","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Spontaneous Symmetry Breaking in Deep Neural Networks,"['Ricky Fok', 'Aijun An', 'Xiaogang Wang']",Reject,2018,"[2, 25, 14]","[2, 30, 18]","[6, 174, 22]","[1, 117, 7]","[2, 14, 2]","[3, 43, 13]","The paper makes a mathematical analogy between deep neural networks and quantum field theory, and claims that this explains a large number of empirically observed phenomena.

I have a solid grasp of the relevant mathematics, and a superficial understanding of QFT, but I could not really make sense of this paper. The paper uses mathematics in a very loose manner. This is not always bad (an overly formal treatment can make a paper hard to read), but in this case it is not clear to me that the results are even ""correct modulo technicalities"" or have much to do with the reality of what goes on in deep nets.

The first thing I'm confused about is the nature and significance of the symmetries considered in this paper. At a very high level, there are two kinds of symmetries one could consider in DL: transformations of the input space that leave invariant the desired output, and transformations of the weight space that leave invariant the input/output mapping. These are not necessarily related. For instance, a translation or rotation of an image is an example of the former, whereas an arbitrary permutation of hidden units (and corresponding rows/columns of weight matrices) is an example of the latter. This paper is apparently dealing with groups that act on the input as well as the weight space, seemingly conflating the two.

Section 2.2 defines the action of symmetries on the input and weight space. For each layer t, we have a matrix Q_t in G, where G is an unspecified Lie group. Since all Q_t are elements of the same group, they have the same dimension, so all layers must have the same dimension as well. This is somewhat unrealistic. Furthermore, from the definitions in 2.2 it seems that in order to get covariance, the Q_t would have to be the same for all t, which is probably not what the authors had in mind.

For symmetries like rotation/translation of images, a better setup would probably involve a single group with different group actions or linear group representations for each layer. In that case, covariance of the weight layers is not automatic, but only holds for certain subspaces of weight space. For permutation or scale symmetries in weight space, a more sensible setup would be to say that each layer has a different group of symmetries, and the symmetry group of the whole network is the direct product of these groups.

It is stated that transformations in the affine group may not commute with nonlinearities, but rotations of feature maps do. This is correct (at least up to discretization errors), but the paper continues to talk about affine and orthogonal group symmetries. Later on an attempt is made to deal with this issue, by splitting the feature vectors into a part that is put to zero by a ReLU, and a part that is not, and the group is split accordingly. However, this does not make any sense because the pattern of zeros/non-zeros is different for each input, so one cannot speak of a ""remnant symmetry"" for a layer in general.

The connection between DL and QFT described in 2.3 is based on some kind of ""continuous limit"" of units and layers, i.e. having an uncountably infinite number of them. Even setting aside the enormous amount of technical difficulty involved in doing this math properly, I'm a bit skeptical that this has anything to do with real networks.

As an example of how loose the math is, ""theorem 1"" is only stated in natural language: ""Deep feedforward networks learn by breaking symmetries"". The proof involves assuming that the network is a sequence of affine transformations (no nonlinearities). Then it says that if we include a nonlinearity, it breaks the symmetry. Thus, since neural nets use nonlinearities, they break symmetries, and therefore learning works by breaking symmetries and the layers can learn a ""more generalized representation"" than an affine network could. The theorem is so vaguely stated that I don't know what it means, and the proof is inscrutable to me.

Theorem 2 states ""Let x^T x be an invariant under Aff(D)"". Clearly x^T x is not invariant under Aff(D).

The paper claims to explain many empirical facts, but it is not exactly clear which are the conspicuous and fundamental facts that need explaining. For instance, the IB phase transition claimed to happen in deep learning was recently called into question [1]. It appears that this phenomenon does not occur in ReLU nets but only in sigmoid nets, but the current paper purports to explain the phenomenon while assuming ReLUs. I would further note that the paper claims to explain a suspiciously large number of previously observed phenomena (Appendix A), but as far as I can tell does not make novel testable predictions.

The paper makes several strong claims, like ""we [...] illustrate that spontaneous symmetry breaking of affine symmetries is the sufficient and necessary condition for a deep network to attain its unprecedented power"", ""This phenomenon has profound implications"", ""we have solved one of the most puzzling mysteries of deep learning"", etc. In my opinion, unless it is completely obvious that this is indeed a breakthrough, one should refrain from making such statements.

[1] On the information bottleneck theory of deep learning. Anonymous ICLR2018 submission.","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
The Set Autoencoder: Unsupervised Representation Learning for Sets,['Malte Probst'],Reject,2018,[5],[10],[21],[6],[11],[4],"Summary:

This paper proposes an encoder-decoder framework for learning latent representations of sets of elements. The model utilizes the neural attention mechanism for set inputs proposed in (Vinyals et al., ICLR 2016) to encode a set into a fixed-length latent representation, and then employs an LSTM decoder to reconstruct the original set of elements, in which a stable matching algorithm is used to match decoder outputs to input elements. Experimental results on synthetic datasets show that the model learns meaningful representations and effectively handles permutation invariance.

Major Concerns:

1. Although the employed Gale-Shapely algorithm facilitates permutation-invariant set reconstruction, it has O(n^2) computational complexity during each back-propagation iteration, which might prevent it from scaling to sets of fairly big sizes. 

2. The experiments are only evaluated on synthetic datasets, and applications of the set autoencoder to real-world applications or scientific problems will make this work more interesting and significant.

3. The main contribution of this work is the adoption of the stable matching algorithm in the decoder. A strong set autoencoder baseline will be, the encoder employs the neural attention mechanism proposed in (Vinyals et al., ICLR 2016), but the decoder just uses a standard LSTM as in a seq2seq framework. Comparisons to this baseline will reveal the contribution of the stable matching procedure in the whole  framework of  the set autoencoder for learning representations. 

Minor issues:

On page 5, above Section 4, d_j -> o_j ?

the footnote on page 5: we not consider -> we do not consider?

on page 6 and 7,   6.000, 1.000 and 10.000 training examples ->  6000, 1000 and 10,000 training examples","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Now I Remember! Episodic Memory For Reinforcement Learning,"['Ricky Loynd', 'Matthew Hausknecht', 'Lihong Li', 'Li Deng']",Reject,2018,"[19, 10, 'no_match', 28]","[24, 15, 'no_match', 30]","[11, 53, 'no match', 329]","[6, 25, 'no match', 188]","[5, 24, 'no match', 36]","[0, 4, 'no match', 105]","The paper addresses an important problem of how ML systems can learn episodic memory.
Authors, first, criticize the existing approaches and benchmarks for episodic memory, arguing that the latter do not necessarily test episodic memory to the full extent of human-level intelligence.
Then, a new external memory augmented network (MEM) is proposed which is similar in the spirit to content-based retrieval architectures such as DNC and memory networks, but allows to explicitly exclude certain dimensions of memory vectors from matching. 
Authors evaluate the proposed MEM together with DNC and simple LSTM baselines on the game of Concentration where they find MEM to outperform concurrent approaches.

Unfortunately, I did not find enough of novelty, clarity or at least rigorous and interesting experiments in the paper to recommend acceptance.

Detailed comments:
1) When a new architecture is proposed, it is good to describe in detail, at least in the appendix. Currently, it is introduced only implicitly and a reader should infer the details from fig. 2.
2) It looks like the main difference between DNC and MEM is the way of addressing memories that allow explicit masking. If so, then to me this is a rather minor novelty and to justify it's importance authors should run a control experiment with the exact same architecture as in DNC, but with a masked similarity kernel. Besides that, an analysis of that is learned to be masked should be provided, how ""hard"" (i.e. strictly 0 and 1) are the masks, what influences them etc.
3) While the game of concentration clearly requires episodic memory to some extent, this only task is not enough for testing EM approaches, because there is always a risk that one of the evaluated systems somehow overfitted to this task by design. Especially to reason about human-level intelligence we need a variety of tasks.
4) To continue the previous point, humans would not perform well in the proposed task with random card labels, because it is very likely that familiar objects on cards help building associations and remembering them. Thus it is impossible to make a human baseline for this task and decide on how far are we below the human level. ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Disentangled activations in deep networks,"['Mikael Kågebäck', 'Olof Mogren']",Reject,2018,"[5, 11]","[5, 16]","[10, 30]","[5, 16]","[4, 11]","[1, 3]","The authors propose a penalization term that enforces decorrelation between the dimensions of the representation 
They show that it can be included as additional term in cost functions to train generic models.
The idea is simple and it seems to work for the presented examples.

However, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the 
proposed term depending on the parameters of the model (and this depends on the model!). On the other hand, 
given the expression of the proposed regulatization,
it seems to lead to non-convex optimization problems which are hard to solve. Any comment on that?.

Moreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the ""related work"" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis).

Minor points: Fig.1 conveys not that much information.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
A Deep Learning Approach for Survival Clustering without End-of-life Signals,"['S Chandra Mouli', 'Bruno Ribeiro', 'Jennifer Neville']",Reject,2018,"[4, 18, 17]","[9, 23, 22]","[11, 122, 139]","[5, 57, 86]","[6, 51, 34]","[0, 14, 19]","Pros:
The paper is a nice read, clearly written, and its originality is well stated by the authors, “addressing the lifetime clustering problem without end-of-life signals for the first time”. I do not feel experienced enough in the field to evaluate the significance of this work.

The approach proposed in the manuscript is mainly based on a newly-designed nonparametric loss function using the Kuiper statistic and uses a feed-forward neural network to optimize the loss function. This approach does challenge some traditional assumptions, such as the presence of end-of-life signals or the artificial defined timeouts. Instead of giving a clear end-of-life signal, the authors specify a probability of end-of-life that permits us to take into account the associated uncertainty. By analyzing a large-scale social network dataset, it is shown that the proposed method performs better on average than the other two traditional models.

Cons:       
I think that the main drawback of the paper is that the structure of the neural network and the deep learning techniques used for optimizing the loss function are not explained in sufficient detail. ","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[1, 5, 4]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning,"['Aras Dargazany', 'Kunal Mankodiya']",Reject,2018,"[9, 10]","[12, 15]","[10, 70]","[1, 40]","[7, 14]","[2, 16]","The paper falls far short of the standard expected of an ICLR submission. 

The paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like “In this figure, VBP, VBP with FBA, and ITD using FBA for VBP…” are painful to read. 


","[2, 3, 2]","[' Strong rejection', ' Clear rejection', ' Strong rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Combining Model-based and Model-free RL via Multi-step Control Variates,"['Tong Che', 'Yuchen Lu', 'George Tucker', 'Surya Bhupatiraju', 'Shane Gu', 'Sergey Levine', 'Yoshua Bengio']",Reject,2018,"[7, 'no_match', 6, 2, 0, 10, 31]","[12, 'no_match', 10, 4, 1, 15, 36]","[38, 'no match', 75, 10, 1, 743, 975]","[14, 'no match', 34, 5, 1, 326, 405]","[18, 'no match', 39, 5, 0, 396, 454]","[6, 'no match', 2, 0, 0, 21, 116]","This paper presents a model-based approach to variance reduction in policy gradient methods.  The basic idea is to use a multi-step dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.  The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks.

This paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission. But right now (at least as of the version I'm reviewing), the paper reads as being half-finished.  Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of ""embedding"" an ""imaginary trajectory"" remains completely opaque to me.  Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is ""we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates,"" but I see literally no section that hints at anything like this (no mention of ""dynamic unfolding"" or ""latent space trajectory embedding"" ever occurs later in the paper).

In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.  But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick.  But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about.

The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).  I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here.  But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper.

Finally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently.  The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach.  And the experiments are described with very little detail or discussion about the experimental setup.

Nor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix.  Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.  My honest impression is simply that this is still work in progress and that the write up was done rather hastily.  I think it will eventually become a good paper, but it is not ready yet.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION,"['Suraj Nandkishor Kothawade', 'Sumit Baburao Tamgale']",Reject,2018,"['no_match', 'no_match']","['no_match', 'no_match']","['no match', 'no match']","['no match', 'no match']","['no match', 'no match']","['no match', 'no match']","
As one can see by the title, the originality (application of DCNN) and significance (limited to ATM domain) is very limited. If this is still enough for ICLR, the paper could be okay. However, even so one can clearly see that the architecture, the depth, the regularization techniques, and the evaluation are clearly behind the state of the art. Especially for this problem domain, drop-out and data augmentation should be investigated.

Only one dataset is used for the evaluation and it seems to be very limited and small. Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected. Looking into the referece (to get the details of the dataset -  from a workshop of the IEEE International Conference on Computer Vision Workshops (ICCVW) 2017) reveals, that it has only 25 subjects and 10 disguises. This makes it even likely that the same subject with the same disguise appears in the training and test set.

A very bad manner, which unfortunately is often performed by deep learning researchers with limited pattern recognition background, is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is reported. As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set. 

Minor issues:
make sure that the capitalization in the references is correct (ATM should be capital, e.g., by putting {ATM} - and many more things).","[2, 1, 3]","[' Strong rejection', ' Trivial or wrong', ' Clear rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
A Goal-oriented Neural Conversation Model by Self-Play,"['Wei Wei', 'Quoc V. Le', 'Andrew M. Dai', 'Li-Jia Li']",Reject,2018,"[14, 8]","[19, 13]","[299, 74]","[143, 28]","[145, 42]","[11, 4]","Summary: The paper proposes a self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model.

Contributions:

While there are architectural changes (e.g. the customer agent and client agent have different roles and parameters; the parameters of both agents are updated via self-play training), the information isolation claim is not clear. Both the previous work (Lewis et al., 2017) and the proposed approach pitch two agents against each other and the agents communicate via language utterances alone (e.g. rather than exchanging hidden states). In the previous work, the two agents share a set of initial conditions (the set of objects to be divided; this is required by the nature of the task: negotiation), but the goals of each agent are hidden and the negotiation process and outcome are only revealed through natural language. Could you expand on your claim regarding information isolation? Could you design an experiment which highlights the contribution and provide a comparison with the previous approach?

Furthermore, divergence from natural language when optimizing the task reward remains an issue. As a result, both methods require alternate training between the supervised loss and the reinforcement loss.

Experiments:

1. Minor question: During self-play ""we conduct 1 supervised training using the training data every time we make a reinforcement update"". One iteration or one epoch of supervised training?

2. The method is only evaluated on a toy dataset where both the structure of the dialog is limited (see figure 2) and the sentences themselves (the number of language templates is not provided). The referenced negotiation paper uses data collected from mechanical turk ensuring more diversity and the dataset is publicly available. Couldn't your method be applied to that setting for comparison?

3. The qualitative evaluation shows compelling examples from the model. Are the results hand-picked to highlight the two outcomes? I wish more examples and some statistics regarding the diversity of produced dialogs were provided (e.g. how many times to they result in a booked flight vs. unfulfilled request and compare that with the training data).

4. What is the difference between evaluation reward reported in Table 4 and self-play evaluation reward reported in Table 5? (Is the former obtained by conditioning on target utterances?). Is there a reason to not report the itemized rewards in Table 5 as well (Eval flight, Eval action) etc?

5. The use of the value network vs. the policy network is not clarified in the model description nor in the experiments. Is the value network used to reduce the variance in the reward?

Finally, there are several typos or grammatical errors, including:
- Page 4, t and i should be the same.
- Page 4. Use p(u_t |  t_{<t-1}; \theta) instead of p(u_t |  t_{<t-1} | \theta).
- Page 2, second paragraph: ""which correctness"" -> ""whose correctness"".
- Page 2, second-to-last paragraph: ""access to a pieces"" -> ""access to pieces"", ""to the best of it can"" -> ""as good as it can"".
- Page 4. ""feeded"" -> fed
- Page 5, second-to-last paragraph: ""dataset is consists of"" -> ""dataset consists of"".
- Page 7/8: Both examples are labeled ""Sample dialog 1""
- Dataset & experiments: Table 3 and Table 3
- Experiments: ""to see how our model performs qualitative"" -> ""to see how our model performs qualitatively""
- Related work: ""... of studying dialog system is to ..."" -> ""dialog systems""
- Conclusion: ""In those scenario"" -> ""In those scenarios""","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Achieving Strong Regularization for Deep Neural Networks,"['Dae Hoon Park', 'Chiu Man Ho', 'Yi Chang']",Reject,2018,"[12, 1, 16]","[14, 5, 21]","[21, 22, 218]","[11, 9, 139]","[6, 13, 44]","[4, 0, 35]","The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks. Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16. The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.

The proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on.

One limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported. 
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Improving generalization by regularizing in $L^2$ function space,"['Ari S Benjamin', 'Konrad Kording']",Reject,2018,"[1, 19]","[5, 24]","[9, 107]","[2, 29]","[6, 35]","[1, 43]","I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision.
It seems that the authors have made an effort to accommodate reviewers' comments. I upgraded the rating.

-----------------------------------------------------------------------------------------------------------------------

Summary: The paper considers the use of natural gradients for learning. The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training. The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.

The paper is structured as follows:
1. KL divergence is used as a similarity measure between two distributions.
2. Regularizing the objective with KL div. seems promising, but expensive.
3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix.
4. However, computing and inverting the Fisher information matrix is computationally expensive.
5. One solution is to approximate the solution F^{-1} J using gradient descent. However, still we need to calculate F. There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').
6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a ""good"" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective. 
7. No large matrices need to be formed or inverted, however more passes needed per outer step.

Importance:
Somewhat lack of originality and poor experiments lead to low importance.

Clarity:
The paper needs major revision w.r.t. presenting and highlighting the new main points. E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS).

Originality/Novelty:
The paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks. Beyond this, the paper does not provide any futher original idea. So, slight to no novelty.

Main comments:
1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help? This would require the combination of two Hessian matrices.

2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results. 

Overall:
Rejection.
","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,"['Runyao Chen', 'Kun Wu', 'Ping Luo']",Reject,2018,"['no_match', 20, 'no_match']","['no_match', 25, 'no_match']","['no match', 106, 'no match']","['no match', 53, 'no match']","['no match', 14, 'no match']","['no match', 39, 'no match']","The paper proposes a generalization of an algorithm by Yin et al. (2017), which performs SGD with adaptive batch sizes. The present paper generalizes the algorithm to SGD with momentum. Since the original algorithm was already formulated with a general utility function, the proposed algorithm is similar in structure but replaces the utility function so that it takes momentum into account. Experiments on an image classification task show improvements in the training loss. However, no test accuracies are reported and the learning curves have suspicious artifacts, see below. Experiments on a relation extraction task show little improvement over SGD with momentum and constant batch size.


COMMENTS:

The paper discusses a relevant issue. While adaptive learning algorithms are popular in deep learning, most algorithms adapt the learning rate or the momentum coefficient, but not the batch size. It appears to me that the main idea and the overall structure of the proposed algorithm is the same as in the one published by Yin et al. (2017), and that only few changes were necessary to include momentum. Given the incremental process, I find the presentation unnecessarily involved, and experiments not convincing enough.

Concerning the presentation, the paper dedicates two full pages on a review of the algorithm by Yin et al. (2017). The first page of this review states that, for large enough batch sizes, the change of the objective function in SGD is normal distributed with a variance that is inversely proportional the batch size. It seems to me that this is a direct consequence of the central limit theorem. The derivation, however, is quite technical and introduces some quantities that are never used (e.g., $\vec{\xi}_j$ is never used individually, only the combined term $\epsilon_t$ defined below Eq. 12 is). The second page of the review seems to discuss the main part of the algorithm, but I could not follow it. First, a ""state"" $s_t$ (also written as $S$) is introduced, which, according to the text, is ""the objective value"", which was earlier denoted by $F$. Nevertheless, the change of $s_t$, Eq. 5, appears to obey a different probability distribution than the change of $F$. The paper provides a verbal explanation for this discrepancy, saying that it is possible that $S$ is first reduced to the minimum $S^*$ of the objective and then increased again. However, in my understanding, the minimum of the objective is only realized at a singular point in parameter space. Crossing this point in an update step should have zero probability as long as the model has more than one parameter. The explanation also does not make it clear why the argument should apply to $S$ (or $s$) but not to $F$.

Page 5 provides pseudocode for the proposed algorithm. However, I couldn't find an explanation of the code. The code suggests that, for each update step, one gradually increases the batch size until it becomes larger or equal than a running estimate of the optimal batch size. While this may be a plausible strategy in practice, it seems to have a bias that is not addressed in the paper: the algorithm recalculates a noisy estimate of the optimal batch size after each increase of the batch size, and it terminates as soon as the noisy estimate happens to be small enough, resulting in a bias towards a smaller than optimal batch size. A probably more important issue is that the algorithm is sequential and hard to parallelize, where parallelization is usually the main motivation to use larger batch sizes. As the gradient noise scales inversely proportional to the batch size, I don't see why increasing the batch size should be preferred over decreasing the learning rate unless optimizations with a larger batch size can be parallelized. The experiments don't compare the two alternatives.

Concerning the experiments, it seems peculiar that the learning curves in Figure 1 remain at a constant value for a long time at the beginning of the optimization before they begin to drop. Do the authors understand this behavior? It could indicate that the magnitude of the random initialization was chosen too small. I.e., the parameters might have been initialized too close to zero, where the loss is stationary due to symmetries. Also, absolute values of the training loss can be deceptive since there is often no natural scale. A better indicator of convergence would be the test accuracy. The identification of the ""batch size boom"" is interesting.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Multi-task Learning on MNIST Image Datasets,"['Po-Chen Hsieh', 'Chia-Ping Chen']",Reject,2018,"[2, 17]","[1, 22]","[1, 96]","[1, 79]","[0, 8]","[0, 9]","This paper presents a multi-task neural network for classification on MNIST-like datasets.

The main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited.

The experiments do not bring too much insights.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
A Bayesian Nonparametric Topic Model with Variational Auto-Encoders,"['Xuefei Ning', 'Yin Zheng', 'Zhuxi Jiang', 'Yu Wang', 'Huazhong Yang', 'Junzhou Huang']",Reject,2018,"[2, 7, 3, 20, 13]","[7, 11, 6, 25, 18]","[46, 49, 7, 546, 392]","[20, 17, 1, 331, 213]","[20, 20, 3, 57, 113]","[6, 12, 3, 158, 66]","""topic modeling of text documents one of most important tasks""
Does this claim have any backing?

""inference of HDP is more complicated and not easy to be applied to new models""  Really an artifact of the misguided nature of earlier work. The posterior for the $\vec\pi$ of a elements of DP or HDP can be made a Dirichlet, made finite by keeping a ""remainder"" term and appropriate augmentation.  Hughes, Kim and Sudderth (2015) have avoided stick-breaking and CRPs altogether, as have others in earlier work. Extensive models building on simple HDP doing all sorts of things have been developed.

Variational stick-breaking methods never seemed to have worked well.  I suspect you could achieve better results by replacing them as well, but you would have to replace the tree of betas and extend your Kumaraswamy distribution, so it may not work.  Anyway, perhaps an avenue for future work.

""infinite topic models"" I've always taken the view that the use of the word ""infinite"" in machine learning is a kind of NIPSian machismo. In HDP-LDA at least, the major benefit in model performance comes from fitting what you call $\vec\pi$, which is uniform in vanilla LDA, and note that the number of topics ""found"" by a HDP-LDA sampler can be made to vary quite widely by varying what you call $\alpha$, so any statement about the ""right"" number of topics is questionable.  So the claim in 3rd paragraph of Section 2, ""superior"" and ""self-determined topic number"" I'd say are misguided.  Plenty of experimental work to support this.

In Related Work, you seem to only mention HDP for non-parametric topic models.  More work exists, for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and don't rely on the memory hungry HCRP.

Good to see a prior is placed on the concentration parameter.  Very important and not well done in the community, usually.  
ADDED:  Originally done by Teh et al for HDP-LDA, and subsequently done
by several, including Kim et al 2016.   Others stress the importance of this.  You need to
cite at least Teh et al. in 5.4 to show this isn't new and the importance is well known.

The Prod version is a very nice idea.  Great results.  This looks original, but I'm not expert enough in the huge masses of new deep neural network research popping up.

You've upped the standard a bit by doing good experimental work.  Oftentimes this is done poorly and one is left wondering.  A lot of effort went into this.
ADDED:   usually like to see more data sets experimented with

What code is used for HDP-LDA?  Teh's original Matlab HCRP sampler does pretty well because at least he samples hyperparameters and can scale to 100k documents (yes, I tried). The comparison with LDA makes me suspicious. For instance, on 20News, a good non-parametric LDA will find well over 400 topics and roundly beat LDA on just 50 or 200.  If reporting LDA, or HDP-LDA, it should be standard to do hyperparameter fitting and you need to mention what you did as this makes a big difference.
ADDED:   20News results still poor for HPD, but its probably the implementation used ... their
        online variational algorithm only has advantages for large data sets 

Pros:   
* interesting new prod model with good results
* alternative ""deep"" approach to a HDL-LDA model
* good(-ish) experimental work
Cons:
* could do with a competitive non-parametric LDA implementation

ADDED:   good review responses generally
","[7, 3, 5]","[' Good paper, accept', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Network of Graph Convolutional Networks Trained on Random Walks,"['Sami Abu-El-Haija', 'Amol Kapoor', 'Bryan Perozzi', 'Joonseok Lee']",Reject,2018,"[6, 1, 8, 12]","[11, 4, 13, 17]","[36, 13, 82, 68]","[15, 5, 35, 38]","[19, 8, 43, 24]","[2, 0, 4, 6]","The paper presents a Network of Graph Convolutional Networks (NGCNs) that uses
random walk statistics to extract information from near and distant neighbors
in the graph.

The authors show that a 2-layer Graph Convolutional Network, with linear
activation and W0 as identity matrix, reduces to a one-step random walk.
They build on this notion to  introduce the idea to make the GCN directly operate
on random walk statistics to better model information across distant nodes.

Given that it is not clear how many steps of random walk to use a-priori it is
proposed to make a mixture of models whose outputs are combined by a
softmax classifier, or by an attention based mixing (learning the mixing coefficients).

I find that the comparison can be considered slightly unfair as NGCN has k-times
the number of GCN models in it. Did the authors compare with a deeper GCN, or
simply with a mixture of plain GCN using one-step random walk?
The datasets used for comparison are extremely simple, and I am glad that the
authors point out that this is a significant issue for benchmark driven research.
However, doing calibration on a subset of the validation nodes via gradient
descent is not very clean as by doing it one implicitly uses those nodes for training.
The improvement of the calibrated model on 5 nodes per class (Table 3) seems
to hint that this peeking into the validation is indeed happening.

The authors mention that feeding explicitly the information on distant nodes
makes learning easier and that otherwise such information it would be hard to
extract from stacking several GCN layers. While this is true for the small datasets
usually considered it is not clear at all whether this still holds when we will
have large scale graph benchmarks.

Experiments are well conducted but lack a comparison with GraphSAGE and MoNet,
which are the reference models for the selected benchmarks. A comparison would have made the contribution stronger in my opinion. Improvements in performance are minor
except for decimated inputs setting reported in Table 3. In this last case though
no statistics over multiple runs are shown.

Overall I like the interpretation, even if a bit forced, of GCN as using one-step
random walk statistics. The paper is clearly written.
The main issue I have with the approach is that it does not bring a very novel
way to perform deep learning on graphs, but rather improves marginally upon
a well established one.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 2, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Training Autoencoders by Alternating Minimization,"['Sneha Kudugunta', 'Adepu Shankar', 'Surya Chavali', 'Vineeth Balasubramanian', 'Purushottam Kar']",Reject,2018,"[1, 'no_match', 13, 10]","[6, 'no_match', 18, 15]","[17, 'no match', 200, 69]","[5, 'no match', 110, 32]","[9, 'no match', 78, 32]","[3, 'no match', 12, 5]","In this paper an alternating optimization approach is explored for training Auto Encoders (AEs).
The authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase.
Then they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD. The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs.


Some comments on the theoretical part:
-The theoretical part is partly misleading. While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer.
Regarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply.
The authors should mention this point.

-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting. 

-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives. Concretely: Is there any known theory for such objectives? What guarantees can we hope to achieve?


The extension to muti-layer AEs makes sense and seems to works quite well in practice.

The experimental part is satisfactory, and seems to be done in a decent manner. 
It will be useful if the authors could relate to the issue of parameter tuning for their algorithm.
Concretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training,"['Dalei Wu', 'Xiaohua Liu']",Reject,2018,"[16, 18]","[21, 23]","[111, 113]","[57, 67]","[3, 6]","[51, 40]","* Summary *
The paper addresses the instability of GAN training. More precisely, the authors aim at improving the stability of the semi-supervised version of GANs presented in [1] (IGAN for short) . The paper presents a novel architecture for training adversarial networks in a semi-supervised settings (Algorithm 1). It further presents two theoretical results --- one (Theorem 2.1) showing that the generator's gradient vanish for IGAN, and the second (Theorem 3.1) showing that the proposed algorithm does not suffer this behaviour. Finally, experiments are provided (for MNIST and CIFAR10), which are meant to support empirically the claimed improved stability of the proposed method compared to the previous GAN implementations (including IGAN).

I need to say the paper is poorly written and not properly polished. Among many other things:

(1) It refers to non-existent results in other papers. Eq 2 is said to follow [1], meanwhile the objectives are totally different: the current paper seems to use the l2 loss, while Salimans et al. use the cross-entropy;

(2) Does not introduce notations in statements of theorems ($J_\theta$ in Theorem 2.1?) and provides unreadable proofs in appendix (proof of Theorem 2.1 is a sequence of inequalities involving the undefined notations with no explanations). In short, it is very hard to asses whether the proposed theoretical results are valid;

(3) Does not motivate, discuss, or comment the architecture of the proposed method at all (see Section 3).

Finally, in the experimental section it is unclear how exactly the authors measure the stability of training. The authors write ""unexpectedly high error rates and poor generate image quality"" (page 5), however, these things sounds very subjective and the authors never introduce a concrete metric. The authors only report ""0 fails"", ""one or two out of 10 runs fail"" etc. Moreover, for CIFAR10 it seems the authors make conclusions based only on 3 independent runs (page 6).

[1] Salimans et al, Improved Techniques for Training GANs, 2016","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias,['Takuro Kutsuna'],Reject,2018,[9],[14],[20],[12],[4],[4],"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.  Test accuracy is not improved, however.

Overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.

On the theoretical side, the linearly constrained weights are only shown to work for a very special case.  There can be many other approaches to mitigate the impact of angle bias.  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?  When the mean of input is zero, there is no angle bias in the first layer.  Also, what about if we include the bias term so that b + w a is the preactivation value?

On the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.

Minor comments:

In Section 2.2, is Layer 1 the input layer or the next?","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unsupervised Deep Structure Learning by Recursive Dependency Analysis,"['Raanan Y. Yehezkel Rohekar', 'Guy Koren', 'Shami Nisimov', 'Gal Novik']",Reject,2018,"[14, 1, 1, 1]","[18, 5, 5, 5]","[15, 5, 11, 18]","[6, 2, 4, 6]","[7, 3, 7, 12]","[2, 0, 0, 0]","The paper proposes an unsupervised structure learning method for deep neural networks. It first constructs a fully visible DAG by learning from data, and decomposes variables into autonomous sets. Then latent variables are introduced and stochastic inverse is generated. Later a deep neural network structure is constructed based on the discriminative graph. Both the problem considered in the paper and the proposed method look interesting. The resulting structure seems nice.

However, the reviewer indeed finds a major technical flaw in the paper. The foundation of the proposed method is on preserving the conditional dependencies in graph G. And each step mentioned in the paper, as it claims, can preserve all the conditional dependencies. However, in section 2.2, it seems that the stochastic inverse cannot. In Fig. 3(b), A and B are no longer dependent conditioned on {C,D,E} due to the v-structure induced in node H_A and H_B. Also in Fig. 3(c), if the reviewer understands correctly, the bidirectional edge between H_A and H_B is equivalent to H_A <- h -> H_B, which also induces a v-structure, blocking the dependency between A and B. Therefore, the very foundation of the proposed method is shattered. And the reviewer requests an explicit explanation of this issue.

Besides that, the reviewer also finds unfair comparisons in the experiments.

1. In section 5.1, although the authors show that the learned structure achieves 99.04%-99.07% compared with 98.4%-98.75% for fully connected layers, the comparisons are made by keeping the number of parameters similar in both cases. The comparisons are reasonable but not very convincing. Observing that the learned structures would be much sparser than the fully connected ones, it means that the number of neurons in the fully connected network is significantly smaller. Did the authors compare with fully connected network with similar number of neurons? In such case, which one is better? (Having fewer parameters is a plus, but in terms of accuracy the number of neurons really matters for fair comparison. In practice, we definitely would not use that small number of neurons in fully connected layers.)

2. In section 5.2, it is interesting to observe that using features from conv10 is better than that from last dense layer. But it is not a fair comparison with vanilla network. In vanilla VGG-16-D, there are 3 more conv layers and 3 more fully connected layers. If you find that taking features from conv10 is good for the learned structure, then maybe it will also be good by taking features from conv10 and then apply 2-3 fully-connected layers directly (The proposed structure learning is not comparable to convolutional layers, and what it should really compare to is fully-connected layers.) In such case, which one is better? 
Secondly, VGG-16 is a large network designed for ImageNet data. For small dataset such as CIFAR10 and CIFAR100, it is really overkilled. That's maybe the reason why taking the output of shallow layers could achieve pretty good results.

3. In Fig. 6, again, comparing the learned structure with fully-connected network by keeping parameters to be similar and resulting in large difference of the number of neurons is unfair from my point of view.

Furthermore, all the comparisons are made with respect to fully-connected network or vanilla CNNs. No other structure learning methods are compared with. Reasonable baseline methods should be included.

In conclusion, due to the above issues both in method and experiments, the reviewer thinks that this paper is not ready for publication.
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Revisiting Bayes by Backprop,"['Meire Fortunato', 'Charles Blundell', 'Oriol Vinyals']",Reject,2018,"[4, 9, 12]","[8, 14, 17]","[14, 88, 209]","[5, 35, 101]","[8, 48, 98]","[1, 5, 10]","*Summary*

The paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs). The paper first considers the ""Bayes by Backprop"" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model. Several experiments demonstrate the quality of the prediction and the uncertainty over dropout.  

*Originality + significance*

To my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs. However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. 

Nevertheless, the parameterisation of the conditional variational distribution q(\theta | \phi, (x, y)) using recognition model is interesting and could be useful in other models. However, this has not been tested or concretely shown in this paper. The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. 

*Clarity*

The paper is, in general, well-written. However, the presentation in 4 is hard to follow. I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \phi, a variational approximation over both \theta and \phi is needed, and a q that couples \theta, \phi and and the gradient of the log likelihood term wrt \phi is chosen. 

Additional comments:

Why is the variational approximation called ""sharpened""?

At test time, normal VI just uses the fixed q(\theta) after training. It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\theta | \phi, x) in eqs. 19-20 parameterised? The first paragraph of page 5 uses q(\theta | \phi, (x, y)), but y is not known at test time.

What is C in eq. 9?

This comment ""variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty..."" is not precise. EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs. In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational. On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the ""Two problems with variational EM... "" paper by Turner and Sahani (2010).

There are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy? Was there any KL reweighting scheduling as done in the original BBB paper? 

What is the significance of the difference between BBB and BBB with sharpening in the language modelling task? Was sharpening used in the image caption generation task?

What is the computational complexity of BBB with posterior sharpening? Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer? Would be interesting to see the time/accuracy frontier.","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Continuous-fidelity Bayesian Optimization with Knowledge Gradient,"['Jian Wu', 'Peter I. Frazier']",Reject,2018,"[33, 12]","[38, 17]","[374, 110]","[181, 48]","[36, 37]","[157, 25]","This paper studies hyperparameter-optimization by Bayesian optimization, using the Knowledge Gradient framework and allowing the Bayesian optimizer to tune fideltiy against cost.

There’s nothing majorly wrong with this paper, but there’s also not much that is exciting about it. As the authors point out very clearly in Table 1, this setting has been addressed by several previous groups of authors. This paper does tick a previously unoccupied box in the problem-type-vs-algorithm matrix, but all the necessary steps are relatively straightforward.

The empirical results look good in comparison to the competing methods, but I suspsect an author of those competitors could find a way to make their own method look better in those plots, too.

In short: This is a neat paper, but it’s novelty is low. I don't think it would be a problem if this paper were accepted, but there are probably other, more groundbreaking papers in the batch.

Minor question: Why are there no results for 8-cfKG and Hyperband in Figure 2 for SVHN?","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Preliminary theoretical troubleshooting in Variational Autoencoder,"['Shiqi Liu', 'Qian Zhao', 'Xiangyong Cao', 'Deyu Meng', 'Zilu Ma', 'Tao Yu']",Reject,2018,"[2, 7, 4, 14, 2, 'no_match']","[5, 12, 9, 19, 6, 'no_match']","[8, 101, 46, 340, 10, 'no match']","[0, 24, 5, 88, 0, 'no match']","[5, 38, 15, 111, 2, 'no match']","[3, 39, 26, 141, 8, 'no match']","This paper attempts to improve the beta-VAE (Higgins et al, 2017) by removing the trade-off between the quality of disentanglement in the latent representation and the quality of the reconstruction. The authors suggest doing so by explicitly modelling the noise of the reconstructed image Gaussian p(x|z). The authors assume that VAEs typically model the data using a Guassian distribution with a fixed noise. This, however, is not the case. Since the authors are trying to address a problem that does not actually exist, I am not sure what the contributions of the paper are. 

Apart from the major issue outlined above, the paper also makes other errors. For example, it suggests using D_KL(q(z)||p(z)) as a measure of disentanglement, with lower values being indicative of better disentanglement. This, however, is incorrect, since one can have tiny D_KL by encoding all the information into a single latent z_i. Such a representation would be highly entangled while still satisfying all of the conditions the authors propose for a disentangled representation. 

Given the points outlined above and the fact that the paper is hard to read and is excessively long, I do not believe it should be accepted.","[2, 5, 3]","[' Strong rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning Generative Models with Locally Disentangled Latent Factors,"['Brady Neal', 'Alex Lamb', 'Sherjil Ozair', 'Devon Hjelm', 'Aaron Courville', 'Yoshua Bengio', 'Ioannis Mitliagkas']",Reject,2018,"[1, 7, 5, 5, 18, 31, 9]","[5, 12, 9, 9, 23, 36, 14]","[7, 64, 27, 71, 309, 975, 78]","[2, 22, 13, 31, 135, 405, 30]","[5, 38, 13, 37, 160, 454, 46]","[0, 4, 1, 3, 14, 116, 2]","The paper investigates the potential of hierarchical latent variable models for generating images and image sequences. The paper relies on the ALI model from [Dumoulin et al, ICLR'16] as the main building block. The main innovation in the paper is to propose to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The proposed hierarchical model is trained in stages. First stage is an original ALI model as in [Dumoulin et al]. Each subsequent stage is constructed by using the Z variables from the previous stage as the target data to be generated.

The paper constructs models for generatation of images and image sequences. The model for images is a 2-level ALI. The first level is similar to PatchGAN from [1] but is trained as an ALI model. The second layer is another ALI that is trained to generate latent variables from the first layer. 

[1] Isola et al. Image-to-Image Translation with Conditional Adversarial Networks, CVPR'17 

In the the model for image sequences the hierarchy is somewhat different. The top layer is directly generating images and not patches as in the image-generating model.

Summary: I think this paper presents a direct and somewhat straightforward extension of ALI. Therefore the novelty is limited. I think the paper would be stronger if it (1) demonstrated improvements when compared to ALI and (2) showed advantages of hierarchical training on other datasets, not just the somewhat simple datasets like CIFAR and Pacman. 

Other comments / questions: 

- baseline should probably be 1-level ALI from [Dumoulin et al.]. I believe in the moment the baseline is a standard GAN.

- I think the paper would be stronger if it directly reproduced the experiments from [Dumoulin et al.] and showed how hierarchy compares to standard ALI without hierarchy. 

- the reference Isola et al. [1] should ideally be cited since the model for image genration is similar to PatchGAN in [1]

- Why is the video model in this paper not directly extending the image model? Is it due to limitation of the implementation or direclty extending the iamge model didn't work? 
","[6, 3, 4]","[' Marginally above acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Hybed: Hyperbolic Neural Graph Embedding,"['Benjamin Paul Chamberlain', 'James R Clough', 'Marc Peter Deisenroth']",Reject,2018,"[3, 6, 13]","[7, 11, 18]","[46, 47, 131]","[19, 20, 55]","[26, 20, 57]","[1, 7, 19]","== Preamble ==

As promised, I have read the updated paper from scratch and this is my revised review. My original review is kept below for reference. My original review had rating ""4: Ok but not good enough - rejection"".

== Updated review ==

The revised improves upon the original submission in several ways and, in particular, does a much better job at positioning itself within the existing body of literature. The new experiments also indicate that the proposed model offer some improvement over Nickel & Douwe, NIPS 2017).

I do have remaining concerns that unfortunately still prevent me from recommending acceptance:

- Throughout the paper it is argued that we should embed into a hyperbolic space. Such a space is characterized by its metric, but the proposed model do not use a hyperbolic metric. Rather it relies on a heuristic similarity measure that is inspired by the hyperbolic metric. I understand that this may be a practical choice, but then I find it misleading that the paper repeatedly states that points are embedded into a hyperbolic space (which is incorrect). This concern was also raised on this forum prior to the revision.

- The resulting optimization is one of the key selling points of the proposed method as it is unconstrained while Nickel & Douwe resort to a constrained optimization. Clearly unconstrained optimization is to be preferred. However, it is not entirely correct (from what I understand), that the resulting optimization is indeed unconstrained. Nickel & Douwe work under the constraint that |x| < 1, while the proposed model use polar coordinates (r, theta): r in (0, infinity) and theta in (0, 2 pi]. Note that theta parametrize a circle, and therefore wrapping may occur (this should really be mentioned in the paper). The constraints on theta are quite easy to cope with, so I agree with the authors that they have a more simple optimization problem. However, this is only true since points are embedded on the unit disk (2D). Should you want to embed into higher dimensional spaces, then theta need to be confined to live on the unit sphere, i.e. |theta| = 1 (the current setting is just a special-case of the unit sphere). While optimizing over the unit sphere is manageable it is most definitely a constrained optimization problem, and it is far from clear that it is much easier than working under the Nickel & Douwe constraint, |x| < 1.

Other comments:
- The sentence ""even infinite trees have nearly isometric embeddings in hyperbolic space (Gromov, 2007)"" sounds cool (I mean, we all want to cite Gromov), but what does it really mean? An isometric embedding is merely one that preserves a metric, so this statement only makes sense if the space of infinite trees had a single meaningful metric in the first place (it doesn't; that's a design choice).

- In the ""Contribution"" and ""Conclusion"" sections it is claimed that the paper ""introduce the new concept of neural embeddings in hyperbolic space"". I thought that was what Nickel & Douwe did... I understand that the authors are frustrated by this parallel work, but at this stage, I don't think the present paper can make this ""introducing"" claim.

- The caption in Figure 2 miss some indication that ""a"" and ""b"" refer to subfigures. I recommend ""a"" --> ""a)"" and ""b"" --> ""b)"".

- On page 4 it is mentioned that under the heuristic similarity measure some properties of hyperbolic spaces are lost while other are retained. From what I can read, it is only claimed that key properties are kept; a more formal argument (even if trivial) would have been helpful.


== Original Review ==

The paper considers embeddings of graph-structured data onto the hyperbolic Poincare ball. Focus is on word2vec style models but with hyperbolic embeddings. I am unable to determine how suitable an embedding space the Poincare ball really is, since I am not familiar enough with the type of data studied in the paper. I have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty:
The paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space. From what I can see, the paper

  Poincaré Embeddings for Learning Hierarchical Representations
  https://arxiv.org/abs/1705.08039

consider an almost identical model to the one proposed here with an almost identical motivation and application set. Some technicalities appear different, but (to me) it seems like the main claimed novelties of the present paper has already been out for a while. If this analysis is incorrect, then I encourage the authors to provide very explicit arguments for this in the rebuttal phase.

Other comments:
*) It seems to me that, by construction, most data will be pushed towards the boundary of the Poincare ball during the embedding. Is that a property you want?
*) I found it rather surprising that the log-likelihood under consideration was pushed to an appendix of the paper, while its various derivatives are part of the main text. Given the not-so-tight page limits of ICLR, I'd recommend to provide the log-likelihood as part of the main text (it's rather difficult to evaluate the correctness of a derivative when its base function is not stated).
*) In the introduction must energy is used on the importance of large data sets, but it appears that only fairly small-scale experiments are considered. I'd recommend a better synchronization.
*) I find visual comparisons difficult on the Poincare ball as I am so trained at assuming Euclidean distances when making visual comparisons (I suspect most readers are as well). I think one needs to be very careful when making visual comparisons under non-trivial metrics.
*) In the final experiment, a logistic regressor is fitted post hoc to the embedded points. Why not directly optimize a hyperbolic classifier?

Pros:
+ well-written and (fairly) well-motivated.

Cons:
- It appears that novelty is very limited as highly similar work (see above) has been out for a while.

","[4, 4, 5, 7]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Model-based imitation learning from state trajectories,"['Subhajit Chaudhury', 'Daiki Kimura', 'Tadanobu Inoue', 'Ryuki Tachibana']",Reject,2018,"[5, 6, 24, 18]","[10, 10, 27, 21]","[46, 34, 20, 60]","[19, 20, 11, 37]","[22, 14, 7, 19]","[5, 0, 2, 4]","The paper presents a model-based imitation learning framework which learns the state transition distribution of the expert. A model-based policy is learned that should matches the expert transition dynamics. The approach can be used for imitation learning when the actions of the expert are not observed, but only the state transitions (which is an important special case).  

Pros:
- The paper concentrates on an interesting special case of imitation learning

Cons:
- The paper is written very confusingly and hard to understand. The algorithm needs to be better motivated and explained and the paper needs proof reading.
- The algorithm is based on many heuristics that are not well motivated. 
- The algorithm is only optimizing the one step error function for imitation learning but not the long term behavior. It heavily relies on the learned transition dynamics of the expert p(s_t+1|s_t). This transition model will be wrong if we go away from the expert's trajectories. Hence, I do not see why we should use p(s_t+1|s_t) to define the reward function. It does not prevent the single step 
errors of the policy to accumulate (which is the main goal of inverse reinforcement learning)
- The results are not convincing
- Other algorithms (such as GAIL) could be used in the same setup (no action observations). Comparisons to other imitation learning approaches are needed.

In summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.


More detailed comments
- It is unclear why a model-based and model-free policy need to be used. Is the model-based policy used at any time in the algorithm? If it is just used as final result, why train it iteratively? Why can we not just also use the model-based policy for data collection?
- It is unclear why the heuristic reward function makes sense. First of all, the defined reward is stochastic as \hat{s}_t+1 is a sample from the next state from the expert's transition model. Why do not we use the mean of the transition model here, then it would not be stochastic any more. Second, a much simpler reward could be used that essentially does the same thing. Instead of requiring a learned dynamics model f_E for predicting the next state, we can just use the experienced next state s_t+1. Note that the reward function for time step t can depend on s_t+1 in an MDP.  
- The objective that is optimized (Eq. 4) is not well defined. A function is not an objective function if we can only optimize part of it for theta while keeping theta fixed for the other part. It is unclear which objective the real algorithm optimizes
- There are quite a few confusions in terms of notation. Sometimes, a stochastic transition model p(s_t+1|s_t, a_t) is used and sometimes a deterministic model f_E(s,a). It is unclear how they relate. 
- Many other imitation learning techniques could be used in this setup including max-entropy inverse RL [1], IRL by distribution matching [2] and the approach given in [3] and GAIL. A comparison to at least a subset of these methods is needed

[1] B. Ziebart et al, Maximum Entropy Inverse Reinforcement Learning, AAAI 2008
[2] Arenz, O.; Abdulsamad, H.; Neumann, G. (2016). Optimal Control and Inverse Optimal Control by Distribution Matching, Proceedings of the International Conference on Intelligent Robots and Systems (IROS)
[3] P Englert, A Paraschos, J Peters, MP Deisenroth, Model-based Imitation Learning by Probabilistic Trajectory Matching, IEEE International Conference on Robotics and Automation","[3, 4, 7]","[' Clear rejection', ' Ok but not good enough - rejection', ' Good paper, accept']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Training Neural Machines with Partial Traces,"['Matthew Mirman', 'Dimitar Dimitrov', 'Pavle Djordjevich', 'Timon Gehr', 'Martin Vechev']",Reject,2018,"[1, 'no_match', 'no_match', 4, 16]","[5, 'no_match', 'no_match', 8, 21]","[12, 'no match', 'no match', 22, 204]","[6, 'no match', 'no match', 19, 137]","[5, 'no match', 'no match', 2, 51]","[1, 'no match', 'no match', 1, 16]","Summary

This paper presents differentiable Neural Computational Machines (∂NCM), an abstraction of existing neural abstract machines such as Neural Turing Machines (NTMs) and Neural Random Access Machines (NRAMs). Using this abstraction, the paper proposes loss terms for incorporating supervision on execution traces. Adding supervision on execution traces in ∂NCM improves performance over NTM and NRAM which are trained end-to-end from input/output examples only. The observation that adding additional forms of supervision through execution traces improves generalization may be unsurprising, but from what I understand the main contribution of this paper lies in the abstraction of existing neural abstract machines to ∂NCM. However, this abstraction does not seem to be particularly useful for defining additional losses based on trace information. Despite the generic subtrace loss (Eq 8), there is no shared interface between ∂NCM versions of NTM and NRAM that would allow one to reuse the same subtrace loss in both cases. The different subtrace losses used for NTM and NRAM (Eq 9-11) require detailed knowledge of the underlying components of NTM and NRAM (write vector, tape, register etc.), which questions the value of ∂NCM as an abstraction.

Weaknesses

As explained in the summary, it is not clear to me why the abstraction to NCM is useful if one still needs to define specific subtrace losses for different neural abstract machines.
The approach seems to be very susceptible to the weight of the subtrace loss λ, at least when training NTMs. In my understanding each of the trace supervision information (hints, e.g. the ones listed in Appendix F) provides a sensible inductive bias we would the NTM to incorporate. Are there instances where these biases are noisy, and if not, could we incorporate all of them at the same time despite the susceptibility w.r.t λ?
NTMs and other recent neural abstract machines are often tested on rather toyish algorithmic tasks. I have the impression providing extra supervision in form of execution traces makes these tasks even more toyish. For instance, when providing input-output examples as well as the auxiliary loss in Eq6, what exactly is left to learn? What I like about Neural-Programmer Interpreters and Neural Programmer [1] is that they are tested on less toyish tasks (a computer vision and a question answering task respectively), and I believe the presented method would be more convincing for a more realistic downstream task where hints are noisy (as mentioned on page 5).

Minor Comments

p1: Why is Grefenstette et al. (2015) an extension of NTMs or NRAMs? While they took inspiration from NTMs, their Neural Stack has not much resemblance with this architecture.
p2: What is B exactly? It would be good to give a concrete example at this point. I have the feeling it might even be better to explain NCMs in terms of the communication between κ, π and M first, so starting with what I, O, C, B, Q are before explaining what κ and π are (this is done well for NTM as ∂NCM in the table on page 4). In addition, I think it might be better to explain the Controller before the Processor. Furthermore, Figure 2a should be referenced in the text here.
p4 Eq3: There are two things confusing in these equations. First, w is used as the write vector here, whereas on page 3 this is a weight of the neural network. Secondly, π and κ are defined on page 2 as having an element from W as first argument, which are suddenly omitted on page 4.
p4: The table for NRAM as ∂NCM needs a bit more explanation. Where does {1}=I come from? This is not obvious from Appendix B either.
p3 Fig2/p4 Eq4: Related to the concern regarding the usefulness of the ∂NCM abstraction: While I see how NTMs fit into the NCM abstraction, this is not obvious at all for NRAMs, particularly since in Fig 2c modules are introduced that do not follow the color scheme of κ and π in Fig 2a (ct, at, bt and the registers).
p5: There is related work for incorporating trace supervision into a neural abstract machine that is otherwise trained end-to-end from input-output examples [2].
p5: ""loss on example of difficulties"" -> ""loss on examples of the same difficulty""
p5: Do you have an example for a task and hints from a noisy source?
Citation style: sometimes citation should be in brackets, for example ""(Graves et al. 2016)"" instead of ""Graves et al. (2016)"" in the first paragraph of the introduction.

[1] Neelakantan et al. Neural programmer: Inducing latent programs with gradient descent. ICLR. 2015. 
[2] Bosnjak et al. Programming with a Differentiable Forth Interpreter. ICML. 2017.","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Using Deep Reinforcement Learning to Generate Rationales for Molecules,"['Benson Chen', 'Connor Coley', 'Regina Barzilay', 'Tommi Jaakkola']",Reject,2018,"[0, 2, 21, 26]","[5, 7, 26, 31]","[6, 67, 247, 294]","[0, 12, 130, 149]","[5, 34, 89, 112]","[1, 21, 28, 33]","In this manuscript, the authors propose an interesting deep reinforcement learning approach via CNNs to learn the rationales associated to target chemical properties. The paper has merit, but in its current form does not match the acceptance criteria for ICLR.

In particular, the main issue lies in the poor performance reached by the systems, both overall and in comparison with baseline methods, which at the moment hardly justifies the effort required in setting up the DL framework. Moreover, the fact that test performances are sometimes (much) better than training results are quite suspicious in methodological terms.
Finally, the experimental part is quite limited (two small datasets), making it hard to evaluate the scalability (in all sense) of the proposed solution to much larger data. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Directing Generative Networks with Weighted Maximum Mean Discrepancy,"['Maurice Diesendruck', 'Guy W. Cole', 'Sinead Williamson']",Reject,2018,"[1, 1, 11]","[5, 2, 16]","[3, 4, 43]","[1, 1, 17]","[2, 3, 16]","[0, 0, 10]","This paper addresses the problem of sample selection bias in MMD-GANs. Instead of having access to an i.i.d. sample from the  distribution of interest, it is assumed that the dataset is subject to sample selection bias or the data has been gathered via a biased sample selection mechanism. Specifically, the observed data are drawn from the modified distribution T(x)P(x) where P(x) is the true distribution we aim to estimate and T(x) is an appropriately scaled ""thinning function"". Then, the authors proposed an estimate of the MMD between two distributions using weighted maximum mean discrepancy (MMD). The idea is in fact similar to an inverse probability weighting (IPW). They considered both when T(x) is known and when T(x) is unknown and must be estimated from the data. The proposed method was evaluated using both synthetic and real MNIST dataset. 

In brief, sample selection bias is generally a challenging problem in science, statistics, and machine learning, so the topic of this paper is interesting. Nevertheless, the motivation for investigating this problem specifically in MMD-GANs is not clear. What motivated you to study this problem specifically for GAN in the first place? How does solving this problem help us understand or solve the sample selection bias in general? Will it shed light on how to improve the stability of GAN? Also, the experiment results are too weak to make any justified conclusion.

Some comments and questions:

- How is sample selection bias related to the stability issue of training GAN? Does it worsen the stability?
- Have estimators in Eq. (2) and Eq. (3) been studied before? Are there any theoretical guarantees that this estimate will convergence to the true MMD? 
- On page 5, why T(men) = 1 and T(women) equals to the sample ratio of men to women in labeled subset?
- Can we use clustering to estimate the thinning function?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Faster Reinforcement Learning with Expert State Sequences,"['Xiaoxiao Guo', 'Shiyu Chang', 'Mo Yu', 'Miao Liu', 'Gerald Tesauro']",Reject,2018,"[6, 8, 10, 8, 32]","[11, 13, 15, 13, 37]","[86, 212, 187, 50, 118]","[39, 106, 86, 24, 67]","[38, 95, 93, 23, 28]","[9, 11, 8, 3, 23]","The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations. The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning. The goal of this process is to extract a function that maps any given state to a subgoal. Subgoals are then used to learn different Q-value functions, one per subgoal. 
To learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal. A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal. Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned).
Finally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation.
The paper is overall well-written, and the proposed idea seems interesting. However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them. From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration. It is not clear how this method would generalize to new situations. There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration. 
The architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g).
The empirical evaluation seems rather mixed. Figure 3 shows that the proposed method learns faster than DQN, but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition. Are these the results after all agents had converged? 
Overall, this is a good paper, but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game. Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Structured Deep Factorization Machine: Towards General-Purpose Architectures,"['José P. González-Brenes', 'Ralph Edezhath']",Reject,2018,"[9, 1]","[10, 2]","[20, 2]","[19, 2]","[0, 0]","[1, 0]","This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)

(\sum x_i \beta_i)^T (\sum x_i \beta_i) - \sum_i x_i^2 beta_i^T beta_i

The projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
A novel method to determine the number of latent dimensions with SVD,"['Asana Neishabouri', 'Michel Desmarais']",Reject,2018,"[4, 32]","[7, 37]","[4, 96]","[4, 72]","[0, 4]","[0, 20]","The manuscript proposes to estimate the number of components in SVD by comparing the eigenvalues to those obtained on bootstrapped version of the input.

The paper has numerous flaws and is clearly below acceptance threshold for any scientific forum. Some of the more obvious issues, each alone sufficient for rejection, include:

1. Discrepancy between motivation and actual work. The method is specifically about determining the rank of a matrix, but the authors motivate it with way too general and vague relationships, such as ""determining the number of nodes in neural networks"". Somewhat oddly, the problem is highlighted to be of interest in supervised problems even though one would expect it to be much more important in unsupervised ones.

2. Complete lack of details for related work. Methods such as PA and MAP are described with vague one-sentences summaries that tell nothing about how they actually work. There would have been ample space to provide the mathematical formulations.

3. No technical contribution. The proposed method is trivial variant of randomised testing, described with single sentence ""Bootstrapped samples R_B are simply generated through random sampling with replacement of the values of R."" with literally no attempt of providing any sort of justification why this kind of random sampling would be good for the proposed task or what kind of assumptions it builds on.

4. Poor experiments using really tiny artificial data sets, reported in unprofessional manner (visual style in plots changes from figure to figure, tables report irrelevant numbers in hard-to-read format etc). No real improvement over the somewhat random choice of comparison methods that do not even represent the techniques people would typically use for this problem.","[1, 3, 2]","[' Trivial or wrong', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Bias-Variance Decomposition for Boltzmann Machines,"['Mahito Sugiyama', 'Koji Tsuda', 'Hiroyuki Nakahara']",Reject,2018,"[9, 24, 24]","[14, 29, 28]","[55, 126, 35]","[27, 59, 10]","[20, 15, 4]","[8, 52, 21]","Summary of the paper:
The paper derives a lower bound on the expected  squared KL-divergence between a true distribution and the sample based maximum likelihood estimate (MLE) of that distribution modelled by an Boltzmann machine (BM) based on methods from information geometry. This  KL-divergence is first split into the squared KL-divergence between the true distribution and MLE of that distribution,  and the expected squared KL-divergence between the MLE of the true distribution and the sample based MLE (in a similar spirit to splitting the excess error into approximation and estimation error in statistical learning theory). The letter is than lower bounded (leading to a lower bound on the overall KL-divergence) by a term  which does not necessarily increase if the number of model parameters is increased. 


Pros:
- Using insights from information geometry  opens up a very interesting and (to my knowledge) new approach for analysing the generalisation ability of ML models.
- I am not an expert on information geometry and I did not find the time to follow all the steps of the proof in detail, but the analysis seems to be correct.

Cons:
- The fact that the lower bound does not necessary increase with a growing number of parameters does not guarantee that the same holds true for the KL-divergence (in this sense an upper bound would be more informative). Therefore, it is not clear how much of insights the theoretical analysis gives for practitioners (it could be nice to analyse the tightness of the bound for toy models).
- Another drawback reading the practical impact is, that the theorem bounds the expected  squared KL-divergence between a true distribution and the sample based MLE, while training minimises the divergence between the empirical distribution and the model distribution ( i.e. the sample based MLE in the optimal case),  and the theorem does not show the dependency on the letter. 

I found some parts difficulty to understand and clarity could be improved  e.g. by
- explaining why minimising KL(\hat P, P_B) is equivalent to minimising the KL-divergence between the empirical distribution and the Gibbs distribution \Phi.
- explaining in which sense the formula on page 4 is equivalent to “the learning equation of Boltzmann machines”.
- explaining what is the MLE of the true distribution (I assume the closest distribution in the set of distributions that can be modelled by the BM).

Minor comments:
- page 1: and DBMs….(Hinton et al., 2006) : The paper describes deep belief networks (DBNs) not DBMs 
- \theta is used to describe the function in eq. (2) as well as the BM parameters in Section 2.2 
- page 5: “nodes H is” -> “nodes H are” 



REVISION:
Thanks to the reviewers for replying to my comments and making the changes. I think they improved the paper. On the other hand the other reviewers raised valid questions, that led to my decision to not change the overall rating of the paper.","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
A dynamic game approach to training robust deep policies,['Olalekan Ogunmolu'],Reject,2018,[-3],[1],[1],[0],[1],[0],"The paper presents a method for evaluating the sensitivity and robustness of deep RL policies, and proposes a dynamic game approach for learning robust policies.

The paper oversells the approach in many ways. The authors claim that ""experiments confirm that state-of-the-art reinforcement learning algorithms fail in the presence of additive disturbances, making them brittle when used in situations that call for robustness"". However, their methods and experiments are only applied to Guided Policy Search (GPS), which seems like a specialized RL algorithm. Conclusions drawn from empirically running GPS on a problem cannot be generalized to all ""state-of-the-art RL algorithms"".

In Fig 3, the authors conclude that ""our algorithm uses lesser number for the GMMs and requires fewer samples to generalize to the real-world"". I'm not sure how this can be concluded from Fig 3 [LEFT]. The two line graphs for different values of gamma almost overlay each other, and the cost seems to go up and down, even with number of samples on a log scale. If this shows the variance in the procedure, then the authors should run enough repeats of the experient to smooth out the variance and show the true signal (with error bars if possible). All related conclusions with regards to the dynamic game achieving higher sample efficiency for GMM dynamics fitting need to be backed up with better experimental data (or perhaps clearer presentation, if such data already exists).

Figures 2 and 3 talk about optimal adversarial costs. The precise mathematical definition of this term should be clarified somewhere, since there are several cost functions described in the paper, and it's unclear which terms are actually being plotted here.

The structure of the global policies used in the experiments should be mentioned somewhere.

Note about anonymity: Citation [21] breaks anonymity, since it's referred to in the text as ""our abstract"". The link to the YouTube video breaks author anonymity. Further, the link to a shared dropbox folder breaks reviewer anonymity, hence I have not watched those videos.","[3, 5, 5]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING,"['jianqi ma', 'hangyu lin', 'yinda zhang', 'yanwei fu', 'xiangyang xue']",Reject,2018,"[3, 3, 'no_match', 9, 20]","[8, 6, 'no_match', 14, 25]","[16, 12, 'no match', 257, 411]","[5, 5, 'no match', 104, 235]","[9, 3, 'no match', 118, 107]","[2, 4, 'no match', 35, 69]","This paper proposes a feature augmentation method for one-shot learning.  The proposed approach is very interesting. However, the method needs to be further clarified and the experiments need to be improved. 

Details:
1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read. 

2. The approach:
(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3.  However, under one-shot learning, won’t this  make each class still have only one instance for training? 

(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation? 

(3) It is not clear how will the vocabulary-information be exploited? In particular, how to ensure the semantic space u to be same as the vocabulary semantic space? How to generate the neighborhood in Neigh(\hat{u}_i) on page 5? 

3.  In the experiments: 
(1) The authors didn’t compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing. 

(2) The results are reported for different numbers of augmented instances. Clarification is needed. 
","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Modifying memories in a Recurrent Neural Network Unit,"['Vlad Velici', 'Adam Prügel-Bennett']",Reject,2018,"[-2, 26]","[1, 31]","[2, 107]","[0, 48]","[2, 21]","[0, 38]","Summary: This paper introduces a model that combines the rotation matrices with the LSTMs. They apply the rotations before the final tanh activation of the LSTM and before applying the output gate. The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations. The paper only provides results on the bAbI task. 

Questions:
Have you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study)
Have you tried on other tasks?
Why did you just apply the rotations only on d_{t}.

Pros:
Uses a simple parametrization of the rotation matrices.

Cons:
Not clear justification and motivations
The experiments are really lacking:
No ablation study
The results are only limited to single toy task.


General Comments:

This paper proposes to use the rotation matrices with LSTMs. However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate. The experiments are seriously lacking, an ablation study should have been made and the results are not good enough. The experiments are only limited to bAbI task which doesn’t tell you much. This paper is not ready for publication, and really feels like it is rushed.

Minor Comment:
This paper needs more proper proof-reading. There are some typos in it, e.g.:
1st page, senstence --> sentence
4th page, the the ... --> the

","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
A Tensor Analysis on Dense Connectivity via Convolutional Arithmetic Circuits,"['Emilio Rafael Balda', 'Arash Behboodi', 'Rudolf Mathar']",Reject,2018,"[3, 11, 29]","[5, 15, 33]","[13, 80, 314]","[7, 45, 221]","[5, 30, 39]","[1, 5, 54]","SUMMARY

Traditional convolutional neural networks consist of a sequence of information processing layers. However, one can relax this sequential design constraint so that higher layers receive inputs from one, some, or all preceding layers. This modification allows information to travel more freely throughout the network and has been shown to improve performance, e.g., in image recognition tasks. However, it is not clear whether this change in architecture truly increases representational capacity or it merely facilitates network training. 

In this paper, the authors present a theoretical analysis of the gain in representational capacity induced by additional inter-layer connections. The authors restrict their analysis to convolutional arithmetic circuits (ConvACs), a class of networks whose representational capacity has been studied previously. An important property of ConvACs is that the network mapping can be recast as a homogeneous polynomial over the input, with coefficients stored in a ""grid tensor"" $\mathcal{A}^y$. The grid tensor itself is a function of the hidden weight vectors $\mathbf{a}^{z,i}$. The authors first extend ConvACs to accommodate ""dense"" inter-layer connections and describe how adding dense connections affects the grid tensor. This analysis gives a potentially useful perspective for understanding the mappings that densely connected ConvACs compute.

The authors' main results (Theorems 5.1-5.3) analyze the ""dense gain"" of a densely connected ConvAC. This quantity roughly captures how much wider a standard ConvAC would need to be in order to represent the network mapping of a generic densely connected ConvAC. This is in a way a measure of the added representational power obtained from dense connections. The authors give upper bounds on this quantity, but also produce a case in which the upper bound is achieved. Importantly, the upper bounds are inversely proportional to a parameter $\lambda \leq 1$ controlling the rate at which hidden layer widths decay with increasing depth. The implication is that indeed densely connected ConvACs can have greater representational capacity, however the gain is limited to the case where hidden layers shrink exponentially with increasing depth.

These results are partly unsurprising, since densely connected ConvACs contain more trainable parameters than standard ConvACs. In Proposition 3, the authors give some criteria for evaluating when it is nonetheless worthwhile to add dense connections to a ConvAC.

COMMENTS

(1.) The authors address an interesting and important problem: explaining the empirical success of densely connected CNNs such as ResNets & DenseNets, relative to standard CNNs. The tensor algebra machinery built around ConvACs is impressive and seems to generate sound insights. However, I feel the current presentation fails to provide adequate intuition and interpretation of the results. Moreover, there is no overarching narrative linking the formal results together. This makes it difficult for the reader to grasp the main ideas and significance of the work without diving into all the details. For example:

- In Proposition 1, the authors comment that including a dense connection increases the rank of the grid tensor for a shallow densely connected convAC. However, the significance of grid tensor rank is not discussed.

- In Proposition 2, the authors do not explain why it is important that the added term $g(\mathbf{X})$ contains only polynomial terms of strictly smaller degree. It is not clear how Propositions 1 & 2 relate to the main Theorems 5.1-5.3. Is the characterization of the grid tensor in Proposition 1 used to obtain the bounds in the later Theorems?

- In Section 5, the authors introduce a parameter $\lambda \leq 1$ controlling how the widths of the hidden layers decay with increasing depth. This parameter seems central to the following bounds on dense gain, yet the authors do not motivate it, and there is no discussion of decaying hidden layer widths in previous sections.

- The practical significance of Proposition 3 is not sufficiently well explained. First, it is not clear how to use this result if all we have is an upper bound for $G_w$, as given by Theorems 5.1-5.2. It seems we would need a lower bound to be able to conclude that the ratio $\Delta P_{stand}/ \Delta P_{dense}$ is large. Second, it would be helpful if the authors commented on the implication for the special case $k=1$ and $r \leq (1/1+\lambda) \sqrt{M}$, where the dense gain is known.

(2.) Moreover, because the authors choose not to sketch the main proof ideas, it is difficult to identify the key novel insights, and how the special structure of densely connected ConvACs factors into the analysis. After studying the proofs in some detail, I have some specific concerns outlined below, which diminish the significance of the results and raise some doubts about soundness.

- In Theorem 5.1, the authors upper bound the dense gain by showing that arbitrary $(L, r, \lambda, k)$ dense ConvACs can be represented as standard $(L, r^\prime, \lambda, 0)$ ConvACs of sufficient width $r^\prime \geq G_w r$. The mechanism of the proof is to relate the grid tensor ranks of dense and standard ConvACs. However, a worst case bound on the grid tensor rank of a dense ConvAC is used, which does not seem to rely on the formulation of dense ConvACs. Thus, this result does not tell us anything in particular about dense ConvACs, but rather is a general result relating the expressive capacity of arbitrary depth-$L$ ConvACs and $(L, r^\prime, \lambda, 0)$ ConvACs with decaying widths.

- Central to Theorem 5.2 is the observation that a densely connected ConvAC can be viewed as a standard ConvAC, only with ""virtually enlarged"" hidden layers (of width $\tilde{r}_\ell = (1 + 1/\lambda)r_\ell$ for $k=1$, using the notation of the paper), and blocks of weights fixed to represent the identity mapping. This is a relatively simple idea, and one that seems to hold for general architectures. Thus, I believe Theorem 5.2 can be shown more simply and in greater generality, and without use of the tensor algebra machinery.

- There is some intuitive inconsistency in Theorem 5.3 which I would like some help resolving. We have seen that dense ConvACs can be viewed as standard ConvACs with larger hidden layers and some weights fixed. Effectively, the proof of Theorem 5.3 argues for a regime on $r, \lambda, M$ where this induced ConvAC uses its full representational capacity. This is surprising to me however, as I would have guessed that having some weights fixed makes this impossible. It would be very helpful if the authors could weigh in on this confusion. Perhaps there is an issue with the application of Lemmas 2 & 3 in the proof of Theorem 5.3. In Lemmas 2 & 3, we assume the tensors $\mathcal{A}$ and $\mathcal{B}$ are random. These Lemmas are applied in the proof of Theorem 5.3 to tensors $\phi^{\alpha, j, \gamma}$ appearing in the construction of the dense ConvAC grid tensor. However, the $\phi^{\alpha, j, \gamma}$ tensors do not seem completely random, as there are blocks of fixed weights. Can the authors please clarify how the randomness assumption is satisfied?

(3.) Lastly, I am concerned that the authors do not at least sketch how to generalize these results to architectures of more practical interest. As the authors point out, there is previous work generalizing theoretical results for ConvACs to convolutional rectifier networks. The authors should discuss whether a similar strategy might apply in this case.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS,"['Ron Levie', 'Federico Monti', 'Xavier Bresson', 'Michael M. Bronstein']",Reject,2018,"[7, 4, 17, 17]","[12, 9, 22, 22]","[35, 27, 97, 301]","[6, 9, 36, 131]","[24, 16, 35, 105]","[5, 2, 26, 65]","Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results.

Opinion: Though the Cayley filters seem to have interesting properties,  I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below:
1. The comparison to Chebyshev filters  (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly).

2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison.

3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them.
Also, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter.

Among the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. 

Overall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.","[4, 6, 8]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Lifelong Learning with Output Kernels,"['Keerthiram Murugesan', 'Jaime Carbonell']",Reject,2018,"[3, 12]","[8, 1]","[27, 1]","[12, 1]","[14, 0]","[1, 0]","The paper proposes a budgeted online kernel algorithm for multi-task learning. The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks. The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine. The proposed algorithm is tested on 3 data sets and compared with several baselines.
  Positives:
- the output kernel update is well justified
- experimental results are encouraging
  Negatives:
- the methodological contribution of the paper is minimal
- the proposed approach to maintain the budget is simplistic
- no theoretical analysis of the proposed algorithm is provided
- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the Generalization Effects of DenseNet Model Structures ,"['Yin Liu', 'Vincent Chen']",Reject,2018,"['no_match', 4]","['no_match', 8]","['no match', 12]","['no match', 8]","['no match', 4]","['no match', 0]","This paper analyzes the role of skip connections with respect to generalization in recent architectures such as ResNets or DenseNets. The authors perform an analysis of the performance of ResNets and DenseNets under data scarcity constraints and noisy training samples. They also run some experiments assessing the importance of the number of skip connections in such networks.

The presentation of the paper could be significantly improved. The motivation is difficult to grasp and the contributions do not seem compelling.

My main concern is about the contribution of the paper. The hypothesis that skip connections ease the training and improve the generalization has already been highlighted in the ResNet and DenseNet paper, see e.g. [a].

[a] https://arxiv.org/pdf/1603.05027.pdf

Moreover, the literature review is very limited. Although there is a vast existing literature on ResNets, DenseNets and, more generally, skip connections, the paper only references 4 papers. Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.

The title suggests that the analysis is performed on DenseNet architectures, but experiments focus on comparing both ResNets and DenseNets to sequential convolutional networks and assessing the importance of skip connections.

In section 3.1. (1st paragraph) proposes adding noise to groundtruth labels; however, in section 3.1.2,. it would seem that noise is added by changing the input images (by setting some pixel channels to 0). Could the authors clarify that? Wouldn’t the noise added to the groundtruth act as a regularizer?

In section 4, the paper claims to investigate the role of skip connections in vision tasks. However, experiments are performed on MNIST, CIFAR100, a curve fitting problem and a presumably synthetic 2D classification problem. Performing the analysis on computer vision datasets such as ImageNet would be more compelling to back the statement in section 4.
","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Structured Exploration via Hierarchical Variational Policy Networks,"['Stephan Zheng', 'Yisong Yue']",Reject,2018,"[3, 12]","[8, 17]","[39, 230]","[10, 101]","[27, 110]","[2, 19]","This paper proposes an approach to improve exploration in multiagent reinforcement learning by allowing the policies of the individual agents to be conditioned on an external coordination signal \lambda. In order to find such parametrized policies, the approach combines deep RL with a variational inference approach (ELBO optimization). The paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other state-of-the-art methods.

Overall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.

=Issues with clarity=

-""This problem has two equivalent solutions"". 
This is not so clear; depending on the movement of the preys it might well be that the optimal solution will switch to the other prey in certain cases?

-It is not clear what is really meant with the term ""structured exploration"". It just seems to mean 'improved'?

-It is not clear that the improvements are due to exploration; my feeling is that is is due to improved statistical strength on a more abstract state feature (which is learned), not unlike:
Geramifard, Alborz, et al. ""Online discovery of feature dependencies."" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.
However, there is no clear indication that there is an improved exploration policy.

-The problem setting is not quite clear:
The paper first introduces ""multi-agent RL"", which seems to correspond to a ""stochastic game"" (also ""Markov game""), but then moves on to restrict to the ""fully cooperative setting"" (which would make it a ""Multiagent MDP"", Boutilier '96).

It subsequently says it deals only with deterministic problems (which would reduce the problem further to a learning version of a multiagent classical planning problem), but in the experiments do consider stochastically moving preys.

-The paper says the problem is fully observable, but fails to make explicit if this is *individually* fully observable, or jointly. I am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.

This is actually a crucial confusion, as it completely changes the interpretation of what the approach does: in the individually observable case, the approach is adding a redundant source of information which is more abstract and thus seems to facilitate faster learning. In the latter case, where agents would have individual observations, it is actually providing the agents with more information.

As such, I would really encourage the authors to better define the task they are considering. E.g., by building on the taxonomies of problems that researchers have developed in the community focusing on decentralized POMDPs, such as:
Goldman, Claudia V., and Shlomo Zilberstein. ""Decentralized control of cooperative systems: Categorization and complexity analysis."" (2004).

-""Compared to the single-agent RL setting, multi-agent RL poses unique difficulties. A central issue
is the exploration-exploitation trade-off""
That now in particular happens to be a central issue in single agent RL too.

-""Finding the true posteriors P (λ t |s t ) ∝ P (s t |λ t )P (λ t ) is intractable in general""
The paper did not explain how this inference task is required to solve the RL problem.

-In general, I found the technical description impossible to follow, even after carefully looking at the appending. For instance, (also) there the term P (λ |s ) is suddenly introduced without explaining what the term exactly is? Why is the term P(a|λ) not popping up here? That also needs to be optimized, right? I suppose \phi is the parameter vector of the variational approximation, but it is never really stated. The various shorthand notations introduced for clarity do not help at all, but only make the formulas very cryptic.

-The main text is not readable since definitions, e.g., L(Q_r,\tehta,\phi), that are in the appendix are now missing.

-It is not clear to me how the second term of (10) is now estimated?

-""Shared (shared actor-critic): agents share a deterministic hidden layer,""
What kind of layer is this exactly? How does it relate to \lambda ?

-""The key difference is that this model does not sample from the shared hidden layer""
Why would sampling help? Given that we are dealing with a fully observable multiagent MDP, there is no inherent need to randomize at all? (there should be a optimal deterministic joint policy?)

-""There is shared information between the agents""
What information is referred to exactly? 
Also: It is not quite clear if for these domains cloned would be better than completely independent learners (without shared weights)?

-I can't seem to find anywhere what is the actual shape (or type? I am assuming a vector of reals) of the used \lambda.

-in figure 5, rhs, what is being shown exactly? What do the colors mean? Why does there seem to be a \lambda *per* agent now?



=Related work=

I think the paper could/should be hugely improved in this respect. 

The idea of casting MARL as inference has also been considered by:

Learning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments.
M Liu, C Amato, EP Anesta, JD Griffith, JP How - AAAI, 2016

Stick-breaking policy learning in Dec-POMDPs
M Liu, C Amato, X Liao, L Carin, JP How
International Joint Conference on Artificial Intelligence (IJCAI) 2015

Wu, F.; Zilberstein, S.; and Jennings, N. R. 2013. Monte-carlo
expectation maximization for decentralized POMDPs. In Proc.
of the 23rd Int’l Joint Conf. on Artificial Intelligence (IJCAI-
13).

I do not think that these explicitly make use of a mechanism to coordinate the policies, since they address to true Dec-POMDP setting where each agent only gets its own observations, but in the Dec-POMDP literature, there also is the notion of 'correlation device', which is an additional controller (say corresponding to a dummy agent), which of which the states can be observed by other agents and used to condition their actions on:

Bernstein DS, Hansen EA, Zilberstein S. Bounded policy iteration for decentralized POMDPs. InProceedings of the nineteenth international joint conference on artificial intelligence (IJCAI) 2005 Jun 6 (pp. 52-57).

(and clearly this could be directly included in the aforementioned learning approaches). 


This notion of a correlation device also highlights to potential relation to methods to learn/compute correlated equilibria. E.g.,:

Greenwald A, Hall K, Serrano R. Correlated Q-learning. In ICML 2003 Aug 21 (Vol. 3, pp. 242-249).


A different connection between MARL and inference can be found in:

Zhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N., ""Conditional Random Fields for Multi-agent Reinforcement Learning"", in (New York, NY, USA: ACM, 2007), pp. 1143--1150.


The idea of doing something hierarchical of course makes sense, but also here there are a number of related papers:

-putting ""hierarchical multiagent"" in google scholar finds works by Ghavamzadeh et al., Saira & Mahadevan, etc.

-Victor Lesser has pursued coordination for better exploration with a number of students.

I suppose that Guestrin et al.'s classical paper:
Guestrin, Carlos, Michail Lagoudakis, and Ronald Parr. ""Coordinated reinforcement learning."" ICML. Vol. 2. 2002.
would deserve a citation, and the MARL field is moving ahead fast, an explanation of the differences with COMA:
Counterfactual Multi-Agent Policy Gradients
J Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson
AAAI 2018
is probably also warranted.








","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Taking Apart Autoencoders: How do They Encode Geometric Shapes ?,"['Alasdair Newson', 'Andres Almansa', 'Yann Gousseau', 'Said Ladjal']",Reject,2018,"[7, 20, 17]","[11, 25, 22]","[30, 108, 36]","[13, 52, 14]","[10, 18, 9]","[7, 38, 13]","1. The idea is interesting, but the study is not comprehensive yet
2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper - Stoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017).]. 
3. What's the effect of training data size? 
4. How do the intermediate feature maps look like? 
5. Is there an effect of number of layers? Maybe the network architecture is too deep for the simple data characteristics and size of training set. 
6. Other shapes are said to be part of future work, but I am not convinced that serious conclusions can be drawn from this study only? 
7. What about the possible effects of Batch normalization and dropout?  
8. size of 'd' is critical for autoencoders, only one example in appendix does not do justice, also it seems other color channels show up in the results (fig 10), wasn't it binary input?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Tracking Loss: Converting Object Detector to Robust Visual Tracker,"['Zhenbin Yan', 'Jimmy Ren', 'Stephen Shaoyi Liao', 'Kai Yang']",Reject,2018,"['no_match', 9, 25, 'no_match']","['no_match', 14, 29, 'no_match']","['no match', 102, 97, 'no match']","['no match', 56, 44, 'no match']","['no match', 39, 3, 'no match']","['no match', 7, 50, 'no match']","In this paper, the authors propose a novel tracking loss to convert the RPN to a tracker. The internal structure of top layer features of RPN is exploited to treat feature points discriminatively. In addition, the proposed compression network speeds up the tracking algorithm. The experimental results on the VOT2016 dataset demonstrate its efficiency in tracking. 

This work is the combination of Faster R-CNN (Ren et al. PAMI 2015) and tracking-by-detection framework. The main contributions proposed in this paper are new tracking loss, network compression and results. 

There are numerous concerns with this work:

1.	The new tracking loss shown in equation 2 is similar with the original Faster R-CNN loss shown in equation 1. The only difference is to replace the regression loss with a predefined mask selection loss, which is of little sense that the feature processing can be further fulfilled through one-layer CNN. The empirical operation shown in figure 2 seems arbitrary and lack of theoretical explanation. There is no insight of why doing so. Simply showing the numbers in table 1 does not imply the necessity, which ought to be put in the experiment sections. 
2.	The network compression is engineering and lack insight as well. To remove part of the CNN and retrain is a common strategy in the CNN compression methods [a] [b]. There is a lack of discussion with the relationship with prior arts.
3.	The organization is not clear. Section 3.4 should be set in the experiments and Section 3.5 should be set at the beginning of the algorithm. The description of the network compression is not clear enough, especially the training details.  Meanwhile, the presentation is hard to follow. There is no clear expression of how the tracker performs in practice.
4.	In addition, VOT 2016, the method should evaluate on the OTB dataset with the following trackers [c] [d].
5.	The evaluation is not fair. In Sec 6, the authors indicate that MDNet runs at 1FPS while the proposed tracker runs at 1.6FPS. However, MDNet is based on Matlab and the proposed tracker is based on C++ (i.e., Caffe).

Reference:
[a] On Compressing Deep Models by Low Rank and Sparse Decomposition. Yu et al. CVPR 2017.
[b] Designing Energy-Efficient Convolutional Neural Network Using Energy-Aware Pruning. Yang et al. CVPR 2017.
[c] ECO: Efficient Convolution Operators for Tracking. Danelljan et al. CVPR 2017.
[d] Multi-Task Correlation Particle Filter For Robust Object Tracking. Zhang et al. CVPR 2017.
","[3, 4, 5]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,"['Alexey Romanov', 'Anna Rumshisky']",Reject,2018,"[7, 17]","[10, 22]","[32, 97]","[21, 53]","[11, 35]","[0, 9]","This paper proposes two regularization terms to encourage learning disentangled representations. One term is applied to weight parameters of a layer just like weight decay. The other is applied to the activations of the target layer (e.g., the penultimate layer). The core part of both regularization terms is a compound hinge loss of which the input is the KL divergence between two softmax-normalized input arguments. Experiments demonstrate the proposed regularization terms are helpful in learning representations which significantly facilitate clustering performance.

Pros:
(1) This paper is clearly written and easy to follow.

(2) Authors proposed multiple variants of the regularization term which cover both supervised and unsupervised settings.

(3) Authors did a variety of classification experiments ranging from time serials, image and text data.

Cons:
(1) The design choice of the compound hinge loss is a bit arbitrary. KL divergence is a natural similarity measure for probability distribution. However, it seems that authors use softmax to force the weights or the activations of neural networks to be probability distributions just for the purpose of using KL divergence. Have you compared with other choices of similarity measure, e.g., cosine similarity? I think the comparison as an additional experiment would help explain the design choice of the proposed function.

(2) In the binary classification experiments, it is very strange to almost randomly group several different classes of images into the same category. I would suggest authors look into datasets where the class hierarchy is already provided, e.g., ImageNet or a combination of several fine-grained image classification datasets.

Additionally, I have the following questions:
(1) I am curious how the proposed method compares to other competitors in terms of the original classification setting, e.g., 10-class classification accuracy on CIFAR10. 
(2) What will happen for the multi-layer loss if the network architecture is very large such that you can not use large batch size, e.g., less than 10? 

(3) In drawing figure 2 and 3, if the nonlinear activation function is not ReLU, how would you exam the same behavior? Have you tried multi-class classification for the case “without proposed loss component” and does the similar pattern still happen or not?

Some typos:
(1) In introduction, “when the cosine between the vectors 1” should be “when the cosine between the vectors is 1”.

(2) In section 4.3, “we used the DBPedia ontology dataset dataset” should be “we used the DBPedia ontology dataset”. 

I would like to hear authors’ feedback on the issues I raised.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Modeling Latent Attention Within Neural Networks,"['Christopher Grimm', 'Dilip Arumugam', 'Siddharth Karamcheti', 'David Abel', 'Lawson L.S. Wong', 'Michael L. Littman']",Reject,2018,"[2, 4, 2, 5, 13, 30]","[6, 9, 7, 9, 17, 35]","[17, 32, 32, 39, 45, 294]","[4, 12, 13, 21, 25, 166]","[11, 19, 18, 18, 17, 75]","[2, 1, 1, 0, 3, 53]","The authors of this paper proposed a data-driven black-box visualization scheme. The paper primarily focuses on neural network models in the experiment section. The proposed method iteratively optimize learnable masks for each training example to find the most relevant content in the input that was ""attended"" by the neural network.  The authors empirically demonstrated their method on image and text classification tasks. 

Strength:
           - The paper is well-written and easy to follow. 
           - The qualitative analysis of the experimental results nicely illustrated how the learnt latent attention masks match with our intuition about how neural networks make its classification predictions.

        Weakness:
           - Most of the experiments in the paper are performed on small neural networks and simple datesets. I found the method will be more compiling if the authors can show visualization results on ImageNet models. Besides simple object recognition tasks, other more interesting tasks to test out the proposed visualization method are object detection models like end-to-end fast R-CNN, video classification models, and image-captioning models. Overall, the current set of experiments are limited to showcase the effectiveness of the proposed method.
           - It is unclear how the hyperparameter is chosen for the proposed method. How does the \beta affect the visualization quality? It would be great to show a range of samples from high to low beta values. Does it require tuning for different visualization samples? Does it vary over different datasets?
  ","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Training Deep AutoEncoders for Recommender Systems,"['Oleksii Kuchaiev', 'Boris Ginsburg']",Reject,2018,"[10, 17]","[15, 22]","[26, 69]","[11, 27]","[12, 40]","[3, 2]","This paper proposed to use deep AE to do rating prediction tasks in recommender systems.
Some of the conclusions of the paper, e.g. deep models perform bettern than shallow ones, the non-linear activation
function is important, dropout is necessary to prevent overfitting, are well known, and hence is of less novelty.
The proposed re-feeding algorithm to overcome natural sparseness of CF is interesting, however, I don't think it is enough to support being accepted by ICLR. 
Some reference about rating prediction are missing, such as ""A neural autoregressive approach to collaborative filtering, ICML2016"". And it would be better to show the performance of the model on implicit rating data, since it is more desirable in practice, since many industry applications have only implicit rating (e.g. whether the user watches the movie or not.).","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network,['Vinci Chow'],Reject,2018,[2],[4],[4],[1],[2],[1],"Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the ""luckiness"" of the license plate number.  The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings. Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.  The proposed pipeline converts each character to an embedding with the only sentence of description being ""Each character is converted by a lookup table to a vector representation, known as character embedding.""   Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined. Finally they find a ""a feature vector for each plate by summing up the output of the last recurrent layer overtime."" and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates. In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better. 

Specific Comments on Sections: 
Comments: Sec 1,2
In these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data.

Sec 3
The author does not mention the following reference: ""Deep learning for stock prediction using numerical and textual information"" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks.

Sec 4
What are the characters embedded with? This is important to specify. Is it Word2vec or something else? What does the lookup table consist of? References should be added to the relevant methods. 

Sec 5
I feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison. LSTMs as well could have been a point of comparison. 

Sec 6
 Nothing too insightful is said about the RNN Model. 

Sec 7
The ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table.

Overall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN. However comparisons are lacking and the paper is not presented very scientifically.  The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.
","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
WHAT ARE GANS USEFUL FOR?,"['Pablo M. Olmos', 'Briland Hitaj', 'Paolo Gasti', 'Giuseppe Ateniese', 'Fernando Perez-Cruz']",Reject,2018,"[10, 4, 10, 23, 27]","[15, 9, 12, 28, 32]","[101, 21, 64, 142, 147]","[29, 7, 35, 69, 66]","[35, 12, 16, 51, 32]","[37, 2, 13, 22, 49]","The main take-away messages of this paper seem to be:

1. GANs don't really match the target distribution. Some previous theory supports this, and some experiments are provided here demonstrating that the failure seems to be largely in under-sampling the tails, and sometimes perhaps in introducing spurious modes.

2. Even if GANs don't exactly match the target distribution, their outputs might still be useful for some tasks.

(I wouldn't be surprised if you disagree with what the main takeaways are; I found the flow of the paper somewhat disjointed, and had something of a hard time identifying what the ""point"" was.)

Mode-dropping being a primary failure mode of GANs is already a fairly accepted hypothesis in the community (see, e.g. Mode Regularized GANs, Che et al ICLR 2017, among others), though some extra empirical evidence is provided here.

The second point is, in my opinion, simultaneously (i) an important point that more GAN research should take to heart, (ii) relatively obvious, and (iii) barely explored in this paper. The only example in the paper of using a GAN for something other than directly matching the target distribution is PassGAN, and even that is barely explored beyond saying that some of the spurious modes seem like reasonable-ish passwords.

Thus though this paper has some interesting aspects to it, I do not think its contributions rise to the level required for an ICLR paper.

Some more specifics:

Section 2.1 discusses four previous theoretical results about the convergence of GANs to the true density. This overview is mostly reasonable, and the discussion of Arora et al. (2017) and Liu et al. (2017) do at least vaguely support the conclusion in the last section of this paragraph. But this section is glaringly missing an important paper in this area: Arjovsky and Bottou (2017), cited here only in passing in the introduction, who proved that typical GAN architectures *cannot* exactly match the data distribution. Thus the question of metrics for convergence is of central importance, which it seems should be important to the topic of the present paper. (Figure 3 of Danihelka et al. https://arxiv.org/abs/1705.05263 gives a particularly vivid example of how optimizing different metrics can lead to very different results.) Presumably different metrics lead to models that are useful for different final tasks.

Also, although they do not quite fit into the framing of this section, Nowozin et al.'s local convergence proof and especially the convergence to a Nash equilibrium argument of Heusel et al. (NIPS 2017, https://arxiv.org/abs/1706.08500) should probably be mentioned here.

The two sample testing section of this paper, discussed in Section 2.2 and then implemented in Section 3.1.1, seems to be essentially a special case of what was previously done by Sutherland et al. (2017), except that it was run on CIFAR-10 as well. However, the bottom half of Table 1 demonstrates that something is seriously wrong with the implementation of your tests: using 1000 bootstrap samples, you should reject H_0 at approximately the nominal rate of 5%, not about 50%! To double-check, I ran a median-heuristic RBF kernel MMD myself on the MNIST test set with N_test = 100, repeating 1000 times, and rejected the null 4.8% of the time. My code is available at https://gist.github.com/anonymous/2993a16fbc28a424a0e79b1c8ff31d24 if you want to use it to help find the difference from what you did. Although Table 1 does indicate that the GAN distribution is more different from the test set than the test set is from itself, the apparent serious flaw in your procedure makes those results questionable. (Also, it seems that your entry labeled ""MMD"" in the table is probably n * MMD_b^2, which is what is computed by the code linked to in footnote 2.)

The appendix gives a further study of what went wrong with the MNIST GAN model, arguing based on nearest-neighbors that the GAN model is over-representing modes and under-representing the tails. This is fairly interesting; certainly more interesting than the rehash of running MMD tests on GAN outputs, in my opinion.

Minor:

In 3.1.1, you say ""ideally the null hypothesis H0 should never be rejected"" – it should be rejected at most an alpha portion of the time.

In the description of section 3.2, you should clarify whether the train-test split was done such that unique passwords were assigned to a single fold or not: did 123456 appear in both folds? (It is not entirely clear whether it should or not; both schemes have possible advantages for evaluation.)","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification,"['Wemerson Marinho', 'Luis Marti', 'Nayat Sanchez-pi']",Reject,2018,"[1, 18, 13]","[6, 22, 17]","[2, 4, 64]","[2, 1, 50]","[0, 1, 1]","[0, 2, 13]","The manuscript proposed to use prefix codes to compress the input to a neural network for text classification. It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.


There are several issues with the paper and I cannot recommend acceptance of the paper in the current state. 
- It looks like it is not finished.
- the datasets are not described properly. 
- It is not clear to me where the baseline results come from.
 They do not match up to the Zhang paper (I have tried to find the matching accuracies there).
- It is not clear to me what the baselines actually are or how I can found more info on those.
- the results are not remarkable. 

Because of this, the paper needs to be updated and cleaned up before it can be properly reviewed. 

On top of this, I do not enjoy the style the paper is written in, the language is convoluted. 
For example: “The effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification “
I do not know which message the paper tries to get across here. 
As a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive.
The acknowledgements should not be included here either. 

","[3, 2, 4]","[' Clear rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Boosting of Diverse Experts,"['Wei Zhang', 'Qiuyu Chen', 'Jun Yu', 'Jianping Fan']",Reject,2018,"[14, 2, 'no_match', 19]","[18, 5, 'no_match', 24]","[49, 11, 'no match', 290]","[26, 5, 'no match', 132]","[12, 3, 'no match', 16]","[11, 3, 'no match', 142]","This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks.

While the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:

- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end.
- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup.
- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners.
- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations.

Other comments:
- Paper would benefit from writing improvements to make it read better.
- ""simply use the weighted error function"": I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted.
-  ""to replace the softmax error function (used in deep learning)"": I don't think we have softmax error function","[2, 6, 5]","[' Strong rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING,['Fei Tian'],Reject,2018,[10],[14],[69],[37],[18],[14],"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se). Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs. The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.  

Overall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak. The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?) for which the reason could be a multitude of issues probably related to hyper-parameter tuning. What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation? Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2. Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos. ","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
LEAP: Learning Embeddings for Adaptive Pace,"['Vithursan Thangarasa', 'Graham W. Taylor']",Reject,2018,"[1, 15]","[6, 20]","[9, 174]","[3, 78]","[6, 77]","[0, 19]","While the idea is novel and I do agree that I have not seen other works along these lines there are a few things that are missing and hinder this paper significantly.

1. There are no quantitative numbers in terms of accuracy improvements, overhead in computation in having two networks.
2. The experiments are still at the toy level, the authors can tackle more challenging datasets where sampling goes from easy to hard examples like birdsnap. MNIST, FashionMNIST and CIFAR-10 are all small datasets where the true utility of sampling is not realized. Authors should be motivated to run the large scale experiments.

","[4, 6, 3]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The Manifold Assumption and Defenses Against Adversarial Perturbations,"['Xi Wu', 'Uyeong Jang', 'Lingjiao Chen', 'Somesh Jha']",Reject,2018,"[11, 2, 6, 27]","[16, 7, 11, 32]","[50, 8, 33, 301]","[19, 4, 15, 175]","[22, 4, 17, 88]","[9, 0, 1, 38]","The authors argue that ""good"" classifiers naturally represent the classes in a classification as well-separated manifolds, and that adversarial examples are low-confidence examples lying near to one of these manifolds. The authors suggest ""fixing"" adversarial examples by projecting them back to the manifold, essentially by finding a point near the adversarial example that has high confidence.

There are numerous issues here, which taken together, make the whole story pretty unconvincing.

The term ""manifold"" is used very sloppily. To be fair, this is unfortunately common in modern machine learning. An actual manifold is a specific mathematical structure with specific properties. In ML, what is generally hypothesized is that the data (often per class) lives ""near"" to some ""low-dimensional"" structure. In this paper, even the low-dimensionality isn't used --- the ""manifold assumption"" is used as a stand-in for ""the regions associated with different classes are well-separated."" (This is partially discussed in Section 6, where the authors point out correctly that the same defense as used here could be used with a 1-nn model.) This is fine as far as it goes, but the paper refs Basri & Jacobs 2016 multiple times as if it says anything relevant about this paper: Basri & Jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds. This reference doesn't add much to the present story.

The essential argument of the paper rests on the ""Postulate: (A good model) F is confident on natural points drawn from the manifolds, but has low confidence on points outside of the manifolds."" 

This postulate is sloppy and speculative. For instance, taken in its strong form, if believe the postulate, then a good model:
1. Can classify all ""natural points"" from all classes with 100% accuracy.
2. Can detect adversarial points with 100% accuracy because all high-confidence points are correct classifications and all low-confidence points are adversarial.
3. All adversarial examples will be low-confidence.

Point 1 makes it clear that no good model F fully satisfying the postulate exists --- models never achieve 100% accuracy on difficult real-world distributions. But the method for dealing with adversarial examples seems to require Points 2 and 3 being true.

To be fair, the paper more-or-less admits that how true these points are is not known and is important. Nevertheless, I think this paper comes pretty close to arguing something that I *think* is not true, and doesn't do much to back up its argument. Because of the quality of the writing (generally sloppy), it's hard to tell, but I believe the authors are basically arguing that:
a. You can generally easily detect adversarial points because they are low confidence.
b. If you go through a procedure to find a point near your adversarial point that is high-confidence, you'll get the ""correct"" (or perhaps ""original"") class back.

I think b follows from a, but a is extremely suspect. I do not personally work in adversarial examples, and briefly looking at the literature, it seems that most authors *do* focus on how something is classified and not its confidence, but I don't think it's *that* hard to generate high-confidence adversarial examples. Early work by Goodfellow et al. (""Explaining and Harnessing Adversarial Examples"", Figure 1, shows an example where the incorrect classification has very high confidence. The present paper only uses Carlini-Wagner attacks. From a read of Carlini-Wagner, it seems they are heavily concerned with finding *minimal* perturbations to achieve a given misclassification; this will of course produce low-confidence adversaries, but I see no reason why this is a general property of all adversarial examples.

The experiments are weak. I applaud the authors for mentioning the experiments are very preliminary, but that doesn't make them any less weak. 

What are we to make of the one image discussed at the end of Section 5 and shown in Figure 1? The authors note that the original image gives low-confidence for the correct class. (Does this mean that the classifier isn't ""good""? Is it evidence against some kind of manifold assumption?) The authors note the adversarial category has significantly higher confidence, and say ""in this case, it seems that it is the vagueness of the signals/data that lead to a natural difficulty."" But the signals and data are ALWAYS vague. If they weren't, machine learning would be easy. This paper proposes something, looks at a tiny number of examples, and already finds a counterexample to the theory. What's the evidence *for* the theory? 

A lot of writing is given over to how this method is ""semantic"", and I just don't buy it. The connection to manifolds is weak. The basic argument here is really ""(1) If our classifiers produce smooth well-separated high-confidence regions, (2) then we can detect adversaries because they're low-confidence, and (3) we can correct adversaries by projecting them back to high-confidence."" (1) seems vastly unlikely to me based on all my experience: neural nets often get things wrong, they often get things wrong with high confidence, and when they're right, the confidence is at least sometimes low. The authors use a sloppy postulate about good models and so could perhaps argue I've never seen a good model, but the methods of this paper require a good model. (2) seems to follow logically from (1). (3) is also suspect --- perturbations which are *minimal* can be corrected as this paper does (and Carlini-Wagner attacks are minimal by design), but there's no reason to expect general perturbations to be minimal.

The writing is poor throughout. It's generally readable, but the wordings are often odd, and sometimes so odd it's hard to tell what was meant. For instance, I spent awhile trying to decide whether the authors assumed common classifiers are ""good"" (according to the postulate) or whether this paper was about a way to *make* classifiers good (I eventually decided the former).","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Understanding Local Minima in Neural Networks by Loss Surface Decomposition,"['Hanock Kwak', 'Byoung-Tak Zhang']",Reject,2018,"[3, 29]","[7, 34]","[11, 309]","[3, 190]","[8, 55]","[0, 64]","The authors propose investigating regions of the the parameter space under which the activations (over the entire training data set) remain unchanged.  They conjecture, and then attempt to argue for a simple network, that, over these regions, the loss function exhibits nice properties:  all local minima are global minima, all other local optima are saddle points, and the function is neither convex nor concave on these regions.  The proof of this statement seems relatively straightforward and appears to be correct.  Unfortunately it only applies to a special case.  Second, the authors argue that the loss function for their simple network has poor local minima.  Finally, the authors conclude with a simple set of experiments exploring the accuracy of random activations.  Overall, I found the main idea of the paper relatively straightforward, but the presentation is a bit awkward in places.

I think the work is heading in an interesting direction, but I found it somewhat incremental.  It's nice to know that the loss function (squared loss in this case) has these properties, but as there are exponentially many regions corresponding to the different activations, it is unclear what the practical consequences of these theoretical observations are.  Could the authors elaborate on this?

Another question:  is it really true that the non-differentiability of the functions involved creates significant issues in practice  (not theoretically) - isn't the set of all points with this property of measure zero?
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On Convergence and Stability of GANs,"['Naveen Kodali', 'James Hays', 'Jacob Abernethy', 'Zsolt Kira']",Reject,2018,"[2, 30, 13, 15]","[1, 35, 18, 20]","[1, 121, 102, 130]","[0, 65, 55, 54]","[1, 43, 40, 71]","[0, 13, 7, 5]","This paper addresses the well-known stability problem encountered when training GANs. As many other papers, they suggest adding a regularization penalty on the discriminator which penalizes the gradient with respect to the data, effectively linearizing the data manifold.

Relevance: Although I think some of the empirical results provided in the paper are interesting, I doubt the scientific contribution of this paper is significant. First of all, the penalty the author suggest is the same as the one suggest by Gulrajani for Wasserstein GAN (there the motivation behind this penalty comes from the optimal transport plan). In this paper, the author apply the same penalty to the GAN objective with the alternative update rule which is also a lower-bound for the Wasserstein distance.

Justification: The authors justify the choice of their regularization saying it linearizes the objective along the data manifold and claim it reduces the number of non-optimal fixed points. This might be true in the data space but the GAN objective is optimized over the parameter space and it is therefore not clear to me their argument hold w.r.t to the network parameters. Can you please comment on this?

Regularizing the generator: Can the authors motivate their choice for regularizing the discriminator only, and not the generator? Following their reasoning of linearizing the objective, the same argument should apply to the generator.

Comparison to existing work: This is not the first paper that suggests adding a regularization. Given that the theoretical aspect of the paper are rather weak, I would at least expect a comparison to existing regularization methods, e.g.
Stabilizing training of generative adversarial networks through regularization. NIPS, 2017

Choice of hyper-parameters: The authors say that the suggested value for lambda is 10. Can you comment on the choice of this parameter and how it affect the results? Have you tried  annealing lambda? This is a common procedure in optimization (see e.g. homotopy or continuation methods).

Bogonet score: I very much like the experiment where the authors select 100 different architectures to compare their method against the vanilla GAN approach. I here have 2 questions:
- Did you do a deeper examination of your results, e.g. was there some architectures for which none of the method performed well?
- Did you try to run this experiment on other datasets?
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 3, 2]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Learning temporal evolution of probability distribution with Recurrent Neural Network,"['Kyongmin Yeo', 'Igor Melnyk', 'Nam Nguyen', 'Eun Kyung Lee']",Reject,2018,"[9, 6, 14, 9]","[13, 11, 19, 13]","[20, 46, 67, 30]","[4, 19, 43, 17]","[10, 23, 12, 0]","[6, 4, 12, 13]","Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay. Unclear if this would work at all in higher-dimensional time series. It is also unclear to me what are the sources of the uncertainties captured.


The author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series. The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts. Instead, the conditional density p(y_t|y_{1:t-1|, \theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM. The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels. Therefore, the uncertainties produced by the model appear to be a black-box. It is probably unlikely that the discretisation method can be generalised to high-dimensional setting?

Quality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth. In the experiments with real data, it's unclear how good the uncertainties produced by the model are. It may be useful to compare to the uncertainty produced by a GP with suitable kernels. In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that?

Clarity: The paper is well-written. The presentations of the ideas are pretty clear.

Originality: Above average. I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever.

Significance: Average. It would be excellent if the authors can extend this to higher dimensional time series.

I'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC.","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DNN Representations as Codewords: Manipulating Statistical Properties via Penalty Regularization,"['Daeyoung Choi', 'Changho Shin', 'Hyunghun Cho', 'Wonjong Rhee']",Reject,2018,"[1, 1, 5, 19]","[4, 6, 10, 24]","[8, 6, 8, 58]","[3, 2, 3, 22]","[3, 4, 3, 21]","[2, 0, 2, 15]","This paper presents a set of regularizers which aims for manipulating the statistical properties like sparsity, variance and covariance. While some of the proposed regularizers are applied to weights, most are applied to hidden representations of neural networks. Class-wise regularizations are also investigated for the purpose of fine-grained control of statistics within each class. Experiments over MNIST, CIFAR10 and CIFAR100 demonstrate the usefulness of this technique.

The following related work also studied the regularizations on hidden representations which are motivated from clustering perspective and share some similarities with the proposed one. It would be great to discuss the relationship.

Liao, R., Schwing, A., Zemel, R. and Urtasun, R., 2016. Learning deep parsimonious representations. NIPS.

Pros:
(1) The paper is clearly written.

(2) The visualizations of hidden activations are very helpful in understanding the effect of different regularizers.

(3) The proposed regularizations are simple and computationally efficient.

Cons:
(1) The novelty of the paper is limited as most of the proposed regularizers are more or less straightforward modifications over DeCov.

(2) When we manipulate the statistics of representations we aim for something, like improving generalization, interpretability. But as pointed out by authors, improvement of generalization performance is not the main focus. I also do not find significant improvement from all experiments. Then the question is what is the main benefit of manipulating various statistics? 

I have an additional question as below:
In measuring the ratio of dead units, I notice authors using the criterion of “not activated on all classes”. However, do you check this criterion over the whole epoch or just some mini-batches?

Overall, I think the paper is technically sound. But the novelty and significance are a bit unsatisfactory. I would like to hear authors’ feedback on the issues I raised.
","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Sequence Transfer Learning for Neural Decoding,"['Venkatesh Elango*', 'Aashish N Patel*', 'Kai J Miller', 'Vikash Gilja']",Reject,2018,"[3, 2, 14, 13]","[3, 7, 18, 18]","[2, 5, 24, 30]","[2, 2, 9, 20]","[0, 1, 0, 0]","[0, 2, 15, 10]","This work addresses brain state  decoding (intent to move) based on intra-cranial ""electrocorticography (ECoG) grids"". ECoG signals are generally of much higher quality than more conventional EEG signals acquired on the skalp, hence it appears meaningful to invest significant effort to decode.  
Preprocessing is only descibed in a few lines in Section 2.1, and the the feature space is unclear (number of variables etc)

Linear discriminants, ""1-state and 2-state"" hidden markov models, and LSTMs are considered for classification (5 classes, unclear if prior odds are uniform). Data involves multiple subjects (4 selected from a larger pool). Total amount of data unclear. ""A validation set is not used due to the limited data size.""  The LSTM setup and training follows conventional wisdom.
""The model used for our analyses was constructed with 100 hidden units with no performance gain identified using larger or stacked networks.""
A simplistic but interesting  transfer scheme is proposed amounting to an affine transform of features(??) - the complexity of this transform is unclear.

While limited novelty is found in the methodology/engineering - novelty being mainly related to the affine transfer mechanism, results are disappointing.  
The decoding performance of the LSTMs does not convincingly exceed that of the simple baselines. 

When analyzing the transfer mechanism only the LSTMs are investigated and it remains unclear how well trans works.

There is an interesting visualization (t-SNE) of the latent representations. But very limited discussion of what we learn from it, or how such visualization could  be used to provide neuroscience insights.

In the discussion we find the claim: ""In this work, we have shown that LSTMs can model the variation within a neural sequence and are a good alternative to state-of-the-art decoders.""  I fail to see how it can be attractive to obtain similar performance with a model of 100x (?) the complexity



","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,"['Wei Wu', 'Can Xu', 'Yu Wu', 'Zhoujun Li']",Reject,2018,"[27, 14, 'no_match', 21]","[32, 19, 'no_match', 26]","[733, 143, 'no match', 69]","[358, 62, 'no match', 44]","[57, 52, 'no match', 6]","[318, 29, 'no match', 19]","The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. For example:

1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning.
2) the formulation in equation 4 seems to be problematic
3) ""simplify pr(ri|si,ai) as pr(ri|ai,ui−1,ui−2) since decoding natural language responses from long conversation history is challenging"" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?
4) about section 3.2, again I didn't get whether the model needs RL for training.
5) ""We train m(·, ·) with the 30 million crawled data through negative sampling."" not sure I understand the connection between training $m(\cdot, \cdot)$ and the entire model.
6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY,"['Yeu-Chern Harn', 'Vladimir Jojic']",Reject,2018,"[4, 15]","[5, 17]","[5, 27]","[0, 19]","[1, 3]","[4, 5]","Summary: This paper studied the conditional image generation with two-stream generative adversarial networks. More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label. During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation. An auxiliary “label difference cost” was further introduced to encourage class information captured by the foreground generator. Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline.

== Novelty/Significance ==
Controllable image generation is an important task in representation learning and computer vision. I also like the unsupervised learning through gating function and label difference cost. However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs).

== Detailed comments ==
The proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region. Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability. For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet. 

Given the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable. However, I didn’t see such detailed analysis as in the other papers on controllable image generation.

In Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp. It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better). Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned. Is it possible to use all the binary attributes in the dataset.

Also, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks).

Overall, I believe the paper is interesting but not ready for publication. I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation. Hopefully, the suggested studies will improve the quality of the paper in the future submission.

== Presentation ==
The paper is readable but not well polished. 

-- In Figure 1, the “G1” on the right should be “G2”;
-- Section 2.2.1, “X_f” should be “x_f”;
-- the motivation of having “z_v” should be introduced earlier;
-- Section 2.2.4, please use either “alpha” or “\alpha” but not both;
-- Section 3.3, the dataset information is incorrect: “20599 images” should be “202599 images”;

Missing reference:
-- Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017.
-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.
-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the difference between building and extracting patterns: a causal analysis of deep generative models.,"['Michel Besserve', 'Dominik Janzing', 'Bernhard Schoelkopf']",Reject,2018,"[12, 18, 24]","[17, 23, 29]","[42, 137, 777]","[16, 58, 380]","[19, 50, 286]","[7, 29, 111]","This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN. The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences. By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies). This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework. The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework. They also claim that this analysis can be used to improve the performance of the models.

Pros:
-- SDR characterisation of the convolutional filters is interesting
-- The authors show that filters with different characteristics are responsible for different aspects of image modelling

Cons:
-- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs
-- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN 
-- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form). Grammatical and spelling mistake are frequent. More background information could be helpful in section 2.2. All figures (but in particular Figure 3) need more informative captions
-- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text. Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning


In summary, the paper is not ready for publication in its current form. The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs.","[2, 7, 7]","[' Strong rejection', ' Good paper, accept', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Evaluation of generative networks through their data augmentation capacity,"['Timothée Lesort', 'Florian Bordes', 'Jean-Francois Goudou', 'David Filliat']",Reject,2018,"[2, 3, 4, 20]","[6, 8, 7, 25]","[30, 14, 12, 112]","[6, 3, 6, 61]","[21, 10, 3, 33]","[3, 1, 3, 18]","The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only. Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. 

In addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for. If the aim is to have realistic samples, a visual turing test is probably the best metric. If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option.

PROS:
The idea is interesting. 

CONS:
1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction). It would be interesting to understand how the different metrics relate. Moreover, the new metric is introduced with the following motivation “[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution”. The mode collapse issue is never discussed elsewhere in the paper. 

2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays. Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. 

3. The authors should clarify if the method is specifically designed for GANs and VAEs. If not, section 2.1 should contain several other works (as in Table 1). 

4. One of the main statements of the paper “Our approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)” is never proved, nor discussed.

5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. Further, how many tau’s one should evaluate? In order to evaluate a generative model one should test on the generated data only (tau=1) I believe. In the worst case, the generator experiences mode collapse and performs badly. Differently, it can memorize the training data and performs as good as the baseline model. If it does actual data augmentation, it should perform better.

6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models. In fact, the limit of training with a fixed dataset is that the model ‘sees’ the data multiple times across epochs with the risk of memorizing. In the proposed protocol, the model ‘sees’ the generated data D_gen (which is fixed before training) multiple time across epochs. This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.


Minor: 
Section 2.2 might be more readable it divided in two (exploitation and evaluation).   
","[3, 5, 3]","[' Clear rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Learning non-linear transform with discriminative and minimum information loss priors,"['Dimche Kostadinov', 'Slava Voloshynovskiy']",Reject,2018,"[5, 21]","[8, 26]","[42, 230]","[26, 160]","[16, 43]","[0, 27]","Overview:
This paper proposes a method for learning representations using a “non-linear transform”. Specifically, the approach is based on the form: Y =~ AX, where X is the original data, A is a projection matrix, and Y is the resulting representation. Using some assumptions, and priors/regularizers on Y and A, a joint objective is derived (eq. 10), and an alternating optimization algorithm is proposed (eq. 11 and 14). Both objective and algorithm use approximations due to hardness of the problem. Theoretical and empirical results on the quality and properties of the representation are presented.
Disclaimer: this is somewhat outside my area of expertise, so this is a rather high-level review. I have not thoroughly checked proofs and claims.

Comments:
-I found the presentation quality to be rather poor, making it hard to fully understand and evaluate the approach. In particular, the motivation and approach are not clear (sec. 1.2), making it hard to understand the proposed method. There is no explicit formulation, instead there are references to other models (e.g., sparsifying transform model) and illustrative figures (fig. 1 and 2). Those are useful following a formal definition, but cannot replace it. The separation between positive and negative elements of the representation is not motivated and explained in a footnote although it seems central to the proposed approach.
- The paper is 17 pages long (24 pages with the appendix), so I had to skim through some parts. Due to the extensive scope, perhaps a journal submission would be more appropriate.

Minors:
- Vu & Monga 2016b and 2016c are the same.
- p. 1: meaner => manner
- p. 1: refereed => referred
- p. 1: “a structural constraints”; p. 2: “a low rank constraints”, “a pairwise constraints”; p. 4: “a similarity concentrations”, “a numerical experiments”, and others...
- p. 2, 7: therms => terms
- p. 2: y_{c_1,k_2} => y_{c_1,k_1}?
- p. 3, 4: “a the”
- p. 5: “an parametric”
- p. 8: ether => either
- Other typos… the paper needs proofreading.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[1, 2, 2]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Image Transformer,"['Ashish Vaswani', 'Niki Parmar', 'Jakob Uszkoreit', 'Noam Shazeer', 'Lukasz Kaiser']",Reject,2018,"[13, 2, 11, 9, 14]","[17, 6, 15, 13, 18]",['skipped'],['skipped'],['skipped'],['skipped'],"In this paper the authors propose an autoregressive image generation model that incorporates a self-attention mechanism. The latter is inspired by the work of [Vaswani et al., 2016], which was proposed for sequences and is extended to 2D images in this work. The authors apply their model to super-resolution of face images, as well as image completion (aka inpainting) and generation, both unconditioned or conditioned on one of a small number of image classes from the CIFAR10 and ImageNet datasets. The authors evaluate their method in terms of visual quality of their generated images via an Amazon Mechanical Turk survey and quantitatively by reporting slightly improved log-likelihoods. 

While the paper is well written, the motivation for combining self-attention and autoregressive models remains unclear unfortunately, even more though as the reported quantitative improvement in terms of log-likelihood are only marginal. The technical exposition is at times difficult to follow with some design decisions of the network layout being quite ad hoc and not well motivated. Expressing the involved operations in mathematical terms would help comprehend some of the technical details and add to the reproducibility of the proposed model. 

Another concern is the experimental evaluation. While the reported log-likelihoods are only marginally better, the authors report a significant boost in how often humans are fooled by the generated images. While the image generation is conditioned on the low-resolution input, the workers in the Amazon Mechanical Turk study get to see the high-resolution images only. Of course, a human observer would pick the one image out of the two shown images which is more realistic although it might have nothing to do with the input image, which seems wrong. Instead, the workers should see the low-res input image and then have to decide which high-res image seems a better match or more likely.

Overall, the presented work looks quite promising and an interesting line of research. However, in its present form the manuscript doesn't seem quite ready for publication yet. Though, I would strongly encourage the authors to make the exposition more self-contained and accessible, in particular through rigorous mathematical terms, which would help comprehend the involved operations and help understand the proposed mechanism.

Additional comments:
- Abstract: ""we also believe to look pretty cool"". Please re-consider the wording here. Generating ""pretty cool"" images  should not be the goal of a scientific work.
","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Baseline-corrected space-by-time non-negative matrix factorization for decoding single trial population spike trains,"['Arezoo Alizadeh', 'Marion Mutter', 'Thomas Münch', 'Arno Onken', 'Stefano Panzeri']",Reject,2018,"[-3, 'no_match', 2, 11, 24]","[1, 'no_match', 1, 16, 29]","[1, 'no match', 1, 21, 84]","[0, 'no match', 0, 5, 21]","[0, 'no match', 0, 9, 7]","[1, 'no match', 1, 7, 56]","In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train. Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition. The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients. An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques). 

The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2. The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability.  A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization.","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Toward predictive machine learning for active vision,['Emmanuel Daucé'],Reject,2018,[21],[25],[24],[13],[4],[7],"In this paper, the authors present a computational framework for the active vision problem. Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal.

I am not convinced about the novelty and contribution of the work. The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013).

The paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Examples: foveate inspection the data (abstract), may allow to (motivation), tu put it clear (motivation), on contrary to animals retina (footnote 1), minimize at most the current uncertainty (perception-driven control), center an keep (fovea-based implementation), degrade te recognition (outlook and perspective). The citations are in non-standard format (section 1.2: Kalman (1960)).

Overall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking. 

1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387–91, 2005.
2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91–107, 2010.
3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013.","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 2]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']"
Parametric Information Bottleneck to Optimize Stochastic Neural Networks,"['Thanh T. Nguyen', 'Jaesik Choi']",Reject,2018,"[6, 13]","[9, 18]","[8, 97]","[1, 51]","[4, 37]","[3, 9]","This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck. A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable. 

The idea is interesting and to my knowledge novel. Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward. It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood. This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective. 

Why is it important to maximise I(X_l, Y) for every layer? Does that impact the MI of the final layer and Y?  

To estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016. Computation of the bound requires integration over multiple layers (equation 15). How is this achieved in practice? With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor.

Mutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17). How is the conditional entropy term estimated? The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large. It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset? What is the variance of the mini batch estimate?

In the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck). 

There is a small typo in (eq 5).
","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples,"['Luke Hewitt', 'Andrea Gane', 'Tommi Jaakkola', 'Joshua B. Tenenbaum']",Reject,2018,"[4, 'no_match', 26, 25]","[8, 'no_match', 31, 30]","[3, 'no match', 294, 610]","[2, 'no match', 149, 353]","[1, 'no match', 112, 226]","[0, 'no match', 33, 31]","This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep, directed generative models with latent variables. Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently. The group-wise posteriors allow amortization of the information cost KL(group posterior || prior) across all examples in the group, which the authors liken to the ""KL annealing"" tricks that are sometimes used to avoid posterior collapse when training models with strong decoders p(x|z) using current techniques for approximate variational inference in deep nets.

The presentation of the core idea is solid, though it did take two read-throughs before the equations really clicked for me. I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group-wise and per-example posteriors are composed in this model, using Equations and pseudo-code for the main training loop, would have saved me some time. For readers less familiar with amortized variational inference in deep nets, the benefit would be larger.

I appreciate that the authors developed extensions of the core method to more complex group structures, though I didn't find the related experiments particularly convincing. 

Overall, I like this paper and think the underlying group-wise posterior construction trick is worth exploring further. Of course, the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized.","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Learning Deep Generative Models With Discrete Latent Variables,"['Hengyuan Hu', 'Ruslan Salakhutdinov']",Reject,2018,"[3, 16]","[8, 21]","[31, 419]","[14, 207]","[16, 201]","[1, 11]","Interesting work, but I’m not convinced by the arguments nor by the experiments. Similar models have been trained before; it’s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad-hoc and not principled. 

Nevertheless, interesting work for everyone interested in RBMs as priors for “binary VAEs”. 

","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures,"['Ding Liu', 'Shi-Ju Ran', 'Peter Wittek', 'Cheng Peng', 'Raul Blázquez García', 'Gang Su', 'Maciej Lewenstein']",Reject,2018,"[18, 1, 10, 'no_match', 'no_match', 13, 25]","[23, 6, 14, 'no_match', 'no_match', 18, 29]","[203, 17, 63, 'no match', 'no match', 32, 20]","[92, 0, 29, 'no match', 'no match', 11, 1]","[42, 16, 17, 'no match', 'no match', 4, 8]","[69, 1, 17, 'no match', 'no match', 17, 11]","Authors of this paper derived an efficient quantum-inspired learning algorithm based on a hierarchical representation that is known as tree tensor network, which is inspired by the multipartite entanglement renormalization ansatz approach where the tensors in the TN are kept to be unitary during training. Some observations are: The limitation of learnability of TTN strongly depends on the physical indexes and the geometrical indexes determine how well the TTNs approximate the limit; TTNs exhibit same increase level of abstractions as CNN or DBN; Fidelity and entanglement entropy can be considered as some measurements of the network.

Authors introduced the two-dimensional hierarchical tensor networks for solving image recognition problems, which suits more the 2-D nature of images. In section 2, authors stated that the choice of feature function is arbitrary, and a specific feature map was introduced in Section 4. However, it is not straightforward to connect (10) to (1) or (2). It is better to clarify this connection because some important parameters such as the virtual bond and input bond are related to the complexity of the proposed algorithm as well as the limitation of learnability. For example, the scaling of the complexity O(dN_T(b_v^5 + b_i^4)) is not easy to understand. Is it related to specific feature map? How about the complexity of eigen-decomposition for one tensor at each iterates. And also, whether the tricks used to accelerate the computations will affect the convergence of the algorithm? More details on these problems are required for readers’ better understanding.

From Fig 2, it is difficult to see the relationship between learnability and parameters such input bond and virtual bond because it seems there are no clear trends in the Fig 2(a) and (b) to make any conclusion. It is better to clarify these relationships with either clear explanation or better examples.

From Fig 3, authors claimed that TN obtained the same levels of abstractions as in deep learning. However, from Fig 3 only, it is hard to make this conclusion. First, there are not too many differences from Fig 3(a) to Fig 3(e).  Second, there is no visualization result reported from deep learning on the same data for comparison. Hence, it is not convincing to draw this conclusion only from Fig 3. 

In Section 4.2, what strategy is used to obtain these parameters in Table 1?

In Section 5, it is interesting to see more experiments in terms of fidelity and entanglement entropy.
","[6, 3, 4]","[' Marginally above acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']"
Learning to play slot cars and Atari 2600 games in just minutes,"['Lionel Cordesses', 'Omar Bentahar', 'Julien Page']",Reject,2018,"[18, 1, 5]","[18, 3, 5]","[10, 4, 4]","[8, 3, 3]","[0, 0, 0]","[2, 1, 1]","In this paper the authors address the very important challenges of current deep learning approaches, which is that these algorithms typically need an extraordinarily large number of training rounds to learn their strategies.  The authors note that in real life, this type of training will outstrip both the training and time budget of most real world problems.  The solution they propose is to take a high level approach and to learn more like humans do by creating strategies that involve relationships between entities rather than trying to build up strategies from pixels. 
The authors credit their reframing of their approach to AI to the “continental philosophers” (e.g. Heidegger) in opposition to the “analytical philosophers” such as Wittgenstein.  The authors associate current machine learning approaches with the analytic philosophers, based on propositions that are either provably true or untrue and their own approach as in opposition to these, however from my reading of this paper what the authors are saying is that if you start learning with higher level concepts (relationships between entities) rather than doing analysis on low level information such as pixels.   Starting with low level concepts makes learning very difficult at first and leads to a path where many trials are required.  Staring from higher level concepts such as relationships between entities allows learning to happen quickly and in a manner much more similar in nature to what humans actually do.
While the authors bring up many valid points, and in essence I believe that they may be correct, the flaw in this paper is that they do not provide methods for teaching computers to learn these higher level concepts.  The algorithms they present all require human knowledge to be encoded in the algorithms to identify the higher level concepts.  The true power of the deep learning approach is that it can actually learn from low level data, without humans hand crafting the higher level entities on their behalf.

While I agree with Dreyfus that understanding what is important and interesting given a situation would be an incredible boon to any AI algorithm, it remains an unsolved problem as to how to teach a computer to understand what is interesting in a scene with the same intuition that a human has.  In the first experiment the authors need to pre-define the concepts of a straight road and a curved road and identify them for the algorithm.  They also need to tell the algorithm exactly how to count the number of sections that the track has.  In the second experiment, to identify the “Me” in the game, the authors instruct the computer to recognize “me” as the things that move when the controller is activated.  While in some ways this is clever, mimicking what a child might do to see what moves in the world when it issues a command to move from its own brain and thus learning what “me” is, children take year to develop a sense of “self” and part of that is learning that a “concept of self” is an interesting and useful thing to have.  In their work the authors know, from their human intelligence, what are the important concepts in the game (again from a human perspective) and devise simple methods for the computer to learn these.  Again the problem here is that the human has to define the important concepts for the computer and define a specific strategy for the computer to learn to identify these important policies.  Data intensive deep learning algorithms are able to infer strategies without these concepts being defined for them.

This reframing does point out a different and perhaps better path for AI, but it is not entirely new and this paper does not present a method for getting from sensed data to higher level concepts.  For each of the experiments, the strategies used rely on human intuition to define policies.  In the first experiment with slot cars, a human needs to provide n laps of driving to imitate.  The authors identify the “shortest lap” and store it for the “AI” to replay.  The only “learning” is from an optimization that minimizes the difference between the AI’s lap time and the best lap time (tbest) of the human by scaling that recorded sample of the human driving.  This results is a strategy that is essentially just trying to replicate (imitate) what the human is doing will not lead to a generalizable learning strategy that could ever exceed a human example.   This is at best a very limited form of imitation learning.  The learning process for the second example is explained in even less detail.
Overall, this paper presents a different way of thinking about AI, one in which the amount of training time and training data required for learning is greatly reduced, however what is missing Is a generalizable algorithmic strategy for implementing this framework.m  

 



","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 2, 1]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', "" The reviewer's evaluation is an educated guess""]"
Simple Fast Convolutional Feature Learning,"['David Macêdo', 'Cleber Zanchettin', 'Teresa Ludermir']",Reject,2018,"[1, 16, 29]","[5, 21, 34]","[45, 110, 274]","[22, 72, 182]","[17, 19, 19]","[6, 19, 73]","This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. 
In the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets.

Pros:
The paper compares different classifiers on three datasets.

Cons:
- Considering an adaptive schedule of the learning decay is common practice in modern machine learning. Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch.
- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers.
- There are many spelling errors
- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore. It is well known that CNN features are much better if enough data is available.
","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
"Model Specialization for Inference Via End-to-End Distillation, Pruning, and Cascades","['Daniel Kang', 'Karey Shi', 'Thao Ngyuen', 'Stephanie Mallard', 'Peter Bailis', 'Matei Zaharia']",Reject,2018,"[14, 'no_match', 'no_match', 'no_match', 9, 13]","[19, 'no_match', 'no_match', 'no_match', 13, 18]","[45, 'no match', 'no match', 'no match', 130, 194]","[13, 'no match', 'no match', 'no match', 39, 92]","[22, 'no match', 'no match', 'no match', 39, 65]","[10, 'no match', 'no match', 'no match', 52, 37]","This paper presents three different techniques for model specialization, i.e. adapting a pretrained network to a more specific task and reduce its computational cost while maintaining the performance. The three techniques are distillation, weight pruning and cascades. Evaluation compares how effective each technique is and how they interact with each other. In certain settings the obtained speed-up reaches 5x without loss of accuracy.

Pros:
- The idea of reducing the computational cost of specialized models makes sense.
- In some setting the speed-up can reach more than 5x, which is quite relevant.

Cons:
- The fact that the models are specialized to simpler tasks is not explicitly used in the approach. The authors should test what would happen when using their cascade for classification on all classes of ImageNet for instance. Would it be the gain in speed much lower?
- It is not clear if the distillation on smaller networks is really improving the models accuracy. The authors compared the distilled models with models trained from scratch. There should be an additional experiment with the small models trained on Imagenet first and then fine-tuned to the task. If in that case there is non gain, then, what is the advantage of distilling in these settings? ImageNet annotations need to be used anyway to train the teacher network.
- In section 3.2 it seems that the filters of a CNN are globally ranked based on their average activation values. Those with the lowest average activation will be removed. However, in my understanding, the ranking can work better if performed layer specific and not globally.
- In section 3.4, the title says ""end-to-end specialization pipeline"", but actually, the specialization is done in 3 steps, therefore in my understanding it is not end-to-end.
- There are some spelling errors, for instance in the beginning of section 4.1
- Pruning does not seem to produce much speed-up.
- The experimental part is difficult to read. In particular Fig. 4 should be better explained. There are some symbols in the legend that do not appear in the graph, and others (baselines only) that appear multiple times, but it is not clear what they represent. Also, at the end of the explanation of Fig. 4 the authors mention a gain of 8%, which in my understanding is not really relevant compared with the total speed-up, which can be in the order of 500%

Overall, the idea of model specialization seem interesting. However, in my understanding the main source of speed-up is a cascade approach with a reduced model, in which is not clear how much speed-up is actually due to the specialized task.","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Hallucinating brains with artificial brains,"['Peiye Zhuang', 'Alexander G. Schwing', 'Oluwasanmi Koyejo']",Reject,2018,"[0, 12, 10]","[5, 17, 15]","[15, 233, 156]","[7, 129, 71]","[8, 99, 71]","[0, 5, 14]","Quality

This is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging.

Clarity

The paper is well-written. Methods and results are clearly described. The authors state significant improvements in classification using generated data. These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets.

Originality

This is one of the first uses of GANs in the context of neuroimaging. 

Significance 

The approach outlined in this paper may spawn a new research direction.

Pros

Well-written and original contribution demonstrating the use of GANs in the context of neuroimaging.

Cons

The focus on neuroimaging might be less relevant to the broader AI community.","[8, 6, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION,"['Prannay Khosla', 'Preethi Jyothi', 'Vinay P. Namboodiri', 'Mukundhan Srinivasan']",Reject,2018,"[2, 10, 15, 6]","[4, 15, 20, 7]","[4, 99, 221, 14]","[1, 62, 109, 11]","[2, 33, 90, 3]","[1, 4, 22, 0]","The contributions made by this paper is unclear. As one of the listed contributions, the authors propose using policy gradient. However, in this setting, the reward is a known differentiable function, and the action is continuous, and thus one could simply backpropagate through to get the gradients on the encoder. Also, it seems the reward is not a function of the future actions, which further questions the need for a reinforcement learning formulation.

The paper is written poorly. For instance, I don't understand what this sentence means: ""We condition the latent variables to come from rich distributions"". Observed accent labels are referred to as latent (hidden) variables.

While the independent Wasserstein critic is useful to study whether models are overfitting (by comparing train/heldout numbers), their use for comparing across different model types is not justified. Moreover, since GAN-based methods optimize the Wasserstein distance directly, it cannot serve as a metric to compare GAN-based models with other models.

All of the models compared against do not use accent information during training (table 2), so this is not a fair comparison.

Overall, the paper lacks any novel technical insight, contributions are not explained well, exposition is poor, and the evaluations are invalid.","[3, 4, 5]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING,"['Ahmed A Aly', 'Joanne Bechta Dugan']",Reject,2018,"[2, 35]","[1, 35]","[1, 55]","[1, 32]","[0, 0]","[0, 23]","This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image. These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain.

From my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges. The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique. This important flaw in the evaluation prevents any fair comparison with the state of the art.

The text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion. In particular, I have serious problems understanding:

1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ? The reference to a GAN architecture seems very forced and out of the scope.

2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid.

3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section.

Finally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning.

I would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision.","[2, 3, 2]","[' Strong rejection', ' Clear rejection', ' Strong rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning,"['Michael Blot', 'Thomas Robert', 'Nicolas Thome', 'Matthieu Cord']",Reject,2018,"[0, 3, 14, 21]","[2, 6, 19, 26]","[2, 11, 124, 282]","[1, 4, 69, 151]","[1, 7, 34, 87]","[0, 0, 21, 44]","the paper adapts the information bottleneck method where a problem has invariance in its structure. specifically, the constraint on the mutual information is changes to one on the conditional  entropy. the paper involves a technical discription how to develop proper estimators for this conditional entropy etc.

this is a nice and intuitive idea. how it interacts with classical regularizers or if it completely dominates classical regularizers would be interesting for the readers.","[7, 5, 4, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Generative Models for Alignment and Data Efficiency in Language,"['Dustin Tran', 'Yura Burda', 'Ilya Sutskever']",Reject,2018,"[4, 5, 12]","[9, 10, 17]","[76, 17, 99]","[31, 8, 49]","[40, 9, 45]","[5, 0, 5]","This paper proposes a generative model called matching auto-encoder to carry out the learning from unaligned data.
However, it is very disappointed to read the contents after the introduction, since most of the contributions are overclaimed.

Detailed comments:
- Figure 1 is incorrect because the pairs (x, z) and (y, z) should be put into two different plates if  x and y are unaligned.

- Lots of contents in Sec. 3 are confusing to me. What is the difference between g_l(x) and g_l(y) if g_l : H_{l−1} → H_l and f_l: H_{l−1} → H_l are the same? What are e_x and e_y? Why is there a λ if it is a generative model?

- If the title is called 'text decipherment', there should be no parallel data at all, otherwise it is a huge overclaim on the decipherment tasks. Please add citations of Kevin Knight's recent papers on deciperment.

- Reading the experiment results of 'Sentiment Transfer' is a disaster to me. I couldn't get much information on 'sentiment transfer' from a bunch of ungrammatical unnatural language sentences. I would prefer to see some results of baseline models for comparison instead of a pure qualitative analysis.

- The claim on ""FMAEs are state of the art for neural machine translation with limited supervision on EN-DE and EN-FR"" is not exciting to me. Semi-supervised learning is interesting, but in the scenario of MT we do have enough parallel data for many language pairs. Unless the model is able to exceed the 'real' state-of-the-art that uses the full set of parallel data, otherwise we couldn't identify whether the models are able to benefit NMT.  Interestingly, the authors didn't provide any of the results that are experimented with full parallel data set. Possibly it is because the introduction of stochastic variables that prevent the models from overfitting on small datasets.

","[4, 2, 5]","[' Ok but not good enough - rejection', ' Strong rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Interpreting Deep Classification Models With Bayesian Inference,"['Hanshu Yan', 'Jiashi Feng']",Reject,2018,"[0, 9]","[5, 14]","[22, 542]","[7, 214]","[13, 235]","[2, 93]","The paper develops a technique to understand what nodes in a neural network are important
for prediction. The approach they develop consists of using an Indian Buffet Process 
to model a binary activation matrix with number of rows equal to the number of examples. 
The binary variables are estimated by taking a relaxed version of the 
asymptotic MAP objective for this problem. One question from the use of the 
Indian Buffet Process: how do the asymptotics of the feature allocation determine 
the number of hidden units selected? 

Overall, the results didn't warrant the complexity of the method. The results are neat, but 
I couldn't tell why this approach was better than others.

Lastly, can you intuitively explain the additivity assumption in the distribution for p(y')","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Deep Function Machines: Generalized Neural Networks for Topological Layer Expression,['William H. Guss'],Reject,2018,[3],[6],[17],[4],[13],[0],"The main idea of this paper is to replace the feedforward summation
y = f(W*x + b)
where x,y,b are vectors, W is a matrix
by an integral
\y = f(\int W \x + \b)
where \x,\y,\b are functions, and W is a kernel. A deep neural network with this integral feedforward is called a deep function machine. 

The motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \x, then one encounters the curse of dimensionality as one obtains finer and finer discretization. The idea of functional PCA is to view \x as a function is some appropriate Hilbert space, and expands it in some appropriate basis. This way, finer discretization does not increase the dimension of \x (nor its approximation), but rather improves the resolution. 

This paper takes this idea and applies it to deep neural networks. Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea. This approach amounts to a change of basis - and therefore the resolution invariance is not surprising. In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components). Unfortunately, this was not done. I suspect that in this case, the results would be very similar. 

","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 1, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']"
Cheap DNN Pruning with Performance Guarantees ,"['Konstantinos Pitas', 'Mike Davies', 'Pierre Vandergheynst']",Reject,2018,"[5, 24, 24]","[9, 29, 29]","[8, 35, 277]","[2, 15, 132]","[6, 7, 69]","[0, 13, 76]","The problem of pruning DNNs is an active area of study.
This paper addresses this problem by posing the Net-trim objective function as  a Difference of convex(DC) function. This allows for an immediate application of DC function minimization using existing techniques. An analysis of Generalization error 
is also given. 

The main novelty seems to be the interesting connection to DC function minimization. The benefits seem to be a faster algorithm for pruning. 

About the generalization error the term C_2 needs to be more well defined otherwise the coefficient of  A would be -ve which may lead to complications.

Experimental investigations are reasonable and the results are convincing.

A list of Pros:
1. Interesting connection to DC function
2. Attempt to analyze generalization error 
3. Faster speed of convergence empirically

A list of Cons:
1. The contribution in posing the objective as a DC function looks limited as it is very straightforward. Also the algorithm is 
direct application
2. The time complexity analysis is imprecise. Since the proposed algorithm is iterative time complexity would depend on the number of iterations.




","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Diffusing Policies : Towards Wasserstein Policy Gradient Flows,"['Pierre H. Richemond', 'Brendan Maginnis']",Reject,2018,"[2, 2]","[7, 3]","[18, 4]","[3, 0]","[15, 3]","[0, 1]","The main object of the paper is the (entropy regularized) policy updates. Policy iterations are viewed as a gradient flow in the small timestep limit. Using this, (and following Jordan et al. (1998)) the desired PDE (Equation 21) is obtained. The rest of the paper discusses the implications of Equation 21 including but not limited to what happens when the time derivative of the policy is zero, and the link to noisy gradients.

Even though the topic is interesting and would be of interest to the community, the paper mainly presents known results and provides an interpretation from the point of view of policy dynamics. I fail to see the significance nor the novelty in this work (esp. in light of  Jordan et al. (1998) and Peyre (2015)).

That said, I believe that exposing such connections will prove to be useful, and I encourage the authors to push the area forward. In particular, it would be useful to see demonstrations of the idea, and experimental justifications even in the form of references would be a welcome addition to the literature.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling,"['Boris Ginsburg', 'Igor Gitman', 'Yang You']",Reject,2018,"[17, 2, 11]","[22, 7, 16]","[69, 8, 102]","[27, 1, 34]","[40, 7, 35]","[2, 0, 33]","This paper provides an optimization approach for large batch training of CNN with layer-wise adaptive learning rates. 
It starts from the observation that the ratio between the L2-norm of parameters and that of gradients on parameters varies
significantly in the optimization,  and then introduce a local learning rate to consider this observation for a more stable and efficient optimization. Experimental results show improvements compared with the state-of-the-art algorithm.

Review:
(1) Pros
The proposed optimization method considers the dynamic self-adjustment of the learning rate in the optimization based on the ratio between the L2-norm of parameters and that of gradients on parameters  when the batch size increases, and shows improvements in experiments compared with previous methods.

(2) Cons
i) LR ""warm-up"" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach. However, it seems that the authors also combine with LR ""warm-up"" in your proposed method in the experimental part, e.g., Table 3. So does it mean that the proposed method cannot handle the problem in general?

ii) There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. In fact, as can be seen in the Algorithm 1, this coefficient can be as an independent hyper-parameter (even is put with the global learning rate together as one fix term).

iii) In the section 6, when increase the training steps, experiments compared with previous methods should be implemented since they can also get better results with more epochs.

iv) Writing should be improved, e.g., the first paragraph in section 6. Some parts are confusing, for example, the authors claim that they use initial LR=0.01, but in Table 1(a) it is 0.02.  ","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
UNSUPERVISED METRIC LEARNING VIA NONLINEAR FEATURE SPACE TRANSFORMATIONS,"['Pin Zhang', 'Bibo Shi', 'JundongLiu']",Reject,2018,"[13, 8, 'no_match']","[17, 10, 'no_match']","[22, 26, 'no match']","[12, 20, 'no match']","[0, 3, 'no match']","[10, 3, 'no match']","This paper proposed a nonlinear unsupervised metric learning framework. The authors combine Coherent Point Drifting and the k-means approaches under the trace minimization framework. However, I am afraid that the novelty and insight of this work is not good enough for acceptance.

Pros:
The paper is well written and easy to follow.

Cons:
1 The novelty of this paper is limited.
The authors mainly combine Coherent Point Drifting and the k-means under the trace minimization framework. The trace minimization is then solved with an EM-like iterative minimization.
However, trace minimization is already well explored and this paper provides little insight. Furthermore, there is not any theoretical guarantee how this iterative minimization approach will converge to.

2 For a method with limited novelty, comprehensive experiments are needed to verify its effectiveness. However, the experimental setting of this paper is biased.
An important line of works, namely deep learning based clustering, are totally missing.
Comprehensive experiments with other deep learning based clustering are required.
","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Adversarial Examples for Natural Language Classification Problems,"['Volodymyr Kuleshov', 'Shantanu Thakoor', 'Tingfung Lau', 'Stefano Ermon']",Reject,2018,"[9, 1, 0, 10]","[14, 6, 3, 15]","[46, 17, 4, 406]","[20, 6, 1, 199]","[22, 11, 3, 200]","[4, 0, 0, 7]","Nice overview of adversarial techniques in natural language classification. The paper introduces the problem of adversarial perturbations, how they are constructed and demonstrate what effect they can have on a machine learning models. 

The authors study several real-world adversarial examples, such as spam filtering, sentiment analysis and fake news and use these examples to test several popular classification models in context of adversarial perturbations. 

Their results demonstrate the existence of adversarial perturbations in NLP and show that several different types of errors occur (syntactic, semantic, and factual). Studying each of these errors type can help defend and improve the classification algorithms via adversarial training.

Pros: Good analysis on real-world examples
Cons: I was expecting more actual solutions in addition to analysis","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,['Jia Bi'],Reject,2018,[17],[21],[16],[9],[5],[2],"The authors present an l-1 regularized SVRG based training algorithm that is able to force many weights of the network to be 0, hence leading to good compression of the model.  The motivation for l-1 regularization is clear as it promotes sparse models, which lead to lower storage overheads during inference. The use of SVRG is motivated by the fact that it can, in some cases, provide faster convergence than SGD.

Unfortunately, the authors do not compare with some key literature. For example there has been several techniques that use sparsity, and group sparsity [1,2,3], that lead to the same conclusion as the paper here: models can be significantly sparsified while not affecting the test accuracy of the trained model.

Then, the novelty of the technique presented is also unclear, as essentially the algorithm is simply SVRG with l1 regularization and then some quantization. The experimental evaluation does not strongly support the thesis that the presented algorithm is much better than SGD with l1 regularization. In the presented experiments, the gap between the performance of SGD and SVRG is small (especially in terms of test error), and overall the savings in terms of the number of weights is similar to Deep compression. Hence, it is unclear how the use of SVRG over SGD improves things. Eg in figure 2 the differences in top-1 error of SGD and SVRG, for the same number of weights is very similar (it’s unclear also why Fig 2a uses top-1 and Fig 2b uses top-5 error). I also want to note that all experiments were run on LeNet, and not on state of the art models (eg ResNets).

Finally, the paper is riddled with typos. I attach below some of the ones I found in pages 1 and 2

Overall, although the topic is very interesting, the contribution of this paper is limited, and it is unclear how it compares with other similar techniques that use group sparsity regularization, and whether SVRG offers any significant advantages over l1-SGD.

typos:
“ This work addresses the problem by proposing methods Weight Reduction Quantisation”
-> This work addresses the problem by proposing a Weight Reduction Quantisation method

“Beside, applying with sparsity-inducing regularization”
-> Beside, applying sparsity-inducing regularization

“Our method that minibatch SVRG with l-1 regularization on non-convex problem”
-> Our minibatch SVRG with l-1 regularization method on non-convex problem

“As well as providing,l1 regularization is a powerful compression techniques to penalize some weights to be zero”
-> “l1 regularization is a powerful compression technique that forces some weights to be zero”

 The problem 1 can
->  The problem in Eq.(1) can

“it inefficiently encourages weight”
-> “it inefficiently encourages weights”

————

[1] Learning Structured Sparsity in Deep Neural Networks
http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf

[2] Fast ConvNets Using Group-wise Brain Damage
https://arxiv.org/pdf/1506.02515.pdf

[3] Sparse Convolutional Neural Networks
https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf


","[4, 4, 2]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Strong rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study,"['David Macêdo', 'Cleber Zanchettin', 'Adriano L. I. Oliveira', 'Teresa Ludermir']",Reject,2018,"[1, 16, 16, 29]","[5, 21, 21, 34]",['skipped'],['skipped'],['skipped'],['skipped'],"This paper describes DReLU, a shift version of ReLU. DReLU shifts ReLU from (0, 0) to (-\sigma, -\sigma). The author runs a few CIFAR-10/100 experiments with DReLU.

Comments:

1. Using expectation to explain why DReLU works well is not sufficient and convincing. Although DReLU’s expectation is smaller than expectation of ReLU, but it doesn’t explain why DReLU is better than very leaky ReLU, ELU etc.
2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc.
3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious. I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU. 

Overall, I don’t think this paper meet ICLR’s novelty standard, although the authors present some good numbers, but they are not convincing. 


","[5, 3, 4]","[' Marginally below acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Combination of Supervised and Reinforcement Learning For Vision-Based Autonomous Control,"['Dmitry Kangin', 'Nicolas Pugeault']",Reject,2018,"[6, 16]","[10, 21]","[24, 77]","[14, 41]","[5, 16]","[5, 20]","This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy.  It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions.  The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning.

This paper is easy to understand but has a couple shortcomings and some fatal (but reparable) flaws:.

1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read.  I would strongly suggest avoiding your notation a(x|\Theta) and using \pi(x) (subscripting theta or making conditional is somewhat less important).  Your a(.) function seems to be the policy here, which is invariable denoted \pi in the RL literature.  There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf. You have no obligation to use this notation but it does make reading of your paper much easier on others in the community.  This is more of a shortcoming than a fundamental issue.

2) More fatally, you have failed to compare your algorithm's performance against benchline implementations of similar algorithms.  It is almost trivial to run DDPG on Torcs using the openAI baselines package [https://github.com/openai/baselines].  I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic.  Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.

3) And finally, perhaps complementary to point 2), you really need to provide examples on more than one environment.  Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities.  Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these.  Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI).  Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)

4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper.  Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.

Overall I think your intuitions and ideas are good, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.  The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space.  Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment.","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Counterfactual Image Networks,"['Deniz Oktay', 'Carl Vondrick', 'Antonio Torralba']",Reject,2018,"[3, 9, 20]","[8, 14, 25]","[8, 122, 391]","[3, 50, 204]","[5, 65, 145]","[0, 7, 42]","This paper proposes a neural network architecture around the idea of layered scene composition.  Training is cast in the generative adversarial framework; a subnetwork is reused to generate and compose (via an output mask) multiple image layers; the resulting image is fed to a discriminator.  An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.

The idea is interesting and different from established approaches to segmentation.  Visualization of learned layers for several scene types (Figures 3, 7) shows that the network does learn a reasonable compositional scene model.

Experiments evaluate the ability to port the model learned in an unsupervised manner to semantic segmentation tasks, using a limited amount of supervision for the end task.  However, the included experiments are not nearly sufficient to establish the effectiveness of the proposed method.  Only two scene types (bedroom, kitchen) and four object classes (bed, window, appliance, counter) are used for evaluation.  This is far below the norm for semantic segmentation work in computer vision.  How does the method work on established semantic segmentation datasets with many classes, such as PASCAL?  Even the ADE20K dataset, from which this paper samples, is substantially larger and has an established benchmarking methodology (see http://placeschallenge.csail.mit.edu/).

An additional problem is that performance is not compared to any external prior work.  Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.  The range of prior work on semantic segmentation is extensive.  How well does the approach compare to supervised CNNs on an established segmentation task?  Note that the proposed method need not necessarily outperform supervised approaches, but the reader should be provided with some idea of the size of the gap between this unsupervised method and the state-of-the-art supervised approach.

In summary, the proposed method may be promising, but far more experiments are needed.
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Estimation of cross-lingual news similarities using text-mining methods,"['Zhouhao Wang', 'Enda Liu', 'Hiroki Sakaji', 'Tomoki Ito', 'Kiyoshi Izumi', 'Kota Tsubouchi', 'Tatsuo Yamashita']",Reject,2018,"[3, 'no_match', 11, 3, 23, 14, 19]","[1, 'no_match', 16, 5, 28, 19, 22]","[1, 'no match', 79, 17, 106, 110, 16]","[1, 'no match', 61, 14, 83, 83, 14]","[0, 'no match', 3, 1, 2, 12, 0]","[0, 'no match', 15, 2, 21, 15, 2]","* PAPER SUMMARY *

This paper proposes a siamese net architecture to compare text in different languages. The proposed architecture builds upon siamese RNN by Mueller and Thyagarajan. The proposed approach is evaluated on cross lingual bitext retrieval.

* REVIEW SUMMARY * 

This paper is hard to read and need proof-reading by a person proficient in English. The experiments are extremely limited, on a toy task. No other baseline than (Mueller and Thyagarajan, 2016) is considered. The related work section lacks important references. It is hard to find positive points that would advocate for a presentation at ICLR.

* DETAILED REVIEW *

On related work, the authors need to consider related work on cross lingual retrieval, multilingual document representation:

Bai, Bing, et al. ""Learning to rank with (a lot of) word features."" Information retrieval 13.3 (2010): 291-314. (Section 4).

Schwenk, H., Tran, K., Firat, O., & Douze, M. Learning Joint Multilingual Sentence Representations with Neural Machine Translation, ACL Workshop on Representation Learning for NLP, 2017

Karl Moritz Hermann and Phil Blunsom.  Multilingual models for compositional distributed semantics. In ACL 2014. pages 58–68.

Hieu Pham, Minh-Thang Luong, and Christopher D. Manning. Learning distributed representations for multilingual text sequences. In Workshop
on Vector Space Modeling for NLP. 2015

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. Cross-lingual sentiment classification with bilingual document representation learning. In ACL 2016

...

On evaluation, the authors need to learn about standard retrieval evaluation metrics such as precision at top 10, etc and use them. For instance, this book will be a good read.

Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. Modern information retrieval. Vol. 463. New York: ACM press, 1999.

On learning objective, the authors might want to read about learn-to-rank objectives for information retrieval, for instance, 

Liu, Tie-Yan. ""Learning to rank for information retrieval."" Foundations and Trends in Information Retrieval 3.3 (2009): 225-331.

Burges, Christopher JC. ""From ranknet to lambdarank to lambdamart: An overview."" Learning 11, no. 23-581 (2010): 81.

Chapelle, Olivier, and Yi Chang. ""Yahoo! learning to rank challenge overview."" Proceedings of the Learning to Rank Challenge. 2011.

Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. ""Large margin rank boundaries for ordinal regression."" (2000).

On experimental setup, the authors want to consider a setup with more than 8k training documents. More importantly, ranking a document set of 1k documents is extremely small, toyish. For instance, (Schwenk et al 2017) search through 1.5 million sentences. (Bai, Bing, et al 2009) search through 140k documents. Since you mainly introduces 2 modifications with respect to (Mueller and Thyagarajan, 2016), i.e  (i) not sharing the parameters on both branch of the siamese and (ii) the fully connected net on top, I would suggest to measure the effect of each of them both on multilingual data and on the SICK dataset used in (Mueller and Thyagarajan, 2016).","[2, 6, 2]","[' Strong rejection', ' Marginally above acceptance threshold', ' Strong rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,"['Eleanor Quint', 'Garrett Wirka', 'Jacob Williams', 'Stephen Scott', 'N.V. Vinodchandran']",Reject,2018,"[0, 'no_match', 2, 24, 25]","[1, 'no_match', 7, 28, 30]","[1, 'no match', 10, 48, 117]","[0, 'no match', 3, 28, 52]","[1, 'no match', 2, 2, 34]","[0, 'no match', 5, 18, 31]","The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). The problem is important and interesting. The authors list the contributions of each part but it seems that only the final contribution, i.e. analysis of the interpretability, is interesting and should be further extended and emphasized. Here with the detailed comments.

1. I think Table 2 does not make sense at all. This is not only because the authors use the label information but also because the authors compare different quantities. The the previous methods evaluate log p(x) while the proposed method evaluates log p(x, y) which should be much lower as the proposed method potentially trains a separated model for each class of the x for evaluation.

2. The generation results of the SVAE shown in Figure 7 in Appendix A seem strange as the diversity of the samples is much less than those from the vanilla VAEs. Could the authors explain this mode collapse phenomenon? 

3. The results in Table 1 are not interesting.  It is most useful to interpret the state-of-the-art classifier while the results of the proposed methods are far from the state-of-the-art even on such simple MNIST dataset.

4. The most interesting results of this paper are shown in Figure 1. However, I think the results on the interpretability should be further extended. Several questions are as follows: 

Why other dimensions are not so interpretable, compared with 21?

Can we also interpret a VAE given labels by varying each dimension of the latent variables without jointly training a DTT? I personally think some of the dimensions of the latent variables of the vanilla VAEs can also be interpreted via interpolation in each dimension. 

Can these results be generalized to other datasets, consisting of natural images? 

Overall, this paper is below the acceptance threshold.
 ","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Code Synthesis with Priority Queue Training,"['Daniel A. Abolafia', 'Quoc V. Le', 'Mohammad Norouzi']",Reject,2018,"[1, 14, 10]","[3, 19, 15]","[6, 299, 136]","[2, 143, 59]","[4, 145, 72]","[0, 11, 5]","This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.

I question the value of program synthesis in a language which is not human-readable. Typically, source code as function representation is desirable because it is human-interpretable. Code written in brainfuck is not  readable by humans. In the related work, a paper by Nachum et al is criticized for providing a sequence of machine instructions, rather than code in a language. Since code in brainfuck is essentially a sequence of pointer arithmetic operations, and does not include any concept of compositionality or modularity of code (e.g. functions or variables), it is not clear what advantage this representation presents. Neither am I particularly convinced by the benchmark of a GA for generating BF code. None of these programs are particularly complex: most of the examples found in table 4 are quite short, over half of them 16 characters or fewer. 500 million evaluations is a lot. There are no program synthesis examples demonstrating types of functions which perform complex tasks involving e.g. recursion, such as sorting operations.

There is also an odd attitude in the writing of this paper, reflected in the excerpt from the first paragraph describing that traditional approaches to program synthesis “… typically do not make use of machine learning and therefore require domain specific knowledge about the programming languages and hand-crafted heuristics to speed up the underlying combinatorial search. To create more generic programming tools without much domain specific knowledge …”. Why is this a goal? What is learned by restricting models to be unaware of obviously available domain-specific knowledge? 

All this said, the priority queue training presented here for reinforcement learning with sparse rewards is interesting, and appears to significantly improve the quality of results from a naive policy gradient approach. It would be nice to provide some sort of analysis of it, even an empirical one. For example, how frequently are the entries in the queue updated? Is this consistent over training time? How was the decision of K=10 reached? Is a separate queue per distributed training instance a choice made for implementation reasons, or because it provides helpful additional “regularization”? While the paper does demonstrate that PQT is helpful on this very particular task, it makes very little effort to investigate *why* it is helpful, or whether it will usefully generalize to other domains.

Some analysis, perhaps even on just a small toy problem, of e.g. the effect of the PQT on the variance of the gradient estimates produced by REINFORCE, would go a long way towards convincing a skeptical reader of the value of this approach. It would also help clarify under what situations one should or should not use this. Any insight into how one should best set the lambda hyperparameters would also be very appreciated.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
SIC-GAN: A Self-Improving Collaborative GAN for Decoding Sketch RNNs,"['Chi-Chun Chuang', 'Zheng-Xin Weng', 'Shan-Hung Wu']",Reject,2018,"['no_match', 12]","['no_match', 16]","['no match', 40]","['no match', 26]","['no match', 3]","['no match', 11]","This paper baffles me. It appears to be a stochastic RNN with skip connections (so it's conditioned on the last two states rather than last one) trained by an adversarial objective (which is no small feat to make work for sequential tasks) with results shown on the firetruck category of the QuickDraw dataset. Yet the authors claim significantly more importance for the work than I think it merits.

First, there is nothing variational about their variational RNN. They seem to use the term to be equivalent to ""stochastic"", ""probabilistic"" or ""noisy"" rather than having anything to do with optimizing a variational bound. To strike the right balance between pretension and accuracy, I would suggest substituting the word ""stochastic""  everywhere ""variational"" is used.

Second, there is nothing self-improving or collaborative about their self-improving collaborative GAN. Once the architecture is chosen to share the weights between the weak and strong generator, the only difference between the two is that the weak generator has greater noise at the output. In this sense the architecture should really be seen as a single model with different noise levels at alternating steps. In this sense, I am not entirely clear on what the difference is between the SIC-GAN and their noisy GAN baseline - presumably the only difference is that the noisy GAN is conditioned on a single timestep instead of two at a time? The claim that these models are somehow ""self-improving"" baffles me as well - all machine learning models are self-improving, that is the point of learning. The authors make a comparison to AlphaGo Zero's use of self-play, but here the weak and strong generators are on the same side of the game, and because there are no game rules provided beyond ""reproduce the training set"", there is no possibility of discovery beyond what is human-provided, contrary to the authors' claim.

Third, the total absence of mathematical notation made it hard in places to follow exactly what the models were doing. While there are plenty of papers explaining the GAN framework to a novice, at least some clear description of the baseline architectures would be appreciated (for instance, a clearer explanation of how the SIC-GAN differs from the noisy GAN). Also the description of the soft $\ell_1$ loss (which the authors call the ""1-loss"" for some reason) would benefit from a clearer mathematical exposition.

Fourth, the experiments seem too focused on the firetruck category of the QuickDraw dataset. As it was the only example shown, it's difficult to evaluate their claim that this is a general method for improving variety without sacrificing quality. Their chosen metrics for variety and detail are somewhat subjective, as they depend on the fact that some categories in the QuickDraw dataset resemble firetrucks in the fine detail while others resemble firetrucks in outline. This is not a generalizable metric. Human evaluation of the relative quality and variety would likely suffice.

Lastly, the entire section on the strong-weak collaborative GAN seems to add nothing. They describe an entire training regiment for the model, yet never provide any actual experimental results using that model, so the entire section seems only to motivate the SIC-GAN which, again, seems like a fairly ordinary architectural extension to GANs with RNN generators.

The results presented on QuickDraw do seem nice, and to the best of my knowledge it is the first (or at least best) applications of GANs to QuickDraw - if they refocused the paper on GAN architectures for sketching and provided more generalizable metrics of quality and variety it could be made into a good paper.","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Avoiding Catastrophic States with Intrinsic Fear,"['Zachary C. Lipton', 'Kamyar Azizzadenesheli', 'Abhishek Kumar', 'Lihong Li', 'Jianfeng Gao', 'Li Deng']",Reject,2018,"[5, 3, 'no_match', 16, 19, 28]","[10, 8, 'no_match', 20, 24, 30]","[203, 90, 'no match', 170, 542, 329]","[77, 26, 'no match', 89, 263, 188]","[117, 56, 'no match', 66, 247, 36]","[9, 8, 'no match', 15, 32, 105]","The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that “DQNs  are susceptible to periodically repeating mistakes”. I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues.

The paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions. In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel. 

Still, many of the design choices appear quite arbitrary and can most likely be improved upon. In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal. Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest. The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong. 

To conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning. A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. 
","[7, 5, 5]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Reinforcement Learning via Replica Stacking of Quantum Measurements for the Training of Quantum Boltzmann Machines,"['Anna Levit', '\x06 Daniel Crawford', 'Navid Ghadermarzy', 'Jaspreet S. Oberoi', 'Ehsan Zahedinejad', 'Pooya Ronagh']",Reject,2018,"[3, 4, 6, 8, 4, 4]","[3, 4, 7, 10, 6, 9]","[3, 4, 7, 7, 6, 19]","[0, 1, 0, 2, 0, 0]","[2, 2, 5, 4, 5, 12]","[1, 1, 2, 1, 1, 7]","The paper is easy to read  for a physicist, but I am not sure how useful it would be for ICLR... it is not clear for me it there is an interest for quantum problems in this conference. This is something I will let to the Area Chair to deceede. Other than this, the paper is interesting, certainly correct, and provides a nice perspective on the future of learning with quantum computers. I like the  quantum ""boltzmann machine"" problems. 

I feel, however, but it might be a bit far from the main interest of the conference.

Comments:

* What the authors called ""Free energy-based reinforcement learning"" seems to me just the minimization / maximiation of the free energy. This is simply maximum likelihood applied to the free energy and I think that calling it ""reinforcement learning"" is not only wrong, but also is very confusing, given this is usually reserved to an entirely different learning process.

* While i liked the introduction of the quantum Boltzmann machine, I would be happy to learn what they can do? Are these useful, for instance, to study correlated fermions/bosons? The paper does not explain why one should be concerns with these devices.

* The fact that the simulation on a classical computer agrees with the one on a quantum computer is promising, but I would say that this shows that, so far, there is not yet a clear advantage in using a quantum computer. This might change, but in the mean time, what is the benefits for the ICLR community?
","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Kernel Graph Convolutional Neural Nets,"['Giannis Nikolentzos', 'Polykarpos Meladianos', 'Antoine J-P Tixier', 'Konstantinos Skianis', 'Michalis Vazirgiannis']",Reject,2018,"[4, 4, 3, 4, 26]","[9, 5, 8, 8, 31]","[55, 19, 30, 20, 316]","[28, 14, 14, 12, 181]","[21, 5, 16, 7, 77]","[6, 0, 0, 1, 58]","The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms.  These are then transformed into vector representation using graph kernels and fed to a neural network. Multiple graph kernels may serve as different channels. The approach is evaluated on synthetic and real-world graphs.

The article is well-written and easily comprehensible, but suffers from several weak points:

* Features are not learned directly from the graphs, but the approach merely weights graph kernel features.
* The weights refer to the RKHS and filters are not easily interpretable.
* The approach is similar in spirit to Niepert, Ahmed, Kutzkov, ICML 2016 and thus incremental.
* The experiments are not convincing: The improvement over the existing work is small on real-world data sets. The synthetic classification task essentially is to distinguish a clique from star graph and not very meaningful. Moreover, a comparison to at least one of the recent approaches similar to ""Convolutional Networks on Graphs for Learning Molecular Fingerprints"" (Duvenaud et al., NIPS 2015) or ""Message Passing Neural Networks"" (Gilmer et al., 2017)  would be desirable.

Therefore, I cannot recommend the paper for acceptance.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
On the Use of Word Embeddings Alone to Represent Natural Language Sequences,"['Dinghan Shen', 'Guoyin Wang', 'Wenlin Wang', 'Martin Renqiang Min', 'Qinliang Su', 'Yizhe Zhang', 'Ricardo Henao', 'Lawrence Carin']",Reject,2018,"[2, 2, 16, 12, 10, 17, 15, 22]","[6, 7, 20, 17, 15, 22, 20, 27]","[66, 65, 64, 79, 61, 107, 136, 602]","[31, 28, 33, 37, 30, 55, 72, 306]","[34, 36, 24, 32, 23, 51, 51, 173]","[1, 1, 7, 10, 8, 1, 13, 123]","This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models. Average and/or max pooling over word embeddings (which are initialized from pretrained embeddings) is used to obtain a fixed-length representation for natural language sequences, which is then fed through a single layer MLP classifier. In many of the 9 evaluation tasks, this approach is found to match or outperform single-layer CNNs or RNNs.

The varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis.

My only criticism would be the fact that the study is limited to English, even though the conclusions are explicitly scoped in light of this. Moreover, I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets.

Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets!

Minor things:
- It wasn't entirely clear how the text matching tasks are encoded. Are the two sequences combined into a single sequence before applying the model, or something else? I might have missed this detail.

- Given the two ways of using the Glove embeddings for initialization (direct update vs mapping them with an MLP into the task space), it would be helpful to know which one ended up being used (i.e. optimal) in each setting.

- Something went wrong with the font size for the remainder of the text near Figure 1.

** Update **
Thanks for addressing my questions in the author response.

After following the other discussion thread about the novelty claims, I believe I didn't weigh that aspect strongly enough in my original rating, so I'm revising it. I remain of the opinion that this paper offers a useful systematic comparison that goes sufficiently beyond the focus of the two related papers mentioned in that thread (fasttext and Parikh's).
","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Discovering the mechanics of hidden neurons,"['Simon Carbonnelle', 'Christophe De Vleeschouwer']",Reject,2018,"[1, 23]","[5, 28]","[5, 164]","[2, 88]","[3, 33]","[0, 43]","--------------------
Review updates:
Rating 6 -> 7
Confidence 2 -> 4

The rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I’ve increased my score.
--------------------

I want to love this paper. The results seem like they may be very important. However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions. I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained.

Unfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1. Without understanding this first result, it’s difficult to decide to what extent the rest of the paper’s results are to be believed.

Fig 1 shows “the histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers.” Let’s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons? For now let’s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value. The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs. Let’s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, …]. Now what do we do with this list? As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1. But we can’t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect?

Further in this direction, Section 4.1 claims “Zero partial derivatives are ignored to make the signal more clear.” Are these zero partial derivatives of the post-relu or pre-relu? The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once). Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? In this case we would be excluding a large set (about half!) of the gradient values, and it didn’t seem from the context in the paper that this would be desirable.

It would be great if the above could be addressed. Below are some less important comments.

Sec 5.1: great results!

Fig 3: This figure studies “the first and last layers of each network”. Is the last layer really the last linear layer, the one followed by a softmax? In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). Or is the layer shown (e.g. “stage3layer2”) the penultimate layer? Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from.

Sec 5.2 states “neuron partitions the inputs in two distinct but overlapping categories of quasi equal size.” This experiment only shows that this is true in aggregate, not for specific neurons? I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct? Perhaps this statement could be qualified.

Table 1: “52th percentile vs actual 53 percentile shown”. 

> Table 1: The more fuzzy, the higher the percentile rank of the threshold

This is true for the CIFAR net but the opposite is true for ResNet, right?
","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
TD Learning with Constrained Gradients,"['Ishan Durugkar', 'Peter Stone']",Reject,2018,"[6, 29]","[10, 34]","[23, 671]","[12, 437]","[11, 121]","[0, 113]","Summary: This paper tackles the issue of combining TD learning methods with function approximation. The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule. Results with linear and non-linear function approximation highlight the attributes of the method.

Quality: The quality of the writing, notation, motivation, and results analysis is low. I will give a few examples to highlight the point. The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed. There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma. This is a very active area of research and the paper needs to justify their approach. The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar. The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others.

The experiments have serious issues. Exp1 seems to indicate that the new method does not converge to the correct solution. The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed. Again exp3 provides little information about the experimental setup.

Clarity: The clarity of the text is fine, though errors make things difficult sometimes. For example The Bhatnagar 2009 reference should be Maei.
 
Originality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable. 

Significance: Because of all the things outlined above, the significance is below the bar for this round. ","[2, 3, 4]","[' Strong rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
A Neural Method for Goal-Oriented Dialog Systems to interact with Named Entities,"['Janarthanan Rajendran', 'Jatin Ganhotra', 'Xiaoxiao Guo', 'Mo Yu', 'Satinder Singh']",Reject,2018,"[4, 4, 6, 10, 28]","[9, 8, 11, 15, 33]","[31, 29, 86, 187, 291]","[11, 13, 39, 86, 189]","[18, 13, 38, 93, 72]","[2, 3, 9, 8, 30]","Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation. Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks. This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments. 

The proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process. All we have a besides text is a small figure (figure 1). Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies?

The initial description (section 2)  leaves way too many unanswered questions:
- What embeddings are used for words detected as NE? Is it the same as the generated representation?
- What is the exact mechanism of generating a representation for NE EECS545? (end of page 2)
- Is it correct that the same representation stored in the NE table is used twice? (a) To retrieve the key (a vector) given the value (a string)  as the encoder input. (b) To find the value that best matches a key at the decoder stage?
- Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive?
- How is the system supervised? Do we need to give the name of the column the Attention-Column-Query attention should focus on? Because of this unknown, I could not understand the experiment setup and data formatting!

The list goes on...

For such a complex architecture, the authors must try to analyze separate modules as much as possible. As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level

The Q&A task could be used to describe a simpler system with only a decoder accessing the DB table. Complexity for solving the Babi tasks could be added later.
","[3, 6, 4]","[' Clear rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
On Optimality Conditions for Auto-Encoder Signal Recovery,"['Devansh Arpit', 'Yingbo Zhou', 'Hung Q. Ngo', 'Nils Napp', 'Venu Govindaraju']",Reject,2018,"[8, 9, 20, 14, 30]","[13, 14, 25, 18, 35]","[45, 93, 127, 46, 352]","[20, 41, 63, 32, 261]","[25, 46, 29, 5, 12]","[0, 6, 35, 9, 79]","*Summary*
The paper studies recovery guarantees within the context of auto-encoders. Assuming a noise-corrupted linear model for the inputs x's, the paper looks at some sufficient properties (e.g., over the generating dictionary denoted by W) to recover the true underlying sparse signals (denoted by h). Several settings of increasing complexity are considered (from binary signals with no noise to noisy continuous signals). Evaluations are carried out on synthetic examples to highlight the theoretical findings.

The paper is overall difficult to read. Moreover, and importantly, no algorithmic perspectives are presented in the paper, in the sense that we do not know whether practical procedures would lead to W's satisfying the appropriate properties (unlike (not-mentioned) recent results for dictionary learning/ICA; see detailed comments). Also, assumptions are made (e.g., knowledge about expectations of h and x) for which it is unclear to see how practical/limiting they are. Finally (and as further discussed below), the paper does not sufficiently discuss related work.

(note: I have not reviewed the appendix and supplementary material)

*Detailed comments*

-I think there is an insufficient literature review about recent recovery results in the context of sparse coding, dictionary learning and ICA (see some references at the bottom of the review). I think this is all the more important as the paper tries to draw connections with ICA (see Sec. 4.4).
Given that the paper positions itself on a theoretical level, detailed comparisons with existing sample complexities obtained in previous work for related models (e.g., sparse coding, dictionary learning and ICA) must be provided.

-To the best of my understanding of the paper, the guarantees are about h_hat and the true h. It therefore seems that the paper's approach is very close to standard sparse inverse problems, up to the difference due to the (non-identity) activation function. If this is indeed the case, the paper should discuss its results when the activation is identity to see whether known results are recovered. 

-""...we consider linear activation s_d because it is a more general case."": Just after this statement, it is mentioned that non-linear activations are used in practice. Could this statement be therefore clarified?

-Sec. 2 is unclear. For instance, it is not easy to see how one go from (1) to (2). Moreover, the concept of ""AE framework"" is not well defined.

-In the bottom of page 3, why are p_i and (1-p_i) discarded?

-In practice, how can we set the appropriate value of b_i?

-What is the practical sense of being able to have access to E_h[x], E_x[x], and E_h[h]?

-In Proposition 1 and 2, if the noise e is indeed random, it means the right-hand sides are also random variables. Then, what does the probability statement Pr mean on the left-hand side? Is is conditioned on the draw of e? Some clarifications are required.

-Typo page 7: ""...that used to generate the data."" --> ""... used to generate the data.""
-Typo page 9: ""...data are then generate..."" --> ""...data are then generated...""

-In Sec. 5.3, to match W_hat and W, the Hungarian algorithm can probably be used.

*References*

(Arora2012) Arora, S.; Ge, R.; Moitra, A. & Sachdeva, S. Provable ICA with unknown Gaussian noise, with implications for Gaussian mixtures and autoencoders Advances in Neural Information Processing Systems (NIPS), 2012, 2375-2383

(Arora2013) Arora, S.; Ge, R. & Moitra, A. New algorithms for learning incoherent and overcomplete dictionaries preprint arXiv:1308.6273, 2013

(Chatterji2017) Chatterji, N. S. & Bartlett, P. L. Alternating minimization for dictionary learning with random initialization preprint arXiv:1711.03634, 2017

(Gribonval2015) Gribonval, R.; Jenatton, R. & Bach, F. Sparse and spurious: dictionary learning with noise and outliers IEEE Transactions on Information Theory, 2015, 61, 6298-6319

(Sun2015) Sun, J.; Qu, Q. & Wright, J. Complete dictionary recovery over the sphere Sampling Theory and Applications (SampTA), 2015 International Conference on, 2015, 407-410
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Tree2Tree Learning with Memory Unit,"['Ning Miao', 'Hengliang Wang', 'Ran Le', 'Chongyang Tao', 'Mingyue Shang', 'Rui Yan', 'Dongyan Zhao']",Reject,2018,"[1, 'no_match', 0, 2, 1, 15, 'no_match']","[6, 'no_match', 3, 7, 6, 20, 'no_match']","[18, 'no match', 5, 88, 17, 365, 'no match']","[8, 'no match', 5, 44, 10, 158, 'no match']","[10, 'no match', 0, 42, 7, 44, 'no match']","[0, 'no match', 0, 2, 0, 163, 'no match']","This paper presents a model to encode and decode trees in distributed representations. 
This is not the first attempt of doing these encoders and decoders. However, there is not a comparative evalution with these methods.
In fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see ""Decoding Distributed Tree Structures"" and ""Distributed tree kernels"".
The paper should present a comparison with such kinds of models.
","[4, 5, 2]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Convergence rate of sign stochastic gradient descent for non-convex functions,"['Jeremy Bernstein', 'Kamyar Azizzadenesheli', 'Yu-Xiang Wang', 'Anima Anandkumar']",Reject,2018,"[2, 3, 8, 13]","[7, 8, 13, 18]","[23, 90, 143, 419]","[10, 26, 62, 154]","[13, 56, 68, 223]","[0, 8, 13, 42]","The paper presents convergence rate of a quantized SGD, with biased quantization - simply taking a sign of each element of gradient.

The stated Theorem 1 is incorrect. Even if the stated result was correct, it presents much worse rate for a weaker notion of convergence.

Major flaws:
1. As far as I can see, Theorem 1 should depend on 4th root of N_K, the last (omitted) step from the proof is done incorrectly. This makes it much worse than presented.
2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11). That is enormous difference, particularly in settings where such gradient compression can be relevant. Also, it is lot more worse than just d times:
3. Again in eq (11), you compare different notions of convergence - E[||g||_1]^2 vs. E[||g||_2^2]. In particular, the one for signSGD is the weaker notion - squared L1 norm can be d times bigger again. If this is not the case for some reason, more detailed explanation is needed.

Other than that, the paper contains several attempts at intuitive explanation, which I don't find correct. Inclusion of Assumption 3 would in particular require better justification.

Experiments are also inconclusive, as the plots show convergence to significantly worse accuracy than what the models converged to in original contributions.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']"
Deep Lipschitz networks and Dudley GANs,"['Ehsan Abbasnejad', 'Javen Shi', 'Anton van den Hengel']",Reject,2018,"[8, 12, 21]","[13, 17, 26]","[90, 170, 398]","[40, 67, 166]","[41, 61, 149]","[9, 42, 83]","The authors propose a different type of GAN--the Dudley GAN--that is related to the Dudley metric. In fact, it is very much like the WGAN, but rather than just imposing the function class to have a bounded gradient, they also impose it to be bounded itself. This is argued to be more stable than the WGAN, as gradient clipping is said not necessary for the Dudley GAN. The authors empirically show that the Dudley GAN achieves a greater LL than WGAN for the MNIST and CIFAR-10 datasets.

The main idea [and its variants] looks solid, but with the plethora of GANs in the literature now, after reading I'm still left wondering why this GAN is significantly better than others [BEGAN, WGAN, etc.]. It is clear that imposing the quadratic penalty in equation (3) is really the same constraint as the Dudley norm? The big contribution of the paper seems to be that adding some L_inf regularization to the function class helps preclude gradient clipping, but after reading I'm unsure why this is ""the right thing"" to do in this case. We know that convergence in the Wasserstein metric is stronger than the Dudley metric, so why is using the weaker metric overweighed by the benefits in training?

Nits: Since the function class is parameterized by a NN, the IPM is not actually the Dudley metric between the two distributions. One would have to show that the NN is dense in Dudley unit ball w.r.t. L_inf norm, but this sort of misnaming had started with the ""Wasserstein"" GAN.","[5, 8, 5]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]"
Reward Estimation via State Prediction,"['Daiki Kimura', 'Subhajit Chaudhury', 'Ryuki Tachibana', 'Sakyasingha Dasgupta']",Reject,2018,"[6, 5, 18, 7]","[10, 10, 21, 10]","[34, 46, 60, 34]","[20, 19, 37, 12]","[14, 22, 19, 15]","[0, 5, 4, 7]","The authors propose to solve the inverse reinforcement learning problem of inferring the reward function from observations of a behaving agent, i.e. trajectories, albeit without observing state-action pairs as is common in IRL but only with the state sequences. This is an interesting problem setting. But, apparently, this is not the problem the authors actually solve, according to eq. 1-5. Particularly eq. 1 is rather peculiar. The main idea of RL in MDPs is that agents do not maximize immediate rewards but instead long term rewards. I am not sure how this greedy action should result in maximizing the total discounted reward along a trajectory. 
Equation 3 seems to be a cost function penalizing differences between predicted and observed states. As such, it implements a sort of policy imitation, but that is quite different from the notion of reward in RL and IRL. Similarly, equation 4 penalizes differences between predicted and observed state transitions. 
Essentially, the current manuscript does not learn the reward function of an MDP in the RL setting, but it learns some sort of a shaping reward function to do policy imitation, i.e. copy the behavior of the demonstrator as closely as possible. This is not learning the underlying reward function. So, in my view, the manuscript does a nice job at policy fitting, but this is not reward estimation. The manuscript has to be rewritten that way. 
One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say:
C. A. Rothkopf, C. Dimitrakakis. Preference elicitation and inverse reinforcement learning. ECML 2011
Overall the manuscript leverages on deep learning’s power of function approximation and the simulation results are nice, but in terms of the soundness of the underlying RL and IRL theory there is some work to do.","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Convolutional Mesh Autoencoders for 3D Face Representation,"['Anurag Ranjan', 'Timo Bolkart', 'Michael J. Black']",Reject,2018,"[4, 7, 33]","[9, 11, 38]","[40, 55, 383]","[19, 25, 205]","[20, 24, 116]","[1, 6, 62]","Paper summary:
Authors extend [1] to form an auto-encoder CNN network for face mesh representation. Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space. Chebyshev polynomial is used for faster computations. Max pooling on graph is done by using Graclus multilevel clustering algorithm. Binary tree generated in pooling layers are kept for unpooling layers in decoder network. Authors captured a new facial dataset for their evaluation and reported better results than PCA.

Positive points:
Authors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning.

Negative points:
Although proposed idea is interesting, paper has a number of critical problems. Firstly, experiments are the main weakness of the paper. Set of experiments does not prove claims of the paper. 
- It is not clear how authors uses PCA to reconstruct faces in the test set.
- Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE). 
- How network behaves by introducing noise on vertices?
- What is the effect of network hyper-parameters?

Secondly, paper has a lack of novelty. It is a simple extension of [1] without considering and solving problems in [1]. Also, it is not mentioned what is the loss function to train the network. I suppose it is L2 norm loss, but it must be clear in the paper.

[1] M. Defferrard, X. Bresson, and P. Vandergheynst.  Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016.
[2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces. In European Conference on Computer Vision, pp. 297–312, 2014a.

After rebuttal:
The current version of the paper still needs significant amount of work regarding the experimental part.","[2, 6, 4]","[' Strong rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,"['Prameesha Sandamal Weerasinghe', 'Tansu Alpcan', 'Sarah Monazam Erfani', 'Christopher Leckie']",Reject,2018,"['no_match', 18, 8, 28]","['no_match', 23, 13, 33]","['no match', 216, 127, 308]","['no match', 123, 73, 190]","['no match', 37, 45, 36]","['no match', 56, 9, 82]","Although the problem addressed in the paper seems interesting, but there lacks of evidence to support some of the arguments that the authors make. And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference. Detailed critiques are as following:
1. The idea proposed by the authors seems too quite simple. It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM.
2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5. It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change.
3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that. And PCA as a non-random projection would a nice baseline to compare against.
4. The paper seems to misuse the term “False positive rate” as the y label of figure 3(d/e/f). The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive. So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a). From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified. The paper should come up with a better term for that evaluation.
5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d). Need more experiment on more dimensionality to prove that. 
6. In the appendix B results part, sometimes the word ’S_attack’ is typed wrong. And the values in  “distorted/distorted” columns in Table 5 do not match up with the ones in Figure 3(c).","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Learning to navigate by distilling visual information and natural language instructions,"['Abhishek Sinha', 'Akilesh B', 'Mausoom Sarkar', 'Balaji Krishnamurthy']",Reject,2018,"[8, 1, 13, 31]","[13, 2, 18, 36]","[102, 2, 28, 97]","[38, 1, 12, 45]","[48, 1, 16, 51]","[16, 0, 0, 1]","Interesting Problem, but Limited Novelty and Flawed Evaluation


The paper considers the problem of following natural language instructions given an first-person view of an a priori unknown environment. The paper proposes a neural architecture that employs an RNN to encode the language input and a CNN to encode the visual input. The two modalities are fused and fed to an RNN policy network. The method is evaluated on a new dataset consisting of short, simple instructions conveyed in simple environments.

The problem of following free-form navigation instructions is interesting and has achieved a fair bit of attention, previously with ""traditional"" structured approaches (rule-based and learned) and more recently with neural models. Unlike most existing work, this paper reasons over the raw visual input (vs., a pre-processed representation such as a bag-of-words model). HoA notable exception is the work of Chaplot et al. 2017, which addresses the same problem with a nearly identical architecture (see below). Overall, this paper constitutes a reasonable first-pass at this problem, but there is significant room for improvement to address issues related to the stated contributions and flawed evaluations.

The paper makes several claims regarding the novelty and expressiveness of the model and the contributions of the paper that are either invalid or not justified by the experimental results. As noted, a neural approach to instruction following is not new (see Mei et al. 2016) nor is a multimodal fusion architecture that incorporates raw images (see Chaplot et al.). The paper needs to make the contributions and novelty relative to existing methods clear (e.g., those stated in the intro are nearly identical to those of Mei et al. and Chaplot et al.). This includes discussion of the attention mechanism, for which the contributions and novelty are justified only by simple visualizations that are not very insightful. Related, the paper omits a large body of work in language understanding from the NLP and robotics domains, e.g., the work of Yoav Artzi, Thomas Howard, and Stefanie Tellex, among others (see below). While the approaches are different, it is important to describe this work in the context of these methods.


There are important shortcomings with the evaluation. First, one of the two scenarios involves testing on instructions from the training set. The test set should only include held-out environments and instructions, which the paper incorrectly refers to as the ""zero-shot"" scenario. This test set is very small, with only 19 instructions. Related, there is no mention of a validation set, and the discussion seems to suggest that hyperparameters were tuned on the test set. Further, the method is compared to incomplete implementations of existing baselines that admittedly don't attempt to replicate the baseline architectures. Consequently, it isn't clear what if anything can be concluded from the evaluation. There is a



Comments/Questions

* The action space does not include an explicit stop action. Instead, a run is considered to be finished either when the agent reaches the destination or a timeout is exceeded. This is clearly not valid in practice. The model should determine when to stop, as with existing approaches.

* The paper makes strong claims regarding the sophistication of the dataset that are unfounded. Despite the claims, the environment is rather small and the instructions almost trivially simple. For example, compare to the SAIL corpus that includes multi-sentence instructions with an average of 5 sentences/instruction (vs. 2); 37 words/instruction (vs. a manual cap of 9); and a total of 660 words (vs. 40); and three ""large"" virtual worlds (vs. 10x10 grids with 3-6 objects).

* While the paper makes several claims regarding novelty, the contributions over existing approaches are unclear. For example, Chaplot et al. 2017 propose a similar architecture that also fuses a CNN-based representation of raw visual input with an RNN encoding of language, the result of which is fed to a RNN policy network. What is novel with the proposed approach and what are the advantages? The paper makes an incomplete attempt to evaluate the proposed model against Chaplot et al., but without implementing their complete architecture, little can be inferred from the comparison.

* The paper claims that the fusion method realizes a *minimalistic* representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn't clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?).

* It isn't clear that much can be concluded from the attention visualizations in Figs. 6 and 7, particularly regarding its contribution. Regarding Fig 6. the network attends to the target object (large apple), but not the smaller apple, which would be necessary to reason over their relative size. Further, the attention figure in Fig. 7(b) seems to foveate on both bags. In both cases, the distractor objects are very close to the true target, and one would expect the behavior to be similar irrespective of which one was being attended to.

* The conclusion states that the method is ""highly flexible"" and able to handle a ""rich set of natural language instructions"". Neither of these claims are justified by the discussion (please elaborate on what makes the method ""highly flexible"", presumably the end-to-end nature of the architecture) or the experimental results.

* The significance of randomly moving non-target objects that the agent encounters is unclear. What happens when the objects are not moved, as in real scenarios?

* A stated contribution is that the ""textual representations are semantically meaningful"" but the importance is not justified.

* Figure captions should appear below the figure, not at top.

* Figures and tables should appear as close to their first reference as possible (e.g., Table 1 is 6 pages away from its reference at the beginning of Section 7).


* Many citations should be enclosed in parentheses.



References:

* Artzi and Zettlemoyer, Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions, TACL 2013

* Howard, Tellex, and Roy, A Natural Language Planner Interface for Mobile Manipulators, ICRA 2014

* Chung, Propp, Walter, and Howard, On the performance of hierarchical distributed correspondence graphs for efficient symbol grounding of robot instructions, IROS 2015

* Paul, Arkin, Roy, and Howard, Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators, RSS 2016

* Tellex, Kollar, Dickerson, Walter, Banerjee, Teller and Roy, Understanding natural language commands for robotic navigation and mobile manipulation, AAAI 2011","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,"['Anderson Y. Zhang', 'Miao Lu', 'Deguang Kong', 'Jimmy Yang']",Reject,2018,"[4, 16, 12, 8]","[9, 21, 17, 13]","[16, 41, 51, 5]","[1, 18, 40, 1]","[12, 9, 7, 1]","[3, 14, 4, 3]","Minor comments:
- page 3. “The observation equation and transition equations together (i.e., Equation (1,2,3)) together define “ - one “together” should be removed
- page 4. “From Figure 2, the joint distribution (i.e., the likelihood function ” - there should be additional bracket
- page 7. “We can further integral out αn “ -> integrate out

Major comments:
The paper is well-written. The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.

Such type of parametric models are widely considered in econometric literature, see e.g.
[1] Jalles, João Tovar, Structural Time Series Models and the Kalman Filter: A Concise Review (June 19, 2009). FEUNL Working Paper No. 541. Available at SSRN: https://ssrn.com/abstract=1496864 or http://dx.doi.org/10.2139/ssrn.1496864 
[2] Jacques J. F. Commandeur, Siem Jan Koopman, Marius Ooms. Statistical Software for State Space Methods // May 2011, Volume 41, Issue 1.
[3] Scott, Steven L. and Varian, Hal R., Predicting the Present with Bayesian Structural Time Series (June 28, 2013). Available at SSRN: https://ssrn.com/abstract=2304426 or http://dx.doi.org/10.2139/ssrn.2304426 
[4] Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, Farshid Vahid-Araghi, Forecasting time series with multiple seasonal patterns, In European Journal of Operational Research, Volume 191, Issue 1, 2008, Pages 207-222, ISSN 0377-2217, https://doi.org/10.1016/j.ejor.2007.08.024.
[5] A.C. Harvey, S. Peters. Estimation Procedures for structural time series models // Journal of Forecasting, Vol. 9, 89-108, 1990
[6] A. Harvey, S.J. Koopman, J. Penzer. Messy Time Series: A Unified approach // Advances in Econometrics, Vol. 13, pp. 103-143.

They also use Kalman filter and MCMC-based approaches to sample posterior to estimate hidden components.

There are also non-parametric approaches to extraction of components from quasi-periodic time-series, see e.g.
[7] Artemov A., Burnaev E. Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence // 16th International Conference on Data Mining Workshops (ICDMW), IEEE Conference Publications, pp. 29 - 36, 2016. DOI: 10.1109/ICDMW.2016.0013
[8] Alexey Artemov, Evgeny Burnaev and Andrey Lokot. Nonparametric Decomposition of Quasi-periodic Time Series for Change-point Detection // Proc. SPIE 9875, Eighth International Conference on Machine Vision, 987520 (December 8, 2015); 5 P. doi:10.1117/12.2228370;http://dx.doi.org/10.1117/12.2228370

In some of these papers models of structural brakes and change-points are also considered, see e.g. 
- page 118 in [6]
- papers [7, 8]

There were also Bayesian approaches for change-point detection, which are similar to the model of change-point, proposed in the considered paper, e.g.
[9] Ryan Prescott Adams, David J.C. MacKay. Bayesian Online Changepoint Detection // https://arxiv.org/abs/0710.3742
[10] Ryan Turner, Yunus Saatçi, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. In Zaïd Harchaoui, editor, NIPS Workshop on Temporal Segmentation, Whistler, BC, Canada, December 2009.

Thus,
- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,
- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,
- the proposed model itself looks very similar to what can be found across econometric literature,
- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.

Therefore, also the paper is well-written, 
- it lacks novelty,
- its topic does not perfectly fit topics of interest for ICLR,
So, I do not recommend this paper to be published.","[4, 5, 6]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,"['Melike Nur Mermer', 'Mehmet Fatih Amasyali']",Reject,2018,"['no_match', 13]","['no_match', 18]","['no match', 60]","['no match', 39]","['no match', 6]","['no match', 15]","This paper addresses an interesting problem of curriculum/self-paced versus random order of samples for faster learning. Specifically, the authors argue that adding samples in random order is as beneficial as adding them with some curriculum strategy, i.e. from easiest to hardest, or reverse. 
The main learning strategy considered in this work is learning with growing sets, i.e. at each next stage a new portion of samples is added to the current available training set. At the last stage, all training samples are considered. The classifier is re-learned on each stage, where optimized weights in the previous stage are given as initial weights in the next stage. 

The work has several flaws. 
-First of all, it is not surprising that learning with more training samples at each next stage (growing sets) gets better - this is the basic principle of learning. The question is how fast the current classifier converges to the optimal Bayes level when using Curriculum strategy versus Random strategy. The empirical evaluations do not show evidence/disprove regarding this matter. For example, it could happen that the classifier converges to the optimal on the first stage already, so there is no difference when training in random versus curriculum order with growing sets. 
-Secondly, easyness/hardness of the samples are defined w.r.t. some pre-trained (external) ensemble method. It is not clear how this definition of easiness/hardness translates when training the 3-layer neural network (final classifier). For example, it could well happen that all the samples are equally easy for training the final classifier, so the curriculum order would be the same as random order. In the original work on self-paced learning, Kumar et al (2010), easiness of the samples is re-computed on each stage of the classifier learning. 
-The empirical evaluations are not clear. Just showing the wins across datasets without actual performance is not convincing (Table 2). 
-I wonder whether the section with theoretical explanation is needed. What is the main advantage of learning with growing sets (when re-training the classifier)  and (traditional) learning when using the whole training dataset (last stage, in this work)? 

","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']"
Dense Recurrent Neural Network with Attention Gate,"['Yong-Ho Yoo', 'Kook Han', 'Sanghyun Cho', 'Kyoung-Chul Koh', 'Jong-Hwan Kim']",Reject,2018,"[12, 15, 'no_match', 24]","[16, 16, 'no_match', 29]","[21, 19, 'no match', 262]","[10, 14, 'no match', 170]","[1, 0, 'no match', 19]","[10, 5, 'no match', 73]","The authors propose an RNN that combines temporal shortcut connections from [Soltani & Jang, 2016] and Gated Recurrent Attention [Chung, 2014]. However, their justification about the novelty and efficacy of the model is not well demonstrated in the paper. The experiment part is modest with only one small dataset Penn Tree Bank is used. The results are not significant enough and no comparisons with models in [Soltani & Jang, 2016] and [Chung, 2014] are provided in the paper to show the effectiveness of the proposed combination. To conclude, this paper is an incremental work with limited contributions.

Some writing issues:
1. Lack of support in arguments,
2. Lack of referencing to previous works. For example, the sentence “By selecting the same dropout mask for feedforward, recurrent connections, respectively, the dropout can apply to the RNN, which is called a variational dropout” mentions “variational dropout” with no citing. Or “NARX-RNN and HO-RNN increase the complexity by increasing recurrent depth. Gated feedback RNN has the fully connection between two consecutive timesteps” also mentions a lot of models without any references at all.
3. Some related papers are not cited, e.g., Hierarchical Multiscale Recurrent Neural Networks [Chung, 2016]
","[4, 2, 4]","[' Ok but not good enough - rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER,['Mahdi Azarafrooz'],Reject,2018,[9],[9],[10],[3],[4],[3],"This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. This is equivalent to adding noise to the gradient update, using the duality of reproducing kernels. Unfortunately, the evaluation here is wholly unsatisfactory to justify the manuscript's claims. No concrete practical algorithm specification is given (only a couple of ideas to inject noise listed), only a qualitative one on a 2-dimensional latent space in MNIST, and an inconclusive one using the much-doubted Parzen window KDE method. The idea as stated in the abstract and introduction may well be worth pursuing, but not on the evidence provided by the rest of the manuscript.","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
GraphGAN: Generating Graphs via Random Walks,"['Aleksandar Bojchevski', 'Oleksandr Shchur', 'Daniel Zügner', 'Stephan Günnemann']",Reject,2018,"[2, 2, 4, 10]","[7, 6, 9, 15]","[38, 15, 39, 274]","[17, 6, 18, 145]","[19, 9, 20, 104]","[2, 0, 1, 25]","This paper proposes a WGAN formulation for generating graphs based on random walks. The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks.

The model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models. 

The primary claims of the paper are as follows:
i) The proposed approach is a generative model of graphs, specifically producing ""sibling"" graphs
ii) The learned latent representation provides an interpretation of generated graph properties
iii) The model generalizes well in terms of link and node classification

The proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs). However, from a graph generation perspective, the problem formulation and evaluation do not sufficiently demonstrate the utility of proposed method. 

First, wrt claim (i) the problem of generating ""sibling"" graphs is ill-posed. Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution. The notion of a ""sibling"" graph used in this paper is not clearly defined, but it seems to only be useful if the sibling graphs are likely under the distribution. Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated. On the other hand, since many of the edges are shared the ""siblings"" may be nearly isomorphic to the input graph, which is not useful from a graph modeling perspective. 

For claim (i), the comparison to related work is far from sufficient to demonstrate its utility as a graph generation model. There are many graph models that are superior to DC-SBM, including KPGMs, BETR, ERGMs, hierarchical random graph models and latent space models. Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings. 

Next, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics. However, since the properties are measured based on a set of random walks it is still difficult to interpret the impact on the generated graphs (since an arbitrary node in the final graph will have some structure determined from each of the regions). Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)?   

Lastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize. This is the weakest claim of the paper. The learned node embeddings appear to do significantly worse than node2vec, and the full model is worse than DC-SBM. Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive. 

Overall, while the paper includes a wide range of experimental evaluation, they are aimed too broadly (and the results are too weak) to support any specific claim of the work. If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding). If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).  
","[4, 6, 7]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Pointing Out SQL Queries From Text,"['Chenglong Wang', 'Marc Brockschmidt', 'Rishabh Singh']",Reject,2018,"[9, 9, 10]","[14, 14, 14]","[129, 75, 133]","[59, 41, 62]","[36, 29, 50]","[34, 5, 21]","This paper proposes a model for solving the WikiSQL dataset that was released recently.

The main issues with the paper is that its contributions are not new.

* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time. I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular. Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017.

So at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution.

* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017. If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results.

Moreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold. the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. The authors re-invent this and find it works better than randomly choosing a gold token or taking the max. But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing.

* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance. In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.","[3, 4, 7]","[' Clear rejection', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Semi-supervised Outlier Detection using Generative And Adversary Framework,"['Jindong Gu', 'Matthias Schubert', 'Volker Tresp']",Reject,2018,"[1, 19, 29]","[6, 24, 34]","[39, 111, 308]","[13, 84, 166]","[26, 18, 105]","[0, 9, 37]","The idea of the paper is to use a GAN-like training to learn a novelty detection approach. In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers. To establish such a behavior, the authors propose early stopping as well as other heuristics. 

I like the idea of the paper, however, this paper needs a revision in various aspects, which I simply list in the following:
* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc
* The model selection using the AUC of ""inlier accepted fraction"" is not well motivated in my opinion. This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data. The latter is important for the GAN-like training.
* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. 
* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)
* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)
* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)


Minor comments:
* Citations should be fixed (use citep to enclose them in ())
* The term ""AI-related task"" sounds a bit too broad
* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection.
* Where is Table 1?
* There are quite a lot of typos.

*After revision statement*
I thank the authors for their revision, but I keep my rating. The clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Graph Topological Features via GAN,"['Weiyi Liu', 'Hal Cooper', 'Min-Hwan Oh']",Reject,2018,"[27, 4]","[32, 8]",['skipped'],['skipped'],['skipped'],['skipped'],"The authors try to combine the power of GANs with hierarchical community structure detections. While the idea is sound, many design choices of the system is questionable. The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers. I do not recommend the acceptance of this draft.

Compared with GAN, traditional graph analytics is model-specific and non-adaptive to training data. This is also the case for hierarchical community structures. By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic. In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm. 

The evaluation remains superficial with minimal quantitative comparisons. Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic. They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs. 

The writing of the draft leaves much to be desired. The description of the architecture is confusing with design choices never clearly explained. Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification. Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services.","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Networks,['Hiroshi Inoue'],Reject,2018,[22],[26],[95],[40],[4],[51],"In this paper it is described a method that can be used to speed up the prediction process of ensembles of classifiers that output probabilistic predictions. The method proposed is very simple and it is based on the observation that in the case that the individual predictors are very sure about the potential class label, ensembling many predictions is not particularly useful. It seems it is most useful when the individual classifier are most unsure, as measured by the output probabilities. The idea proposed by the authors is to compute an estimate of the probability that the class with the highest probability will not change after querying more predictors from the ensemble. This estimate is obtained by using a t-student distribution for the distribution of the average maximum probability.

The paper is generally well written with a few mistakes that can be easily corrected using any spell checking tool.

The experiments carried out by the authors are convincing. It seems that their proposed approach can speed up the predictions of the ensemble by an important factor. The benefits of using ensemble methods are also evident, since they always improve the performance of a single classifier.

As far as I know this work is original. However, it is true that several similar ensemble pruning techniques exist for multi-class problems in which one uses majority voting for computing the combined prediction of the ensemble. Therefore it is unclear what are the advantages of the proposed method with respect to those ones. This is, in my opinion, the weakest point of the paper.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Neural Tree Transducers for Tree to Tree Learning,"['João Sedoc', 'Dean Foster', 'Lyle Ungar']",Reject,2018,"[3, 0, 31]","[8, 3, 36]","[70, 7, 226]","[32, 5, 138]","[36, 2, 38]","[2, 0, 50]","The authors propose to tackle the tree transduction learning problem using recursive NN architectures: the prediction of a node label is conditioned on the ancestors sequence and the nodes in the left sibling subtree  (in a serialized order)
Pros:
- they identify the issue of locality as important (sequential serialization distorts locality) and they move the architecture closer to the tree structure of the problem
- the architecture proposed moves the bar forward in the tree processing field
Cons: 
- there is still a serialization step (depth first) that can potentially create sharp dips to null probabilities for marginal changes in the conditioning sequence (the issue is not addressed or commented by the authors) 
- the experimental setup lacks a perturbation test: rather than a copy task, it would be of greater interest to assess the capacity to recover from noise in the labels (as the noise magnitude increases)
- a clearer and more articulated comparison of the pros/cons w.r.t. competitive architectures would improve the quality of the work: what are the properties (depth, vocabulary size, complexity of the underlying generative process, etc) that are best dealt with by the proposed approach? 
- it is not clear if the is the vocabulary size in their model needs to increase exponentially with the tree depth: a crucial vocabulary size  vs performance experiment is missing
","[7, 2, 3]","[' Good paper, accept', ' Strong rejection', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"
Explaining the Mistakes of Neural Networks with Latent Sympathetic Examples,"['Riaan Zoetmulder', 'Efstratios Gavves', ""Peter O'Connor""]",Reject,2018,"[-3, 9, 22]","[1, 14, 23]","[1, 137, 32]","[0, 64, 20]","[0, 62, 5]","[1, 11, 7]","Summary: The authors propose a method for explaining why neural networks make mistakes by learning how to modify an image on a mistaken classification to make it a correct classification. They do this by perturbing the image in an encoded latent space and then reconstructing the perturbed image.  The explanation is the difference between the reconstructed perturbed encoded image and the reconstructed original encoded image.

The title is too general as this paper only offers an explanation in the area of image classification, which by itself, is still interesting.  

A method for explaining the results of neural networks is still open ended and visually to the human eye, this paper does offer an explanation of why the 8 is misclassified. However, if this works very well for MNIST, more examples should be given. This single example is interesting but not sufficient to illustrate the success of this method.  

The examples from CelebA are interesting but inconclusive. For example, why should adding blue to the glasses fix the misclassification. If the explanation is simply visual for a human, then this explanation does not suffice. And the two examples with one receding and the other not receding hairlines look like their correct classifications could be flipped.

Regarding epsilon, it is unclear what a small euclidean distance for epsilon is without more examples. It would also help to see how the euclidean distance changes along the path.  But also it is not clear why we care about the size of epsilon, but rather the size of the perturbation that must be made to the original image, which is what is defined in the paper as the explanation. 

Since it is the encoded image that is perturbed, and this is what causes the perturbations to be selective to particular features of the image, an analysis of what features in the encoded space that are modified would greatly help in the interpretability of this explanation. The fact that perturbations are made in the latent space, and that this perturbation gets reflected in particular areas in the reconstructed image, is the most interesting part of this work.  More discussion around this would greatly enhance the paper, especially since the technical tools of this method are not very strong.

Pros: Interesting explanation, visually selects certain parts of the image relevant to classification rather than obscure pixels

Cons: No discussion or analysis about the latent space where perturbations occur. Only one easy example from MNIST shown and examples on CelebA are not great. No way (suggested) to use this tool outside of image recognition. ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']"
Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient,"['Lei Wu', 'Zhanxing Zhu', 'Cheng Tai', 'Weinan E']",Reject,2018,"[21, 10, 6, 19]","[26, 15, 10, 24]","[336, 100, 12, 111]","[149, 48, 2, 17]","[38, 44, 6, 64]","[149, 8, 4, 30]","This paper focuses on enhancing the transferability of adversarial examples from one model to another model. The main contribution of this paper is to factorize the adversarial perturbation direction into model-specific and data-dependent. Motivated by finding the data-dependent direction, the paper proposes the noise reduced gradient method. 

The paper is not mature. The authors need to justify their arguments in a more rigorous way, like why data-dependent direction can be obtained by averaging; is it true factorization of the perturbation direction? i.e. is the orthogonal direction is indeed model-specific? most of explanations are not rigorous and kind of superficial.


","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']"

review,reasoning,sentiment_score,politeness_score
"The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","The review acknowledges the importance of the topic and describes the work as a ""good baseline"". It also mentions the lack of ""strong novel methods"", which slightly lowers the positivity. Overall, the tone is balanced and suggests moderate positivity. The language used is formal and respectful, without any negative or overly critical remarks.",50.0,80.0
"This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.","The review acknowledges the merits of the paper, such as its exploration of different comparison methods and its achievement of good performance. However, it also points out a significant weakness, stating that the work is incremental and lacks innovation. The reviewer suggests a qualitative evaluation that would make the work more interesting. Overall, the feedback is balanced, suggesting a sentiment leaning slightly towards the positive side due to the acknowledged merits. The language used is objective and professional, without any harsh or negative phrasing.",20.0,80.0
"Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.","The review starts with praising the paper's clarity and contribution to the field. While it acknowledges a concurrent work, it still highlights the novelty and significance of the presented approach. The reviewer also asks clarifying questions and suggests minor improvements, which indicates a positive engagement with the paper. Overall, the tone is constructive and appreciative.",75.0,100.0
"This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.","The reviewer explicitly states their willingness to champion the paper, indicating a positive sentiment. While they mention a lack of expertise in the specific area, they find the results compelling and the addressed challenge relevant. The criticism focuses on improving clarity and presentation, not questioning the validity or novelty of the work. The language used is constructive, professional, and suggestive, not accusatory, making it polite.",60.0,80.0
"I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.","The reviewer starts with positive feedback, highlighting the good idea and consistent gains. However, they raise several concerns and ask for more analysis and experiments. The tone is mostly constructive and suggestive, not demanding. The presence of minor comments also indicates a positive attitude towards the paper's potential.",50.0,70.0
"This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: *The capturing the similarities* -- typo in line 2 of intro. *Recently, (Wieting et al.,2016) learned* -- use citet instead of parenthesized citation","The review starts with positive remarks, highlighting the merits of the paper and its interesting aspects. While it acknowledges some limitations (NB-SVM's superior performance), it doesn't diminish the paper's value. The tone remains constructive throughout, suggesting improvements and pointing out minor issues politely. ",60.0,80.0
"The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question*s sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary. This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs. The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.","The review is positive about the paper. It praises the clarity of the hypothesis, the refreshing approach, and the potential for future impact. The reviewer does suggest an improvement (exploring more datasets) but frames it constructively. The language is formal and respectful.",75.0,80.0
"This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion.","The review starts with positive statements such as ""interesting idea"", ""well-written"", and ""easy to follow"". It also acknowledges the theoretical soundness and competitive results. While it points out a drawback (non-convex optimization), it still recommends acceptance. Overall, the tone is encouraging and constructive. Therefore, the sentiment is positive, and the language is polite.",75.0,80.0
"This paper has two main contributions: (1) Applying adversarial training to imagenet, a larger dataset than previously considered (2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution. This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.","The review is overwhelmingly positive. The reviewer explicitly states the contributions of the paper, praises its clarity, writing quality, empirical approach, and contribution to the field. There is no negative sentiment expressed. The language used is formal and respectful, typical of scientific peer reviews.",90.0,100.0
"The authors propose to apply virtual adversarial training to semi-supervised classification. It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world. In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP). Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup). Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.","The review acknowledges the paper's merits (""The paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice.""), but also points out significant shortcomings regarding the lack of comparison with existing literature and potential novelty issues. The language is critical but professional and not disrespectful.",20.0,50.0
This is a parallel work with BiGAN. The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result.,"The review acknowledges the work as a ""parallel work"" and not entirely novel. However, it calls the idea ""promising"" based on the results. This suggests a generally positive sentiment, but not overly enthusiastic. Therefore, the sentiment score is moderately positive. The language used is neutral and professional, lacking any strong positive or negative connotations.",60.0,50.0
"This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. In particular, experiments are shown in a synthetic denoising task as well as in machine translation. I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different? If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I*d like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline. The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi* with interpolation, penalize the variance, reducing the value of rare actions, etc. Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. It seems that the authors need more work to make the model work without so many heuristics. The authors also mentioned several optimization difficulties (some of which are non-intuitive), 1) why does the critic assign very high value to actions with very low probability according to the actor? 2) why is a lower square error on Q resulting in much worst performance? The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework. Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this? The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. I*ll revise my score if the authors address my questions. In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.","The reviewer finds the idea interesting but has major concerns regarding the experimental evaluation and the clarity of the paper. They find the results not convincing, missing important baselines and comparisons, and relying on too many heuristics. The language used is quite direct and critical, but not overly rude or disrespectful. Therefore, the sentiment is rather negative, and the politeness is slightly below neutral.",-30.0,-10.0
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise. The original Bell & Sejnowski infomax framework only considered the no noise case. Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods. This seems like an interesting and potentially more general approach to unsupervised learning. However the paper is quite long and it was difficult for me to follow all the twists and turns. For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going. *Hierarchical* is probably not the right terminology here because it*s not like a deep net hierarchy, it*s just decomposing the tuning curve function into different parts. I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document. Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related. They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.","The reviewer finds the paper interesting and potentially general, suggesting it's a positive sentiment. However, they also express difficulty in following the paper and suggest significant revisions. This mixture of positive and negative points leads to a sentiment closer to neutral than extremely positive. The language used is constructive and professional, suggesting politeness.",50.0,70.0
"In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably. One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates. Pros: The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work. The experiments are good proofs of concept, but do not go beyond that i.m.h.o. Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning). Cons: As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell. The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016). Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task. Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.","The reviewer acknowledges the paper's contributions but points out limitations and suggests several improvements. They find the work interesting but have concerns about its novelty and significance. The language is formal and academic, with constructive criticism but no overtly negative phrasing.",30.0,60.0
"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don*t discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads *log p(topic proportions)* which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn*t the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not *error*; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.","The reviewer starts with positive remarks, stating they ""like the direction"" and find NVI ""promising."" However, they raise several methodological concerns, questioning the clarity and significance of the results. The numerous suggestions for improvement, while constructive, indicate a somewhat negative sentiment due to the perceived flaws. The language remains polite throughout, employing a professional and undramatic tone.",20.0,60.0
"EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score. This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM*s predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers. Comments: - Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it*s not clear at that point that the answer is an entity within the document. - 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?) - 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction? - how does performance of the pattern matching change with different cutoff constant values? - 5.2: are there questions whose answers are not entities? - how could the proposed approach be used when predictions aren*t made at every word? is there any extension for, say, sentence-level sentiment classification?","The reviewer finds the paper's core idea interesting and potentially useful, leading to a generally positive sentiment. While they initially had concerns about clarity and the evaluation's scope, the reviewer acknowledges the thorough revisions and improved clarity. The language used is constructive and professional throughout, indicating politeness.",65.0,80.0
"This is a solid paper with good results. However, there aren*t many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing. The second paragraph of the introduction is very confusing. It*s clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn*t familiar with similar approaches. The authors keep referring to *previously popular attention paradigms* without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches *summarize each modality into a single vector.* That*s one of the most incorrect descriptions I*ve yet seen for attention mechanisms. First, I don*t know what model works over several modalities in a single attention pass. Maybe the authors don*t know what a *modality* is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper*s supposedly new way of using attention is pretty much exactly the standard way. Both modeling and modelling spellings are in the text. I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a *modeling layer (RNN)*... It*s just an RNN, you don*t need to give an RNN another name, especially one that*s as nondescript as *modeling layer* all layers are part of a model? Typo: *let*s the modeling (RNN) layer to learn* This paragraph is supposed to give an overview of the model but just confuses readers. I would delete it. *Phrase embedding layer* -- terrible word choice as you are not embedding phrases here. It*s a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to *contextual word embedding layer* or something less incorrect. Your phrase layer embeddings only show single words, as expected in Table 2. Section 2: point 4. Second sentence needs citations for *popular* Typo: *from both *of* the context and query word* Typo: *aveaged* It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn*t quite accurate. I*d say you*re changing one module or part of your model. Section 4: attention isn*t countable (no *a* in front of *huge attention*). Also, academic writing usually doesn*t include such adjectives in the first place.","The review starts with a positive statement, acknowledging the paper as ""solid"" with ""good results."" However, it quickly transitions into criticism, pointing out a lack of novelty and significant issues with the writing and clarity. The reviewer uses strong language like ""most incorrect descriptions I've yet seen"" and questions the authors' understanding of fundamental concepts. While the reviewer offers constructive feedback and specific recommendations, the overall tone is quite critical.",-10.0,20.0
"This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data. I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature. Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 *Finally, let*s whose discriminative power*. I*m not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.","The reviewer provides a largely positive overview of the paper, highlighting its contribution to addressing a major shortcoming of GANs. They acknowledge the paper's strengths, such as the elegant mathematical exposition and the promising results on toy data. While they point out limitations regarding scalability and accuracy, these are presented as areas for further exploration rather than outright criticisms. The reviewer also offers constructive feedback on the writing, suggesting a title change to better reflect the paper's significance. The tone throughout is professional and respectful.",75.0,90.0
"This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular. Pros: * This paper addresses an important question I and many others would have liked to know the answer to but didn*t have the computational resources to thoroughly attack it. This is a nice use of Google*s resources to help the community. * The work appears to have been done carefully so that the results can be believed. * The basic answer arrived at (that, in the *typical training environment* LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful. Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper. * The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work. In sum, they*re much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity. * The point about the near-equivalence of capacity at equal numbers of parameters is very useful. * The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures. * The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it*s a practical problem that everyone working with these networks has but often isn*t addressed. * The paper text is very clearly written. Cons: * The work on the UGRNNs and the +RNNs seems a bit preliminary. I don*t think that the authors have clearly shown that the +RNN should be *recommended* with the same generality as the GRU. I*d at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel). In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important. I don*t really mind having them in the paper though. I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don*t know what the ICLR area chairs will think. * The paper gives short shrift to the details of the HP algorithm itself. They do say: *Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs* and give some good references, but I expect that actually trying to replicate this involves a lot of missing details. * I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness. * The neuroscience reference (*4.7 bits per synapse*) seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained. I guess it*s just in the discussion, but it seems gratuitous. Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be *in agreement* here between computational architectures and neuroscience, but perhaps they could say something like -- *We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.*)","The review is overwhelmingly positive. The reviewer explicitly states the paper is well-done, addresses an important question, and is clearly written. While there are constructive criticisms, these are presented gently and are outweighed by the positive feedback. ",85.0,90.0
"This paper introduces a continuous relaxation of categorical distribution, namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients. The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models. The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments. The paper is well-written, and I applaud the details provided in the appendix. The main application seems to be semi-supervised situations where you really want categorical variables. - P1: *differentiable sampling mechanism for softmax*. *sampling* => *approximate sampling*, since it*s technically sampling from the Gumbal-softmax. - P3: *backpropagtion* - Section 4.1: Interesting experiments. - It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported. - It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.","The reviewer finds the paper's contribution (a new method) to be valuable and effective. They praise the paper's clarity and the detailed appendix. While they offer constructive suggestions and point out areas for improvement, their overall tone is positive and encouraging. ",75.0,80.0
"The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class?","The reviewer provides a generally positive overview, acknowledging the simplicity and effectiveness of the proposed method. While they point out a limitation (the independence assumption for hidden activations), they do so constructively, prompting further investigation rather than outright criticism. The reviewer also raises specific questions about the results, seeking clarification and further analysis. Overall, the tone is engaged and constructive, suggesting a positive outlook on the work.",60.0,80.0
"This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.","The review is overwhelmingly positive. The reviewer explicitly states the paper is ""very nicely written,"" the results are ""new and insightful,"" and the empirical section is ""very well explained."" The reviewer also predicts a strong impact, saying the paper ""will likely influence a broad swath of RL."" While there's a minor comment, it's framed as non-crucial for the current version. The reviewer concludes by advocating for acceptance. All of this points to a very high sentiment score. The language used is entirely professional and courteous, resulting in a high politeness score.",90.0,100.0
"Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.","The review is overwhelmingly positive. The reviewer highlights the significance of the paper's topic, praises the effectiveness of the proposed method, and appreciates the simplicity of the algorithm. The suggestions for revision are minor and aim to further strengthen the paper. There is no use of negative or overly critical language.",90.0,100.0
"This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling. Comments: - p. 3 “can be seen has a way” -> “can be seen as a way” (?) - p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts? - p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations. - p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.","The reviewer finds the paper clearly written and acknowledges the demonstrated improvement in performance across tasks. However, they express concern about the method's simplicity and lack of theoretical grounding, using phrases like ""too simple"" and ""sounds ad hoc."" This suggests a mixed sentiment leaning towards the skeptical side. The reviewer's language remains constructive and polite throughout, focusing on specific areas for improvement and posing questions for clarification.",50.0,70.0
"This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets. The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction. While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset. The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question. Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.","The review acknowledges the interesting aspects and good execution of the work, using phrases like ""interesting idea"", ""seems to work well"", and ""well-executed work."" However, it also points out limitations in novelty and impact beyond the specific application. The criticism is presented constructively with suggestions for improvement, making the overall tone balanced. Therefore, the sentiment is slightly positive, and the language is polite.",60.0,70.0
"The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it*s a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB. The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model *deeper*, whereas adding linear bias terms isn*t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). I think that this paper is more proper for NLP conferences.","The review starts with positive statements, praising the writing, results, and analysis ('very well written', 'impressive experimental results'). However, it quickly transitions into a negative sentiment, criticizing the novelty and significance of the work ('hardly novel', 'trivial step', 'isn't confirmed to work'). The reviewer suggests the paper is a better fit for a different conference, indicating a lack of enthusiasm. Overall, the negative points outweigh the initial positive ones. The language used is critical but professional, avoiding overtly rude or unprofessional language.",-30.0,40.0
"The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.","The review is overwhelmingly positive. This is evident through the use of terms like ""extremely well-written"", ""thorough mathematical analysis"", and ""stands out"". The reviewer clearly expresses their positive view of the paper's quality and contribution. There is no use of negative or critical language. ",95.0,100.0
"Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.","The review is mostly positive, highlighting the clear benefits and cleverness of the approach. However, it does point out a significant missing comparison that would have strengthened the paper. The language is neutral, lacking strong positive or negative adjectives, and maintains a professional tone.",60.0,0.0
"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning. The framework is nice and appealing. However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning* for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.","The review starts with positive words like ""nice"" and ""appealing"" but then raises serious concerns about the novelty and significance of the work. The reviewer questions the need for deep learning on the chosen datasets and criticizes the authors for ignoring comparisons with simpler methods. The tone, while critical, maintains a professional and academic decorum.",-10.0,60.0
"An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. *Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).* It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.",,,
"The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF. Presentation: * Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter. * I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example. It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. Experiments: * Why is the run time not reported in table 1? * What are the *difficulties around convergence* encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? * Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. * How difficult is it to get the inference to work (eg HMC) when we don*t have full control over the computational graph structure and sampler? * It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models. * What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example? Minor comments: * Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I*m not sure why alpha=-1 was chosen here. * How do you handle discrete distributions (eg Figure 5)? * x_real is not defined in Figure 7. * I would suggest highlighting M in Figure 8. * Comma instead of period after *rized), In* on page 8. In conclusion I would say that the software developments presented here are quite exciting, and I*m glad the authors are pushing towards practical and accessible *inference for all*. In its current form though I am forced to give the submission itself a score of 5.",,,
"This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn*t discuss how priors over the beta*s can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity. My compliments for the amount of detail put in the paper and appendix. The experiments are on toy examples, but show promise. - Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized). - Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve. - Eq 9: “This interpretation implies the factorization of the recognition model:..” The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)",,,
"Update: raised the score, because I think the arguments about adversarial examples are compelling. I think that the paper convincingly proves that this method acts as a decent regularizer, but I*m not convinced that it*s a competitive regularizer. For example, I don*t believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc. I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply). ---- Summary: If I understand correctly, this paper proposes to take the *bottleneck* term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective. The argument is that this is an effective regularizer and increases robustness to adversarial attacks. Pros: -The presentation is quite good and the paper is easy to follow. -The idea is reasonable and the relationship to previous work is well described. -The robustness to adversarial examples experiment seems convincing, though I*m not an expert in this area. Is there any way to compare to an external quantitative baseline on robustness to adversarial examples? This would help a lot, since I*m not sure how the method here compares with other regularizers in terms of combatting adversarial examples. For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)? Cons: -MNIST accuracy results don*t seem very strong, unless I*m missing something. The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%. So the 1.13% error rate listed here doesn*t necessarily prove that the method is a competitive regularizer. I also suspect that tuning this method to make it work well is harder than other regularizers like dropout. -There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z. For example, the output could directly follow z, or there could be several layers between z and the output. As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it*s not obvious why this choice was made. Did it work best empirically? Other: -I wonder what would happen if you *trained against* the discovered adversarial examples while also using the method from this paper. Would it learn to have a higher variance p(z | x) when presented with an adversarial example?",,,
"This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain! The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code. I see this paper as being above the threshold for acceptance.",,,
"This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks. Pros: The use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users. The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice. The ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited. DeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures. Cons: There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow). This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API. (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.) The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax. This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years -- see a ~15 line definition of AlexNet [1]. Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format. DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework. Some examples: “This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.” This separation is not unique -- it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there). “The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.” There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL. I asked about this statement, and the response didn’t convince me otherwise. The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution. This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2]. It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this. Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise. [1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24 [2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92 ===================== Update: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review. I*ve upgraded my rating from 5 (below threshold) to 7 (good paper, accept).",,,
"The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would*ve been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.",,,
"This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. The paper nicely presents this core idea and a way to achieve this - by choosing special *routings* between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration. In summary - the paper is nicely written, results are quite good and the model is interesting - I*m happy to recommend acceptance.",,,
"This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.",,,
"As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston*16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.",,,
"The paper presents an application of deep learning to genomic SNP data with a comparison of possible approaches for dealing with the very high data dimensionality. The approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches. The presentation would benefit from more precise math. Quality: The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable. The experimental evaluation is rather weak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper. Clarity: The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely what was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the *raw end2end* model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used. The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ... Originality: The application and the approach appear quite novel. Significance: There is clearly strong interest for deep learning in the genomics area and the paper seeks to address some of the major bottlenecks here. It is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on. Other comments: I think releasing the code as promised would be a must.",,,
"This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition! The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?) I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.) Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive. Minor: the second paragraph of Section 3 needs a reference to Appendix A.",,,
"In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD. The main contributions of the paper are: 1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating. 2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers. 3) Empirically illustrate the performance of the method, and show: - Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10) - K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet) - Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet) - Favorable scaling properties of K-FAC with mini-batch size Pros: - Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. - Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN). Cons: - “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point. - Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective. The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. Typos: - “updates that accounts for” — “updates that account for” - “Kronecker product of their inverse” — “Kronecker product of their inverses” - “where P is distribution over” — “where P is the distribution over” - “back-propagated loss derivativesas” — “back-propagated loss derivatives as” - “inverse of the Fisher” — “inverse of the Fisher Information matrix” - “which amounts of several matrix” — “which amounts to several matrix” - “The diagram illustrate the distributed” — “The diagram illustrates the distributed” - “Gradient workers computes” — “Gradient workers compute” - “Stat workers computes” — “Stat workers compute” - “occasionally and uses stale values” — “occasionally and using stale values” - “The factors of rank-1 approximations” — “The factors of the rank-1 approximations” - “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.” - “Psi is captures” — “Psi captures” - “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices” - “which is a nested applications of the reshape” — “which is a nested application of the reshape” - “provides a computational feasible alternative” — “provides a computationally feasible alternative” - “according the geometric mean” — “according to the geometric mean” - “analogous to shrink” — “analogous to shrinking” - “applied to existing model-specification code” — “applied to the existing model-specification code” - “: that the alternative parametrization” — “: the alternative parameterization” Minor Issues: - In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example: 1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) 2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231) 3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1) 4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html) 5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl) - Page 2, equation s = WA, is there a dimension issue in this expression? - x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots. - “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.",,,
"Description. This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs. The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. Strong points. - The experiments are carefully done with thorough selection of hyperparameters. - The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014). - The paper is well and clearly written. Weak points: - CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar? Originality: - This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights. Quality: - The experiments are well done. Clarity: - The paper is well written and clear. Significance: - The results go against some of the conclusions from previous work, so should be published and discussed. Overall: Experimental paper with interesting results. Well written. Solid experiments.",,,
"summary The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model. The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights). This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap. Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning. Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally) The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout. The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable. The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced. p6 line 8 typo: expecatation",,,
"Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model*s performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.",,,
"This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting. That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations. <rant Finally a stylistic comment that the authors can feel free to ignore. I don*t like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name. Especially here when the idea is so simple. Why not use words? English words from the dictionary. Instead of *EPOpt* and *EPOpt-e*, you can write *ensemble training* and *robust ensemble training*. Is that not clearer? />",,,
"The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call *monotonic chains of linear segments*, which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very *compatible*. In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the *bound for this case is very loose*. The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio*s group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using *the difference between the ground truth distance ... and the distance computed by the network* seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?",,,
"This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3. The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis? While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it*s an interesting algorithm, nicely composed and very efficient, so it*s reasonable to assume that other readers might have use for some of the ideas presented here.",,,
"This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina. The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim. The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy. Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective. Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim. Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.",,,
"This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function. The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages. I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.",,,

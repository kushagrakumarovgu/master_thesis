reviews,sentiment_scores,politeness_scores,reasonings
"['The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.', 'The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence. It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol. It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language*s speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language. In section 4, the description of the auxiliary decoder setup might benefit from more detail. There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 1. Ogawa, Tetsuji, et al. *Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation.* Proceedings of ICASSP. 2015. 2. Hermansky, Hynek, et al. *Towards machines that know when they do not know.* Proceedings of ICASSP, 2015. 3. Variani, Ehsan et al. *Multi-stream recognition of noisy speech with performance monitoring.* INTERSPEECH. 2013.', 'The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. My comments appear below: 1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold). 2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases). 3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?) 4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required? 5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.']","[50, 50, 20]","[50, 75, 70]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance of the topic and states that the study provides a good baseline and contributes to the community. However, it's not overwhelmingly positive as the reviewer notes that it 'misses strong novel methods for the task'. The politeness score is 50 (moderately polite) because the language used is professional and respectful. The reviewer offers a balanced view, highlighting both strengths and limitations without using harsh or critical language. The tone is constructive and appreciative of the work's contribution, even while pointing out areas for improvement."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the authors' proposal and suggests improvements, indicating a generally positive view of the work. The review starts with a neutral summary and then offers constructive suggestions for improvement, which is a positive approach. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, such as 'It would be great if...' and 'It would be interesting to see...'. The reviewer also provides helpful suggestions and additional references in a courteous manner, without using any harsh or critical language. The overall tone is professional and supportive, aiming to improve the paper rather than criticize it."", ""The sentiment score is slightly positive (20) because while the reviewer provides constructive criticism and suggestions for improvement, they also acknowledge the value of the work and its results. The overall tone is more focused on enhancing the paper rather than criticizing it. The politeness score is relatively high (70) as the reviewer uses respectful language throughout, phrases suggestions as questions or polite requests (e.g., 'It would be useful...', 'I'm curious about...'), and acknowledges the authors' efforts. The reviewer also uses phrases like 'As the authors point out...' which shows they have carefully read and considered the paper. The language is professional and constructive without being overly formal or distant.""]"
"['This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.', 'This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines. The paper is well written overall. A few detailed comments: * page 4, line5: including a some -> including some * What*s the benefit of the preprocessing and attention step? Can you provide the results without it? * Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.', 'The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model. This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I*d strongly recommend acceptance. Detail: - The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections. - If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. *15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy). - Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)? - You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite. - Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.']","[20, 70, 80]","[50, 80, 70]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contributions and its performance on multiple datasets. However, they also point out that it's an incremental work lacking innovation, which tempers the positivity. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language without harsh criticism. They offer constructive feedback and suggestions for improvement, maintaining a respectful tone throughout the review."", ""The sentiment score is 70 (positive) because the reviewer states the paper is 'well written overall' and provides a detailed summary of the paper's content without major criticisms. The few comments provided are constructive and relatively minor. The politeness score is 80 (quite polite) due to the use of respectful language throughout. The reviewer begins with a neutral summary, offers positive feedback, and phrases suggestions as questions or polite requests (e.g., 'Please enhance the quality'). The tone is professional and constructive, avoiding harsh criticism."", ""The sentiment score is 80 (positive) because the reviewer strongly recommends acceptance, praises the work as timely and contributing substantially, and notes that it yields good results. The few criticisms are minor and constructive. The politeness score is 70 (polite) due to the respectful tone, constructive feedback, and use of phrases like 'you should probably' when making suggestions. The reviewer balances praise with specific recommendations for improvement in a professional manner.""]"

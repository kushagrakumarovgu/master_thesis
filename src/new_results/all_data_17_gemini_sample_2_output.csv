reviews,sentiment_scores,politeness_scores,reasonings
"['The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.', 'The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence. It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol. It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language*s speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language. In section 4, the description of the auxiliary decoder setup might benefit from more detail. There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 1. Ogawa, Tetsuji, et al. *Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation.* Proceedings of ICASSP. 2015. 2. Hermansky, Hynek, et al. *Towards machines that know when they do not know.* Proceedings of ICASSP, 2015. 3. Variani, Ehsan et al. *Multi-stream recognition of noisy speech with performance monitoring.* INTERSPEECH. 2013.', 'The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. My comments appear below: 1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold). 2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases). 3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?) 4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required? 5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.']","[20, 60, 60]","[70, 80, 80]","['The review acknowledges the importance of the topic and describes the work as a ""good baseline"". It also mentions that the study ""contributes to the community."" These are positive remarks. However, the statement that the study ""misses strong novel methods"" dampens the positivity, suggesting a lack of significant advancement. Overall, the sentiment leans slightly towards the positive side due to the positive remarks outweighing the criticism.', 'The reviewer provides constructive criticism and suggestions for improvement without using harsh language. They find the paper interesting enough to suggest further exploration of the proposed method in more complex scenarios. The suggestions for improvement and additional references contribute to the overall positive tone.', 'The review is constructive and suggestive. The reviewer provides specific points for improvement and expresses curiosity about certain aspects, indicating a positive engagement with the work. The language is polite and professional throughout, using phrases like ""it would be interesting to know"" and ""I’m curious about"" to frame suggestions.']"
"['This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.', 'This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines. The paper is well written overall. A few detailed comments: * page 4, line5: including a some -> including some * What*s the benefit of the preprocessing and attention step? Can you provide the results without it? * Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.', 'The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model. This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I*d strongly recommend acceptance. Detail: - The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections. - If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. *15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy). - Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)? - You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite. - Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.']","[20, 60, 75]","[50, 80, 100]","['The review acknowledges the merits of the paper, such as its clear framework and the comparison of different matching methods. It points out the use of element-wise subtraction/multiplication as a highlight and acknowledges its good performance. However, it also criticizes the work for being incremental and lacking innovation. The suggestion for qualitative evaluation is neutral, pointing towards potential improvement but not framed as a harsh criticism. Overall, the tone is balanced and suggests areas for improvement without being overly negative.', ""The review starts with positive statements, highlighting the well-written nature of the paper and the extensive experimental results. While it points out areas of improvement, the language used is constructive and suggestive, not demanding. The use of 'please' further adds to the politeness."", 'The reviewer provides a generally positive assessment, stating that the work is ""timely"", ""straightforward and reasonable"", and yields ""good results."" They recommend acceptance despite acknowledging the work as ""bordering on incremental."" The reviewer also lists specific, constructive suggestions for improvement, indicating a desire to see the paper published in a stronger form. The language used is objective and professional throughout.']"

reviews,sentiment_scores,politeness_scores,reasonings
"['First of all, the paper cannot be accepted because it violates the double blind submission policy by including an acknowledgments section.\n\nNonetheless, I will give some brief comments:\n\n The paper proposes a probabilistic hierarchical approach to perform zero-shot learning.\nInstead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy.\n\n The experimental section of the paper is strong enough although more baselines could have been tested. The paper only compares the usual cross entropy loss with their proposed soft-classification framework. \nNonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task.\n\n \nOn the other hand, the theoretical aspect is weak. The proposed method seems to be a straightforward extension of Frome et al., NIPS 2013. The main contribution is that soft probability scores are used to perform classification instead of using only class membership information.\n\nSome weighting strategy is proposed in Section 2.2 but the proposed steps seem very ad hoc with no theoretical justification. The first equation on page 8 has the same problem where some random definition is provided.\n', 'This paper proposes a new soft negative log-likelihood loss formulation for multi-class classification problems. The new loss is built upon the taxonomy graph of labels, which is provided as external knowledge, and this loss provides better semantic generalization ability compared to a regular N-way classifier and yields more accurate and meaningful super-class predictions.\n\nThis paper is well-written. The main ideas and claims are clearly expressed. The main benefits of the new loss are caused by the extra information contained by the taxonomy of labels, and this idea is well-known and popular in the literature. Based on this reason, I think the main contribution of this paper is the discussion on two novel learning settings, which related to the super-classes. However, the formulation of the new soft NLL loss and the SG measurement involves lots of concepts designed based on experiences, so it’s hard to say whether these are the optimal choices. So, I suggest the authors discuss more on these designs.\nAnother thing I concern about is the source of label taxonomy. How to efficiently generate the taxonomy? What if the taxonomy is not perfect and contains noises? Will these significantly affect the models’ performance? I think it’s better to take these problems into consideration. \nIn conclusion, I think this is an interesting paper but can still be improved.', 'SUMMARY\nThe paper presents a method for classification which takes into account the semantic hierarchy of output labels, rather than treating them as independent categories. In a typical classification setup, the loss penalizes the KL-divergence between the model’s predicted label distribution and a one-hot distribution placing all probability mass on the single ground-truth label for each example. The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model’s predicted distribution and this target distribution. This model is used for classification on ImageNet-1k, and for zero-shot classification on ImageNet-21k where a model must predict superclasses seen during training for images of leaf categories not seen during training.\n\nPros:\n- Method is fairly straightforward\n- Modeling relationships between labels is an important problem\n\nCons:\n- Missing references to key prior work in this space\n- Minimal comparison to prior work\n- Confusing experimental setup\n- Paper is difficult to read\n\nMISSING REFERENCES\nThis paper is far from the first to consider the use of a semantic hierarchy to improve classification systems; see for example:\n\nDeng et al, “Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition”, CVPR 2012\n\nDeng et al, “Large-scale object classification using label relation graphs”, ECCV 2014 (Best Paper)\n\nJiang et al, “Exploiting feature and class relationships in video categorization with regularized deep neural networks”, TPAMI 2017\n\nNone of these are cited in the submission. [Deng et al, 2014] is particularly relevant, as it considers not just “is-a” relationships as in this submission, but also mutual exclusion relationships between categories. Without citation, discussion, and comparison with some of these key pieces of prior work, the current submission is incomplete.\n\nCOMPARISON TO PRIOR WORK\nThe only direct comparison to prior work in the paper is the comparison to DeViSE on ILSVRC12 classification performance in Table 3. However since DeViSE was intended to be used for zero-shot learning and not traditional supervised classification, this comparison seems unfair.\n\nInstead the authors should compare their method against DeViSE and ConSE for zero-shot learning. Indeed, in Section 4.3 the authors construct a test set “in a [sic] same manner defined in Frome et al” but do not actually compare against this prior work.\n\nI suspect that the authors chose not to perform this comparison since unlike DeViSE and ConSE their method cannot predict category labels not seen during training; instead it is constrained to predicting a known supercategory when presented with an image of a novel leaf category. As such, the proposed method is not really “zero-shot” in the sense of DeViSE and ConSE.\n\nEXPERIMENTAL SETUP\nFrom Section 3.1, “we adopt a subset of ImageNet the ILSVRC12 dataset which gather [sic] 1K classes [...]”. The 1000 category labels in ILSVRC12 are mutually exclusive leaf nodes; when placed in the context of the WordNet hierarchy there are 820 internal nodes between these leaves and the WordNet root. As a result, for the method to make sense I assume that all models must be trained to output classification scores for all 1820 categories rather than the 1K leaf categories. This should be made more explicit in the paper, as it means that none of the performance metrics reported in the paper are comparable to other results on ILSVRC12 which only measure performance on the 1K leaf categories.\n\nThe experiments on zero-shot learning are also confusing. Rather than following the existing experimental protocol for evaluating zero-shot learning from [Frome et al, 2013] and [Norouzi et al, 2013] the authors evaluate zero-shot learning by plotting SG-hit vs SG-specificity; while these are reasonable metrics, they make it difficult to compare with prior work.\n\nPOOR WRITING\nThe paper is difficult to follow, with confusing notation and many spelling and grammatical errors.\n\nOVERALL\nOn the whole, the paper addresses an important problem and presents a reasonable method. However due to the omission of key references and incomplete comparison to prior work, the paper is not suitable for publication in its current form.']","[-20, 50, -60]","[20, 75, 20]","[""The sentiment score is slightly negative (-20) because the reviewer starts by stating the paper cannot be accepted due to a policy violation, and points out some weaknesses in the theoretical aspect. However, they also acknowledge the strong experimental section and improvements in performance, which prevents the score from being more negative. The politeness score is slightly positive (20) as the reviewer uses professional language throughout, offers 'brief comments' despite the policy violation, and provides balanced feedback on both strengths and weaknesses. They avoid harsh criticism and use phrases like 'could have been' instead of more direct negative statements."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper as 'interesting' and 'well-written' with clear ideas and claims. However, they also suggest improvements and express some concerns, indicating a balanced view. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I suggest' and 'I think' to soften their recommendations, and conclude with a positive note despite pointing out areas for improvement."", ""The sentiment score is -60 because the review is overall quite critical, pointing out several major flaws like missing key references, minimal comparison to prior work, and a confusing experimental setup. However, it does acknowledge some positives like addressing an important problem and presenting a reasonable method. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout and use phrases like 'I suspect' and 'On the whole' to soften some critiques. They also begin by listing pros before cons. The language is not overtly rude, but rather matter-of-fact in pointing out issues.""]"
"['The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack. In general I find the writing poor, as it is not exactly clear what the focus of the paper is - the evaluation or the new attack? The experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.\n\t\nMajor remarks:\n- Linking the code and asking the reviewers not to look seems like bad practice and close to violating double blind, especially when considering that the cleavhans library is well known. Should have just removed the link and cleavhans name and state it will be released after review. \n\n- It is unclear what the focus of the paper is, is it the evaluation methodology or the new attack? While the evaluation methodology is presented as the main topic in title, abstract and introduction most of the paper is dedicated to the attack.\n\n- The evaluation methodology is a good idea but is quiet trivial. Also, curves are nice visually but hard to compare between close competitors. A numeric value like area-under-the-curve should be better.\n\n- The theoretical guarantees is also quiet trivial, more or less saying that if a confident adversarial attack exists then finding the most confident attack will be successful. Besides that the third part of the proof can be simplified significantly.\n\n- The experiments are very lacking. The authors do not compare to any other attack so there is no way to evaluate the significance of their proposed method\n\n- That being said, the max-confidence attack by itself sounds interesting, and might be useful even outside confidence thresholding.\n\n- One interesting base-line experiment could be trying this attack on re-calibrated networks e.g. “On Calibration of Modern Neural Networks” \n\n- Another baseline for comparison could be doing just a targeted attack with highest probability wrong class.\n\n- I found part 4.2 unclear \n\n- In the conclusion, the first and last claims are not supported by the text in my mind. \n\n\n\nMinor remarks:\n\n- The abstract isn’t clear jumping from one topic to the next in the middle without any connection.\n\n- Having Fig.1 and 2 right on the start is a bit annoying, would be better to put in the relevant spot and after the terms have been introduced.\n\n-In 6.2 the periodically in third line from the end seems out of place.\n\n', 'This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.\n\nAlthough the idea behind this paper is fairly simple, the paper is very difficult to understand.  I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method. Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2. However, in Figure 2, it is used for evaluating defense schemes. Again, this confuses me on what is the main topic of this paper. Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case? Intuitively, it should provide similar results to the success-failure curve.\n\nThe paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"". If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.\n\nMeanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations.', 'This paper introduces a family of attack on confidence thresholding algortihms. Such algorithms are allowed to refuse to make predictions when their confidence is below a certain threshold. \n\nThere are certainly interesting links between such models and KWIK [1] algorithms (which are also supposed to be able to respond \'null\' to queries), however they are not mentioned in this paper, which focuses mainly on evaluation methodologies.\n\nThe definition of the metric is certainly natural: you would expect some trade-off between performance in the normal versus the adversarial regime. I am not certain why the authors don\'t simply measure the success rate on both natural and adversarial conditions, so as to have the performance metric uniform. Unfortunately the paper\'s notationleaves something to be desired, as it fails to concretely define the metric.\nLet me do so instead, and consider the classification accuracy of a classification rule $P_t$ using a threshold $t$ under a (possibly adaptive) distribution $Q$ to be $U(P,Q)$. Then, we can consider $Q_N, Q_A$ as the normal and adversarial distribution and measure the corresponding accuracies. \n\nEven if we do this, however, the authors do not clarify how they propose to select the classification rule. Should they employ something like a convex combination:\n\\[\nV(P_t) := \\alpha U(P_t, Q_N) + (1 - \\alpha) U(P_t, Q_A) \n\\]\nor maybe take a nimimax approach\n\\[\nV(P_t) := \\min \\{U(P_t, Q) | Q = Q_A, Q_N\\}\n\\]\n\nIn addition, the authors simply plot curves for various choices of $t$, however it is necessary to take into account the fact that measuring performance in this way and selecting $t$ aftewards amounts to a hyperparameter selection [2]. Thus, the thresholding should be chosen on an independent validation set in order to optimise the chosen performance measure, and then the choice should evaluated on a new test set with respect to the same measure $V$\n\nThe MaxConfidence attack is not very well described, in my opinion. However, it seems it simply wishes to find to find a single point $x \\in \\mathbb{S}$ that maximises the probability of misclassification. It is not clear to me why performance against an attack of this type is interesting to measure.\n\nThe main contribution of the paper seems to be the generalisation of the attack by Goodfellow et al to softmax regression. The proof of this statement is in a rather obscure place in the paper. \n\nI am not sure I follow the idea for the proof, or what they are trying to prove. The authors should follow a standard Theorem/Proof organisation, clearing stating assumptions and what the theorem is showing us. It seems that they want to prove that if a solution to (1) exists, then MaxConfidence() finds it. But the only definition of MaxConfidence is (1). Hence I think that their theorem is vacuous. There are quite a few details that are also unclear such as what the authors mean by \'clean example\' etc. \n\nHowever the authors do not explain their attack very well, their definition of the performance metric is not sufficiently formal, and their evaluation methodology is weak. Since evaluation methodology is the central point of the paper, this is a serious weaknes. Finally, there doesn\'t seem to be a lot of connection with the conference\'s topic.\n\n[1] Li, Lihong, Michael L. Littman, and Thomas J. Walsh. ""Knows what it knows: a framework for self-aware learning."" Proceedings of the 25th international conference on Machine learning. ACM, 2008.\n\n[2] Bengio, Samy, Johnny Mariéthoz, and Mikaela Keller. ""The expected performance curve."" International Conference on Machine Learning, ICML, Workshop on ROC Analysis in Machine Learning. No. EPFL-CONF-83266. 2005.\n']","[-60, -60, -50]","[-20, -20, 20]","[""The sentiment score is -60 because the review is predominantly negative. The reviewer criticizes the writing quality, lack of clarity in focus, trivial methodology, and insufficient experiments. Phrases like 'poor writing', 'lacking', and 'trivial' indicate a negative sentiment. However, there are some positive aspects mentioned (e.g., 'good idea', 'sounds interesting'), which prevent the score from being extremely negative. The politeness score is -20 because while the reviewer isn't overtly rude, the tone is quite direct and critical. The use of phrases like 'bad practice', 'very lacking', and 'isn't clear' without much softening language contributes to a somewhat impolite tone. However, the reviewer does offer constructive suggestions and uses some neutral language, preventing the score from being extremely negative in terms of politeness."", ""The sentiment score is -60 because the reviewer expresses significant confusion and criticism about the paper. They state that the paper is 'very difficult to understand', question the purpose and usage of the proposed method, and point out a lack of experimental results. The reviewer also suggests that the main conclusion is not particularly novel or significant. These criticisms indicate a generally negative sentiment towards the paper.\n\nThe politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite direct and critical. Phrases like 'I have no idea' and 'this confuses me' are somewhat blunt. The reviewer also doesn't offer many positive comments or soften their criticisms, which contributes to a slightly impolite tone. However, they do use some neutral language and provide specific suggestions for improvement, which prevents the score from being extremely negative."", ""The sentiment score is -50 because the review is generally critical of the paper, pointing out several weaknesses such as unclear definitions, weak evaluation methodology, and poor explanation of the attack. However, it's not entirely negative as it acknowledges some interesting aspects of the work. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout, using phrases like 'I am not certain why' and 'Unfortunately' rather than harsh language. They also offer constructive suggestions and references to improve the paper. The reviewer's language is slightly more polite than neutral, hence the slightly positive score.""]"
"['Pros:\n\nThis paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \\phi_i, and then is formulated as a GP. \n\nCons:\n\nHowever, there are some key issues.\n1. The so-called “infinite width” is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details.\n \n[S1] Generalization Properties of Learning with Random Features, NIPS 2017.\n \n2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing.\n\nSummary:\nThe paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results.\n\nResponse to rebuttal:\nThe authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. \n\nTherefore the paper is very borderline. However, I would like to bump my rating a bit higher.\n\n \n \n ', 'Summary:\nI\'m not very familiar with the work this is building off of, but my summary is as follows:\nThe authors look at the problem of defining multilayer infinite width neural networks. The main challenge is that the weights (which are in some sense now a function) must be appropriately sampled to ensure that norms don\'t explode. \n\nThis has only been done for two layers before, and the authors derive how to do this for more than two hidden layers using RKHSs. This initialization is called Win-Win, and is compared to different initializations on a few different datasets.\n\nClarity: This paper is quite technical and hard to follow without knowledge of the prior work. I think the authors could have been a little clearer on some of the challenges. E.g. instead of talking about the weights needing to be ""in the same function space"", it would be helpful to remphasise the norm issue.\n\nComments:\nI\'m not sure about the high level motivation for developing networks of this kind. In particular, none of the performance numbers are near state of the art (not necessary for developing new promising methods!) but I don\'t know exactly what the initialization buys in this setting.\n\nAlso, it would have been nice to see whether the Win-Win initialization is only useful for larger width networks compared to smaller width i.e. do other initialization schemes work better in this latter setting?\n\nThe derivations are interesting though, so I still recommend accept.', 'In this paper, the authors propose deep neural networks of infinite width. The primary challenge in such networks is defining a distribution over the weights connecting two layers of infinite width. The authors tackle this by using Gaussian Processes for these distributions with the covariance functions defined in a canonical manner. Inspired by these networks, the authors propose weight initialization schemes for finite width networks.\n\nWhile the idea proposed in the paper is interesting, the paper appears quite incomplete. In particular, the authors do not mention a single example of a kernel that can be constructed using the process outlined in Section 3. Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. It will perhaps be more interesting if the authors can use the kernels obtained for supervised learning tasks using kernel machines (as done in Cho & Saul 2009) or as the covariance function of a Gaussian process (as done in Wilson et al. 2014).\n\nMoreover, the experiments conducted on finite width networks are not enough to justify the utility of this initialization scheme. It will be useful if the authors can test the performance of the state-of-the-art networks for CIFAR-10/100 and ImageNet, where the weights of the last fully connected layer have been sampled from different distributions. An extension to initialization for convolutional layers will further strengthen the paper.']","[-30, 50, -20]","[20, 60, 50]","[""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('Pros' section), they express significant concerns ('Cons' section) and describe the paper as 'very borderline'. The overall tone is more negative than positive, but not extremely negative. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They present their concerns as 'issues' rather than failures, and use phrases like 'I would like to bump my rating a bit higher', which shows a willingness to be fair. However, the review is not overly polite or effusive, maintaining a professional tone."", 'The sentiment score is 50 (slightly positive) because while the reviewer acknowledges the technical merit and interesting derivations of the paper, recommending acceptance, they also express some concerns about clarity and motivation. The reviewer suggests improvements but overall seems to view the work favorably. The politeness score is 60 (moderately polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions without harsh wording. They acknowledge their own potential lack of familiarity with the background work and frame their comments as suggestions rather than demands, which contributes to a polite tone.', ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'interesting', they also state that the paper appears 'quite incomplete' and list several significant shortcomings. The reviewer suggests multiple improvements, indicating that the current state of the paper is not satisfactory. However, the criticism is not entirely negative, hence the score is not deeply negative. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, acknowledging the interesting aspects of the work, and framing criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'It will perhaps be more interesting if...' and 'It will be useful if...' contribute to the polite tone. The reviewer maintains a professional and constructive approach, even while pointing out significant areas for improvement.""]"
"['- Does the paper present substantively new ideas or explore an under-explored or highly novel question? \n\nThe paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. \n\n- Does the results substantively advance the state of the art?\n\nThe results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).\n\n- Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\nYes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper’s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.\n\n- Would I send this paper to one of my colleagues to read? \n\n  I would definitely send the paper to my colleagues to read. \n\n\n- In terms of quality:  \n\nClear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. \n\n- I terms of clarity:  \n\nEasy to read.–Experimental evaluation is clearly presented.\n\n- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor’s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).\n\n- In terms of significance:  \n\n The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   \n\n\n\n', 'The paper looks at the problem of generalization across physical parameter varaition in learning for continuous control. The paper presents a method to develop a sampling based curriculum over env. settings for training robust agents. \n\n\n* The paper makes an interesting observation on inadvertent generalization in robust policy learning. \nHowever, the examples in both the cartpole and the pendulum cases seem not to be watertight. \nFor instance, the authors claim that \nBut from a dynamical system perspective in both cases, the controller is operating near limits. \nThe solution and subsequent generalization depend more on the topology of the solution space. \nA heavy Pendulum is an overdamped system and required the policy to operate at the limits of action to generate momentum for swing up. Hence a solution for a lighter pendulum in implicitly included. Similarly, the rolling ball is an underdamped system, and where the policy operates near zero limits in light ball case to prevent the system from going unstable. Adding mass results in damping which makes it easier. In this case, as well the solution space is implicitly contained.\n\n\nBut this is not a novel observation. Similar observations have been made for Robust control and Model-Reference Adaptive Control. \nThe paper also overlooks a number of related works in model-free randomization [4], adaptive randomization [3], adversarial randomization [5,6]. The method also does not compare with model-based methods for adaptive policy learning and iLQR based methods to handle this problem [2, 7].\n\n\nThe argument that the method is model-free is perhaps not as acceptable since the model parameters need to be known apriori for adaptation. The policy itself may be model-free but that is a design choice. \nA good experimental evaluation for this is generalization across known unknowns and unknown unknowns. \n\n\n* The algorithm itself is reasonable but the problem setup and choice of a discrete dynamics parameter choices are questionable. The bandit style method operates over a discrete decision set. \nIt also assumes in the multi-parameter setting that they are independent, which may not be true very often. \n\nThe algorithm proposed itself isnt novel, but would have been justified if the results supported the use of such a method. \n\n* Experiments are quite weak. \nBoth the experimental domains are rather simplistic with smooth nonlinear dynamics. There are more sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2].  \n\nIt would be useful to see how tis method works in more complicated domains and how the performance compares with simpler methods such as joint brute-force randomization both in performance and in computation.  \n\nQuestions: \n1. Please provide details of Algorithm 1. How are the quantities K and M related? \n2. What is the process of task initialization? What information is required and what priors are used. Uniform prior over what range?\n\n\nIn summary, the authors explore an interesting adaptive curriculum design method. However, in its current form, the work needs more thought and empirical evaluation for the sake of completeness. \n\n\nReferences:\n1. Model Reference Adaptive Control [https://doi.org/10.1007/978-1-4471-5102-9_116-1\n]\n2. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems [https://arxiv.org/abs/1707.04674]\n3. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles [https://arxiv.org/abs/1610.01283]\n4. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\n[https://arxiv.org/abs/1610.01283]\n5. Certifying Some Distributional Robustness with Principled Adversarial Training [https://arxiv.org/pdf/1710.10571.pdf]\n6. Adversarially Robust Policy Learning: Active Construction of Physically-Plausible Perturbations [http://vision.stanford.edu/pdf/mandlekar2017iros.pdf]\n7. Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization [https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf]\n', ""This paper investigated the robustness of RL policies learning under different environmental conditions. \n\nBased on the observations that policies learnt in different experimental settings lead to different generalizability, the authors proposed an EXP3 based reward-guided curriculum for improving policy robustness. The algorithm was tested on inverse pendulum, cart-pole balancing, and ball-pushing in OpenAI gym.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding. Papers in the references were not properly cited.\n\nIt is an interesting discovery that different environment brewed different policies with different robustness/generalizability in daily life. However, these are also easily derivable in physics, especially in the three experiments tested in the paper. It would be more complete to compare with PID controllers.\n""]","[70, -20, 50]","[80, 50, 75]","[""The sentiment score is 70 (positive) because the reviewer expresses overall positive views about the paper. They note that the combination of ideas is novel, the results advance the state of the art, and they would definitely send the paper to colleagues. The reviewer also mentions that the paper is interesting and presents a promising direction. However, they do point out some limitations, such as the simplicity of the examples and the question of scalability, which prevents the score from being higher. The politeness score is 80 (polite) because the reviewer uses respectful and constructive language throughout. They acknowledge the paper's strengths and provide balanced feedback, using phrases like 'interesting application' and 'promising direction'. The reviewer also offers suggestions for improvement in a considerate manner, without using harsh or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting aspects of the paper, they also point out several significant weaknesses and areas for improvement. The review highlights issues with the examples, lack of novelty, overlooked related works, questionable problem setup, and weak experiments. However, it's not entirely negative as the reviewer recognizes the interesting observation and reasonable algorithm.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'interesting observation', 'reasonable algorithm', and provide specific suggestions for improvement. The critique is delivered in a matter-of-fact manner without harsh language. The reviewer also asks clarifying questions and provides references, which is helpful and courteous. However, the score is not higher because the review is quite direct in pointing out flaws and doesn't use many overtly polite phrases or soften criticisms."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper as 'well-organized and easy to understand' and finds the discovery 'interesting'. However, they also point out some limitations, such as the lack of comparison with PID controllers and improper citations. The politeness score is 75 (fairly polite) as the reviewer uses respectful language throughout, offering constructive criticism without harsh words. They balance positive comments with suggestions for improvement, maintaining a professional tone. The reviewer avoids direct criticism, instead framing suggestions as ways to make the paper 'more complete'.""]"
"['This work proposes a defence based on class-conditional feature distributions to turn deep neural networks into robust classifiers.\n\nAt present this work lacks even the most rudimentary evidence to support the claims of robustness, and I hence refrain from providing a full review. In brief, model robustness is only tested against adversarials crafted from a standard convolutional neural network (i.e. in a transfer setting, which is vastly different from what the abstract suggests). Unsurprisingly, the vanilla CNN is less robust than the density-based architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla CNN are. No direct attacks - neither gradient-based, score-based or decision-based attacks - have been used to evaluate robustness. Please check [1] for how a thorough robustness evaluation should be performed.\n\n[1] Schott et al. “Towards the first adversarially robust neural network model on MNIST”.', 'This paper proposes geometrically robust networks (GRN), which applies geometric perspective and unsupervised model augmentation to transform traditional deep neural networks into adversarial robust classifiers. Promising experimental results against several adversarial attacks are presented as well.\n\nThe BNP-MFA are applied twice in the framework: one for getting the soft labels, and the other for getting the predictions through MAP estimation. There are existing works which are in the same line as the second part: deep kNN [1], and simple cache model [2] for example, where similarities to training examples are used to derive the test prediction and substantial increase of the robustness against adversarial attacks considered in this work have also been shown. \n\nThese raise two questions:\n(1) How much does the soft label encoding help increase the robustness?\n(2) How does the proposed model compare with the deep kNN and the simple cache model, which are much simpler?\n\nSome minor issues:\n- The unsupervised learning for label encoding is performed on the input space, the image pixel for example. But it is known that they are not good features for image recognition.\n- It is unclear which part of the network is considered as ""feature extraction"" part which is used for MAP estimation in the experiments.\n- It would be nicer to have results with different architectures.\n\n\n[1] N. Papernot and P. McDaniel. Deep k-nearest neighbors: towards confident, interpretable and robust deep learning. arXiv:1803.04765.\n[2] E. Orhan. A simple cache model for image recognition. arXiv:1805.08709.', 'The paper is working on a robust classifier that consists of two stages. The first stage performs unsupervised conditional kernel density estimates (KDE) of the covariate vectors, and the second stage is feature extractions and classification. I appreciate the authors\' efforts to clarify the intuition, but more technical details and experiments can be provided to support their arguments. My questions and comments are below.\n\n1. Page 2. ""this means the stochastic gradient descent training algorithm minimizing..."" Is the problem because of SGD or the structure of NN? I think the reason might be the latter, consider logistic regression, which can be seen as a single-layer NN, does not suffer such a problem. \n2. I know the KDE part is from an existing paper, but more technical details can make the paper clearer and some statements are questionable. Specifically, what basis vectors are used for (3)? Is it really speedy and scalable (Page 4, Section 3.1) for BNP-MFA if using Gibbs sampling? Is it the reason why the experiments in Table 1 is incomplete?\n3. For Eqn (7), how do you calculate \\beta\'s to ""scale the correct class label higher than incorrect classes for the cases...?""\n4. Is the proposed model robust to all kinds of attacks, like gradient based noise, and outliers which locates far away from the corresponding cluster?\n5. Can you provide some experiments to show the advantage over other approaches?[1]\n\n\nI highly encourage the use of BNP KDE which has many advantages as stated in the paper. But the authors may have to solve the problem of scalability and show advantages over other approaches.\n\n[1]Uličný, Matej, Jens Lundström, and Stefan Byttner. ""Robustness of deep convolutional neural networks for image recognition."" International Symposium on Intelligent Computing Systems. Springer, Cham, 2016.']","[-80, 20, -20]","[-20, 60, 60]","[""The sentiment score is highly negative (-80) because the reviewer states the work 'lacks even the most rudimentary evidence' and they 'refrain from providing a full review' due to the inadequacies. They criticize the testing methodology as insufficient and suggest the results are unsurprising and easily explained. The politeness score is slightly negative (-20) because while the language is not overtly rude, the tone is quite dismissive and critical. The reviewer bluntly states the work's shortcomings without much softening language, though they do provide a reference for improvement which slightly mitigates the negative tone."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's promising experimental results and the novel approach of applying geometric perspective and unsupervised model augmentation. However, the reviewer also raises questions and points out some issues, which tempers the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, framing criticisms as questions or 'minor issues' rather than direct criticisms. The reviewer also acknowledges the paper's contributions before raising concerns, which is a polite approach. The language is professional and constructive, without any harsh or rude phrasing."", ""The sentiment score is slightly negative (-20) because while the reviewer appreciates the authors' efforts, they express several concerns and suggest that more technical details and experiments are needed. The review ends with a statement encouraging the use of BNP KDE but also pointing out that the authors need to address scalability issues and demonstrate advantages over other approaches. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, starting with 'I appreciate the authors' efforts' and using phrases like 'I highly encourage'. The reviewer also frames their criticisms as questions or suggestions rather than direct criticisms. However, it's not extremely polite as it does contain direct critiques and doesn't use overly deferential language.""]"
"['This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.\n\nThe proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.\n\nI like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between ""compound questions"" instead of ""multi-hop questions"". In my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.\n\nMy only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:\n\n(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).\n\n(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include ""Search-based Neural Structured Learning for Sequential Question Answering"" and ""ComplexWebQuestions"". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.', 'This paper proposes a knowledge-based QA system that learns to decompose compound questions into simple ones. The decomposition is modeled by assigning each token in the input question to one of the partitions and receiving reward signal based on the final gold answer. The model achieves the state-of-the-art performance on the MetaQA dataset. \n\nMy main complaint about the paper is its lack of technical details and analysis of empirical results. Parts of the paper seem quite unclear, for example:\n\nIn the last paragraph of Section 3.1, it says “We do not assume that nay question should be divided into exactly three parts. … See section 4 for case study.” Does this mean that the model can have <=3 partitions, but not more? How is this number decided?\n\nSection 3.2 describes the simple-question answer. From Eq (4), it seems that the answerer only uses the current partition, is that the case? Moreover, how is the gold relation r obtained?\n\nIt would be nice to add more explanation to the caption of Figure 4 to make it self-contained.\n\nThe case study section (4.3) only contains a single example. It would be very helpful to include more examples of question partitions (there is enough space). Error analysis would also be helpful to understand, for example, why the proposed model is worse than VRN (Zhang et al. 2017) on 1- and 2-hop questions.\n', 'Summary: the paper is interested in parsing compound questions for querying on knowledge graph, e.g. MetaQA by Zhang et al. (2017). The paper proposes to have two modules, one that segments the question into partitions (up to three) and the other that looks at each segment to get the relation. The relations are merged to obtain a single KG path, which is queried to obtain the answer. Since the segmentation is a non-differentiable process, the paper uses reinforcement learning to propagate gradient to the segmentation model. The segmentation is a process of classifying each word for which partition it should be tied to. Answering is a process of classifying the partition into one of the possible relation edges. The model shows expected results in a synthetic arithmetic dataset, and obtains the state of the art in MetaQA, improving nearly 5% over the baseline. The model especially does much better on 3-hop questions, with nearly 20% improvement.\n\nStrengths: the paper is well-written. The model is simple yet effective and is a novel contribution to compound question answering on KG. Especially, the improvement on 3-hop category is nearly 20%, which is substantial and quite impressive. \n\nWeaknesses: My biggest concern is the lack of discussions on its relevance to  (Iyyer et al., 2016), which also proposed to decompose question into simpler ones for WIkiTableQuestions. Also, I think it would be good to mention Semantic Role Labeling as related literature, which is about tagging each word with its role in the sentence. The partition index can be somewhat considered as a “role” in the sentence.\n\nQuestions:\n1. How do you obtain x^(k)? Is it the last state of the LSTM?\n2. Why did you have to augment “NO_OP” relation in the MetaQA dataset?\n3. Why +1 reward has lower variance than probabilistic reward? Explanation or citation would be needed.\n4. What if two partitions need to share a word? The current setup necessitates that a word participates in only one partition. Wouldn’t this be problematic?\n5. I am a bit confused about how the simple question answering module is trained. Is it directly trained by the gold relation label?\n\nTypos and Suggestions:\n- Second paragraph of 2.1: in stead -> instead\n- Third paragraph of 2.1: research. -> research\n- c_t + h_t: would be good to explicitly mention that the circled plus sign is concatenation.\n- Last paragraph on page 4: “leave to be”?\n- Second last paragraph of 4.1: he -> The\n- Second paragraph of 4.2: “if exists a proper meaning”?\n- First paragraph of page 7: be either assume -> either assume\n- Last paragraph of Section 5: generalizing -> generalize\n- I think you should not put acknowledgment in a double-blind submission.\n\nM Iyyer, W Yih, MW Chang. Answering complicated question intents expressed in decomposed question sequences. 2016 (https://arxiv.org/abs/1611.01242)\n']","[60, -20, 70]","[80, 50, 80]","[""The sentiment score is 60 (moderately positive) because the reviewer expresses liking the proposed idea and approach, calling it a 'straightforward solution' and praising the clarification of terminology. However, they do have some concerns about the evaluation, which prevents a higher score. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms constructively as questions or suggestions, and acknowledges positive aspects before raising concerns. They use phrases like 'I like', 'Could you explain', and 'I think the authors should' which maintain a collegial tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's achievements ('achieves the state-of-the-art performance'), they express a 'main complaint' about lack of technical details and analysis. They also point out several unclear aspects and suggest improvements, indicating overall dissatisfaction with the current state of the paper. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, such as 'it would be nice' and 'it would be very helpful', even when expressing criticisms. They frame their feedback as suggestions rather than demands, maintaining a respectful tone. The reviewer also balances criticism with positive acknowledgment of the paper's contributions."", ""The sentiment score is 70 (positive) because the reviewer highlights several strengths of the paper, including that it is well-written, presents a novel and effective model, and shows impressive improvements over baselines. The reviewer also notes some weaknesses and questions, but these are presented constructively. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and provides helpful suggestions for improvement. The reviewer also acknowledges the paper's strengths before discussing weaknesses, which is a polite approach. The use of phrases like 'I think it would be good to' and 'I am a bit confused about' further contribute to the polite tone.""]"
"[""# Positive aspects of this submission\n\n- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.\n\n- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.\n\n- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.\n\n# Criticism\n\n- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.\n\n- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.\n\n- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem."", ""This paper proposes a method for multi-document abstractive summarization. The model has two main components, one part is an autoencoder used to help learn encoded document representations which can be used to reconstruct the original documents, and a second component for the summarization step which also aims to ensure that the summary is similar to the original document. \n\nThe biggest problem with this paper is in its evaluation methodology. I don't really know what any of the three evaluation measures are actually measuring, and there is no human subject evaluation back them up.\n- Rating Accuracy seems to depend on the choice of CLF used, and at best says whether the summary conveys the same average opinion as the original reviews. This captures a small amount about the actual contents of the reviews. For example, it does not capture the distribution of opinions, or the actual contents that are conveyed.\n- Word Overlap with the original documents does not seem to be a good measure of quality for abstractive systems, as there could easily be abstractive summaries with low overlap that are nevertheless very good exactly because they aggregate information and generalize. It is certainly not appropriate to use to compare between extractive and abstractive systems.\n-There are many well-known problems with using log likelihood as a measure of fluency and grammaticality, such as biases around length, and frequency of the words.\nIt also seems that these evaluation measures would interact with the length of the summary being evaluated in ways which systems could game.\n\nOther points:\n- Multi-Lead-1: The lead baseline works very well in single-document news summarization. Since this model is being applied in a multi-document setting to something that is not news, it is hard to see how this baseline is justified.\n\n- Despite the fact that the model is only applied to product reviews, and there seem to be modelling decisions tailored to this domain, the paper title does not specify so, which in my opinion is a type of over-claiming.\n\nHaving a paper with poor evaluation measure may set a precedent that causes damage to an entire line of research. For this reason, I am not comfortable with recommending an accept.\n\n\n---\nThank you for responding to my comments and updating the paper. I have slightly raised my score to reflect this effort.\n\nThere are new claims in the results section that do not seem to be warranted given the human evaluation. The claim is that the human evaluation results validate the use of the automatic metrics. The new human evaluation results show that the proposed abstractive model performs on par with the extractive model in terms of conveying the overall sentiment and information (Table 2), whereas it substantially outperforms the extractive model on the automatic measures (Table 1). This seems to be evidence that the automatic measures do not correlate with human judgments, and should not be used as evaluation measures.\n\nI am also glad that the title was changed to reflect the scope of the experiments. I would now suggest comparing against previous work in opinion summarization which do not assume gold-standard summaries for training. Here are two representative papers:\n\nGanesan et al. Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions. COLING 2010.\nCarenini et al. Multi-Document Summarization of Evaluative Text. Computational Intellgience 2012."", ""Overall and positives:\n\nThe paper investigates the problem of multidocument summarization\nwithout paired documents to summary data, thus using an unsupervised\napproach. The main model is constructed using a pair of locked\nautoencoders and decoders. The model is trained to optimize the\ncombination of 1. Loss between reconstructions of the original reviews\n(from the encoded reviews) and original the reviews, 2. And the\naverage similarity of the encoded version of the docs with the encoded\nrepresentation of the summary, generated from the mean representation\nof the given documents.\n\nBy comparing with a few simple baseline models, the authors were able\nto demonstrate the potential of the design against several naive\napproaches (on real datasets, YELP and AMAZON reviews). \nThe necessity of several model components is demonstrated\nthrough ablation studies. The paper is relatively well structured and\ncomplete. The topic of the paper fits well with ICLR. The paper\nprovides decent technical contributions with some novel ideas about\nmulti-doc summary learning models without a (supervised) paired\ndata set.\n\nComments / Issues\n\n[ issue 6 is most important ]\n\n1.  Problem presentation. The problem was not properly introduced and\nelaborated. In fact, there is not a formal and mathematical\nintroduction of the problem, input, output, dataset and model\nparameters. The notations used are not very clearly defined and are\nquite handwavy, (e.g. what is V, dimensions of inputs x_i was not\nmentioned until much later in the paper). The authors should make\nthese more precise. Similar problem with presentations of the models,\nparameters, and hyperparameters.\n\n3.  How does non-equal weighted linear combinations of l_rec and l_sim\nchange the results? Other variation of the overall loss function? How\ndo we see the loss function interaction in the training, validation\nand test data? With the proposed model, these could be interesting to\nobserve.\n\n4.  In equation two, the decoder seems to be very directly affecting\nthe quality of the output summary. Teacher forcing was used to train\nthe decoder in part (1) of the model, but without ground truth, I\nwould expect more discussions and experiments on how the Gumbel\nsoftmax trick affect or help the performance of the output.\n\n5.  Baseline models and metrics\n\n(1) There should be more details on how the language model is trained,\nsome examples, and how the reviews are generated from the language\nmodel as a base model (in supplement?).\n\n(2). It is difficult to get a sense of how these metrics corresponds\nto the actual perceived quality of the summary from the\npresentation. (see next)\n\n(3). It will be more relevant to evaluate the proposed design\nvs. other neural models, and/or more tested and proved methods.\n\n6. The rating classifier (CLF) is intriguing, but it's not clearly\nexplained and its effect on the evaluation of the performance is not\nclear: One of the key metrics used in the evaluation relies on the\noutput rating of a classifier, CLF, that predicts reader ratings on\nreviews (eg on YELP).  The classifier is said to have 72%\naccuracy. First, the accuracy is not clearly defined, and the details\nof the classifier and its training is not explained (what features are\nits input, is the output ordinal regression).  Equation 4 is not\nexplained clearly: what does 'comparing' in 'by comparing the\npredicted rating given the summary rating..' mean?  The classifier may\nhave good performance, but it's unclear how this accuracy should\naffect the results of the model comparisons.\n\nThe CLF is used to evaluate the rating of output\nreviews from various models. There is no justification these outputs\nare in the same space or generally the same type of document with the\ntraining sample (assuming real Yelp reviews).  That is probably\nparticularly true for concatenation of the reviews, and the CLF classifier\nscores the concatenation very high (or  eq 4 somehow leads to highest value\nfor the concatenation of reviews )... It's not clear whether such a classifier is \nbeneficial in this context.\n\n7. Summary vs Reviews. It seems that the model is built on an implicit\nassumption that the output summary of the multi-doc should be\nsufficiently similar with the individual input docs.  This may be not\ntrue in many cases, which affects whether the approach generalizes.\nDoc inputs could be covering different aspects of the review subject\n(heterogeneity among the input docs, including topics, sentiment etc),\nor they could have very different writing styles or length compared to\na summary.  The evaluation metrics may not work well in such\nscenarios.  Maybe some pre-classification or clustering of the inputs,\nand then doing summarization for each, would  help?  In the conclusions section, the\nauthors do mention summarizing negative and positive reviews\nseparately.\n\n\n\n\n\n""]","[70, -60, 20]","[80, 20, 60]","[""The sentiment score is 70 (positive) because the review begins with strong praise for the submission, highlighting its novelty, creativity, and usefulness. The reviewer also commends specific aspects like the alternative metrics and model variations. While there are criticisms, they are presented as limitations rather than severe flaws. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, balancing praise with constructive criticism. They acknowledge the paper's contributions even when pointing out limitations, using phrases like 'This shouldn't overshadow the bigger contributions of this paper.' The review maintains a professional and courteous tone, offering specific feedback without harsh or dismissive language."", ""The sentiment score is -60 because the reviewer expresses significant concerns about the paper's evaluation methodology, stating it's the 'biggest problem' and that they are 'not comfortable with recommending an accept.' They also mention that poor evaluation measures could damage an entire line of research. However, it's not entirely negative as they acknowledge some improvements in the revised version. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'Thank you for responding to my comments' and offer constructive suggestions for improvement. The language is not overtly polite, but it avoids rudeness and maintains a respectful, academic tone."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges some strengths of the paper, such as its novel approach, decent technical contributions, and good fit with ICLR. However, the review also lists several significant issues and areas for improvement, which tempers the overall positivity. The politeness score is moderately high (60) as the reviewer uses professional and respectful language throughout, offering constructive criticism without harsh or dismissive comments. The reviewer acknowledges the paper's merits before delving into issues, and phrases suggestions as recommendations rather than demands. The tone remains objective and focused on improving the paper rather than criticizing the authors personally.""]"
"[""The whole model can be simplified by this: using auto-encoders for X and Y's reconstruction, then use Triple GAN loss to match the joint distribution of (X, Y).  However, the deterministic model with GAN loss looks problematic to me.\n\nquestions:\n\n1. Although the authors showed strong evidence in their experiment part, they still failed to compare models with Bicycle-GAN, i.e., how Bicycle GAN performs on these two dataset?\n\n2. missing some comparison: why use simplified Triple-GAN loss (i.e. without two regularization terms)  instead of Triangle-GAN, which is addressed to be better? I think the authors need to discuss about this. Also, the authors need to use MMD and other methods mentioned in the original WAE paper.\n\n3. In table 1, without triple-GAN loss, the whole model is deterministic, but the authors can still show the FID score for the generalization ability, which is better than all other cycle-GAN based models, why is that possible? Is this equivalent to claim that auto-encoder has the ability to generate realistic images just by sampling z? \n(If I understand the experiment correctly, the author's synthesized images is generated by $y_hat = G_2(E_1(X))$, no sampling z required)\n\n4. Can the authors show the generalization ability of JWAE? For example, with input X, we can have different correct corresponding Ys, just like Bicycle-GAN did.\n\n"", 'This paper proposes a joint Wasserstein Auto-Encoder (JWAE) method to solve the problem of joint distribution matching. Instead of ﬁnding a coupling, the paper seeks a decoupling to make the primal problem of Wasserstein distance tractable. The decoupled version of joint Wasserstein distance is used for empirical reconstruction losses of within-domain Auto-Encodings and cyclic mappings. In addition, two GAN divergences are used to learn the cross-domain mappings such that the generated distributions are close to the real distribution, and another GAN divergence is imposed to align the latent distributions generated by two Auto-Encoders. Later, the paper applies the proposed model on the interpolation based video-to-video synthesis problem. \n\nAs far as I understand, the paper can be thought of revisiting the Cycle-Consistent Adversarial Networks (CycleGAN) from the joint Wasserstein Auto-Encoder point of view. In other words, it essentially extends the CycleGAN using additional within-domain auto-encoding reconstruction losses and the latent code alignment loss. Accordingly, the proposed model can be naturally applied to image-to-image translation. I have no idea why the paper merely applies it to interpolation based video-to-video translation. In addition, as the paper tries to apply the relaxed optimal Wasserstein distance to Auto-Encoder and cycle consistency losses, why not apply such Wasserstein distance to the distribution divergence as well. To study the generative power of the proposed generative model using the relaxed Wasserstein distance, it is quite necessary to evaluate the use of exit Wasserstein distance based VAE (e.g., Wasserstein AE) and GAN (e.g., Wasserstein GAN and spectral normalized Wasserstein GAN) losses. ', 'This paper studies the joint distribution matching problem where given data samples in two different domains, one is interested in learning a bi-directional mapping between unpaired data elements in these domains. The paper proposes a joint Wasserstein auto-encoder (JWAE) to solve this problem. The paper shows that under the decomposable cost metric and deterministic decoding maps, the optimization problem associated with the JWAE formulation can be reduced to a tractable optimization problem. The paper also establishes a generalization bound for the JWAE formulation. Finally, the paper conducts an experimental evaluation of the proposed solution with the help of a video-to-video synthesis problem and show improved performance as compared to the existing results in the literature.\n\nOverall, the reviewer finds that the paper considers an important problem and proposes some interesting ideas to tackle the problem. However, in its current form, there is a large scope for improvement in the presentation of the paper. The paper is full of errors/typos which make it an extremely difficult read (see my comments below). That said the paper fairs quite well as compared to other existing methods. Since the reviewer is not very much familiar with this field, the reviewer leaves it to the other reviewers to decide the significance of these results.\n\nPros: \n\n1) The paper aims to provide a theoretical treatment of the joint distribution matching problem which has many interesting applications, including image-to-image translation and video-to-video synthesis. \n\n2) The proposed method in the paper had good empirical performance on the real world datasets.\n\nCons:\n\nThe paper is very poorly written with many typos and (possibly) mistakes. Some of the comments in this direction are as follows.\n\n1) The paper does not formally define the underlying problem before diving into the details of the proposed solution. The authors only informally talk about the problem in the introduction. Given that the ICLR has a wide audience, it would have been nice if the authors have made the presentation of the paper self-contained.\n\n2) In the same vein, the paper talks about many important quantities without introducing them first. E.g., what are $E_{A}(f^*)$, $E_B^g(f^*)$ etc. in the statement of Theorem 2? These quantities are first defined inside the proofs in the supplementary material!\n\n3) Some of the notation in the paper is also very confusing. For example, cross-domain mapping have two different sets of notations. $(E1oG2, E2oG)$ in Sec. 4.2 and $(G2oE1, G1oE2)$ in Section 5. It should be latter. Similarly, Sec. 4.2 refers to $E1oG1$ and $E2oG2$ as auto-encoders, which should be $G1oE1$ and $G2oE2$, respectively. In Sec. 3, the authors refer to $N$ and $M$ as the number of samples in the $X$ and $Y$ domain, respectively. This is then reversed in Theorem 2 and 4. These are only a small list of large number of such typos. Also, what is the notion defined in the last line of Sec. 3?\n\n4) It is not clear to me why the sets $Q_1$ and $Q_2$ in Theorem 1 are define in their current forms. In particular, it is not clear why the equality hold in Eq. (17) in the proof of Theorem 1. \n\n5) One line in the proof of Lemma 1 says, ""Specifically, we choose its equality, then we have"". Could the authors elaborate on this?\n\n6) Eq. (24) should be inequality?\n\n7) Given that the authors write a regularized problem in (4). Does that mean now sets $Q_1$ and $Q_2$ are different from how they are defined in the statement of Theorem 1?\n\n#########################\n\nPost rebuttal: The authors have addressed most of my concerns regarding the poor presentation of the earlier version. I have updated my score.\n\n']","[-20, -20, -20]","[50, 50, 50]","[""The sentiment score is slightly negative (-20) because the reviewer expresses some concerns about the model and points out several missing comparisons and explanations. The opening statement suggests that the model can be simplified, which implies some criticism. However, the reviewer also acknowledges the 'strong evidence in their experiment part,' which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They phrase their concerns as questions rather than direct criticisms, which is a polite approach. The reviewer also uses phrases like 'I think the authors need to discuss about this' instead of making demands, which contributes to the politeness. The absence of harsh or accusatory language also supports this score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions, they express skepticism about the limited application and suggest several improvements. The reviewer questions the focus on video-to-video translation and suggests evaluating against existing methods. This indicates some reservations about the paper's current state. The politeness score is moderately positive (50) as the reviewer uses neutral language and phrases criticisms as suggestions or questions rather than direct criticisms. They use phrases like 'As far as I understand' and 'I have no idea why' which maintain a respectful tone while expressing concerns. The review is constructive and professional, avoiding harsh or rude language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting ideas', 'good empirical performance'), they express significant concerns about the paper's presentation, describing it as 'very poorly written' with 'many typos and (possibly) mistakes'. The reviewer also states there is 'large scope for improvement'. However, the score is not deeply negative as the reviewer recognizes the paper's merits and leaves final judgment to other reviewers. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'the reviewer finds' and 'it would have been nice if', even when pointing out flaws. They also balance criticism with positive comments and provide specific, constructive feedback rather than harsh judgments.""]"
"['This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network\'s feature vector (output of layer before last) and learn a direct mapping to the GAN\'s latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. \n\nInference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate ""edited"" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN\'s performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple.\n\nIn the experimental part, the authors first compare their approach to the \'naive encoder\' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder\'s latent space is added, and the rest of the discriminator\'s layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN\'s is precluded by the fact that different examples are chosen. This oversight does not affect the paper\'s relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained.\n\nThe article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there\'s a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869.\n\nOverall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm\'s simplicity, most of the paper is devoted to experiments and discussions.\n \n', 'The paper proposes using the GAN discriminator for inference mapping, mapping an image to a latent code that would be used to generate the image by the encoder, based on the argument that the discriminator can be used as a powerful feature extractor because it has seen both real and fake data during training. The paper compares the proposed approach to several approaches that train an inference model together with a generator. \n\nWhile the paper compares its approach to several baselines, they are not the most relevant ones. In fact, the most relevant baseline is not cited and compared. As a result, the novelty of the paper is not justified. Specifically, the baselines the paper compare to are mostly methods that jointly learn an inference model and a generation model, while the proposed approach first learns a generation model and then fits an inference model (it is referred to as the connection network in the paper). In this regard, the paper should compare its approach to methods that first learns a generation model and then learns an inference model. The iGAN work by Zhu et. al. ECCV 2016 is arguably most relevant approach. Especially, they also use the discriminator architecture for the inverse mapping. Unfortunately, the work is neither cited nor compared.\n\nIn addition, pretrained networks such as VGG and ResNet have been known to be powerful feature extractor. It would be ideal the paper can compare the proposed approach to that using VGG and ResNet for finding the z for a given image.\n\nFinally, the paper seems to lack of comprehensive knowledge on how the inference mapping has been investigated in the GAN literature. For example,  the statement that ""BEGAN (Berthelot et al., 2017) made the first attempt to solve the inverse mapping from x to z using the non-convex optimization"" in the introduction section is incorrect. The scheme is used in at least two 2016 papers (Liu and Tuzel NIPS 2016 and Zhu et. al. ECCV 2016).', 'Paper Summary: \nThis paper proposes to reconstruct the generated images to the their corresponding latent code. As claimed, the goal is to improve the accuracy and efficiency of inference mapping better than other inference mapping techniques, while maintaining their generation quality.\n Instead of using an independent encoder, the authors propose to share the encoder parameters with the discriminator: a Connection Network (CN) is built on top of the features extracted by the discriminator. The weight-sharing machisme shows better performance in Figure 1.\nThe proposed method has two benefits: : a) manipulating the image by disentangling the latent space and b) suggesting a new metric for assessing the GAN model by measuring reconstruction errors of real data.\n\nGeneral Comments:\nIn term of algorithm, the paper essentially adds the conscontruction term (CN) to the standard GAN loss, and partially shares the weights of the “encoder” and discriminator. However, it is almost identical to the existing works, which are NOT cited, and the connections are not discussed.\n\nConnection to InfoGAN: To relate the generated images to the latent code,  the proposed method employs the reconstruction loss, InfoGAN employs the mutual information. Note that reconstruction loss = negative log likelihood, and effectively is equivalent to Mutual Information and Conditional Entropy in the case. Please see the discussion in Lemma 3 and Appendix A of [3] for detailed discussion.  Further, InfoGAN has proposed to to sharing weights of the encoder and discriminator, exactly the same with this submission. The claimed advantage is to disentangle the latent space. It is not surprise at all, once the authors see the connection to InfoGAN, which was originally proposed to disentangle the latent codes.\n\nConnection to CycleGAN: CycleGAN consists of four losses: two reconstruction losses and two standard GAN losses. As shown in Section 4 of [3] “Connecting ALI and CycleGAN”, one reconstruction loss and  one standard GAN loss is sufficient to achieve CycleGAN’s objective, the other two losses would only help to accelerate. In another word, the proposed method is exactly half of the CycleGAN losses.\n\nThe author mention in Abstract that “the bidirectional generative models introduce an encoder to establish the inverse path of the generation process. Unfortunately, their inference mapping does not accurately predict the latent vector from the data because the imperfect generator rather interferes the encoder training.” This is the non-identifiable issue of ALI/BiGAN discovered in [3]. Please clarify. \n\nThe proposed method should compare with [1] and [2] in great detail, to demonstrate its own advantages. Given the missing literature, the current experimental comparisons seem not that meaningful, because the baseline methods are not really the competitors. \n\nOne interesting contribution of the submission is to consider the reconstruction errors to measure the quality of GANs. To my best knowledge, it is original. \n\n\nReferences:\n\n[1] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, NIPS 2016\n[2] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV 2017\n[3] ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching, NIPS 2017\n']","[80, -50, -50]","[70, 20, 20]","[""The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper, describing it as 'novel', 'significant', and 'technically sound'. They highlight the paper's contributions and usefulness in the field. The few criticisms are minor and constructive. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, offering constructive feedback and suggestions without harsh criticism. They acknowledge the paper's strengths while politely pointing out areas for improvement. The tone is professional and courteous, with phrases like 'I wish the authors would have' and 'Overall, I find this paper provides...' indicating a respectful approach to feedback."", ""The sentiment score is -50 because the review is generally critical, pointing out significant shortcomings in the paper's comparison to relevant baselines and its lack of comprehensive knowledge on the topic. However, it's not entirely negative as it acknowledges the paper's proposal and comparisons to some baselines. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional and respectful language throughout. They offer specific suggestions for improvement and explain their concerns clearly without using harsh or rude language. The reviewer uses phrases like 'it would be ideal' and 'unfortunately' which soften the criticism slightly."", ""The sentiment score is -50 because the reviewer points out significant issues with the paper, particularly its lack of originality and failure to cite relevant existing work. The reviewer states that the proposed method is 'almost identical to the existing works' and questions the meaningfulness of the experimental comparisons. However, the score is not lower because the reviewer does acknowledge one 'interesting contribution' of the paper. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use neutral language like 'please clarify' and 'should compare' rather than harsh or dismissive phrasing. The reviewer also provides constructive feedback by suggesting specific papers for comparison and explaining the connections to existing work in detail.""]"
"['The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. \n\nClarity: \nSection 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.\n\nOriginality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.\n\nEvaluation: \nThe evaluation is not very strong for the following reasons:\n\n1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960\n\n2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956\n\n3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.\n\nThis makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.\n\nAlso, which test set are you using?\n\nFinally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? \nPlease reference this figure in the text.', ""Differentiable Expected BLEU for Text Generation\n\nPaper Summary:\n\nNeural translation systems optimizes training data likelihood, not the end metric of interest BLEU. This work proposes to approximate BLEU with a continuous, differentiable function that can be optimized during training.\n\nReview:\n\nThe paper reads well. It has a few but crucial missing references. The motivation is easy to understand and a relevant problem to work on. The main weaknesses of the work lies in its very loose derivations, and its weak empirical results.\n\nFirst on context/missing references: the author ignores approaches optimizing BLEU with log linear models (Franz Och 2003), and the structured prediction literature in general, both for exact (Tsochantaridis et al 2004) and approximate search (Daume and Marcu 2005). This type of approach has been applied to NMT recently (Edunov et al 2018). Your paper also misses important references addressing BLEU optimization with reinforcement strategies (Norouzi et al 2016) or (Bahdanau et al 2017). Although not targeting BLEU directly (Wiseman and Rush 16) is also a reference to cite wrt optimizing search quality directly. \n\nOn empirical results, you chose to work IWSLT in the de-en direction while most of the literature worked on en-de. It prevents comparing your results to other papers. I would suggest to switch directions and/or to report results from other methods (Ranzato et al 2015; Wiseman and Rush 2016; Norouzi et al 2016; Edunov et al 2018). De-en is generally easier than en-de (generating German) and your BLEU scores are particularly low < 25 for de-en while other methods ranges in 26-33 BLEU for en-de (Edunov et al 2018).\n\nOn the method itself, approximating BLEU with a continuous function is not easy and the approach you take involves swapping function composition and expectation multiple times in a loose way. You acknowledge that (7) is unprincipled but (10) is also problematic since this equation does not acknowledge that successive ngrams overlap and cannot be considered independent. Also, the dependence of successive words is core to NMT/conditional language models and the independence hypothesis from the footnote on page 4 can be true only for a bag of word model. Overall, I feel that given the shortcuts you take, you need to justify that your approximation of BLEU is still correlated with BLEU. I would suggest to sample from a well trained NMT system to collect several hypotheses and to measure how well your BLEU approximation correlate with BLEU. How many times BLEU decides that hypA > hypB but your approximation invert this relation? is it true for large difference, small difference of BLEU score? at low BLEU score, high BLEU score?\n\nFinally, you do not mention the distinction between expected BLEU  \\sum_y P(y|x) BLEU(y, ref) and the BLEU obtained by beam search which only look at (an estimate of) the most likely sequence y* = argmax P(y|x) . Your approach and most reinforcement strategy targets optimizing expected BLEU, but this has no guarantee to make BLEU(y*, ref) any better. Could you report both an estimate of expected BLEU and beam BLEU for different methods? In particular, MERT (), beam optimization (Wiseman and Rush 2016) and  structured prediction (Edunov et al 2018) explicitly make this distinction. This is not a side issue as this discussion is in tension with your motivations.\n\nReview Summary:\n\nThe paper misses important references. It chooses an empirical setup which prevents comparison with related work, and the report results on de-en seem weak. The proposed approach does not bound or estimate how far from BLEU is the proposed approximation. This means that the authors need to justify empirically that it preserves correlation with BLEU, which is not shown in the paper.\n\nMissing references\n\nAn Actor-Critic Algorithm for Sequence Prediction (ICLR 2017)  Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio\n\nHal Daume III and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. ICML 2005.\n\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato\nClassical Structured Prediction Losses for Sequence to Sequence Learning, NAACL 18\n\nMinimum Error Rate Training in Statistical Machine Translation Franz Josef Och. 2003 ACL\n\nI. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, Support Vector Machine Learning for Interdependent and Structured Output Spaces, ICML 2004.\n\nMohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Reward Augmented Maximum Likelihood for Neural Structured Prediction, 2016\n\nSequence-to-Sequence Learning as Beam-Search Optimization, Sam Wiseman and Alexander M. Rush., EMNLP 2016\n"", 'This paper proposed a differentiable metric for text generation tasks inspired by BLEU and a random training method by the Gumbel-softmax trick to utilize the proposed metric. Experiments showed that the proposed method improves BLEU compared with simple cross entropy training and policy gradient training.\n\nPros:\n* The new metric provides a direct perspective on how good the conjunction of a generated sentence is, which has not been provided other metric historically used on language generation tasks, such as cross-entropy. \n\nCons:\n* Too many approximations that blur the relationship between the original metric (BLEU) and the derived metric.\n* Not enough experiments and poor discussion. Authors should consume more space in the paper for experiments.\n\nThe formulation of the metric consists of many approximations and it looks no longer BLEU, although the new metric shares the same motivation: ""introducing accuracy of n-gram conjunction"" to evaluate outputs. Selecting BLEU as the starting point of this study seems not a reasonable idea. Most approximations look gratuitously introduced to force to modify BLEU to the final metric, but choosing an appropriate motivation first may conduct more straightforward metric for this purpose.\n\nIn experiments on machine translation, its setting looks problematic. The corpus size is relatively smaller than other standard tasks (e.g., WMT) but the size of the network layers is large. This may result in an over-fitting of the model easily, as shown in the results of cross-entropy training in Figure 3. Authors mentioned that this tendency is caused by the ""misalignment between cross entropy and BLEU,"" however they should first remove other trivial reasons before referring an additional hypothesis.\nIn addition, the paper proposed a training method based on Gumbel softmax and annealing which affect the training stability through additional hyperparameters and annealing settings. Since the paper provided only one training case of the proposed method, we couldn\'t discuss if the result can be generalized or just a lucky.\n\nIf the lengths of source and target are assumed as same, the BP factor becomes always 1. Why the final metric (Eq. 17) maintains this factor?']","[-30, -50, -30]","[20, 20, 20]","[""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('nice idea'), they express significant concerns about the evaluation, weak baselines, and limited experiments. The overall tone suggests skepticism about the significance of the results. The politeness score is 20 because the reviewer uses generally polite language and phrases criticisms as suggestions or questions ('Please rewrite...', 'Why is...', 'Please reference...'). However, the directness of some critiques prevents a higher politeness score. The reviewer maintains a professional tone throughout, balancing constructive feedback with clear expression of concerns."", ""The sentiment score is -50 because while the reviewer acknowledges some positives ('The paper reads well', 'motivation is easy to understand'), they primarily focus on criticisms and weaknesses. They point out 'crucial missing references', 'very loose derivations', and 'weak empirical results'. The reviewer also questions the methodology and suggests significant changes. The politeness score is 20 because the reviewer uses generally polite language ('I would suggest', 'Could you report') and acknowledges some positives, but is also quite direct in their criticisms. They maintain a professional tone throughout without using overly harsh language, balancing critique with constructive suggestions."", ""The sentiment score is -30 because the review is generally critical, with more cons than pros mentioned. The reviewer points out several issues with the paper's methodology, experiments, and conclusions. However, it's not entirely negative as it does acknowledge some positive aspects, such as the new perspective provided by the metric. The politeness score is 20 because the language used is professional and constructive, avoiding harsh or personal criticisms. The reviewer uses neutral phrases like 'Authors should' and 'looks problematic' rather than more confrontational language. The tone is academic and objective, offering specific suggestions for improvement, which is a polite approach in peer reviews.""]"
"[""Summary\nThis paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.\nThe main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.\nSpecifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.\nBy performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.\n\nStrength\nLearning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\nThis paper tackles this problem and showed results on various segmentation problems.\n\nWeakness\nThe proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.\n\nThis paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.\nFor example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. \nIn addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.\nFor example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.\n\nThere are some strong arguments that require further justification. \n- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).\nHowever, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.\n- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.\n\nOverall comment\nI believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. \nEspecially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.\n"", 'Summary:\nThis paper proposed a few-shot learning approach for interactive segmentation. Given a set of user-annotated points, the proposed model learns to generate dense segmentation masks of objects. To incorporate the point-wise annotation, the guidance network is introduced. The proposed idea is applied to guided image segmentation, semantic segmentation, and video segmentation.\n\nClarity:\nOverall, the presentation of the paper can be significantly improved. First of all, it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images T in the first equation); It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two. Also, it is not clear how the authors incorporate the unannotated images for training. \n\nThe descriptions on model architecture are also not quite clear, as it involves two components (g and f) but start discussing with g without providing a clear overview of the combined model (I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer). The loss functions are introduced in the last part of the method, which makes it also very difficult to understand. \n\nOriginality and significance:\nThe technical contribution of the paper is very limited. I do not see many novel contributions in terms of both network architecture and learning perspective.\n\nExperiment:\nOverall, I am not quite convinced with the experiment results. The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016). \n\nThe experiment settings are also not clearly presented. For instance, what is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets? How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance? \n\nThe performance of the guided semantic segmentation is also quite low, limiting the practical usefulness of the method. Finally, the paper does not present qualitative results, which are essential to understanding the performance of the segmentation system. \n\nMinor comments:\n1. There are a lot of grammar issues. Please revise your draft.\n2. Please revise the notations in equations. For instance, \n    T = {{(x_1, L_1),...} \\cup {\\bar{x}_1,...}\n    L_s = {(p_j,l_j):j\\in{1,...,P}, l\\in{1,...,K}\\cup{\\emptyset}}\n    Also, in the next equation, j\\in\\bar{x}_q} -> p_ j\\in\\bar{x}_q} (j is an index of pixel)\n', ""To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks. In this paper's sense, segmentation. It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query. Cases of semantic, interactive and video segmentation are applied. Experiments are very thorough.\n\nWe see too many variants of few-shot learning papers on mini-imagenet or omniglot. For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work. I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n\nComments:\n\n- what is interactive segmentation? I looked through the related work, it just mentioned some previous work without defining or describing it.\n\n- z is the network output of g? is there any constraint on z? Like Gaussian distributions like what z is like in VAE models. \n\n""]","[-60, -60, 90]","[20, 20, 70]","[""The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges the interesting nature of the problem, they express significant concerns about the novelty, methodology, and conclusions of the paper. The reviewer uses phrases like 'relatively simple', 'closely related to existing approach', 'question the novelty', and 'rather incremental', indicating a negative sentiment. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I believe', 'I question', and 'Overall comment' to soften their criticisms, and they acknowledge the paper's strengths before discussing its weaknesses. The language is not overtly polite, but it avoids rudeness and maintains a constructive tone."", ""The sentiment score is -60 because the review is predominantly critical, pointing out numerous issues with clarity, originality, and experimental results. The reviewer states that the paper's presentation 'can be significantly improved,' the technical contribution is 'very limited,' and they are 'not quite convinced with the experiment results.' However, it's not entirely negative as the reviewer does provide constructive feedback and suggestions for improvement. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'I would suggest' and 'Please revise' which add a degree of politeness. The reviewer also provides specific, actionable feedback which is helpful and considerate. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score."", ""The sentiment score is 90 because the reviewer expresses strong positive sentiment towards the paper, praising it as the first to apply few-shot learning to high-level computer vision tasks and stating it 'deserves an acceptance'. They also mention it will inspire follow-up work, indicating high regard for its impact. The politeness score is 70 as the reviewer uses respectful and professional language throughout, acknowledging the paper's contributions and providing constructive feedback. The comments are framed as questions rather than criticisms, which is a polite approach. However, it's not overly formal or excessively polite, maintaining a neutral professional tone, hence not scoring higher.""]"
"['This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given.\n\nThis paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong.\n\nTheorem 3:\n""Note that $\\theta^i_t$ and $\\hat \\theta^i_t$ are initialized with the same initial distribution µ0 = ν0 and we can also set $\\theta^i_0$ to be independent of $\\hat \\theta^i_0$, we can have $\\gamma(0) = 0$. $\\gamma(0) = E \\|\\theta^i_0 - \\hat \\theta^i_0 \\|^2$."" - this doesn\'t seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density.\n\n""Then according to the Gronwall Lemma, we have"" - I don\'t see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and  $\\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case.\n\n""... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption ""We can tune the bandwidth of the RBF kernel to make ∇K ≤ H_∇K, which is omitted in the Assumption due to the space limit."" First, there is a missing norm, since ∇K is a vector and H_∇K is I believe a scalar constant. Second, c1 = H_∇K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn\'t mean that part of assumptions can be moved to the supplement).\n\nI find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don\'t understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations).\n\n""Note the first bullet indicates U to be a convex function and W to be ... "" I think it should be K, not W.\n\nTheorems 3-6 could be lemmas, while there should be a unifying theorem for the bound.\n\nFinally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process.\n\nOther comments:\n\nExample in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn\'t necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative.\n\nIt is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data.\n', 'This paper proposes a particle-based inference algorithm, the optimal update for each particle is the summation of the standard SGLD direction and SVGD velocity.  The work further analyzes non-asymptotic properties of SPOS. The results appear theoretically interesting and of potential practical value in designing inference algorithms. I did not go through the proofs in the supplementary. \n\n[Experimental results are not convincing] \n\n[BNN] I noticed the test RMSE and test LL of SVGD are directly copied from the original SVGD paper. However, the performance critically depends on:\n1.    Running time, or training epochs\n2.    Data partitions\nTo be a fair comparison, the authors should keep at least the training epochs and random partitions the same. Especially for the dataset Year, for which only one random partition is conducted. It’s highly likely that the performance gain is due to favored data partition rather than the superiority of the algorithm.\n\n[RL] Average rewards are significantly lower than the scores reported in the original SVPG paper?\n1.    From figure 3, SPOS only outperforms SVPG on envs Cartpole Swing Up and Double Pendulum. The best reward for env Cartpole Swing Up reported in this paper is around 200. However, the score is ~400 in the original SVPG paper. For the env Double Pendulum, there’s also very large performance gap. I am aware the code for SVPG is now publicly available, the authors may consider conducting the experiments with the same settings (e.g. same seed?). Otherwise, it’s hard to tell whether the performance gain is significant while the baseline is much worse than it should be.\n2.    Only 3 envs are reported, the authors may also consider reporting all the envs are used in the SVPG paper\n\n[Figure 1] The authors may consider reporting the exact settings of this case, otherwise, it’s hard to believe that SVGD would collapse on a simple 1D case.\n\nIf the authors can fully address the concerns above, I will consider changing the scores.  \n\nOther comments:\n\n-    Related papers:\n     Stein Variational Message Passing for Continuous Graphical Models,  Wang et al., ICML18 (https://arxiv.org/abs/1711.07168)\n     Stein Variational Gradient Descent as Moment Matching, Liu et al., NIPS18 (https://arxiv.org/abs/1810.11693)\n\n-    Page 30 crashes my browser all the time\n', 'Two promising methods for scalable sample-based Bayesian inference are:\n1) SGLD: simply discretize a standard Langevin dynamics to construct a Markov chain that approximate the correct invariant distribution. This reads: \n\nx_{t+1} = x_t + \\nabla \\log \\pi(x_t) \\delta + \\sqrt(2 \\delta) \\xi\n\n2) SVGD: the method can be expressed as a type of gradient descent of an appropriate functional on the space of probability distributions. A cloud of particles {x_i}_{I=1}^M evolves according to:\n\nx^i_{t+1} = x^i_t + (some functional of all the particles) \\, \\delta\n\nThe method proposed in the article is not very different from alternating the two above mentioned update, which is indeed quite a natural idea, and can work pretty well I think. The method reads:\n\nx^i_{t+1} = x^i_t + [ \\nabla \\log \\pi(x_t) \\delta + (some functional of all the particles) \\, \\delta ] + \\sqrt(2 \\delta) \\xi.\n\nPROS:\n- yes, I think that the method can work quite OK since it may be borrowing the strengths of both SGLD and SVGD.\n- It seems that the meat of the paper consists in proving some (non-asymptotic) convergence result. Unfortunately, this went above my head and I cannot claim that I have read the details of the proofs. \n\nCONS:\n- it is (very) difficult to fairly evaluate this type of methods in high-dimensional settings. I thus appreciate that the numerical section starts with a toy very simple Gaussian model. I would have been much more interested  in fair and extensive simulations in this type of settings where it is relatively easy to compare the proposed method with SGLD and SGVD. In other words, after reading the paper, I must say that I am not at all convinced that the method does bring something over SGLD or SVGD (although it is very possible that it does).  For example, comprehensive and fair comparisons with SGLD and SVGD  in Gaussian settings (not necessarily one-dimensional) could have been presented. The delicate tuning of the different methods, the speed of convergence wrt algorithmic time, the speed of comparison wrt the number of particles, etc.. could have been investigated numerically: this would have been, I think, much more convincing.\n\nMINOR comments:\n- I did check the proof of Theorem 2, which seems hand-wavy and overly complicated.  What is the function G? It seems that the proof of Theorem 2 simply consists in establishing that if each particle x_i follows the dynamics dx = F(x)*dt then the associated densities satisfy \\partial_t \\mu_t = -\\partial_x(F(x) * \\mu_t(x))  , which is obvious. But the situation in the paper is indeed more delicate since the particles are interacting, etc... Reading this proof got me very worried and did not motivate me to read the rest of the paper.\n\nSUMMARY:\n- the method is not terribly original -- this is a simple hybrid SVGD / SGLD -- but may work very well.\n- unfortunately, the numerical experiments are not convincing.\n\n']","[-50, -50, -20]","[20, 20, 50]","[""The sentiment score is -50 because the review starts positively but then becomes quite critical. The reviewer states 'This paper was a pleasant read until I decided to check the proof of Theorem 3,' after which they list several issues and errors they found. This indicates a significant shift from positive to negative sentiment. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'Could you please elaborate' and 'I suggest' which are polite ways of requesting clarification or suggesting changes. However, the reviewer is also direct in pointing out errors, which prevents the score from being higher. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the slightly positive politeness score."", ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('theoretically interesting and of potential practical value'), the majority of the review focuses on criticisms and concerns about the experimental results. The reviewer states that the 'Experimental results are not convincing' and raises several issues with the BNN and RL experiments. The politeness score is 20 because the reviewer uses generally professional language and offers constructive feedback. They use phrases like 'The authors may consider...' and 'If the authors can fully address the concerns above, I will consider changing the scores,' which indicate a willingness to reconsider their evaluation. However, the overall tone is critical, which prevents a higher politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('PROS'), they express significant concerns in the 'CONS' section. They state they are 'not at all convinced that the method does bring something over SGLD or SVGD' and find the numerical experiments 'not convincing'. The reviewer also expresses worry about the proof of Theorem 2. However, they do mention that the method 'may work very well', which prevents the score from being more negative. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They acknowledge potential strengths of the paper and frame criticisms constructively (e.g., 'I would have been much more interested in...'). The reviewer also admits when certain aspects are beyond their expertise ('this went above my head'). While critical, the tone remains courteous and constructive.""]"
"['Post-rebuttal\n------------------\nI have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:\n- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.\n- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.\n\n=========================================\nPros\n-------\n[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.\n\nLimitations\n----------------\n[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. \n\n[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. \n\n[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?\n\n[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn\'t the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \\lambda^* > 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. \n\n[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. \n\n\nGeneral comments\n---------------------------\n[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as ""gradient descent in Euclidean space"" (although the authors would need to also precisely explain what they mean by that statement).\n\n[Originality] Some related work that the authors should position their proposed problem/solution against:\n- There is some work on ""max-kernel search"" which can perform similarity search with general notions of similarity (than just Euclidean metrics).\n- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.\n\nMinor comments/typos\n---------------------------------\n- The authors should present the precise SL2G algorithm given the graph in the manuscript.\n- l^2 --> \\ell_2\n- gradient decent --> gradient descent\n- Table 1, f(q, x) --> f(x, q)', 'This work extends the approximate nearest neighbor search (ANNS) algorithm to a more general setting. Instead of search with a ""separable"" similarity measure, the authors propose Optimal Binary Functional Search (OBFS), where the scoring function f() is in general non-separable. The exact construction of the Binary Function Graph wrt f() and X is computationally expensive. The specific approximate algorithm of OBFS proposed in the paper is to:\n1) First construct an L2 Delaunay graph for based on the dataset X only and;\n2) Perform greedy search with the L2 Delaunay graph.\n\nThe authors also discuss various conditions under which, the approximation method can achieve close to optimal value.\n\nSome of the concerns I have with this work:\n\n1) The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in  Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.\n\n2) Lately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches.\n\nOverall, this research direction is interesting, but this specific work falls short for a publication at ICLR.\n\n']","[-20, -50]","[50, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('I am going to raise my rating by one point', 'presents a novel solution'), there are several criticisms and limitations pointed out. The reviewer mentions that the solution is 'somewhat obvious', the manuscript is 'hard to parse', and there are multiple areas needing clarification or improvement. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They acknowledge positive aspects, use phrases like 'I believe' and 'it would be good to', and provide constructive feedback without harsh criticism. The reviewer also offers specific suggestions for improvement, which is a polite way to provide feedback."", ""The sentiment score is -50 because while the reviewer acknowledges the work as 'interesting', they express several concerns and ultimately conclude that the work 'falls short for a publication at ICLR'. This indicates a generally negative sentiment, though not extremely so. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They present their concerns as 'Some of the concerns I have' rather than stating them as definitive flaws. The reviewer also acknowledges the potential of the research direction, which adds a polite tone. However, the review is not overly complimentary or deferential, maintaining a mostly neutral, professional tone.""]"
"['I am a researcher in NLP and know little about vision, so I cannot review this paper. I have contacted general chair about this situation. ', 'The authors propose that the ""checkerboard phenomenon"", whereby the gradients exhibit a repeating pattern over the pixel space, is a source of vulnerability to adversarial examples. They propose to first enhance this vulnerability for pre-trained models with a pre-conditioning layer, and then to evade it by zero padding the image to offset the pattern.\n\nClarity: I found the work difficult to follow in places, and I felt that some material crucial to the paper was relegated to appendices.\n\nOriginality: To my knowledge, the idea is original.\n\nQuality and significance:\nI feel the significance of this work is likely to be low. While the authors report positive ""defense"" results, I strongly suspect this is simply because the attacks considered did not uncover the defense strategy. I expect that this defense would be broken relatively quickly if the paper is accepted. The authors did not present evidence to suggest that their method reduces the probability that a misclassified example lies close to the training or test examples. As such, the defense seems to rely on the attacker being ""tricked"".\n\nSpecific comments:\n\n1) Throughout the paper, I was unclear what specific kinds of attacks the method was intended to defend against.\n2) In section 2.2, key definitions are relegated to appendix C.\n3) Section 3.2-> p = 0.3 is still 30% of the pixels. Could the authors provide a baseline with a random 30% of pixels?\n4) Adaptive attack scenario: I would recommend that the authors also included a measure of the attack-ability under random noise in the epsilon ball. This would demonstrate whether the defense actually removes adversarial examples or just ""attacks the attackers"".', ""I have to emphasize first that this is not my area of expertise so I am going to review it as an outsider. \n\nThe authors argue that the checkerboard phenomenon can be exploited to make neural networks robust against adversarial attacks. They propose to enhance the checkerboard pattern by first adding a layer, called Artificial Checkerboard Enhancer (ACE), and then evading the attacks by zero-padding the image. The authors’ argument is that enhancing the checkerboard phenomenon will make attacks more targeted towards certain pixels, which can be evaded by shifting the image. \n\nOverall, I think the paper is difficult to read and is not suitable for publication. In terms of clarity, the authors do not use precise terminology that would allow the reader to reproduce their work. They allude to vague statements. For example, they introduce two KEY terminologies that are repeatedly used throughout the paper but are not properly defined (see for instance the “definition” of “Gradient Overlap” in Appendix C). \n\nIn addition, in terms of the experiements, it certainly does not help to say that they were “reproduced by [the authors themselves]”. What does this mean?\n \nIn terms of originality, I agree with the first reviewer that the defense strategy seems to be easily breakable. The authors propose that they enhance the checkerboard phenomenon so that adversarial attacks become easier to implement by targeting individual pixels (the pixels in the checkerboard artifacts). Then, they pad the image with zero pixels to shift it to the right. I don’t understand how shifting the pixels would make it harder to attack (especially when the adversary knows the system). \n\nIt would be really appreciated if the authors elaborate on the following points to help me understand their contribution: \n- The entire discussion about ACE in Section 4.1 is ad-hoc and not well-motivated. Why would ACE enhance the checkerboard patterm? Can you please explain why it works? This is not mentioned anywhere in the paper. The experiment in Section 4.2 helps a bit but it does not answer this question. \n\n- What wouldn't an adversary remove the padded pixels before generating the attack? In defense strategies, it is often assumed that the adversary knows the system. Can you please explain why that is not possible in this setting? \n\n- \n\nIn Figure 4, the axes are \\bar i and \\bar j in the main body, but they are x and y in the figure. Please use the same notation. ""]","[0, -50, -80]","[50, 50, -20]","[""The sentiment score is 0 (neutral) because the reviewer is neither positive nor negative about the paper itself, but simply stating their inability to review it due to lack of expertise. The politeness score is 50 (somewhat polite) because the reviewer is straightforward and honest about their limitations, and has taken the responsible action of contacting the general chair. The language is not overly formal or polite, but it's professional and respectful. The reviewer doesn't use any rude or dismissive language, which contributes to the slightly positive politeness score."", ""The sentiment score is -50 because the reviewer expresses several concerns about the paper, particularly regarding its clarity and significance. They state that the work is 'difficult to follow' and that its significance is 'likely to be low.' They also suspect that the defense method proposed could be 'broken relatively quickly.' However, the score is not lower because the reviewer acknowledges some positive aspects, such as the originality of the idea.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I found,' 'I feel,' and 'I would recommend,' which soften their criticisms. The reviewer also provides specific, constructive feedback and suggestions for improvement. However, the score is not higher because the language, while polite, is not overly deferential or exceptionally courteous."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'difficult to read and is not suitable for publication', criticizes the lack of clarity and precise terminology, questions the reproducibility of the work, and expresses skepticism about the originality and effectiveness of the proposed method. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism and skepticism that come across as somewhat impolite. For example, the reviewer directly states that the paper is not suitable for publication and questions the authors' statement about reproducing experiments. However, the reviewer does offer constructive feedback and asks for elaboration on specific points, which prevents the score from being even lower.""]"
"['In this paper, the authors relate the architectures of recurrent neural\nnetworks with ODEs and defines a way to categorize the RNN architectures by\nlooking at non-linearity order and temporal memory scale. They further\npropose QUNN, a RNN architecture that is more stable and has less complexity\noverhead in terms of input dimension while comparing with LSTM. \n\nAlthough this paper provides a new view point of RNN architectures and relates\nRNNs with ODEs, it fails to provide useful insight using this view point.\nAlso, it is not clear what advantage the new proposed architecture QUNN has\nover existing models like LSTM or GRU. \n\nThe paper is well presented and the categorization method is well defined.\nHowever, how the order of non-linearity or the length of temporal memory\naffect the behavior and performance of RNN architectures are not studied.\n\nIt is proved that QUNN is guaranteed existence and its Jacobian eigen values\nwill always have zero real part. It would be easier to understand if the\nauthors could construct a simple example of QUNN and conduct at least some \nsynthetic experiments.\n\nIn general I think this paper is interesting but could be extended in various\nways. \n', 'The authors make connections between RNN dynamics and those of a class of ODEs similar to RNNs (ORNN) that has different orders of nonlinearity and order of gradients in time. They show that typical RNN architectures can be described as members of the ORNN family. They then make the connection that quantum mechanical systems can be described as following the Schrodinger equation which can be cast as a series of coupled 1st order ODEs of the evolution of wavefunctions under an influencing Hamiltonian. They then claim that these discretized equations can be represented by a RNN similar to a unitary RNN. They go on to outline a RNN structure inspired by this insight that has time-dependent activations to increase the scale of temporal dependence. \n\nThe main challenge of this paper is that it does not present or support its arguments in a clear fashion, making it difficult to judge the merit of the claims. Given the nuance required for their arguments, a more robust Background section in the front that contextualizes the current work in terms of machine learning nomenclature and prior work could dramatically improve reader comprehension. Also, while the parallels to quantum mechanics are intriguing, given that the paper is arguing for their relevance to machine learning, using standard linear algebra notation would improve over the unnecessary obfuscation of Dirac notation for this audience. While I\'m not an expert in quantum mechanics, I am somewhat proficient with it and very familiar with RNNs, and despite this, I found the arguments in this paper very hard to decipher. I don\'t think this is a necessity of the material, as the URNN paper (http://proceedings.mlr.press/v48/arjovsky16.pdf) describes very similar concepts with a much clearer presentation and background. \n\nFurther, despite claims of practical benefits of their proposed RNN structure, (reduced parameter counts required to achieve a given temporal correlation), no investigations or analyses (even basic ones) are performed to try and support the claim. For example, the proposed scheme requires a time varying weight matrix, which naively implemented would dramatically grow the parameter count over a standard LSTM. I can understand if the authors prefer to keep the paper strictly a theory paper, but even the main proof in Theorem 4 is not developed in detail and is simply stated with reference to the URNN paper. \n\nThere are some minor mistakes as well including a reference to a missing Appendix A in Theorem 3, ""Update rule of Eq. (15)-(15)"", ""stble regime"". Finally, as a nit, the claim of ""Universal computing"" in the name, while technically true like other neural networks asymptotically, does not seem particularly unique to the proposed RNN over others, and doesn\'t provide much information about the actual proposed network structure, vs. say ""Quantum inspired Time-dependent RNN"".', 'This paper attempts to do three things:\n\t1) introduce a generalization / formalism for describing RNN architectures\n\t2) demonstrate how various popular RNN architectures fit within the proposed framework\n\t3) propose a new RNN architecture within the proposed framework that overcomes some limitations in LSTMs\nThe ultimate goal of this work is to develop an architecture that:\n\t1) is better able to model long-term dependencies\n\t2) is stable and efficient to train\n\nSome strengths, concerns, and questions loosely ordered by section:\n\nStable RNNs\n\t- it\'s not clear to me where equations (2) and (3) come from; what is the motivation? Is it somehow derived from this Runge-Kutta method (I\'m not familiar with it)? \n\t- I don\'t understand what this t^th order time-derivative amounts to in practice. A major claim (in Table 4) is that LSTMs are time-order 2 whereas QUNNs are time-order L and the implication is that this means LSTMs are worse at modeling long term structure than QUNNs ; but how does that actually relate to practical ability to model long-term dependencies? It certainly doesn\'t seem correct to me to say that LSTMs can only memorize sequences of length 2, so I don\'t know why we should care about this time-derivative order.\n\t- I thought this section was poorly written. The notation was poorly chosen at times, e.g. the t_k notation and the fact that some $l$ have subscripts and some don\'t. There were also some severe typos, e.g. I think Claim 1 should be ""L-2-ORNN"". Furthermore, there were crucially omitted definitions: what is reversibility and why should we care? Relatedly, the ""proofs"" are extremely hand-wavy and just cite unexplained methods with no further information.\n\nQUNNs\n\t- The practical difference between QUNNs and LSTMs seems to be that the weights of the QUNN are dynamic within a single forward prop of the network, whereas LSTM weights are fixed given a single run (although the author does admit that the forget gates adds some element of dynamism, but there\'s no concrete evidence to draw conclusions about differences in practice).\n\t- I don\'t understand the repeated claim that LSTMs don\'t depend on the data; aren\'t the weights learned from data?\n\t\nThere may be something interesting in this paper, but it\'s not clear to me in its current incarnation and I\'m not convinced that an eight-page conference paper is the right venue for such a work. There\'s a substantial of amount of exposition that needs to be there and is currently missing. I suspect the author knows this, but due to space constraints had to omit a lot of definitions and explanations.\n\nI don\'t think all papers need experiments, but this paper I think would have greatly benefited from one. Community knowledge of LSTMs has reached a point where they are in practice easy to train and fairly stable (though admittedly with a lot of tricks). It would have been much more convincing to have simple examples where LSTMs fail due to instability and QUNNs succeed. Similarly, regarding long-term dependencies, my sense is that LSTMs are able to model some long-term dependencies. Experimental evidence of the gains offered by QUNNs would have also been very convincing.\n\nNote: It looks like there\'s some funkiness in the tables on page 8 to fit into the page limit.\n', 'This paper presents a new framework to describe and understand the dynamics of RNNs inspired by quantum physics. The authors also propose a novel RNN architecture derived by their analysis. \n \nAlthough I found the idea quite interesting, my main concern is that the jargon used in the paper makes it hard to understand. I suggest that the authors to add an in-depth ""background"" section, so the reader becomes more familiar with the terms that will be introduced later. \n \nDespite this paper is mainly a theory paper, it would have a lot more strength if the authors provide some experiments to demonstrate the strength of the proposed architecture over LSTMs. \n \nAs a minor suggestion, the term ""universal"" should be removed from ""UNIVERSAL COMPUTING NEURAL NETWORK"" as all recurrent neural networks are, in theory, universal. \n\n']","[-20, -50, -50, -20]","[50, 20, 20, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The paper is well presented', 'interesting'), they express several criticisms and concerns. The reviewer points out that the paper 'fails to provide useful insight' and questions the advantages of the proposed architecture. However, it's not entirely negative as they suggest ways to improve and extend the work. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and balances negative points with positive ones. They use phrases like 'it would be easier to understand if' and 'I think this paper is interesting but could be extended', which maintain a polite and professional tone even while critiquing the work."", ""The sentiment score is -50 because the review is generally critical of the paper, pointing out several issues with clarity, presentation, and lack of supporting evidence. However, it's not entirely negative as it acknowledges some intriguing aspects of the work. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer constructive feedback. They use phrases like 'The main challenge' and 'could dramatically improve' rather than harsh or rude language. The reviewer also acknowledges their own potential limitations ('While I'm not an expert in quantum mechanics'). The slightly positive politeness score reflects the reviewer's effort to provide helpful suggestions for improvement while maintaining a respectful tone."", ""The sentiment score is -50 because the review is generally critical, pointing out several weaknesses and concerns about the paper. The reviewer mentions that the paper is poorly written in parts, lacks crucial definitions, and would have greatly benefited from experimental evidence. However, it's not entirely negative as the reviewer acknowledges there might be something interesting in the paper. The politeness score is 20 because while the reviewer is direct in their criticisms, they use relatively polite language such as 'It's not clear to me' and 'I don't understand', rather than making blunt negative statements. The reviewer also offers constructive suggestions for improvement, which is a polite approach to criticism. However, the overall tone is more neutral than overtly polite, hence the moderate positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer finds the idea 'quite interesting', they express significant concerns about the paper's clarity and lack of experimental evidence. The reviewer suggests major improvements, indicating that the paper in its current form is not satisfactory. The politeness score is moderately positive (60) as the reviewer uses polite language throughout, such as 'I suggest', 'As a minor suggestion', and frames criticisms constructively. They also acknowledge positive aspects ('I found the idea quite interesting') before presenting concerns. The tone is professional and respectful, offering constructive feedback without harsh criticism.""]"
"['This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. \n\nIt is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? \n\nArguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. \n\nAnother way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?\n', 'In this interesting study, the authors propose a score (BrainScore) to (1) compare neural representations of an ANN trained on imagenet with primate neural activity in V4 and IT, and (2) test whether ANN and primate make the same mistakes on image classification.  They also create a shallow recurrent neural network (Cornet) that performs well according to their score and also reasonably well on imagenet classification task given its shallow architecture.\n\nThe analyses are rigorous and the idea of such a score as a tool for guiding neuroscientists building models of the visual system is novel and interesting.\n\nMajor drawbacks:\n\n1. Uncertain contribution to ML: it remains unclear whether architectures guided by the brain score will indeed generalize better to other tasks, as the authors suggest.\n\n2. Uncertain contribution to neuroscience: it remains unclear whether finding the ANN resembling the real visual system most among a collection of models will inform us about the inner working of the brain.\n\n\nThe article would also benefit from the following clarifications:\n\n3. Are the recurrent connections helping performance of Cornet on imagenet and/or on BrainScore?\n\n4. Did you find a correlation between the neural predictivity score and behavioral predictivity score across networks tested? If yes, it would be interesting to mention.\n\n5. When comparing neural predictivity score across models, is a model with more neurons artificially advantaged by the simple fact that there is more likely a linear combination of neurons that map to primate neural activations? Is cross-validation enough to control for this potential bias?\n\n6. Fig1: what are the gray dots?\n\n7. “but it also does not make any assumptions about significant differences in the scores, which would be present in ranking. “\nWhat does this mean?\n\n8. How does Cornet compare to this other recent work: https://arxiv.org/abs/1807.00053 (June 20 2018) ?\n\nConclusion:\nThis study presents an interesting attempt at bridging the gap between machine learning and neuroscience. Although the impact that this score will have in both ML and Neuroscience fields remains uncertain, the work is sufficiently novel and interesting to be published at ICLR. I am fairly confident in my evaluation as I work at the intersection of deep learning and neuroscience.\n', 'Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work\'s novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper\'s novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning and\nbiological vision.  I have experience with neural network models and\nvisual neurophysiology.\n\n2. This paper makes two contributions: 1) It develops Brain-Score - a\ndataset and error metric for animal visual single-cell recordings.  2)\nIt develops (and brain-scores) a new shallow(ish) recurrent network\nthat performs well on ImageNet and scores highly on brain-score. \n\n3. The development of Brain-Score is a useful invention for the field.  A nice\naspect of Brain-Score is that responses in both V4 and IT as well as behavioral\nresponses are provided.   I think it could be more useful if the temporal dynamics (instead of the\nmean number of spikes) was included.    This would allow to compare temporal\nresponses in order to compare ""brain-like"" matches.\n\n4. This general idea is somewhat similar to a June 2018 Arxiv paper\n(Task-Driven Convolutional Recurrent Models of the Visual System)\nhttps://arxiv.org/abs/1807.00053\nbut this is a novel contribution as it is uses the Brain-Score dataset.\n\nOne limitation of this approach relative to the June 2018 ArXiv paper\nis that the Brain-Score method is just representing the mean neural\nresponse to each image - The Arxiv paper shows that different models\ncan have different temporal responses that can also be used to decide\nwhich is a closer match to the brain.\n\n5. More analysis of why CORNET-S is best among compact models would greatly\nstrengthen this paper.  What do the receptive fields look like?  How do they compare\nto the other models.  What about the other high performing networks (e.g. DenseNet-169)?\nHow sensitive are the results to each type of weight in the network?   What about feedback connections\n(instead of local recurrent connections)? \n\n6. This paper makes a significant contribution, in part due to the\ndevelopment and open-sourcing of Brain-Score.  The significance of the\ncontribution of the CORnet-S architecture is limited by the\nlack of analysis into what aspects make it better than other models.\n\n\n\n']","[50, 50, 60]","[75, 75, 80]","[""Sentiment Score (50): The review starts positively, calling the paper 'interesting' and praising its goal and writing. However, it then raises several concerns and suggestions for improvement, balancing the positive aspects with constructive criticism. This mix of positive and negative elements results in a slightly positive overall sentiment.\n\nPoliteness Score (75): The reviewer maintains a professional and respectful tone throughout. They use phrases like 'interesting paper', 'well written', and 'interesting take', which are polite and encouraging. Even when raising concerns, the language remains constructive and suggestive rather than demanding, using phrases like 'It is a little unclear', 'Making these steps clearer is crucial', and 'Perhaps a better thing is to add'. The reviewer also acknowledges the authors' own recognition of limitations. This consistently polite and constructive approach warrants a high politeness score."", ""The sentiment score is 50 (slightly positive) because the reviewer describes the study as 'interesting' and 'novel', and states that it's 'sufficiently novel and interesting to be published'. However, they also mention 'major drawbacks' and uncertainties about the contribution, balancing out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledges the rigor of the analyses, and frames criticisms as suggestions for improvement (e.g., 'The article would also benefit from the following clarifications'). The reviewer also expresses confidence in their evaluation without being arrogant. The tone is professional and constructive throughout."", ""The sentiment score is 60 (moderately positive) because the reviewer acknowledges the paper's significant contributions, particularly the development of Brain-Score and its usefulness to the field. They also mention that the paper makes a 'significant contribution.' However, they point out some limitations and areas for improvement, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and balances positive feedback with suggestions for improvement. They use phrases like 'useful invention,' 'nice aspect,' and 'would greatly strengthen this paper,' which maintain a professional and courteous tone. The reviewer also follows the provided rubric closely, demonstrating respect for the review process.""]"
"[""The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important.  To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there’s also an experiment on ImageNet\nIt looks to me that the proposed method is overly complicated. It is also described in a complicated manner.  I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers.\nThe results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated"", 'This is an architecture design paper. It proposes a general structure called Selective Convolutional Unit that the authors claim to be useful for various CNN models. The SCU structure contains two major parts: CD and NC. CD for compressing/pruning channels and NC for multiplicative noise. The paper gives a measure, called expected channel damage score, on the change of the output for SCU. It also shows the effectiveness of SCU on CIFAR-10, CIFAR-100 and imagenet.\n\nSome questions and concerns:\n\n1. The paper spends too much space introducing the bottleneck structures and a whole lot of the details on the optimization of NC and CD are put in the appendix. I would suggest to reduce the section of introductory part and put a shorter version of appendix A and B to the main text so that the readers know more about the architecture and how it is optimized. In particular, the description on NC is confusing since without looking at the appendix it is not clear how the prior p(\\theta) is used. \n\n2. The experiment shows improvement on densenet and resnetXT, but the result is not the state-of-the-art. Wide-Resnet seems to get better accuracy on both CIFAR-10 and CIFAR-100 compared to the best accuracy reported in this paper. Also the number reported by the original densenet paper on imagenet seems to be better (densenet-264 has an error rate of 22.15/20.80)\n\n3. In your CD design, channel assignment \\pi is a discrete variable. How is it optimized in the training process?\n\n4. The proof of proposition 1 does not look correct to me. The optimization procedure makes use of the data X to determine your NC variable \\theta so \\theta depends on X. In this way you cannot factorize the expectation in the equation below (20) in your appendix.\n', 'This paper propose Selective Convolutional Unit (SCU), which can replace the bottleneck in Resnet block. The difference between SCU and bottleneck is that SCU adds Channel Distributor (CD) and Noise Controller (NC) to reduce and replace the channels. This paper also propose Expected channel damage score (ECDS) to measure the importance of a channel to decide weather remove or replace it. Then the experiment shows result on cifar10/100 and imagenet data set with different network architectures.\nThe idea is interesting, however, the parameter flops reduced rate seems not very impressive. The SCU seems too complicated,so I want to know that if the SCU could accelerate the forward process on modern GPU or mobile devices? \nThe result of these networks seems not the state-of-the-art, if the result can be improved, the SCU could be more convincing.\n']","[-30, -20, -20]","[20, 60, 50]","[""The sentiment score is -30 because the reviewer expresses several criticisms and doubts about the paper's approach and results. They describe the method as 'overly complicated' and 'not clear', and state that the improvements shown are 'not great'. However, they do acknowledge some 'interesting ideas', which prevents the score from being more negative. The politeness score is 20 because the reviewer uses relatively neutral language and avoids harsh criticism. They present their concerns as personal observations (e.g., 'It looks to me', 'I don't see') rather than absolute statements. The reviewer also acknowledges positive aspects, such as 'interesting ideas' and 'decent improvement', which contributes to a more polite tone. However, the overall critique is still direct, preventing a higher politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions, they express several concerns and questions about the work. The review starts with a neutral summary but then lists four significant issues, indicating more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses professional and respectful language throughout. They phrase their concerns as 'questions and concerns' rather than outright criticisms, and use polite suggestions like 'I would suggest'. The reviewer also acknowledges the paper's contributions and effectiveness in certain areas, showing a balanced and courteous approach despite the criticisms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'interesting', they express concerns about the complexity of the SCU, the unimpressive parameter flops reduction rate, and the lack of state-of-the-art results. These criticisms outweigh the initial positive comment. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, starting with a positive comment and framing criticisms as questions or suggestions rather than direct negative statements. They use phrases like 'I want to know' and 'if the result can be improved', which maintain a respectful tone while still conveying their concerns.""]"
"[""- Summary\nThis paper presents a minor improvement over the previous Deli-GAN framework (Gurumurthy et al CVPR'17). Specifically, the work proposes to use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence. It shows little results (seems to be positive) using this new distance measure other than traditional ones.  Except that, I didn't see any other contribution from this paper.\n\n- Suggestions\nThe paper is poorly written and seems to be a rush submission to ICLR. For example:\n* a lot of grammatical errors throughout the paper\n* only 6.5 papes out of 8 pages are utilized\n* the introduction is not convincing -- what problem are you going to address? any summary of your methodology? why it is expected to outperform existing frameworks? what distinguishes your work from existing works? and what're your main results? I cannot conclude after reading the intro.\n* the results are very minor and not convincing. It seems the authors conducted a very limited set of experiments and concluded that the proposed Deli-Fisher GAN is better. If you claim that the proposed framework can generate better images, at least the framework should be compared to the latest state-of-the-art GANs (e.g. spectral GANs, etc.)\n* The writing is not polished. \n\nOverall, the paper is far from ready to be submitted to ICLR, not mentioning acceptance. I would recommend the authors to conduct more experiments and comparisons and do a better job before submitting it to future conferences.\n\n"", ""This paper is a straightforward combination of two previous works, Deli-GAN and Fisher GAN. Deli-GAN has a mixture prior distribution in the latent space, while Fisher GAN uses Fisher IPM instead of JSD as objective.\nInception score on CIFAR-10 is used to empirically measure quality.\n\nCons:\nThe exposition of the ideas is lacking. What's wrong with Deli-GAN? What is this paper trying to accomplish by incorporating fisher metric?\nNo theoretical justification while empirical results are sparse and unconvincing.\nWriting quality could be improved throughout the paper in terms of both structure and language.  \n\nIn summary, this paper is not of the quality that should be accepted by ICLR. "", 'The paper proposes to combine two ideas from previous publications, Fisher-GAN and Deli-GAN, i.e., use a mixture noise (Deli) with Fisher IPM metric for training GAN.\n\nThe extension of the previous work is trivial and the combination of the two ideas lack of any motivation. The experimental results are also weak. It is certainly below the bar of acceptance.\n \n']","[-70, -70, -80]","[-20, -20, -20]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper presents only a 'minor improvement', shows 'little results', and is 'poorly written'. They also mention that the paper is 'far from ready to be submitted'. These comments indicate a strong negative sentiment towards the paper. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite direct and critical. Phrases like 'poorly written', 'rush submission', and 'not convincing' are used without much softening language. However, they do provide specific suggestions for improvement, which prevents the score from being even lower."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper is 'not of the quality that should be accepted by ICLR' and lists several cons without any pros. The only neutral statement is the description of the paper's content in the first paragraph. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without any softening language or positive feedback. Phrases like 'The exposition of the ideas is lacking' and 'Writing quality could be improved' are blunt criticisms. The review doesn't use any polite phrases or acknowledge potential merits, which contributes to its slightly impolite tone."", ""The sentiment score is -80 because the review is highly negative. The reviewer states that the paper's extension is 'trivial', lacks motivation, and has 'weak' experimental results. The concluding statement that it is 'certainly below the bar of acceptance' indicates a strong negative sentiment. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't use any softening language or acknowledge any positive aspects, which makes the tone somewhat impolite. However, it doesn't use explicitly rude language, so it's not at the extreme end of impoliteness.""]"
"['This submission proposes an energy-based method to correct mislabelled examples. Intuitively, the authors claim that contradictions between energy and noisy labels can be used to identify label noise. To make the idea reliable, the authors propose to compute the energy by using learned (commonly shared) features. Experiment results look good. The presentation is also clear. My concerns are as follows:\n\n(1) By learning discriminative features and then correcting the label noise, the authors have implicitly assumed that the label noise strongly dependent on the discriminative features. This assumption may be strong as most labels are provided according to the original instance (features). In the experiment section, it is very unclear about how the noise is generated, e.g., how to select x_i according to variance? How to set the threshold? What is ""col"" in Tables 1 and 2? What are the overall label noise rates? Note that if the threshold for the variance is large. It means that the noise is only added to the discriminative features, making the experiment too ad-hoc. \n\n(2) The theory of why the residual energy can be used to identify label noise is elusive. How to set the threshold for identifying label noise with a theoretical guarantee is also unclear. Two recent papers on learning with instance-dependent label noise are surprisingly missed, e.g., Menon, Aditya Krishna, Brendan Van Rooyen, and Nagarajan Natarajan. ""Learning from binary labels with instance-dependent corruption."" arXiv preprint arXiv:1605.00751 (2016). and Cheng, Jiacheng, et al. ""Learning with Bounded Instance-and Label-dependent Label Noise."" arXiv preprint arXiv:1709.03768 (2017). The latter one proposes algorithms to identify label noise with theoretical guarantees. The authors should compare the proposed method with them.\n\n(3) There are methods provided for choose the values of hyperparameters. Most of them are empirically set, which is not convincing.\n\n(4) The authors reported that with discriminative features learned by employing noisy data, the proposed method also provide good performance. It would be interesting to see how corrected labels will recursively help better learn the discriminative features. Illustrating figures are preferred.', 'Need improvements\n\n[Summary]\n\nThis paper addresses the problem of correcting noisy labels for binary classification. It assumes the exists of fully clean data, trains an energy-based autoencoder using contrastive learning objective, and use the estimated energy to determine if a training label is corrupted or not. \n\n[Pros]\n\n1.\tThe paper summarizes different types of label noise in a sensible way. And, it is reasonable to bootstrap the learning process with a small fully clean dataset.\n2.\tThe proposed method shows encouraging results under controlled noise.\n\n[Cons]\n\n1.\tIt is not well-motivated why a contrastive objective or an energy-based autoencoder can be a good solution for label correction. In particular, the connections are not established between the discriminative feature learned by an energy-based model and the label correctness. The proposed method looks more like a binary classifier with a better-regularized structure, but still, it is unclear why an energy-based autoencoder is a good choice. \n2.\tThe proposed method is limited to binary classification, and there is no obvious way to extend it to multiple classes. \n3.\tThe experiments are on toy/small-scale datasets with controlled label noise (but the way to control the noise is not clear). To show the effectiveness of the proposed methods, experiments need to be done on larger-scale datasets with truly realistic unknown noise, Establishing state-of-the-art classification accuracy using a large-scale dataset with noisy labels can serve as strong support for this paper.\n', 'This paper addresses Type III label noise correction problem in which the labeling noise depends on the features. They assume that we can obtain a small amount of cleanly labeled data, and use an energy-based semi-supervised learning approach to bootstrap the relabeling process. \n\nPros:\n- Problem is well-motivated with a reasonably good overview of this research area. \n- Paper is generally well-written with enough details to follow and good experimental result discussion.\n\nCons:\n- The energy-based approach based on contrastive divergence is pretty straightforwardly defined, but it will make the paper much stronger if the authors can have more analysis on this approach and/or provide theoretical guarantee on generalization. \n- It is not obvious to me how to extend the proposed approach to multi-class problems. \n- It will be beneficial to test the approach on more real-world problems on top of the toy-data-alike binary classification problems. \n\nMinor clarification questions: \n- What amount of cleanly labeled data is sufficiently required for the proposed approach to work? The authors have some pre-selected percentage in experiments but it is non-trivial to establish that for different applications.\n- Related to the previous comment, how much clean data were used in AE (known) columns in all experiments? \n- In Fig 2 between the two subgraphs, why is the left one showing positive thetas while the right one showing negative thetas?\n- Were the hyperparameters a & b chosen from cross-validation or from std of E terms in all experiment results? \n- In Table 1, for Breast Cancer dataset, how can AE (known) be better than the upper-bound LR-C? \n- It would be good to vary the noise parameters and show how robust the proposed approach is in dealing with different levels of noise. ']","[-20, -30, 50]","[50, 50, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Experiment results look good. The presentation is also clear.'), they express several significant concerns and criticisms about the methodology, assumptions, and lack of comparisons to existing work. The overall tone suggests the paper needs substantial revisions. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, framing criticisms as 'concerns' and using phrases like 'It would be interesting to see' rather than making demands. They also begin with positive comments before listing concerns, which is a polite approach in academic reviews."", ""The sentiment score is -30 because while the review acknowledges some positive aspects ('encouraging results', 'sensible summary'), it primarily focuses on areas needing improvement and limitations of the paper. The 'Need improvements' header and the longer list of cons compared to pros indicate a generally negative sentiment, though not extremely so. The politeness score is 50 because the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They use phrases like 'it is reasonable' and 'the paper summarizes... in a sensible way', which are polite ways of giving feedback. The reviewer also provides constructive suggestions for improvement rather than just pointing out flaws."", ""The sentiment score is 50 (slightly positive) because the review begins with a neutral summary of the paper's content, followed by a balanced list of pros and cons. The pros indicate appreciation for the paper's motivation, writing, and experimental results. The cons, while critical, are presented constructively as areas for improvement rather than outright flaws. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions for improvement (e.g., 'it will make the paper much stronger if...', 'It will be beneficial to...'). The reviewer also asks clarification questions in a polite manner, showing engagement with the work. The overall tone is professional and constructive, aiming to improve the paper rather than dismiss it.""]"
"['This paper proposes to use n-ary representations for convolutional neural network model quantization. A novel strategy of nested-means clustering is developed to update weights. Batch normalization is also considered in the activation quantization. Experiments on both weight quantization and activation quantization are conducted and show effectiveness.\n\nStrengths:\n1.\tThe idea of nested-means clustering is interesting, which somehow shows its effectiveness.\n2.\tState-of-the-art experimental results.\n3.\tThe representation is excellent, and it is easy to follow.\n\nConcerns:\n1.\tThough the experiment study seems solid, an ablation study is still missing. For example, how important is the nested-means clustering technique? What is the effect if replacing it with the original one or with other clustering methods? What will happen if expanding the interval in the quantization of activation? All these kinds of questions are hard to answer without an ablation study.\n2.\tIt is not clear how the weight and activation quantization are addressed together.\n3.\tIf counting the first and last layers, what is the size of the model (the number of parameters)?\n4.\tSimilarly, what are the FLOPs in different settings of experiments? This seems missing.\n5.\tWhen discussing the related work about model compression, there are important references missing. I just list two references in the latest vision and learning literature:\n[Ref1] X. Lin et al. Towards accurate binary convolutional neural network. NIPS 2017\n[Ref2] Z. Liu et al. Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm. ECCV 2018.\n', ""This paper is about CNN model compression and inference acceleration using quantization. The main idea is to use 'nest' clustering for weight quantization, more specifically, it partitions the weight values by recurring partitioning the weights by arithmetic means and negative of that of that weight clustering.\n\nI have several questions for this paper:\n\n1) the main algorithm is mainly based on the hypothesis that the weights are with Gaussian distribution. What happens if the weights are not Gaussian, such as skewed distribution? Seems the outliners will bring lots of issues for this nest clustering  for partitioning the weight values.\n\n2) Since the paper is on inference acceleration, there is no real inference time result. I think having some real inference time on these quantized models and showing how their inference time speedup is will be interesting.\n\n3) Activation quantization in Section 4 is a standard way for quantization, but I am curious how to filter out the outliner, and how to set the clipping interval?\n\n4) I am not sure what does the 'sparsity' mean in Table 2? Does this quantization scheme introduce many zeros? Or sparsity is corresponding to the compression ratio? If that is the case, then many quantization algorithms can actually achieve better compression ratios with 2 bits quantization."", 'Summary: This paper proposes a technique for quantizing the weights and activations of a CNN. The main contribution is in replacing the heuristic to find good quantization intervals of (Zhu et al, 2016) with a different heuristic based on a hierarchical clustering algorithm, and empirically validating its effectiveness.\n\nStrenghts:\n- The proposed nested-means heuristic is simple and makes sense intuitively.\n- The experiments on two modern architectures seem solid and demonstrate good empirical performance.\n\nWeaknesses:\n- The main weakness is the limited novelty of this paper. The proposed setup is almost identical to the one in (Zhu et al, 2016), except for the replacement of the heuristic to find quantization intervals with another one. While the experiments demonstrate the empirical effectiveness of the method as a whole, what is missing is a direct, controlled comparison between the original heuristic and the proposed one. Now it is hard to tell whether the accuracy increases are obtained through the proposed adaptation or because of other factors such as a better implementation or longer training.\n- In section 4, it is not made clear whether the activations are quantized according to the same scheme as the weights (apart from the issue of selecting a good clipping interval, which is addressed).\n- The paper is a bit short on references, considering the many recent works on quantized neural networks.\n\nMinor comments and questions:\n- The wording is sometimes imprecise, making some arguments hard to follow. Two examples:\n-- ""Lowering the learning rate for re-training can diminish heavy changes in the weight distribution, at the cost of longer time to converge and the risk to get stuck at plateau regions, which is especially critical for trainable scaling factors""\n-- ""This approach is beneficial because it defines cluster thresholds which are influenced by large weights that were shown to play a more important role than smaller weights (Han et al., 2015b)""\n- The title says ""for compression and inference acceleration"", so it would be nice if the paper reports some compression and timing metrics in the experiments section.\n- The notation in section 3.1 overly complicated, could probably be simplified a bit for readability.\n- Section 3.3: ""However, having an additional hyperparameter t_i for each scaling factor alpha_i renders the mandatory hyperparameter tuning infeasible."" -> From section 4.2 in (Zhu et al, 2016), I believe the constant factor t is shared across all layers, making it only a single hyperparameter.\n- Last paragraph of section 4: ""(Cai et al., 2017) experimentally showed that the pre-activation distribution after batch normalization are all close to a Gaussian with zero mean and unit variance. Therefore, we propose to select a fixed clipping parameter gamma."". -> But what about the activations *before* the batchnorm layer where the assumption of zero mean and unit variance does not hold?']","[50, -20, -20]","[75, 50, 50]","[""The sentiment score is 50 (slightly positive) because the review begins by highlighting the paper's strengths, including 'interesting' ideas, 'state-of-the-art experimental results', and 'excellent' representation. However, it also lists several concerns, which balance out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing concerns as questions or suggestions rather than direct criticisms. Phrases like 'It is not clear' and 'This seems missing' are used instead of more confrontational language. The reviewer also acknowledges the paper's strengths before presenting concerns, which is a polite approach in academic reviews."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's topic and main idea, they raise several questions and concerns about the methodology and results. The reviewer points out potential limitations and missing information, which suggests a somewhat critical stance. However, the tone is not overtly negative, hence the score is only slightly below neutral. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They frame their concerns as questions rather than direct criticisms, which is a polite approach. The reviewer also uses phrases like 'I am curious' and 'I think,' which maintain a respectful tone. The absence of harsh or dismissive language contributes to the positive politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out significant weaknesses, particularly the limited novelty and lack of direct comparison with previous work. The review is more critical than positive overall. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, balancing critiques with acknowledgments of the paper's strengths. They offer constructive feedback and pose questions rather than making harsh judgments. The reviewer maintains a neutral, academic tone without using overly formal or informal language.""]"
"['The authors are providing an information theoretic viewpoint on the behavior of DNN based on the information bottleneck.  The clarity of the paper is my main concern.  It contains quite a number of typos and errors.  For example, in section 6, the results of MNIST in the first experiment was presented after introducing the second experiment.  Also, the results shown in Fig 1b seems to have nothing to do with Fig. 1a.  It makes use of some existing results from other literature but it is not clearly explained how and why the results are being used.   It might be a very good paper if the writing could be improved.   The paper also contains some experimental results.  But they are too brief and I do not consider the experiments as sufficient to justify the correctness of the bounds proved in the paper.', ""This paper interprets the optimization of deep neural networks in terms of a two phase process: first a drift phase where gradients self average, and second a diffusion phase where the variance is larger than the square of the mean. As argued by first by Tishby and Zaslavsky and then by Shwartz-Ziv and Tishby (arxiv:1703.00810), the first phase corresponds to the hidden layers becoming more informative about the labels, and the second phase corresponds to a compression of the hidden representation keeping the informative content relatively fixed as in the information bottleneck of Tishby, Pereira, and Bialek. \n\nA lot of this paper rehashes discussion from the prior work and does not seem sufficiently original. The main contribution seems to be a bound that is supposed to demonstrate representation compression in the diffusion phase. The authors further argue that this shows that adding hidden layers lead to a boosting of convergence time.\n\nFurthermore, the analytic bound relies on a number of assumptions that make it difficult to evaluate. One example is using the continuum limit for SGD (1), which is very popular but not necessarily appropriate. (See, e.g., the discussion in section 2.3.3 in arxiv:1810.00004.)\n\nAdditionally, there has been extensive discussion in the literature regarding whether the results of Shwartz-Ziv and Tishby (arxiv:1703.00810) hold in general, centering in particular on whether there is a dependence on the choice of the hyperbolic tangent activation function. I find it highly problematic that the authors continue to do all their experiments using the hyperbolic tangent, even though they claim their analytic bounds are supposed to hold for any choice of activation. If the bound is general, why not include experimental results showing that claim? The lack of discussion of this point and the omission of such experiments is highly suspicious.\n\nPerhaps more importantly, the authors do not even mention or address this contention or even cite this Saxe et al. paper (https://openreview.net/forum?id=ry_WPG-A-) that brings up this point. They also cite Gabrie et al. (arxiv:1805:09785) as promising work about computing mutual information for deep networks, while my interpretation of that work was pointing out that such methods are highly dependent on choices of binning or regulating continuous variables when computing mutual informations. In fact, I don't see any discussion at all this discretization problem, when it seems absolutely central to understanding whether there is a sensible interpretation of these results or not.\n\nFor all these reasons, I don't see how this paper can be published in its present form."", 'This paper presents some results about the information bottleneck view of generalization in deep learning studied in recent work by Tishby et al.\nSpecifically this line of work seeks to understand the dynamics of stochastic gradient descent using information theory. In particular, it quantifies the mutual information between successive layers of a neural network. Minimizing mutual information subject to empirical accuracy intuitively corresponds to compression of the input and removal of superfluous information.\nThis paper further formalizes some of these intuitive ideas. In particular, it gives a variance/generalization bound in terms of mutual information and it proves an asymptotic upper bound on mutual information for the dynamics of SGD.\n\nI think this is an intriguing line of work and this paper makes an meaningful contribution to it. The paper is generally well-written (modulo some typos), but it jumps into the technical details (stochastic calculus!) without giving much intuition to help digest the results or discussion of how they relate to the broader picture. (Although I appreciate the difficulty of working with a page limit.) \n\nTypos, etc.:\np1. ""ereas"" should be ""whereas""\np2. double comma preceeding ""the weights are fixed realizations""\np5. extra of in ""needed to represent of the data""\nThm 1. L(T_m) has not been formally defined when T_m contains a set of representations rather than data points.']","[-20, -80, 60]","[50, -20, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the potential of the paper ('It might be a very good paper if the writing could be improved'), they express significant concerns about clarity, typos, errors, and insufficient experimental results. The overall tone suggests the paper needs substantial improvement. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'my main concern' and 'It might be a very good paper if...', which soften the criticism. They also acknowledge the paper's potential value. The reviewer provides specific examples of issues, which is helpful and constructive, rather than being harshly critical."", ""The sentiment score is -80 because the review is highly critical and negative. The reviewer states that the paper 'rehashes discussion from prior work', lacks originality, relies on questionable assumptions, and fails to address important criticisms in the field. The reviewer concludes that the paper cannot be published in its present form, indicating a strongly negative sentiment. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism ('highly problematic', 'highly suspicious') and the overall message is quite harsh. The reviewer does not soften their criticisms or offer many positive comments, leading to a somewhat impolite tone, though not extremely rude."", ""The sentiment score is 60 (positive) because the reviewer describes the paper as making a 'meaningful contribution' to an 'intriguing line of work' and states that it is 'generally well-written'. However, it's not extremely positive as the reviewer also points out some shortcomings, such as the paper jumping into technical details without providing much intuition. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledges the difficulty of working within page limits, and frames criticisms constructively. The reviewer also provides helpful feedback on typos in a neutral manner. The tone is professional and courteous throughout, without being overly formal or effusive.""]"
"['This paper proposed a bio-inspired sparse coding algorithm where iterations\nfor dictionary updates take into account the past updates. It is argued\nthat time takes a crucial rule in learning.\n\nThe paper is quite well written and contains an extensive literature review\ndemonstrating a good understanding of previous literature in both ML/DL and biological\nvision.\n\nThe idea of using a ""non-linear gain normalization"" to adjust atom selection\nin sparse coding is interesting and as far as I know novel, while providing\ninteresting empirical results: The system learns in an unsupervised way faster.\n\nMisc:\n\n- Using < > for latex brakets is not ideal. I would recommend: $\\langle\\,,\\rangle$\n\n- ""derivable"" I guess you mean ""differentiable""\n\n- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the\nreference to Pedregosa et al. for sklearn is missing.', ""Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work's novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning,\ncomputational neuroscience and biological vision.  I have experience\nwith neural network models and visual neurophysiology.\n\n2. This paper develops and tests an adaptive homeostatic algorithm for\nunsupervised visual feature learning (for example for learning models\nof early visual processing/V1).\n\n3.The work spends a lot of pages describing the general problem of\nunsupervised feature learning and the history of the base algorithms.\nThe literature review is quite extensive.  The new content appears to\nbe in section 2.2 (Histogram Equalization Homeostasis - HEH), where a\nsimple idea to keep all units with balanced activity over the set of\nnatural images.  The authors also develop a computationally cheaper\nversion they call HAP (Homeostasis on Activation Probability) The\nauthors show that their F function is optimized quicker with the HEH\nand HAP algorithms.  I would like to see how these curves vary with\nthe number of neurons (e.g. can you add X% more neurons and get\nsimilar convergence speed -- and if so which is more computationally\ncostly)?\n\n4. Many groups have developed various homeostatic algorithms for\nunsupervised learning, though I have not seen this exact one before.\n\n5.  The experiments reveal the resulting receptive fields and show the \ndecrease in the F function (error function).    The resulting receptive fields\ndo not seem that different to me between the different methods.  I am also not\nthat convinced that the faster convergence as a function of learning step is that important\nespecially as the learning steps may be more computationally expensive for this method.\n\n6. I am not sure how interesting this work will be for the ICLR audience,\nas it is not clear how important the faster convergence and more even\nutilization of neurons is (and how it would compare computationally\nwith just having more neurons).\n\n"", 'This paper discusses the addition of a regularizer to a standard sparse coding/dictionary learning algorithm to encourage the atoms to be used with uniform frequency.    I do not think this work should be accepted to the conference for the following reasons:\n\n1: The authors show no benefit of this scheme except perhaps faster convergence.  If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.  SPAMS (http://spams-devel.gforge.inria.fr/) can train a model on image patches as the authors do here in a few tens of seconds on a modern computer.  On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.\nIn my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.  It is not even clear that the final compression of the baselines would not be better.  Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.\n\n2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.   The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset.  Even without the ""train to convergence"" question above, I don\'t think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.   ']","[80, -20, -80]","[70, 50, -20]","[""The sentiment score is 80 (positive) because the reviewer expresses a generally positive view of the paper. They describe it as 'quite well written' with an 'extensive literature review' and 'interesting empirical results'. The idea is called 'interesting and novel'. The politeness score is 70 (polite) due to the use of respectful language throughout. The reviewer offers constructive feedback and suggestions without harsh criticism. They use phrases like 'I would recommend' instead of demanding changes. The only slightly negative comments are phrased as minor suggestions ('Misc:') rather than major criticisms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (extensive literature review, novel algorithm), they express skepticism about the significance and relevance of the work. The reviewer questions the importance of faster convergence and is unsure about the interest level for the ICLR audience. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, offers constructive feedback, and suggests additional experiments. They avoid harsh criticism and use phrases like 'I would like to see' and 'I am not sure' to soften their critiques. The reviewer also acknowledges their own expertise in the field, which adds credibility to their comments without being boastful."", ""The sentiment score is -80 because the reviewer explicitly states they do not think the work should be accepted and provides multiple criticisms without any positive comments. The reviewer points out lack of benefits, weak empirical evaluation, and insufficient evidence for the claims made. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without any softening language or positive feedback. The reviewer uses phrases like 'I do not think' and 'The authors should' which come across as somewhat blunt. The overall tone is professional but lacks the courtesy often found in more polite reviews.""]"
"['This review will unfortunately be very short because I am afraid there is not much to say about this well written paper, which seems to have been sent to the wrong conference. The scientific problem is interesting, namely the detection of topological artifacts in images showing biological phenomena (which I don’t know much about). The relevant literature here is basically literature from this field, which is not machine learning and not even image processing. The contribution of the paper, in terms of machine learning, is to apply a well known neural model (YOLO) to detect bounding boxes of objects in images, which are very specific. The contribution here does not lie in machine learning, I am afraid.\n\nThis is thus a purely experimental paper on a single application, namely object detection in specific images. Unfortunately the experiments are not convincing. The results are validated against a “traditional method”, which has never been cited, so we do not know what it is.\n\nThe performance gain obtained with YOLO seems to be minor, although the difference in time complexity is quite enormous (to the advantage of YOLO).\n\nThe contribution is thus minor and for me does not justify publication at ICLR.\n\nThe grant number is mentioned in the acknowledgments, which seems to violate double blind policy.\n', 'In this paper the authors apply methods developed in computer vision towards the identification of topological defects in nematic liquid crystals. Typically, defects are identified using a costly algorithm that is based on numerically computing the winding number at different locations in the image to identify defects. The authors demonstrate that a deep learning approach offers improvement to both the identification accuracy and rate at which defects can be identified. Finally, the authors do some work investigating the limitations of the model and show that breakdown occurs near the edge of the field of view of the microscope. They show that this also happens with a conventional approach.\n\nOverall, this seemed like a nice application of machine learning to a subject that has received significant attention from soft matter community. The results appear to be carefully presented and the analysis seems solid. However, it does not seem to me that ICLR is a particularly appropriate venue for this work and it is unclear exactly what this paper adds to a discussion on machine learning. While there is nothing wrong with taking an existing architecture (YOLO) and showing that it can successfully be applied to another domain, it does limit the machine learning novelty. It also does not seem as though the authors pushed particularly hard in this direction. I would have been interested, for example, in seeing some analysis of the features learned by the architecture trained to classify defects appropriately.\n\nI would encourage the authors to either submit this work to a journal closer to soft matter or to do some work to determine what insights and lessons might help machine learning researchers working on other applied projects. The closest I got from the paper was the discussion of bounding box sizes and subclassification in section 3. It would have been nice to see some work discussing the dependence on this choice and what physical insights one might be able to glean from it. \n', 'Summary:\nThis paper applies deep learning model YOLO to detect topological defects in 2D active nematics. Experimental results show that YOLO is robust and accurate, which outperforms traditional state-of-the-art defect detection methods significantly.\n\nPros:\n+ Detecting defects in 2D active nematics is an important task to study. \n+ YOLO is effective in object detection and shows good results for defect detection. \n+ The experiment shows that YOLO appears to outperform traditional state-of-the-art defect detection methods. \n\nCons:\n-\tThe technical contribution seems not enough. YOLO is state-of-the-art object detection method and has been widely used. However, this paper directly applies YOLO for this task, while few variants have been specifically designed or modified for the defect detection tasks. \n-\tThe experiments may miss some details. For example, what is the traditional method used for comparison? What is the difference between traditional method and YOLO? The paper should provide some explanations and introductions. \n-\tSince the training data set is imbalanced, does the proposed model utilize some strategy to overcome this problem?\n-\tThe detection rate comparison is not convincing. As shown in the experiments, traditional model and YOLO is operated by different machines, therefore, the detection rate comparison is not convincing.\n-\tThe paper contains some minors. For example, in table 1 and table 2, +1/2 defects should be -1/2.\n']","[-70, -20, -20]","[20, 60, 50]","[""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's suitability for the conference and its overall contribution. Phrases like 'unfortunately be very short', 'sent to the wrong conference', 'contribution here does not lie in machine learning', and 'does not justify publication' indicate a strongly negative sentiment. However, it's not entirely negative as the reviewer acknowledges the paper as 'well written' and the problem as 'interesting'. The politeness score is 20 because while the reviewer is direct in their criticism, they use polite language such as 'I am afraid' and 'unfortunately' to soften their negative feedback. They also start with a positive comment about the paper being well-written. However, the overall tone remains professional rather than overtly polite, hence the relatively low positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as a 'nice application' with 'carefully presented' results and 'solid' analysis, they express concerns about its suitability for the ICLR venue and its limited novelty in machine learning. The reviewer suggests submitting to a different journal or expanding on machine learning insights.\n\nThe politeness score is moderately positive (60) as the reviewer uses respectful language throughout. They acknowledge the paper's strengths before presenting criticisms, use phrases like 'I would encourage' and 'It would have been nice', and offer constructive suggestions rather than harsh criticisms. The tone is professional and considerate, even when expressing reservations about the paper's fit for the venue."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), there are more substantial criticisms ('Cons') that outweigh the positives. The reviewer points out several limitations in the paper's methodology, experimental design, and overall technical contribution. The politeness score is moderately positive (50) as the reviewer maintains a professional and objective tone throughout. They present both pros and cons in a balanced manner, using neutral language like 'The paper should provide' rather than more accusatory phrasing. The reviewer also frames criticisms as suggestions for improvement rather than outright dismissals of the work.""]"
"['The authors proposed a new method improving a previous Bayesian optimization approach for chemical design (Gomez-Bombarelli et al., 2016b) by addressing the problem that data points need to have valid molecular structures. The main contribution is a constrained Bayesian optimization approach that take into account the constraint on the probability of being valid.\n\nMy biggest concern of this paper is that it is not really using sequential evaluations to do automatic design of experiments on molecules. The experiments do not seem to fully support the proposed approach in terms of being able to adaptively do sequential evaluations. \n\nDetailed comments:\n1. The term inside the expectation of the EI criterion in Sec 2.5 should be max(0, f(m)-\\eta) rather than max(0, \\eta - f(m)).\n2. The EIC criterion the authors adopted uses Pr(C(m)) if the constraint is violated everywhere with high probability. It seems like Pr(C(m)) does not have the ability to explore regions with high uncertainty. How does this approach compare to Bayesian level set estimation approaches like \nB. Bryan, R. C. Nichol, C. R. Genovese, J. Schneider, C. J. Miller, and L. Wasserman, “Active learning for identifying function threshold boundaries,” in NIPS, 2006 \nI. Bogunovic, J. Scarlett, A. Krause, and V. Cevher, “Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation,” in NIPS, 2016.\n3. It would be good to explain in more detail how a constraint is labeled to be valid or invalid. \n4. What is the space of m in Sec 2.3 and the space of m in Sec 2.4? It seems like there is a discrepancy: Sec 2.3 is talking about m as a molecule but Sec 2.4 describes f as a function on the latent variable? It would be good to be clear about it.\n5. In the experiment, I think it is very necessary to show the effectiveness of the constrained BO approach in terms of how the performance varies as a function of the number of evaluations on the constraint and the function. The current empirical results only support the claim that the constrained BO approach is able to output more valid latent variables and the function values from constrained BO is higher than vanilla BO under the same number of training data. It is also strange why there is a set of test data. \n\n\nTypos/format:\n1. citation format problems across the paper, e.g. \n""Gomez-Bombarelli et al. (Gomez-Bombarelli et al., 2016b) presented Automatic Chem""\n""SMILES strings Weininger (1988) are a means of representing molecules as a character sequence.""\nIt\'s likely a problem of misuse of \\cite, \\citep.\n2. no period at the end of Sec 2.4', 'This paper proposes to improve the chemical compound generation by the Bayes optimization strategy, not by the new models. \nThe main proposal is to use the acquisition that switches the function based on the violation of a constraint, estimated via a BNN. \n\nI understand that the objective function, J_{comp}^{QED} is newly developed by the authors, but not intensively examined in the experiments. \nThe EIC, and the switching acquisition function is developed by (Schonlau+ 1998; Gelbard, 2015). \nSo I judge the technical novelty is somewhat limited. \n\nIt is unfortunate that the paper lacks intensive experimental comparisons with ""model assumption approaches"". \nMy concern is that the baseline is rely on the SMILE strings. \nIt is well known that the string-based generators are much weaker than the graph-based generators. \nIn fact, the baseline model is referred as ""CVAE"" in (Jing+, 2018) and showed very low scores against other models. \n\nThus, we cannot tell that these graph-based, ""model-assumption"" approaches are truly degraded in terms of the validity and the variety of generated molecules, \ncompared to those generated by the proposed method. \nIn that sense, preferable experimental setting is that to \ntest whether the constrained Bayesian optimization can boost the performance of the graph-based SOTA models. \n\n\n+ Showing that we can improve the validity the modification of the acquisition functions\n- Technical novelty is limited. \n- No comparison with SOTA models in ""graph-based, model assumption approaches"". \n', ""Summary:\nThis paper proposes a novel method for generating novel molecules with some targeted properties. Many studies on how to generate chemically valid molecular graphs have been done, but it is still an open problem due to the essential difficulty of generating discrete structures from any continuous latent space. From this motivation, the 'constrained' Bayesian optimization (BO) is applied and analyzed. Posing 'constraints' on the validity is realized by probability-weighting onto the expected improvement scores in BO. The 'validity' probability is learned beforehand by Bayesian neural nets in a supervised way. As empirical evaluations, two case studies are presented, and quality improvements of generated molecules are observed.\n\nComment:\n- The presentation would be too plain to find what parts are novel contributions. Every part of presentations seems originated from some past studies at the first glance. \n\n- In this paper, how to pose the validity 'constraint' onto Bayesian optimization would be the main concern. Thus if it is acquired through supervised learning of Bayesian neural nets in advance, that part should be explained more in details. How do we collect or setup the training data for that part? Is it valid to apply such trained models to the probability weighting P(C(m)) on EI criteria in the test phase? Any information leakage does not happen?\n\n- The implementations of constrained BO is just directly borrowed from Gelbart, 2015 including parallel BO with kriging believer heuristics? The description on the method is totally omitted and would need to be included.\n\n- How training of Bayesian neural nets for 'Experiment II' are performed? What training datasets are used? Is it the same as those for 'Experiment I' even though the target and problem are very different?\n\nPros:\n- a constrained Bayesian optimization with weighing EI by the probabilities from pre-trained Bayesian neural nets applied to the hot topic of valid molecule generations.\n- Experiments observe the quality improvements\n\nCons:\n- unclear and insufficient descriptions of the method and the problem\n- novel contributions are unclear\n""]","[-30, -20, -30]","[50, 50, 20]","[""The sentiment score is -30 because while the reviewer acknowledges the authors' contribution, they express significant concerns about the paper's methodology and experimental support. The opening paragraph is somewhat positive, but the 'biggest concern' mentioned and the numerous detailed comments indicate overall negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'It would be good to explain' and 'I think it is very necessary to show' which are polite ways of pointing out areas for improvement. The reviewer also provides detailed comments and suggestions, which is a courteous way to help the authors enhance their work."", 'The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects (\'improve the chemical compound generation\', \'Showing that we can improve the validity\'), they express more concerns and limitations (\'technical novelty is somewhat limited\', \'lacks intensive experimental comparisons\', \'we cannot tell that these graph-based, ""model-assumption"" approaches are truly degraded\'). The overall tone suggests the paper has significant shortcomings.\n\nThe politeness score is moderately positive (50) because the reviewer uses respectful and professional language throughout. They phrase criticisms constructively (\'It is unfortunate that...\', \'My concern is...\') and offer suggestions for improvement (\'preferable experimental setting is...\'). The reviewer also acknowledges the authors\' contributions (\'newly developed by the authors\') while pointing out limitations, maintaining a balanced and courteous tone.', ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('Experiments observe the quality improvements'), they express several concerns and criticisms about the paper's clarity, novelty, and methodology. The review lists more cons than pros and uses phrases like 'unclear and insufficient descriptions' and 'novel contributions are unclear'. The politeness score is slightly positive (20) as the reviewer maintains a professional tone throughout, using neutral language like 'would need to be included' rather than harsh criticisms. They also balance negative points with positive observations. The reviewer asks questions for clarification rather than making accusatory statements, which contributes to the polite tone.""]"
"['This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:\n\n- There are versions of attention mechanisms that are spare and differentiable. See here: \nFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\nAndré F. T. Martins, Ramón Fernandez Astudillo \n\n- Why is ""rationalizing textual matching"" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.\n\n- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:\nD. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018\nhttp://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf\n\n- I don\'t agree that explanations are sufficient if removing them doesn\'t degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:\nAdversarial Examples for Evaluating Reading Comprehension Systems.\nRobin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf\n\n- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.\n\n- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.\n\n- What is the ""certificate of exclusion of unselected parts"" that the proposed method has?\n\n- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it.', '\nThis paper tackles the problem of generating rationales for text matching problems (i.e., two pieces of text are given). The approach is in a similar spirit as (Lei et al, 2016) while the latter mainly focuses on one piece of text for text classification problems and this work focuses on generating pairs of rationales. The approach has been evaluated on NLI and QA datasets, which demonstrates that the generated rationales are sensible and comes at a cost of accuracy.\n\nThe approach employs a generation-encoding-generation schema: it firsts generates the rationale from one side as a sequence tagging problem, re-encodes the rationales and predicts the rationale on the other side as a span prediction problem. Leveraging a match-LSTM framework and generated rationales for prediction, the model can be trained using a policy gradient method.\n\nOverall, I think this problem is novel and interesting. However, I am not fully convinced whether the proposed solution (and its implementation) is the right way to do so. Also, the paper writing needs to much improved.\n\nFirst of all, there is certainly a drop in the end task performance while it is unclear whether the derived rationales are really that useful (if the goal is interpretability) in the current evaluation. I am not convinced by the noisy SciTrail evaluation for rationales -- the noisy part p’ can be totally irrelevant and assume that the rationale generation component learns some sort of alignment between two parts, so it is not surprising that the model will not select words from p’ and it doesn’t really show that the rationales are useful. I think it is necessary to conduct some human evaluation for generate rationales and also provide some simple baselines for comparison (for example, just converting the soft-attention in math-LSTM to some hard selections) and see if this interpretability (at a cost of task performance) is really worthy or not.\n\nSecondly, I am not sure that whether the current way of generating the rationale pairs really makes sense or not.\nIt casts the rationale generation on one side as a tagging problem while the rationale generation on the other side as a span prediction problem. Why is that? Do you make any assumption that the two pieces of texts are not symmetric (e.g., one side is much longer than the other side like most of the current QA setup)?\n\nThere is a regularization term for both x and y but it seems that there isn’t any constraint that the generated rationales on the y side are not overlapping. Is it a problem or not? I don’t know how this is dealt with in the implementation.\n\nUnderstanding sec 3 takes some efforts and I think the presentation could be much improved. For example, q * {x^k} is not defined -- I assume it means extracting the subset of q based on the 1’s in {x^k}. The equations in Sec 3.2 can be made clearer.\n\nFinally, it is also unclear that how the 3 datasets were chosen. There are so many NLI and QA datasets (some of them are more popular and more competitive) at this point. Is there a reason that these datasets were chosen? There is a setup called ‘no rationalization w/ re-encoding’ which means that the rationale is already provided on one side, but is unclear that whether the OpenIE tuple and the searchQA queries can be used as rationales directly.\n\nMinor points:\n- Distal supervision -> distant supervision\n- The first paragraph of Introduction, “absent attention or rationale mechanisms”, what does it mean by ‘absent attention’? Isn’t it the case that all the models used attention mechanisms?\n', 'This paper is about learning paired rationales that include the corresponding relevant spans of the (question, passage) or (premise, hypothesis).  Experimental results show the same or better accuracies using just the fraction of the input selected as when the whole input is used.\n\nWhile there has been prior work on learning rationales, this is the first I have seen that included this fine-grained pairing.  The paper also learns these rationales without explicitly labeled rationales but rather with only the distant supervision of the overall question answering or natural language inference task.\n\nThis paper could be made stronger by including an experimental evaluation of accuracy in an adversarial setting.  The model developed here might be well-suited for adversarial SquAD examples in which an extra sentence has been added.  It would be interesting to see these results.  This paper does include a somewhat similar adversarial evaluation (Section 4.3) but adds extra information to NLI examples.  Since for NLI, unlike QA, the extra sentence can change the correct label (can flip from entailment to contradiction), accuracy was not able to be evaluated.\n\nExperimentally, it would be good to compare against some prior work that doesn\'t include the pairing.  Perhaps an interpretability model based on the passage only without fine-grained pairing with the question?  My apologies if this corresponds to ""Independent"", I was somewhat confused by descriptions of the baseline.\n\nThe descriptions of the baselines was the least clear part of this paper.  It would be helpful to improve the clarity of Section 4.1 (perhaps adding a figure).\n\nOptional suggestion: consider breaking up the experiment section into two subsections: one for the cases in which the question rationales are provided (results in Table 1), and one for the cases in which the question-side rationales are learned as well (Table 2).  By putting all descriptions together, the paper explains two different settings and then needs to discuss which baselines are applicable to each setting and dataset and why.']","[-60, -20, 60]","[20, 50, 80]","[""The sentiment score is -60 because the review is predominantly critical, pointing out several significant issues with the paper such as lack of human evaluation, no direct comparison to related work, and questionable assumptions. However, it's not entirely negative as it acknowledges the paper's proposal and offers constructive feedback. The politeness score is 20 because while the reviewer maintains a professional tone and offers specific suggestions for improvement, the language is direct and doesn't use many polite phrases or softeners. The reviewer states disagreements plainly ('I don't agree') and points out flaws, but does so in a respectful, academic manner without personal attacks or overly harsh language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the problem as 'novel and interesting', they express several concerns and are 'not fully convinced' by the proposed solution. They point out issues with the evaluation, methodology, and presentation, indicating more criticism than praise. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, phrases criticisms as personal opinions ('I am not convinced', 'I think'), and offers constructive feedback. They also acknowledge positive aspects before presenting criticisms. The tone is professional and courteous, avoiding harsh or rude language, but it's not overly formal or deferential either."", ""The sentiment score is 60 (moderately positive) because the reviewer expresses appreciation for the novelty of the work and its contributions, while also providing constructive suggestions for improvement. The overall tone is supportive, but not overwhelmingly enthusiastic. The politeness score is 80 (quite polite) due to the respectful language used throughout, such as 'This paper could be made stronger by...' and 'My apologies if...'. The reviewer offers suggestions in a considerate manner and acknowledges potential misunderstandings on their part. The language is professional and courteous throughout, without any harsh criticisms.""]"
"[""I don't quite see how the proposed approach addresses non-linear canonical correlation analysis. In particular:\n\n1) The main motivation is the minimization of the conditional mutual information I(X;Y|Z), where X and Y correspond to the two views and Z is latent. First of all, what uncertainty does this expression has when X and Y are observations and Z is given? My understanding is that the main objective of any CCA problem should be to find some transformations, say f(X) and g(Y), with some (to be defined) desirable properties. For example, these would correspond to linear transformations, say Ax and By, for classical CCA. Therefore, should not one be interested in minimizing something like I(f(X);g(Y)|Z)?\n\n2) Assuming that the minimization of the conditional mutual information I(X;Y|Z) would be the goal, I don't quite see why the formulation in equation (6) would actually be equivalent (or be some reasonable approximation)? \n\n3) It is well known that differential entropy can be negative (e.g., Cover and Thomas, 2006). Why would the conditional mutual information in equation (4) be non-negative? Alternatively, what would negative values of I(X;Y|Z) mean in the CCA context? My understanding is that one should be interested in minimizing I(X;Y|Z), or its variants with transformations, in *absolute value* to ensure some closeness to conditional independence.\n\n4) Expressions in equation (5)-(6) are general and hold with no assumptions whatsoever for any random variables X, Y, Z (given the expectations/integrals exist). It is therefore not clear what are the variables of this minimization problem? (parameters? but what is the parametric model?)\n\n5) Assuming solving (6) is the goal, this formulation as mentioned by the authors is actually is quite a challenging problem involving latent variables. Some form of this approach explanation would \n\nI can not quite see how the proposed adversarial version would correct or supplement any of these questions.\n\nOther comments:\n\n1) It would be appropriate to cite the probabilistic CCA paper by Bach and Jordan (2005); a better citation for classical CCA would be Hotelling (1936).\n\n2) I find the multiple mentioning of the *non-linear* (in-)dependence confusing. Is this in statistical sense? And how exactly is this related to CCA? Does it have anything to do with the fact that the third and higher order cumulants are zero only for independent variables unless they are Gaussian? Moreover, does this linear independence have any connection with the non-linearity of the proposed CCA approach?\n\n3) What exactly is the *linear correlation criterion* and how does it enter the classical CCA or PCCA formulation (Introduction; bullet point 2)?\n\n4) It would be helpful to introduce the original CCA problem emphasizing that each view, X and Y, are *different* linear transformation of *the same* latent codes z. Moreover, the full description of the models (classical CCA/ PCCA) wouldn't take more than one-two paragraphs and would help the readers to avoid any misunderstanding.\n\n5) Are any assumptions necessary to ensure existence?\n"", ""This paper proposes to improve deep variational canonical correlation analysis (VCCA, Bi-VCCA) by 1) applying adversarial autoencoders (Makhzani et al. ICLR 2016) to model the encoding from multiple data views (X, Y, XY) to the latent representation (Z); and 2) introducing q(z|x,y) to explicitly encode the joint distribution of two views X,Y. The proposed approach, called adversarial canonical correlation analysis (ACCA), is essentially the application of adversarial autoencoder to multiple data views. Experiments on benchmark datasets, including the MNIST left right halved dataset, MNIST noisy dataset, and Wisconsin X-ray microbeam database, show the proposed ACCA result in higher dependence (measured by the normalized HSIC) between two data views compared to Bi-VCCA. \n\nThis paper is well motivated. Since adversarial autoencoder aims to improve based on VAE, it's natural to make use of adversarial autoencoder to improve the original VCCA. The advantage of ACCA is well supported by the experimental result.\n\nIn ACCA_NoCV, does the author use a Gaussian prior? If so, could the author provide more intuition to explain why ACCA_NoCV would outperform Bi-VCCA, which 1) also use a Gaussian prior; and 2) also does not use the complementary view XY? Why would adversarial training improve the result?\n\nIn ACCA, does the form of the prior distribution have to be specified in advance, such as Gaussian or the Gaussian mixture? Are the parameters of the prior learned during the training?\n\nWhen comparing the performance of different models, besides normalized HSIC, which is a quite recent approach, does the author compute the log-likelihood on the test set for Bi-VCCA and different variants of ACCA? Which model can achieve the highest test log-likelihood?\n\nAccording to equation (6), in principle, only q(z|x,y) is needed to approximate the true posterior distribution p(z|x,y). Did the author try to remove the first two terms in the right hand side of Equation (11), i.e., the expectation w.r.t. q_x(z) and q_y(z), and see how the model performance was affected?\n\nDoes adversarial training introduce longer training time compared to the Bi-VCCA?"", 'In this paper, the authors attempt to provide a perspective on CCA that is based on implicit distributions.  The authors compare and discuss several variants on CCA that have been proposed over the years, ranging from Linear CCA to Deep CCA and autoencoder variants.  In order to overcome the prior/likelihood distribution assumptions, the authors propose a CCA view that is based on learning implicit distributions, e.g, by using generative adversarial networks.   The authors further motivate their work by comparing with (Bi-)VCCA, claiming that the underlying assumptions lead to inconsistent constraints (or idealistic).  I think the work has merit, and I like the motivation.  Nevetheless, I think stronger experiments are required, as well as improvements in terms of clarity in the writing of the paper, and stronger support for the motivation.   Figure 2 should be better explained in text.  The MNIST experiment is useful, but using GANs usually results in sharper images than say VAE.  Also, comparisons with (i) other models besides Bi-VCCA, and (ii) on other multi-view real-world data (besides the MNIST_LR) would be very useful in terms of communicating the true benefits of this model.']","[-60, 60, 20]","[20, 80, 60]","[""The sentiment score is -60 because the reviewer expresses significant doubts and criticisms about the proposed approach, pointing out multiple issues and gaps in understanding. The review starts with 'I don't quite see how...' and continues to list several major concerns, indicating a generally negative sentiment. However, it's not entirely dismissive, as the reviewer offers suggestions and asks for clarifications, hence not reaching the lowest possible score. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'I don't quite see' and 'It would be helpful' rather than harsh or rude language. The reviewer also offers constructive feedback and suggestions for improvement, which contributes to a slightly positive politeness score."", ""The sentiment score is 60 (positive) because the reviewer states that the paper is 'well motivated' and that the 'advantage of ACCA is well supported by the experimental result.' This indicates a generally positive view of the paper. However, it's not extremely high as the reviewer also raises several questions and suggestions for improvement. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, framing their comments as questions or suggestions rather than criticisms. Phrases like 'could the author provide more intuition' and 'did the author try' are polite ways of suggesting improvements. The reviewer also acknowledges the paper's strengths before raising questions, which is a polite approach to peer review."", ""The sentiment score is slightly positive (20) because the reviewer states that the work 'has merit' and they 'like the motivation'. However, they also mention that 'stronger experiments are required' and improvements are needed, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while providing constructive criticism. They use phrases like 'I think' to soften their critiques and offer specific suggestions for improvement rather than harsh criticism. The overall tone is professional and supportive, even while pointing out areas for enhancement.""]"
"['The paper studies the problem of question generation from sparql queries. The motivation is to generate more training data for knowledge base question answering systems to be trained on. However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:\n- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745\n- Globally Coherent Text Generation with Neural Checklist Models\nChloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032\nThus the main novelty claim of the paper needs to be hedged appropriately. Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.\n\nSome other points:\n- How is the linearization of the inout done? It  typically matters\n- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.\n- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013\n- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used. Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.\n- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. ""what job did jefferson have"" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone. While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express. What were the guidelines used?\n', '\n\nThis paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework. The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.\n\nMajor points:\n\nOverall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty. I don’t recommend to accept this paper, at least in the current format.\n\nThe paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is “possible to generate a large high quality (SPARQL query, local form) dataset”. For the second contribution, there isn’t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications. I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.\n\nFor the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?\n\nIn general I find Section 3 pretty difficult to follow. What does “keeping notes” mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage. I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode. \\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.\n\nMinor points:\n- tau Yih et al, 2016 --> Yih et al, 2016\n- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.\n- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.\n', ""Overall:\nThis paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form. The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a “scratchpad” memory to keep track of what has been generated so far and to guide future generation. \n\nQuality and Clarity:\n-- The paper is well-written and easy to read. \n-- Consider using a standard fonts for the equations. \n\n\nOriginality :\nThe idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful. \nCompared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write “decoding information” to the “encoder” output. The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively “writes” a message (“coverage”) to the encoder's hidden states.\nIn the original coverage paper (Tu et.al, 2016), they also proposed a “neural network based coverage model” where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder. However, the modification is slightly marginal but seems quite effective. It is better to explain the major difference and the motivation of updating the hidden states.\n\n-------------------\nComments:\n-- In Equation (13), is there an activation function between W1 and W2?\n-- Based on Table 1, why did not evaluate the proposed model with beam-search?\n\n""]","[-50, -70, 60]","[50, 20, 70]","[""The sentiment score is -50 because the reviewer expresses significant concerns about the novelty of the paper's approach and points out several issues with the methodology and evaluation. However, it's not entirely negative as the reviewer acknowledges the paper's motivation and provides constructive feedback. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, offering specific suggestions for improvement without using harsh language. The reviewer uses phrases like 'I would propose' and 'Some other points:' which are polite ways of offering criticism. The overall tone is critical but constructive, balancing negative feedback with suggestions for improvement."", ""The sentiment score is -70 because the reviewer clearly states they do not recommend accepting the paper, calling it 'not good enough for an ICLR paper' and criticizing its contributions and clarity. However, it's not maximally negative as they do acknowledge some improvements and analysis. The politeness score is 20 because while the reviewer is direct in their criticism, they use relatively polite language like 'I think' and 'I believe', and provide specific recommendations for improvement. They avoid harsh or rude phrasing, maintaining a professional tone throughout, even while delivering negative feedback."", ""The sentiment score is 60 (positive) because the reviewer expresses overall positive views about the paper, noting it is 'well-written and easy to read' and describing the idea as 'interesting and useful'. They also mention the model seems 'quite effective'. However, it's not extremely positive as they point out some similarities to existing work and suggest some improvements. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive feedback and suggestions rather than harsh criticism. They use phrases like 'Consider using' and 'It is better to explain' which are polite ways of suggesting improvements. The reviewer also acknowledges the strengths of the paper before offering critiques, which is a polite approach to reviewing.""]"
"['The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).\n\nThere is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.\n\n- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.\n\n- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?', 'This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation:  d – p_data; d(y|x) – p_d(y|x); m(y|x) – p_theta(y|x)\n\nmax E_x~d E_y~d(y|x) [ log m (y|x) ]                 \t\t\t\t               (1) \nmax E_x~d { E_y~d(y|x) ) [ log d(y|x) ]}  -  E_y~d(y|x) [ log m (y|x) ]}        (2)\nmax { E_y~d [  log  (y) ]  -  E_y~d  log E_x~d(x|y) [ m (y|x) ]}                        (3)\n\nFirst error is that the “max” in (2) and (3) should be “min”. I will assume this minor error is corrected in the following.\nThe equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term  in (2) does not depend on model.  The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following:\n\n min { E_y~d [  log d (y) ]  -  E_y~d  E_x~d(x|y) [ log m (y|x) ]}                     (3’)\n\nNotice the swap of “E_x” and “log”. By Jensen’s nequality, we have \n\n log E_x~d(x|y)  m (y|x) ]  > E_x~d(x|y) [ log m (y|x)\n -  E_y~d  log E_x~d(x|y)  [ m (y|x) ]    < -  E_y~d  E_x~d(x|y) [ log m (y|x) ]                    \n\nSo, minimizing (3) amounts to minimizing a lower bound of the correct objective (3’). It does not make sense at all.\n', 'Summary:\n\nThis paper proposes maximizing the “reflective likelihood,” which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term.  The paper derives the reflective likelihood for classification models and unsupervised latent variable models.  Choices for \\alpha are also discussed, and connections are made to ranking losses.  Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling.\n\nPros:\n\nI like how the paper frames the reflective likelihood as a ranking loss.  It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed ‘focus’ the model on the dependent relationship y|x.  Can this be further formalized?  I would be very interested in seeing a derivation of this kind.    \n\nI like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration.\n\nCons:\n\nMy biggest issue with the paper is that I find much of the discussion lacks rigor.  I followed the argument through to Equation 3, but then I became confused when the discussion turned to ‘dependence paths’: “we want our learning procedure to follow the dependence path—the subspace in Θ for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3—the independence path” (p 3).  What are these dependence paths?  Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3?  Surely any conditional model x-->y has a ‘dependence path’ flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure?\n\nMoving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I’m not sure how exactly that final form was settled upon.  There seems to be a connection to maximum entropy methods?  That is,   E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well.  Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy.  As there is a long history of maximum entropy methods / classifiers, I’m surprised there were no mentions or references to this literature.  Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent).  I’m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references.   \n\nThe data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for.  I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood.  As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word ‘calibration’ is never mentioned.  More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus.  It would be great to show that this method provides good calibration directly during optimization and doesn’t need the post-hoc calibration steps that most methods require. \n\nEvaluation:  While the paper has some interesting ideas, they are not well defined, making the paper unready for publication.  Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper’s argument.  ']","[-20, -80, -50]","[50, -20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('experiments... are preliminary but encouraging'), they also point out significant issues ('the setting... does not make any sense'). The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions ('Please reconsider...', 'Please try to provide...') rather than harsh criticism. The reviewer maintains a professional tone, balancing critique with acknowledgment of the paper's potential merits."", ""The sentiment score is -80 because the review is highly critical, stating the paper is 'technically flawed' and pointing out a 'MAJOR ERROR'. The reviewer uses strong negative language like 'does not make sense at all'. The politeness score is -20 because while the reviewer is direct and somewhat blunt in their criticism, they don't use overtly rude language. They do attempt to correct a 'minor error' before proceeding, which shows some consideration. However, the use of capitalization for emphasis ('MAJOR ERROR') and the concluding statement about the paper not making sense at all come across as somewhat impolite in an academic context."", ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('Pros'), there are significant criticisms ('Cons') that outweigh the positives. The reviewer states that the paper is 'unready for publication' and that 'much of the discussion lacks rigor', indicating a generally negative sentiment. However, it's not entirely negative as the reviewer sees potential in some ideas, hence the score is not at the extreme negative end.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I like how' and 'I would be very interested in seeing' when discussing positive aspects. Even when critiquing, they use polite language such as 'My biggest issue' and 'I would have liked to have seen'. The reviewer offers constructive feedback and suggestions for improvement rather than harsh criticism. However, the score is not at the highest end of politeness as the review is direct in its criticisms without excessive softening language.""]"
"['This paper tries to propose a so-called hybrid algorithm to eliminate the gradient delay of asynchronous methods. The authors propose algorithm 1 and a simplified version algorithm 2 and prove the convergence of algorithm 2 in the paper.  The paper is very hard to follow, especially the algorithm description part. What I can understand is that the authors want to let the fast workers do more local updates until the computation in the slowest worker is done. The idea is similar to EASGD except that it forces the workers to communicate the server once the slowest one has completed their job.\n\nThe following are my concerns:\n1. Do you consider the overhead in constructing the communication between machines? in your method,  workers are keeping notifying servers that they are done with the computation. \n2. In Algorithm 1 line 9 and line 23, there are two assignments: x_init =x and x_init=ps.x, is there any conflict? \n3. In Algorithm 2,  at line 6 workers wait to receive ps.x, at line 20 server wait for updates. I think there is a bug, and nothing can be received at both ends.\n4. The experiments are too weak. There is no comparison between other related methods, such as downpour, easgd.\n5. The authors test resnet50 on cifar10,  however, there is no accuracy result. They show the result by using googlenet, why not resnet50? I am curious about the experimental settings.\n\nAbove all, the paper is hard to follow and the idea is very trivial. Experiments in the paper are also very weak. ', ""The paper proposes an algorithm to restrict the staleness in ASGD (asynchronous SGD), and also provides theoretical analysis. This is an interesting and important topic. However, I do not feel that this paper solves the fundamental issue - the staleness will be still very larger or some workers need to stay idle for a long time in the proposed algorithm if there exists some extremely slow worker. To me, the proposed algorithm is more or less just one implementation of ASGD, rather than a new algorithm. The key trick in the algorithm is collecting all workers' gradients in the master machine and update them at once, while hard limiting the number of updates in each worker. The theoretical analysis is not brand new. The\nline 6 in Algorithm 1 makes the delay a random variable related to the speed of a worker. The faster a worker is, the larger the tau is, which invalidates the assumption implicitly used in the theoretical analysis.\n\nThe experiment is done with up to 4 workers, which is not sufficient to validate the advantages of the proposed algorithm compared to state of the art ASGD algorithms. The comparison to other ASGD implementations is also missing, such as Hogwild! and Allreduce.\n\nIn addition, I am so surprised that this paper only have 10 references (the last one is duplicated). The literature review is quite shallow and many important work about ASGD are missing, e.g.,\n\n- Parallel and distributed computation: numerical methods, 1989.\n- Distributed delayed stochastic optimization, NIPS 2011.\n- Hogwild!, NIPS 2011\n- Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization, NIPS 2015\n- An asynchronous mini-batch algorithm for regularized stochastic optimization, 2016."", 'Overall, this paper is well written and clearly present their main contribution.\nHowever, the novel asynchronous distributed algorithm seems not be significant enough.\nThe delayed gradient condition has been widely discussed, but there are not enough comparison between these variants.\n']","[-70, -50, -20]","[-20, 20, 50]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's clarity, methodology, and experimental results. Phrases like 'very hard to follow', 'the idea is very trivial', and 'experiments are too weak' indicate a strong negative sentiment. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and direct without much attempt to soften the criticism. The reviewer lists concerns without any positive feedback or constructive suggestions, which comes across as somewhat impolite in academic discourse. However, the language isn't explicitly rude, which is why the score isn't lower."", ""The sentiment score is -50 because while the reviewer acknowledges the topic as 'interesting and important', they express significant concerns about the paper's contribution, methodology, and literature review. The reviewer states that the paper doesn't solve the fundamental issue, lacks comparison to state-of-the-art algorithms, and has insufficient experimental validation. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I do not feel that' and 'To me' to soften criticisms. However, they also express surprise at the limited references, which could be perceived as slightly impolite. The reviewer provides constructive feedback and suggestions for improvement, which contributes to the overall polite tone despite the negative sentiment."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is well-written and clearly presented, they express concerns about the significance of the novel algorithm and the lack of comparison with existing variants. The initial positive comment is outweighed by the subsequent criticisms. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, starting with a positive comment and using phrases like 'However' to soften criticism. They avoid harsh or direct negative statements, instead framing their concerns as observations or suggestions for improvement.""]"
"['This paper presents a new semi-supervised method for bilingual dictionary induction and proposes a new metric to measure isometry between embedding spaces.\n\nPros:\n- The paper proposes to use a new metric, the Gromov-Hausdorff distance to measure how isometric two word embedding spaces are.\n- The toy example is useful for motivating the use case of the method.\n- The approach achieves convincing results on the dataset.\n\nCons:\n- Beyond the isometry metric, the main innovation as far as I can see seems to be the hubness filtering, which is incremental and not ablated, so it is not clear how much improvement it yields. The weak orthogonality constraint has already been used in [2].\n- It is not clear to me what the proposed metric adds beyond the eigenvector similarity metric proposed in [1]. The authors should compare to this metric at least.\n- The authors might want to add the results of [3] for an up-to-date comparison.\n\n[1] Søgaard, A., Ruder, S., & Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.\n[2] Zhang, M., Liu, Y., Luan, H., & Sun, M. (2017). Adversarial Training for Unsupervised Bilingual Lexicon Induction. In Proceedings of ACL.\n[3] Artetxe, M., Labaka, G., & Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.', 'This paper presents a new semi-supervised method to learn cross-lingual word embeddings mappings combining unsupervised distribution matching, alignment over a training dictionary, and a weak orthogonality constraint. The paper also shows that the underlying isometry assumption in orthogonal mappings weakens as the languages involved are more distant, proposes a new method to quantify the strength of the said assumption, and argues that the proposed semi-supervised mapping method is particularly suited for these more challenging cases.\n\nI think that this is a solid work that explores an interesting research direction within cross-lingual embedding mappings. While the basic ingredients of the proposed method are not new, their combination is certainly original. In that regard, I think that the paper is rather incremental, but still has enough substance to make an interesting contribution. However, I think that some parts of the paper are too obscure, and I am not fully convinced by the experiments. I would appreciate if the authors could address my concerns below, and I would be happy to modify my score accordingly:\n\n1) My understanding is that the proposed method (the one named BLISS in the experiments) only makes use of the proposed semi-supervised framework (Section 3.2) and is not followed by the iterative procrustes refinement (Section 3.3), but this is not clear at all from the paper. Could you clarify this?\n\n2) It is well known that the retrieval method can have a big impact in bilingual dictionary induction due to the hubness problem. However, the paper does not detail which retrieval method is used in the experiments. I assume that MUSE uses CSLS and Vecmap uses nearest neighbor over cosine. Is this correct? What retrieval method does BLISS use?\n\n3) I assume that when you talk about the ""CSLS metric"" in page 7 and 13 you refer to the unsupervised validation criterion of Lample et al. (Section 3.5 in their paper), and not to CSLS itself (Section 2.3 in their paper). In either case, this needs some clarification.\n\n4) Unlike the ""train"" dictionaries, the ""full"" dictionaries from MUSE as provided at github also include the test set. Do you preprocess them to exclude the test set? If so, this should be clearly stated in the paper. If not, this would invalidate all these experiments.\n\n5) The authors use different language pairs in their different result tables, which I find very confusing. For instance, none of the language pairs in table 5 (except for en-ru), are included in the main results (table 3), so we do not know how the different baselines and variants perform in them. Is there any reason for that?\n\n6) Could you include all MUSE variants in Table 4?\n\n7) While you compare your method to different versions of Vecmap (Artetxe et al., ACL 2017 & AAAI 2018), the last one (Artetxe et al., ACL 2018) (http://aclweb.org/anthology/P18-1073) is missing. That paper reports 48.1% and 48.2% accuracy for en-it and en-de in the unsupervised case, which is substantially better than your results for en-it (45.9%) and at par for en-de (48.3%). This goes against the main motivation of the paper (i.e. unsupervised distribution information and supervision from dictionaries can be combined for best results), as a completely unsupervised method seems to perform better than (or at least at par with) the proposed semi-supervised method. I think that the paper should include some discussion on this. In particular, I would like to know whether you have any argument to believe that both works are complementary.', 'Summary: \n\nThe paper propose a semi-supervised algorithm for bilingual lexicon induction (BLI) problem. Prior works on BLI problem usually impose orthogonality constraint on the linear transformation in order to obtain a ""reversible"" mapping and to preserve the monolingual performance. However, from both modeling and generalization perspective, recent works do not impose this constraint while learning the mapping (Doval et al 2018, Jawanpuria et al 2018, Joulin et al 2018, Sogaard et al 2018, among others). The present work argues for the removal of the orthogonality constraint when language spaces are non-isometric, and proposes to employ the Gromov Hausdroff (GH) distance to validate this condition. Overall, the paper employs  an objective function which is the sum of the (unsupervised) adversarial distribution matching objective (Lample et al 2018b), (supervised) the BLI loss function (typically the square loss), and a consistency loss (Hoshen and Wolf 2018). Empirically, the proposed method shows better results than unsupervised method of Lample et al (2018b) and the Procrustes solution.\n\nThe pros:\n\n- Existing works have shown that some BLI techniques perform better than the other in *some* pair of languages. Hence, it seems that there may not be ""one size fit all"" BLI technique. The proposed usage of GH distance is in the direction to quantitatively categorize pairs of languages. Based on a carefully crafted metric, the practical systems may chose to use one BLI algorithm over another for a given pair of languages.\n\nThe cons:\n\n- From modeling perspective, the utility of weak orthogonality constraint in the objective function is unclear. Does it improve generalization performance? Is it for preserving monolingual performance? The cited works (in the above summary) show that removing the strong/weak orthogonality constraint improves the BLI accuracy while preserving the monolingual performance.\n- The baselines chosen for experiments are not state-of-the-art. In addition, Artetxe et al. (2017, 2018) results are with NN/ISF retrieval procedure. These baselines should be rerun with CSLS retrieval procedure (codes are available in the author\'s website), which is now a standard for BLI task. Refer to Artetxe et al (2018b), Joulin et al (2018), Jawanpuria et al (2018), Gravel et al (2018) for state-of-the-art (semi-supervised/ unsupervised) results on MUSE and Vecmap datasets.\n- Experiments with varying data (Table 6) does not provide a clear picture without discussing unsupervised/semi-supervised baselines.\n- The logic behind experiments on GH distance (Table 2) is unclear. Why should a high correlation with *a baseline* suggest that GH distance correlates well with the degree of isometry of the two languages? Does GH distance has high correlation with *any* baseline for BLI?\n\n\nArtetxe et al (2018b): A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.\nJoulin et al (2018): Loss in translation: Learning bilingual word mapping with a retrieval criterion.\nJawanpuria et al (2018): Learning multilingual word embeddings in latent metric space: a geometric approach.\nHoshen and Wolf (2018): Non-adversarial unsupervised word translation.\nDoval et al (2018): Improving cross-lingual word embeddings by meeting in the middle.\n']","[20, -20, -20]","[50, 60, 50]","[""The sentiment score is slightly positive (20) because the review begins by highlighting the paper's contributions and lists several pros before moving on to the cons. The reviewer acknowledges the paper's new method, metric, and convincing results. However, the score is not higher due to the significant criticisms in the 'Cons' section, which point out limitations and areas for improvement. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They present both pros and cons objectively without harsh criticism, and use phrases like 'The authors might want to' which suggests improvements in a non-confrontational manner. The review maintains a constructive tone, offering specific recommendations for improvement rather than outright dismissal of the work."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'solid work' with 'enough substance to make an interesting contribution', they also describe it as 'rather incremental' and express concerns about parts being 'too obscure' and not being 'fully convinced by the experiments'. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's merits and framing criticisms as requests for clarification or additional information. Phrases like 'I would appreciate if the authors could address my concerns' and 'I would be happy to modify my score accordingly' demonstrate a constructive and courteous approach. The reviewer also provides detailed, specific feedback, which is a polite way to help improve the paper."", 'The sentiment score is slightly negative (-20) because while the reviewer acknowledges some pros of the paper, they list more cons and point out several significant shortcomings. The reviewer suggests that the baselines chosen are not state-of-the-art, questions the utility of certain aspects of the methodology, and finds some of the experimental logic unclear. However, the tone is not entirely negative as they do recognize some positive aspects.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to present their critiques, avoiding harsh or personal criticism. The review is structured and objective, focusing on specific aspects of the paper rather than making sweeping negative statements. The reviewer also acknowledges the potential contributions of the paper, which adds to the politeness of the review.']"
"['This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.\n\nIn active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?\n\nThe novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.\n\nThe paper is well-written, has good experiments, and has a comprehensive related work section.\n\nOverall, this paper is good, but is not novel or important enough for acceptance.', 'The paper shows that Bayesian neural networks, trained with Dropout MC (Gal et al.) struggle to fully capture the posterior distribution of the weights.\nThis leads to over-confident predictions which is problematic particularly in an active learning scenario.\nTo prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.\nThe proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.\n\n\nThe paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce. \n\nHowever, the paper contains only little novelty and does not provide sufficiently new scientific insights.\nIt is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.\nFor instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores. Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.\nIt would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].\n\n\nAnother weakness of the paper is that the empirical evaluation is not sufficiently rigorous: \n\n1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.\n\n 2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M. \n \n 3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures. The same holds for the type of data, since the paper only shows results for image classification benchmarks.\n \n 4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.\n \n\n\n\n[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\nBalaji Lakshminarayanan, Alexander Pritzel, Charles Blundel\nNIPS 2017\n\n[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger\n    Snapshot Ensembles: Train 1, get {M} for free}\n    ICLR 2017\n\n[3] Bayesian Optimization with Robust Bayesian Neural Networks\n    J. Springenberg and A. Klein and S.Falkner and F. Hutter\n    NIPS 2016\n \n[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams\n    Scalable Bayesian Optimization Using Deep Neural Networks\n    ICML 2015\n\n[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling\n    Carlos Riquelme, George Tucker, Jasper Snoek\n    ICLR 2018', 'The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning. They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation. The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem. \n\nThe paper is clearly written and easy to follow. It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning. \n \nThe major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore. Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not. Therefore, it is a little misleading to still call it Bayesian active learning. Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective. \n\nThe motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout. So it seems not a reasonable solution for the mode collapse problem of MC-Dropout. It is not clear to me why we need to add MC-Dropout to the ensemble. What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?\n\nIn terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout. While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.\n\nThe labels of figures are hard to read. \n']","[-50, -50, 20]","[50, 20, 60]","[""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('well-written', 'good experiments', 'comprehensive related work section'), the overall tone is critical. The reviewer states that the improvements are 'rather minimal', the novelty is 'minimal', and concludes that the paper is 'not novel or important enough for acceptance'. This indicates a generally negative sentiment, though not extremely so.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They provide specific critiques and suggestions without using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper, which contributes to a polite tone. However, the review doesn't go out of its way to be exceptionally polite either, maintaining a neutral-to-positive professional courtesy."", ""The sentiment score is -50 because while the reviewer acknowledges the paper addresses an interesting research direction, they express significant criticisms. They state the paper 'contains only little novelty and does not provide sufficiently new scientific insights.' They also point out several weaknesses in the empirical evaluation. However, it's not entirely negative as they do recognize some value in the work. The politeness score is 20 because the reviewer uses generally polite language, avoiding harsh or rude expressions. They use phrases like 'It would be great if...' and 'I would like to have seen...', which are constructive and polite ways of suggesting improvements. However, the criticism is direct and doesn't go out of its way to be overly polite, hence the moderate positive score."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper is 'clearly written and easy to follow' and finds it 'interesting'. However, they express 'major concern' about the theoretical foundations and question the motivation, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging positives before presenting concerns. They phrase criticisms as questions or observations rather than direct attacks, maintaining a professional tone. The reviewer also offers constructive feedback, such as noting the difficulty in reading figure labels, which contributes to the polite tone.""]"
"['Overview and contributions: The authors propose the ModifAE model that is based on an autoencoder neural network for continuous trait image modifications. ModifAE requires fewer parameters and less time to train than existing generative models. The authors also present experiments to show that ModifAE produces more convincing and more consistent continuous face trait modifications than the current baselines.\n\nStrengths:\n1. Nice presentation of the model.\n2. Good experiments to justify improved running time and fewer number of parameters.\n\nWeaknesses:\n1. I am not completely convinced by the results in Figure 4. It doesn\'t seem like the model is able to pick up on subtle facial expressions and generate them in a flexible manner. In fact, the images look very very similar regardless of the value of the traits. Furthermore, the authors claim that ""In general, as she becomes more emotional, her smile increases, and as she is made more attractive, her smile increases as well, as smiling subjects are judged as more attractive"". I believe attractiveness and emotions are much more diverse and idiosyncratic than just the size of her smile...\n2. From Figure 5 it seems like ModifAE generates images that are lower in quality as compared to StarGAN. Can the authors comment on this point? How can ModifAE be improved to generate higher-quality images?\n\nQuestions to authors:\n1. Weakness points 1 and 2.\n2. This did not affect my rating, but I am slightly concerned by the labelings as seen in Figure 1. Is it reasonable to infer traits like ""trustworthy"", ""attractive"", ""aggressive"", ""responsible"" from images? Are these traits really what we should be classifying people\'s faces as, and are there any possible undesirable/sensitive biases from the dataset that our models could learn? I would like to hear the author\'s opinions on the ethical implications of these datasets and models. \n\nPresentation improvements, typos, edits, style, missing references:\nNone', 'The paper is about changing the attributes of a face image to let it look more aggressive, trustworthy etc. by means of a standalone autoencoder (named ModifAE). The approach is weak starting from the construction of the training set. Since continue social attributes on face images does not exist yet, CelebA dataset is judged by Song et al. (2017) with continuous face ratings and use the predicted ratings to train ModifAE. This obviously introduces a bias driven by the source regression model. The hourglass model is clearly explained. The experiments are not communicating: the to qualitative examples are not the best showcase for the attributes into play (attractive, emotional), and requires to severely magnify the pdf to spot something. This obviously show the Achille’s heel of these works, i.e., working with miniature images. Figure 5, personally, is about who among modifAE and stargan does less bad, since the resulting images are of low quality (the last row speaks loud about that)\nQuantitative results are really speaking user tests, so I will cal it as they are, user tests. They work only on two attributes, and show a reasonable advantage over stargan only for one attribute. \n', ""* The construction of the training dataset is clearly flawed by the use of an automatic algorithm that would certainly introduce a strong bias and noisy labels. Even though the dataset is supposed to encode continuous traits, the validation with human subjects is performed in a binary fashion.\n\n* I miss more formality in the presentation of the methodology. Figure 3. does not seem very self-explanatory, nor does the caption. Which is the dimensionality of the input trait vector?. I assume the input would be the trait ratings predicted by the human subjects. However in the experiments training seems to be done with a maximum of two traits. This makes me wonder how the dense part of the network can handle the dimensionality blow-up to match the latent space dimensionality without suffering from overfitting. I would appreciate some disussion regarding this.\n\n* While I appreciate a section reasoning why the method is supposed to work, those claims should be backed with an ablation study in the experimental section.  \n\n* The qualitative results show a few examples which I find very hard to evaluate due to the low-resolution of the predictions. In both traits there seems to be the same facial features modified and I can't find much difference between trustworthy and aggresssive (the labels could be swapped and I would have the same opinion on the results). I miss additional trait examples that would make clearer if the network is learning something besides generating serious and happy faces.\n\n* The qualitative comparison with StarGAN seems unfair, as if one checks the original paper their results are certainly more impressive than what Figure 5 shows.\n\n* The authors show only two traits in the experiments which makes me a bit suspicious about the performance of the network with the rest of traits. The training datset considers up to 40 traits.\n""]","[-20, -60, -70]","[60, -20, 20]","['The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the results and image quality. The reviewer is not completely convinced by the results in Figure 4 and notes that the ModifAE-generated images seem lower in quality compared to StarGAN. These weaknesses outweigh the positive comments about the model presentation and experiments. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, frames criticisms as questions or areas for improvement, and includes both strengths and weaknesses in their review. They also invite the authors to comment on specific points, showing a willingness to engage in constructive dialogue.', ""The sentiment score is -60 because the review is predominantly negative. The reviewer criticizes the paper's approach as 'weak', points out biases in the training set, and describes the experiments as 'not communicating'. They also state that the qualitative examples are not the best showcase and that the resulting images are of 'low quality'. The few positive aspects mentioned (like 'clearly explained hourglass model') are outweighed by the criticisms. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite blunt and dismissive. Phrases like 'obviously show the Achille's heel of these works' and 'who among modifAE and stargan does less bad' come across as somewhat harsh and impolite in the context of a professional review."", ""The sentiment score is -70 because the review is predominantly critical, pointing out several flaws and expressing skepticism about the methodology and results. The reviewer uses phrases like 'clearly flawed', 'miss more formality', 'hard to evaluate', and 'suspicious about the performance', indicating a negative sentiment. However, it's not entirely negative as the reviewer does acknowledge some positive aspects ('I appreciate a section reasoning...').\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone. They use phrases like 'I would appreciate', 'I miss', and 'I assume' rather than making blunt accusations. The reviewer also offers constructive feedback and suggestions for improvement. However, the politeness is not overly high due to the directness of some criticisms.""]"
"['This paper introduces a domain adaptation approach for structured output data, with a focus here on semantic segmentation. The idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence. This is achieved by defining new patch classes via clustering the source patches according to the semantic information, and making use of an adversarial classifier on the predicted patch-class distributions.\n\nStrengths:\n- Modeling the structure via patches is an interesting idea.\n- The proposed method achieves good results on standard benchmarks.\n\nWeaknesses:\n\nMethod:\n- The idea of relying on patches to model the structure is not new. This was achieved by Chen et al., CVPR 2018, ""ROAD: Reality Oriented Adaptation..."". In this work, however, the patches were assumed to be in correspondence, which leaves some novelty to this submission, although reduced.\n- In essence, the patch-based adversarial alignment remains global; this can be thought of as working at a lower resolution and on a different set of classes, defined by the clusters, than the global alignment. The can be observed by comparing Eq. 3 and Eq. 6, which have essentially the same form. This is fine, but was not clear to me until I reached Eq. 6. In fact, what I understood from the beginning of the paper was an alternative formulation, where one would essentially assign each patch to a cluster and aim to align the distributions of the output (original classes) within each cluster. I suggest the authors to clarify this, and possibly discuss the relation with this alternative approach.\n- I am not convinced by the claimed relationship to methods that learn disentangled representations. Here, in essence, the authors just perform clustering of the semantic information. This is fine, but I find the connection a bit far-fetched and would suggest dropping it.\n\nExperiments:\n- The comparison to the state of the art is fine, but I suggest adding the results of Chen et al, CVPR 2018, which achieves quite close accuracies, but still a bit lower. The work of Saito et al., CVPR 2018, ""Maximum Classifier Discrepancy..."" also reports results on semantic segmentation and should be mentioned here. I acknowledge however that their results are not as good as the one reported here.\n- While I appreciate the ablation study of Section 4.2, it only provides a partial picture. It would be interesting to study the influence of the exact values of the hyper-parameters on the results. These hyper-parameters are not only the weights \\lambda_d, \\lambda^g_{adv} and \\lambda^l_{adv}, but also the number of clusters and the size of the patches used. \n\nSummary:\nI would rate this paper as borderline. There is some novelty in the proposed approach, but it is mitigated by the relation to the work of Chen et al., CVPR 2018. The experiments show good results, but a more thorough evaluation of the influence of the hyper-parameters would be useful.\n', 'This paper proposes a drop-in module of disentangled patch representation learning for adversarial learning-based domain adaptation. The main idea is to encourage the source patch level representation to be disentangled, by creating certain intermediate pseudo-ground truths via clustering the label patch histograms using k-means. This basically creates an alternative, additional view of prediction target of the network outputs. And similar to global network output alignment by Tsai et al., the authors impose an adversarial loss on the additionally introduced view.\n\nClarity: The paper is well-written with good clarity. \n\nResults: This paper has a good experimental validation of proposed module.\n\nConcerns: \n- The idea of using patches in domain adaptation is not completely new. ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes, CVPR 2018 also uses the patch level information to help domain adaptation. Although the ideas are not entirely identical, this paper should at least cite and compare this work.\n\n- The disentangled patch feature learning introduces two additional loss, L_d and L_adv^l, which require three extra parameters, including K in K-means, lambda_d and lambda_adv^l. It will be great if a formal sensitivity analysis on the parameters can be conducted. There are some details missing in the paper too. For example, what is the performance of the VGG source model without adaptation? I am also curious about the learning behavior of the proposed method. Could you show the mIoU v.s. epoch curve for GTA2Cityscapes, or any other benchmarks?\n\n- Although consistently improving over Tsai et al., CVPR18, the introduced methods does not show very significant gain in multiple experiments. On SYNTHIA-to-City, only 0.4 mIoU gain is obtained. In addition, while the proposed method is empirically effective, it is largely task-specific and restricted to domain adaptation for scene parsing only. It seems difficult to generalize the same method to other domain adaptation tasks. The limitation on the performance gain and generalizability somehow reduced the contribution from this work to the community.\n\n- A major concern of this work is the lack of citation and direct comparison to multiple previous SOTAs. For example, the paper should compare the end-system performance with several published works such as:\n1. Zhang et al., Fully convolutional adaptation networks for semantic segmentation, CVPR2018\n2. Zhu et al., Penalizing top performers: conservative loss for semantic segmentation adaptation, ECCV2018\n3. Zou et al., Domain adaptation for semantic segmentation via class-balanced self-training, ECCV2018\nAnd according to the results reported by these works, the proposed joint framework in this paper does not seem very competitive in terms of the UDA performance in multiple settings', ""The authors tackle the unsupervised domain adaptation problem on tasks with structured output (in this case, semantic segmentation) by performing adversarial alignment at two levels: globally, using the entire image, and locally, using patches of the image. Their global alignment method matches previous adversarial adaptation approaches, so the primary contribution appears to be their patch-level alignment method. They cluster source image patches by histogramming the corresponding label patches, then performing K-means clustering on the histogrammed label features. A new model is trained to reproduce the cluster labels from the source image patches, and this model is adversarially optimized so that target image patches produce a matching feature distribution.\n\nThe paper is well-written and concise. It's organized well, and I had very little trouble following the description of their method. The various components of their model are straightforward and well-motivated. They validate their model on multiple synthetic-to-real segmentation tasks, demonstrating strong performance relative to existing baselines, and they also provide a thorough ablation study showing that each of the components of their proposed model is an important part of their final product, which further convinces the reader that the model is sound.\n\nOne quibble is that the authors mention disentanglement quite a bit in this paper, including in the title, though it isn't clear to me what is being disentangled. They claim the use of of label information is a disentangling factor, but that seems to be true of domain adaptation approaches in general, which all attempt to disentangle semantic information from domain-specific details in some form or other. Further clarification on precisely what is being disentangled would be helpful.\n\nAnother question that lingers is whether or not the additional classification module $H$ and the clustering are truly necessary. A baseline I would like to see would be to remove $H$ entirely and simply train another adversarial discriminator similar to $D_g$ directly on patches of $O$ instead of the full output. This sounds similar to the ablation experiment mentioned in 4.2 where $L_d$ is removed, but my understanding is that ablation experiment still uses an additional featurizer $H$. A more rigorous exploration of the clustering process, such as visualizations of learned clusters and a study of how the number of clusters affects performance would serve to further validate the model.\n""]","[-20, -30, 70]","[60, 60, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths ('good results', 'interesting idea'), they also point out several weaknesses and describe the paper as 'borderline'. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive criticism, and acknowledges the paper's merits. They use phrases like 'I suggest', 'I appreciate', and 'I acknowledge', which contribute to a polite tone. The reviewer also provides detailed feedback and suggestions for improvement, which is considerate and helpful to the authors."", ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('good experimental validation', 'well-written with good clarity'), they express several concerns and limitations of the work. These include the lack of novelty, limited performance gains, and absence of comparisons to state-of-the-art methods. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions ('It will be great if...', 'Could you show...'), and balances negative points with positive observations. The reviewer maintains a professional tone without using harsh or dismissive language, even when pointing out significant limitations of the work."", ""The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper. They describe it as 'well-written and concise', 'organized well', and state that the model is 'sound' and demonstrates 'strong performance'. The reviewer does raise some questions and suggestions, but these are presented constructively rather than critically. The politeness score is 80 (polite) due to the reviewer's respectful and professional tone throughout. They use phrases like 'One quibble is...' and 'Another question that lingers...' to introduce their concerns gently. The reviewer also acknowledges the paper's strengths before suggesting improvements, which is a polite approach to criticism. The language is consistently courteous and constructive, avoiding any harsh or dismissive statements.""]"
"['The paper presents a variational inference approach for locally linear dynamical models. In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,  enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO. Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.  \n\nQuality: The experiments appear to be well designed and support the main claims of the paper. \n\nClarity: The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately. It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way. I also struggled a little to understand what is the difference between forward interpolate and filtering. \n\nOriginality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted. In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.  \n\nSignificance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics. \n\nOverall, this appears to be a board-line paper with weak novelty. On the positive side, the experimental validation seems well done. The clarity of this paper needs to be strengthened.  \n\nMinor comments: \n- abstract: uncover nonlinear observation? -> maybe change ""observation"" to ""latent dynamics""?\n\n', 'I\'ll start with a disclaimer: I have reviewed the NIPS 2019 submission of this paper which was eventually rejected. Compared to the NIPS version, this manuscript had significantly improved in its completeness. However, the writing still can be improved for rigor, consistency, typos, completeness, and readability.\n\nAuthors propose a novel variational inference method for a locally linear latent dynamical system. The key innovation is in using a structured ""parent distribution"" that can share the nonlinear dynamics operator in the generative model making it more powerful compared. However, this parent distribution is not usable, since it\'s an intractable variational posterior. Normally, this will prevent variational inference, but the authors take another step by using Laplace approximation to build a ""child distribution"" with a multivariate gaussian form. During the inference, the child distribution is used, but the parameters of the parent distribution can still be updated through the entropy term in the stochastic ELBO and the Laplace approximation. They use a clever trick to formulate the usual optimization in the Laplace approximation as a fixed point update rule and take one fixed point update per ADAM gradient step on the ELBO. This allows the gradient to flow through the Laplace approximation.\n\nSome of the results are very impressive, and some are harder to evaluate due to lack of proper comparison. For all examples, the forward interpolate (really forecasting with smoothed initial condition) provides a lot of information. However, it would be nice to see actual simulations from the learned LLDS for a longer period of time. For example, is the shape of the action potential accurate in the single cell example? (it should be since the 2 ms predictive r^2 shows around 80%).\n\nExcept in Fig 2, the 3 other examples are only compared against GfLDS. Since GfLDS involves nonconvex optimization, it would be reasonable to also request a simple LDS as a baseline to make sure it\'s not an issue of GfLDS fitting.\n\nFor the r^2=0.49 claim on the left to right brain prediction, how does a baseline FA or CCA model perform?\n\nWas input current ignored in the single cell voltage data? Or you somehow included the input current as observation model?\n\nAs for the comment on Gaussian VIND performing better on explaining variance of the data even though it was actually count data, I think this maybe because you are measuring squared error. If you measured point process likelihood or pseudo-r^2 instead, Poisson VIND may outperform. Both your forecasting and the supplementary results figure show that Poisson VIND is definitely doing much better! (What was the sampling rate of the Guo et al data?)\n\nThe supplementary material is essential for this paper. The main text is not sufficient to understand the method.\n\nThis method relies on the fixed point update rule operating in a contractive regime. Authors mention in the appendix that this can be *guaranteed* throughout training by appropriate choices of hyperparameters and network architecture. This seems to be a crucial detail but is not described!!! Please add this information.\n\nThere\'s a trial index suddenly appearing in Algorithm 1 that is not mentioned anywhere else.\n\nIs the ADAM gradient descent in Algorithm 1 just one step or multiple?\n\nMSE -> MSE_k in eq 13\n\nLFADS transition function is not deterministic. (page 4)\n\nlog Q_{phi,varphi} is quadratic in Z for the LLDS case. Text shouldn\'t be \'includes terms quadratic in Z\' (misleading).\n\nregular gradient ascent update --> need reference (page 4)\n\nDue to the laplace approximation step, you don\'t need to infer the normalization term of the parent distribution. This is not described in the methods (page 3).\n\nEq 4 and 5 are inconsistent in notation.\n\nEq (1-6) are not novel but text suggests that it is.\n\nPredict*ive* mean square error (page 2)\n\nIntroduction can use some rewriting.\n\narXiv papers need better citation formatting.', 'This paper discusses a algorithm for variational inference of a non-linear dynamical models. In this paper model assumption is to use single stage Markov model in latent space with every latent variable Z_t to be defined Gaussian distributed with mean depends on Z_(t-1) and time invariant variance matrix lambda. The non linearity in transition is encoded in mean of Guassian distribution. For modeling the likelihood and observation model, the Poisson or Normal distribution are used with X_t being sampled from another Gaussian or Poisson distribution with the non-linearty being encoded in the parameters of distribution with variable Z_t.  This way of modeling resembles so of many linear dynamical model with the difference of transition and observation distribution have nonlinearity term encoded in them. \nThe contribution of this paper can be summarized over following points:\n\n- The authors proposed the nonlinear transition and observation model and introduced a tractable inference model using Laplace approximation in which for every given set of model parameter solves for parameters of Laplace approximation of posteriori and then model parameters get updated until converges\n\n-the second point is to show how this model is successful to capture the non-linearity of the data while other linear models do not have that capabilities \n\n\nNovelty and Quality: \nThe main contribution of this paper is summarized above. The paper do not contain any significant theorem or mathematical claims, except derivation steps for finding Laplace approximation of the posteriori. The main challenge here is to address effectiveness of this model in comparison to other non-linear dynamical system that we can name papers as early as Ghahramani, Zoubin, and Sam T. Roweis. ""Learning nonlinear dynamical systems using an EM algorithm.""\xa0Advances in neural information processing systems. 1999. \nor more recent RNN paper LSTM based papers. I think authors need to distinguish what this paper can give to community beside approximate posteriori of latent variables that other competing models are not capable of. If the aim is to have that posteriori, the authors should show what type of interpretation they have drawn from that in experiments.\nThere are lots of literature exist on speech, language models and visual prediction which can be used as reference as well.\n\nClarity: \nThe paper is well written and some previous relevant methods have been reviewed . There are a few issues that are listed below: \n\n1- as mentioned in Quality sections authors should be more clear about what is distinguished in this paper that other non-linear dynamical systems \n\n2- they used short form RM for Recognition model or FPI for fixed point iteration that need need to be defined before being used\n\n\n\nsignificance and experiments: \nThe experiments are extensive and authors have compared their algorithm with some other linear dynamical systems (LDS) competing algorithms and showed improvement in many of the cases for trajectory reconstruction. \nA few points can be addressed better, it can be seen for many of experiments exhaustive search is used for finding dimension of latent variable. This issue is addressed in Kalantari, Rahi, Joydeep Ghosh, and Mingyuan Zhou. ""Nonparametric Bayesian sparse graph linear dynamical systems.""\xa0arXiv preprint arXiv:1802.07434\xa0(2018). That paper can use non-parametric approaches to find best latent dimension, although the paper applied the technique on linear system, same technique could be adopted to non-linear models. Also that model is capable of finding multiple linear system that model the non linearity by switching between diffrent linear system, for switching linear system, this paper can be named as well: Linderman, Scott, et al. ""Bayesian learning and inference in recurrent switching linear dynamical systems.""\xa0Artificial Intelligence and Statistics. 2017.\n\nIt is shown that the model can reconstruct the spikes very well while linear model do not have that power (which is expected), but it is interesting to see how other non-linear models would compare to this model under those certain conditions\n\nIt is desired and interesting to see how the model behave one step ahead and K-step ahead prediction. Please address why it cannot be done if there is difficulties in that.']","[-20, 20, -20]","[50, 50, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('experiments appear to be well designed', 'experiments are thoroughly conducted'), they also point out significant weaknesses ('clarity is below average', 'technical novelty of this paper rather weak', 'board-line paper with weak novelty'). The overall tone suggests more concerns than praise.\n\nThe politeness score is moderately positive (50) because the reviewer uses respectful and professional language throughout. They provide constructive criticism without harsh or rude phrasing. For example, they use phrases like 'It would be helpful to...' and 'I struggled a little to understand...' instead of more direct or negative statements. The reviewer also balances criticism with positive remarks, which contributes to the polite tone."", ""The sentiment score is slightly positive (20) because while the reviewer acknowledges significant improvements and impressive results, they also point out several areas for improvement and missing information. The overall tone is constructive but with substantial critiques. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging improvements and using phrases like 'it would be nice to see' and 'please add this information' rather than making demands. They also offer explanations for their suggestions and ask clarifying questions instead of making accusations. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a professional tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'The paper is well written', 'The experiments are extensive'), they also raise several concerns and suggest areas for improvement. The reviewer questions the novelty of the work and asks for clearer differentiation from existing methods. They also point out missing comparisons and suggest additional experiments.\n\nThe politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'It is desired and interesting to see' and 'Please address' which are polite ways of making suggestions. The critique is presented constructively, offering specific recommendations for improvement rather than harsh criticism. However, the score is not higher because the review lacks overtly polite language or praise, maintaining a mostly neutral, academic tone.""]"
"['This article presents a novel approach called Integral Pruning (IP) to reduce the computation cost of Deep neural networks (DNN) by integrating activation pruning along with weight pruning. The authors show that common techniques of exclusive weight pruning does compress the model size, but increases the number of non-zero activations after ReLU. This would counteract the advantage of DNN accelerator designs (Albericio et al., 2016; Reagen et al., 2016) that leverage activation sparsity to speed up the computations. IP starts with pruning the weights using an existing technique to mask out weights under a threshold and then fine-tune the network in an iterative fashion to maintain the accuracy. After weight pruning, IP further masks out the activations with smaller magnitude to reduce the computation cost. Unlike weight pruning techniques that use static masks, the authors propose to use dynamic activation masks for activation sparsity in order to account for various patterns that are being activated in DNN for different input samples. In order to do this, the \'winner rate\' measure for every layer (or for a group of layers in deep networks like ResNet32) is defined, to dynamically set the threshold for the generation of activation masks which eventually controls the amount of non-zero activation entries. The article empirically analyzes the sensitivity of activation pruning on validation data by setting different winner rates at every layer in DNN and decides upon a set of winner rates accordingly followed by an iteration of fine-tuning the network to maintain its performance. The authors show that their technique produced lower number of non-zero activations in comparison with the intrinsic sparse ReLU activations and weight pruning techniques. \n\nThe topic of reducing network complexity for embedded implementations of DNNs is highly relevant, in particular for the ICLR community.\n\nThe IP technique yields significantly reduced number of multiply-accumulate operations (MACs) across different models like MLP-3, ConvNet-5, ResNet32 and AlexNet and on different datasets like MNIST, CIFAR10 and ImageNet. They also depicted that pruning the activations with dynamic activation masks followed by fine-tuning the network yields more sparse activations and negligible loss in accuracy when compared against using static activation masks.\n    \n\nStrengths of the paper:\n- The motivation to extend compression beyond the weights to activations in order to support the DNN accelerator designs and the technical details are clearly explained. \n- The proposed technique indeed produces sparser activations than intrinsic ReLu sparse activations and can also applied to any network regardless of the choice of activation function.\n- The proposed technique is evaluated across different network architectures and datasets.\n- The advantage of adapting dynamic activation masks over static ones is clearly demonstrated.\n\n Weaknesses of the paper:\n- The originality of the approach is limited because it is a relatively straightforward combination of existing techniques for weight and activation pruning.\n- The ""winner rate"" measure is defined for every layer and should be explored over different values in order to find the equilibrium to reduce the number of non-zero activations and maintain the accuracy. This search of winner rates will become inefficient as the depth of the network increases. However, the authors used a single winner rate for a group of layers in case of ResNet-32 to reduce the exploration of search space but this choice might lead to suboptimal results.\n- The authors compare the resultant number of MAC operations against numbers from the weight pruning technique. However, there also exist different works on group pruning techniques like Liu et al. (2017), Huang & Wang (2017), Ye et al. (2018) to prune entire channels / feature maps and thus yield more compact networks. Since these approaches prune the channels, they show a direct impact on the computation complexity and greatly reduce the computation time. A proper and fair comparison would be to compare the numbers of IP against such group pruning techniques. This comparison is highly important to highlight the significance of the approach on speeding up the DNNs and it is missing from the paper.\n- At several locations in Section 4, e.g. Sec. 4.1, 4.3, and 4.4. there is no precise statement about the incurred accuracy loss (or no statement at all). The connection to Figures 4 and 5 is not immediately clear and should be made explicit.\n\t\t\nReferences:\n- Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming.\n- Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers\n- Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks.\n        \nOverall Evaluation:\nThe authors integrate activation pruning along with the weight pruning and show that the number of MAC operations are greatly reduced by their technique when compared to the numbers of weight pruning alone. \tHowever, I am not convinced regarding the reported number of MAC operations since the number of MAC operations of sparse weight matrices and activations would remain the same as the original models unless some of the filters/activation maps are pruned from the network.  On the other hand, comparisons against group pruning techniques are highly necessary to evaluate the potential impact of the approach on speeding up of DNNs. My preliminary rating is a weak reject but I am open to revise my rating based on the authors response to the above stated major weaknesses.\n\nMinor comments:\n- Caption of Fig. 4 should mention the task on which the results were obtained.\n- There are occasional grammar errors and typos that should be corrected.', 'The main contribution of the paper is an integral model compression method that handles both weight and activation pruning. Increasing the network weight and activation sparsity can lead to more efficient network computation.  The authors show in the paper that pruning the network weights alone may result in a decrease in activation sparsity, which may not necessarily improve the overall computation. The proposed solution is a 2-stage process that first prunes the weights and then the activation. \n\nPros:\n\n- The results show that the proposed method is effective in reducing the number of multiply-and-accumulate (MAC) compared to weight pruning alone. The improvements are consistent across multiple network architectures and datasets.\n- It also shows that weight pruning alone leads to a slight increase in the number of non-zeros activation.\n\nCons:\n\n- A simple approach with limited novelty.\n- Related work should include other compression techniques, such as low-rank approximation,  weight quantization and varying hidden layer sizes.\n- There is no comparison with other model compression techniques mentioned above.\n ', 'This paper proposes to compress the deep learning model using both activation pruning and weight pruning. Combining both sparsities, the MACs are significantly reduced. \n\nMy main concern is that there is no time comparison. The experiments only show the reduction in terms of the number of non-zeros in weights and activation as well as the MACs. Typically, to deal with sparse activations and sparse weights, there are some overhead computations such as computing indices. Also, dense matrix-matrix(vector) multiplications can be faster by using specially designed libraries.  I would suggest the authors show the improvement for the proposed compression approach in terms of wall-clock time, in CPU, GPU or other hardware platforms. \n\nThe pruning method seems straight-forward to me. I am wondering how to choose the winner rate for each layer. It seems to take a quite long time to pick a set of winner rates for a deep neural network. \n\nThe paper is easy to read in general. However, it is not clear to me how such a compression approach can speed up the training or the inference of deep learning models in practice. \n\n\n']","[-30, 20, -20]","[60, 50, 50]","[""The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper, they ultimately give a 'weak reject' recommendation. The review points out several weaknesses and expresses skepticism about some of the reported results. However, it's not entirely negative as the reviewer is open to revising their opinion based on the authors' response. The politeness score is 60 because the reviewer uses professional and respectful language throughout. They acknowledge the paper's strengths before discussing weaknesses, and phrase criticisms as constructive feedback rather than harsh judgments. The reviewer also uses polite phrases like 'I am open to revise my rating' which contributes to the overall courteous tone."", ""The sentiment score is slightly positive (20) because the review begins by acknowledging the main contribution of the paper and its effectiveness, as evidenced by the 'Pros' section. However, the 'Cons' section balances this with some criticisms, preventing a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral language throughout, presenting both pros and cons objectively without harsh criticism. The reviewer's tone is professional and constructive, offering suggestions for improvement rather than outright dismissal of the work."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's proposal and its potential to reduce MACs, they express several concerns and suggestions for improvement. The main issues raised are the lack of time comparison, questions about the practicality of the approach, and uncertainty about how to choose winner rates. These concerns outweigh the initial positive acknowledgment of the paper's concept.\n\nThe politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I would suggest' and 'I am wondering' which are polite ways to express concerns or ask for clarification. The reviewer also acknowledges positive aspects, such as the paper being 'easy to read in general'. While critical, the feedback is presented in a respectful manner without harsh or rude language.""]"
"['Summary: This paper proposes three new techniques for improving Atari performance over APE (Horgan 2018).  Two of them are closely linked in that they deal with improving stability.  Another involves integrating the use of expert trajectories from DQfD.  \n\nI will summarize each: \n\nTransformed Bellman: This applies a rescaling function (it\'s basically a monotonically increasing version of the sqrt(x) function) to the Q-function and applies the inverse of the function to the max Q-value of the next state (such that the contracting effect h-function is not ""applied"" multiple times when doing the TD backup).  \n\nTemporal Consistency: This encourages the ""next state"" after where the TD-update is applied to not change too much.  This addresses a problem discussed in (Durugkar 2018).  I think the intuition here is that the state which follows the state with the TD update may be visually similar, but it does not impact the value in the past states, so its value function should not have a highly correlated change with the previous state\'s change in value function.    \n\nDQfD: Storing an expert replay buffer and an actor replay buffer.  The expert replay buffer is fixed and the actor replay buffer stores the most recent ""actor processes"".  Train with both a supervised imitation loss (only for the highest return episode) and the original TD loss.  Additionally, the pre-training phase is removed and the ratio of expert-learned trajectories is fixed (both seem like steps in the right direction).  \n\nReview: This paper proposes a few changes to DQN training, two of which are aimed at reducing instability, and one is aimed at improving exploration (expert trajectories).  Because all of these changes are well justified and the experiments are fairly thorough, I recommend acceptance.  My main reservation is that the ideas presented are not very strongly thematically linked.  The presence of ablation studies compensates for this to some extent.  \n\nStrengths: \n\n  -The discussion of related work and comparison to baselines is pretty extensive.  For example I appreciated the ablation study removing ""transformed Q-learning"" and comparison to the pop-art method.  \n\n  -The results, at least for Ape-X DQfD seem impressive to me in that the method works without reward clipping and with a much higher discount factor.  Additionally the results generally outperform DQfD (uses expert trajectories) and Rainbow (no human trajectories).  Additionally evidence was presented that the learned policies often exceed the performance of the human demonstrations (for example in time to achieve rewards).  \n\nWeaknesses: \n\n  -Two of the techniques: ""transformed bellman"" and ""temporal consistency"" seem well-linked thematically, but the expert demonstration idea seems orthogonal.  I would have preferred splitting that idea out into a separate paper, given that the paper is already 20 pages.  \n\n  -The motivation for temporal consistency just references (Durugkar 2018).  The readability of this paper would be improved if it were discussed more here as well.  I also feel like the analysis could be more thorough here, for example a result using the temporal consistency loss on Baird\'s counter example really should be shown (like figure 2 in Durugkar\'s paper).  \n\n-It would be nice to see a visualization or a toy problem with the ""transformed bellman"".  \n\nQuestions: \n\n-Is the ""highest return episode"" idea (3.4) general or is it exploiting the fact that Atari is deterministic?  It seems like in general we\'d want to use many high reward episodes, or the highest reward episodes that go into different parts of state space.  It seems like it could be a very bad idea on certain settings (for example if the reward has a lot of randomness).  \n\n-""Proposition 3.1 shows that in the basic cases when either h is linear or the MDP is deterministic, Th\nhas the unique fixed point h ◦ Q∗"".  From 3.1, it looks if h is linear, then it distributes over r(x,a) + maxh^{-1}(Q) and then it also won\'t effect which is the max, so it would reduce to h*r(x,a) + max(Q) - which means it\'s just rescaling the original reward.  So then this result is trivial?  Please correct me if I misunderstood something here.  \n\n-Could an MDP be constructed which causes the transformed bellman operator to perform badly?  I am imagining something where the MDP is just a single step, and there is a stochastic action which behaves like a lottery.  So perhaps there is a 1-in-1-million chance to win 1-billion dollars by taking an action.  If I understand correctly the transformed bellman operator will destroy the large reward here (because in a single step, there is just r(x,a) which h is applied to).  Which would make the action seem bad even though it\'s actually appealing.  \n\nNotes: \n\n  -I did not read the proofs in the appendix.  ', ""The paper reads well, proposes well motivated modifications to existing methods, and gets what appears to be strong results. I have no experience in RL, and although I read the paper I don't feel able to make meaningful comments."", ""This paper propose a method that aims to solve the following 3 problems: sensitivity to unclipped reward, robustness to the value of the discount factor, and the exploration problem. \n\nPros\nThis paper propose a transformed Bellman operator, and the author proved its convergence under some deterministic MDP conditions. The proposed transformed Bellman operator is interesting since that is analogous to some variance reduction techniques in the policy gradient literature. In the value based method literatures, those techniques have not been well studied.\n\nCons\nI think the main issue of this paper is the experiments can not fully support the advantage claim of the proposed method. \n\t1. With the author's hyper-parameters, the proposed method (Ape-X DQfD) has worse performance than the baseline Ape-X DQN, with the original hyper parameter of Ape-X DQN (Table 1). The author has a version of the baseline with the same hyper parameter as the proposed method, but the modified one is worse than the original baseline, which is not satisfactory. I think in general we should try to keep the original hyper parameter especially the original performance is better. \n\t2. With the same hyper parameters, the performance of the proposed method Ape-X DQfD is better than Ape-X DQN*(with reward clipping, gamma=0.999) with human starts but worse with no-op starts (Table 2). On the whole, their performance I would say, is similar. That makes reader questions about the utility of not use reward clipping, since without reward clipping, we did optimize the true objective, but the final performance is sometimes better and sometimes worse. I am afraid that undo the reward clipping is making the problem unnecessarily harder. \n\t3. The transformed Bellman operator transforms the Q function by a contraction. It's interesting to see what kind of effect of some ad-hoc transformations on the reward will behave. Given the particular function form the author have used, it's especially interesting to see how this transformation: r' = sgn( R) sqrt(abs(r )) will affects the performance. \n\t4. The authors ablates the method on 6 games out of the 42. However, it's mostly qualitative, rather than quantitative. I think it would be more convincing if the leave-one-out experiment could be carried out on all 42 games. \n\t5. The author combines Ape-X DQN with a modified version of DQfD, as mentioned in Section 3.4. For a fair comparison, I think there should be a corresponding modified version of DQfD as a baseline. \n\nI think the author proposed an interesting approach, however, the experiment section, especially the ablation section could be improved. It's hard to tell how much the transformed Bellman operator and the temporal consistency loss contributes on an average case, based on the current results. If the author could provide more information, I'm willing to change my review. "", 'The paper proposed three extensions to DQN to improve the learning performance on Atari games. The extensions are transformed Bellman update, temporal consistency loss and expert demonstration. These three extensions together make the proposed algorithm outperform the state-of-the-art results for Atari games. However, these extensions are relatively straightforward and thus the technical contribution is lean.\n\nThe first extension, transformed Bellman update is similar to reward scaling. While reward scaling is a linear transform, this paper proposed a non-linear transform. It would be great if the paper can compare the transformed Bellman update with reward scaling: For example, normalize the reward based on the best expert performance. In addition, the proposed transformation seems a bit ad-hoc. I feel that many similar transforms will work. For example, would a log transform work? It would be impressive if this transform is learned simultaneously, instead of manually crafted.\n\nThe second extension, TC loss, is a double-edge blade. While it stabilizes the learning process, it can slow down the learning. It is not clear how this paper balances these two? How much weights are placed for the TC loss. I feel that the functionality of the TC loss is somewhat similar to the target network. Will reducing the frequency of updating the target network achieve the similar effect?\n\nThe third extension, bootstrapping from demonstration data, can greatly help the learning process. Although the paper enumerates three differences to DQFD, I am not convinced that these differences lead to significant better results.\n\nOverall, this paper is clearly written. It achieves good results. The contributions are three orthogonal extensions to DQN, which are relatively straightforward. For above reasons, I do not feel strongly about the paper. I am OK if the paper is accepted.\n\n\n']","[60, 80, -20, 20]","[70, 50, 60, 60]","[""The sentiment score is 60 (moderately positive) because the reviewer recommends acceptance and praises the paper's extensive comparisons, impressive results, and thorough experiments. However, they also point out some weaknesses and areas for improvement, tempering the overall positivity. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions for improvement rather than harsh judgments. They use phrases like 'I appreciated' and 'It would be nice to see' which maintain a collegial tone. The reviewer also asks questions in a curious rather than confrontational manner, further contributing to the polite tone."", ""The sentiment score is 80 (positive) because the reviewer states that the paper 'reads well', has 'well motivated modifications', and achieves 'strong results'. These are all positive comments indicating approval of the paper. The score is not 100 as the reviewer admits to not being able to make meaningful comments due to lack of experience in the field. The politeness score is 50 (slightly polite) because the language is professional and respectful, without being overly formal or effusive. The reviewer is honest about their limitations in evaluating the paper, which adds a touch of humility to the tone. However, the review is brief and doesn't include specific praise or encouragement, which prevents it from scoring higher on politeness."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros' section), they express more concerns and criticisms in the 'Cons' section. The reviewer states that the main issue is that the experiments don't fully support the advantage claims of the proposed method, and lists several specific problems with the results and methodology. However, the score isn't deeply negative because the reviewer does recognize the interesting aspects of the approach and expresses willingness to change their review if more information is provided. The politeness score is moderately positive (60) because the reviewer uses professional and respectful language throughout. They present their criticisms constructively, using phrases like 'I think' and 'It would be more convincing if...' rather than making harsh or absolute statements. The reviewer also ends on a polite note, indicating openness to changing their review if more information is provided."", ""The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper's good results and clear writing, they express reservations about the technical contribution being 'lean' and the extensions being 'relatively straightforward'. The reviewer states they are 'OK if the paper is accepted' but 'do not feel strongly about the paper', indicating a mildly positive sentiment overall. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and suggests improvements rather than outright dismissing the work. They use phrases like 'It would be great if...' and 'I feel that...', which maintain a polite tone while expressing their opinions.""]"
"['This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart.\n\nThe experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there\'s little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest.\n\nThe writing is not very clear, especially around equations.\n\nOverall the contribution of this paper is marginally incremental:\n1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified.\n2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf.\n3. Reusing previous level/segment computation with gradient fixed is also not a big innovation.\n\ntypo:\n1. end of page 3, and ""W."" denotes"".\n2. The speed experiment should be put in the main text.', 'This paper proposes a Transformer based RNN structure ""Transformer-XL"" to capture long-range contextual relations and targets on language model task. The idea is straightforward: it splits the input sequence into equal and fixed length segments, and recurrently apply the Transformer over the sequence of segments, in which the hidden states for the previous segment are treated as a memory to attend for the next segment. \n\nThis paper is well-organized and well-written, and easy to follow. The empirical results also demonstrate the proposed model can achieve SoTA performance on several word- and character-based language model benchmarks. \n\n\nPros:\n\n1. The model is designed based on a careful engineering: 1) taking into account the history hidden states for long-term dependency modeling and 2) alignment scores calculated from multiple perspectives for relative position modeling and global significance capturing. In addition, in contrast to the previous Transformer-based language model, benefiting from the recurrent architecture, both training and decoding can be accelerated.\n2. The experimental results show that the proposed Transformer-XL can surpass the baseline model and achieve new state-of-the-art perplexity or bpc on word- or char-based language model task. And, based on the proposed new metric, RECL, the analysis for context length modeling verifies the proposed model can make the best of long-range dependencies.\n\n\nCons:\n\n1. The proposed model is ad-hoc and is only compatible with language model task. Is it possible to extend the proposed model to more general and practical tasks (e.g., seq2seq tasks)?\n2. The absence of a popular language model benchmark, WikiText-2, which has been evaluated in most previous papers.\n3. It is notable that there are no ubiquitous decoding techniques for the language used in both the proposed model and baselines, such as dynamical evaluation and continuous cache pointer. However, these techniques are essential for the RNN-LM baselines to achieve state-of-the-art performance, and has been standardly used in most previous works. Therefore, the comparison seems unfair. \n\nMinor comments: In Figure 1 and 2, it is better to include a legend explaining the meaning of different colors for different nodes.\n', 'This paper proposes a variant of transformer to train language model, it uses two modifications, one is the segment level recurrence with state reuse, the other is relative positional encoding, which significantly enhances the power to model long range dependency. Extensive experiments in terms of perplexity results are reported, specially on WikiText-103 corpus, significant perplexity reduction has been achieved.\n\nPerplexity is not a gold standard for language model, the authors are encouraged to report experimental results on real world applications such as word rate reduction ASR on BLEU score improvement machine translation.  \n\nCiprian Chelba and Frederick Jelinek, Structured language modeling. Computer Speech and Language (2000) 14, 283–332. \n\nPeng Xu, Frederick Jelinek: Random forests and the data sparseness problem in language modeling. Computer Speech & Language 21(1): 105-152 (2007).']","[-30, 70, 60]","[20, 80, 70]","[""The sentiment score is -30 because the review is generally critical, pointing out several limitations and describing the contribution as 'marginally incremental'. However, it does acknowledge some positive aspects like good results on WikiText-103, so it's not extremely negative. The politeness score is 20 because the language is professional and constructive, offering specific feedback and suggestions. The reviewer avoids harsh language, but also doesn't use overtly polite phrases. The tone is matter-of-fact and objective, which is appropriate for a peer review."", ""The sentiment score is 70 (positive) because the reviewer starts with a neutral description of the paper, then explicitly states it is 'well-organized and well-written, and easy to follow.' The reviewer also notes that the empirical results demonstrate state-of-the-art performance. The pros section further reinforces the positive sentiment, highlighting the model's careful engineering and strong experimental results. While there are some cons mentioned, they are presented as constructive criticism rather than major flaws. The politeness score is 80 (polite) because the reviewer uses respectful and professional language throughout. They balance positive feedback with constructive criticism, and phrase their concerns as questions or suggestions rather than harsh criticisms. The use of phrases like 'It is notable that' and 'it is better to' when pointing out potential improvements contributes to the polite tone. The review maintains a formal and courteous style typical of academic peer reviews."", ""The sentiment score is 60 (positive) because the reviewer acknowledges the paper's contributions and significant improvements in perplexity reduction. The reviewer describes the proposed modifications and their benefits, indicating a generally positive view of the work. However, it's not extremely high as the reviewer suggests additional experiments for real-world applications. The politeness score is 70 (polite) because the language used is professional and constructive. The reviewer uses phrases like 'the authors are encouraged to' which is a polite way of suggesting improvements. The tone is respectful throughout, providing specific recommendations and even suggesting relevant literature, which is helpful and courteous.""]"
"['The authors claim contributions in three areas:\n1) Learning representations on physiological signals. The proposed approach  uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms \n LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else?\n\n2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. \n\n3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. \n\nThe experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the “transference” arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare “hospitals”. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. \n\nIt is quite hard to argue from what’s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 “makes sense” as stated in the paper? What is the point that’s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability?\n\nOverall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ', 'Summary of the paper:\nThis paper proposes PHASE, a framework to learn the embeddings for physiological signals from medical records, which can be used in downstream prediction tasks, possibly across domains (i.e. different patient distribution). The authors employ separate LSTMs for each signal channel that are trained to predicts the minimum value of the signal in the fixed future time window (5 minutes in this paper). After training the LSTMs, the learned signal embeddings are fed to gradient boosted trees for a specific prediction task (e.g. predicting whether hypoxemia will occur in 5 minutes). Once the LSTMs are trained, they can be re-used for another dataset; the LSMTs are fixed, and generate embeddings that are fed to a new trainable gradient boosted trees for performing a similar task. The authors also combine existing attribution methods (DeepSHAP and Independent TreeSHAP) to provide some explanation of PHASE. The authors use three different datasets to test PHASE\'s prediction performance, transferability of the embeddings, and interpretation.\n\nPros:\n- The paper is well-motivated, well-organized and clearly written. The reading experience was smooth.\n- Given the importance of physiological signals in ICU settings, transferable embeddings can be an important technique in practice\n- As the authors claim, I am not aware of any notable prior work on transferable physiological signal embeddings. The authors tackle a relatively unexplored territory.\n\nIssues:\n- The authors claim PHASE learns signal embeddings that are transferable. However, the authors train the embeddings to predict the minimum value within the next five time steps, because the downstream tasks are all predicting whether a certain signal goes below some threshold (""hypo""xenia, ""hypo""capnia, ""hypo""tension). This means the authors designed the embedding learning process with a priori knowledge of the downstream tasks, which significantly weakens theirs claim that PHASE learns transferable embeddings. Word embeddings trained on Wikipedia, or ConvNets trained on ImageNet are not designed to be used in a specific type of downstream tasks. What PHASE demonstrates is basically that ""hypo""xxxx predictions can be accurately made with pre-training the embeddings to predict a very relevant task. \n- The authors claim that transferred PHASE embeddings significantly outperform EMA or Raw. But I wouldn\'t call 0.005-0.02 AP improvement ""significant"". Model 12 in Figure 3 shows better performance than model 2 and 4, but the gap is not that large.\n- More importantly, the fact that model 10 and model 12 show similar performance is not very surprising. The two hospitals are in the same city, only miles away. Naturally the distribution of the patients would not be too different. Given this, claiming that PHASE embeddings are transferable does not have a strong ground.\n- The claim for transferable embedding is further weakened by Figure 4. Model 1^p in Figure 4 clearly performs worse than Raw, which means embeddings learned from significantly different setting (hospital P) is actually making it harder for XGB than simply looking at raw signals. If PHASE was learning a robust embeddings, then the learned embeddings should at least not hurt the performance of XGB.\n- Evaluating the interpretation of the model is weak. All the authors did was pick four examples and provide qualitative explanation. And they do not even describe whether this interpretation is from model 9 or 10. It would have been much better if at least one medical expert took a look at more than a few examples. In the current form, we cannot be sure if the model is using the SaO2 signal in a medically meaningful way. Also, if this is the interpretation of model 10 or 12, then we should look at the attributions for other signals as well.\n- Lack of description on experiment setup. The authors do not describe how they pre-trained the LSTMs to obtain Min^h, Auto^h and Hypox^h, which significantly hurts reproducibility. Also I couldn\'t find any description regarding train/test splits or cross validations, or size of the LSTM cells.\n- More description is necessary as to how Raw was used to train XGB. Was the entire sequence of 15 signals fed to XGB?\n- Y-axis of Figure 5 is not on the same scale. This makes it hard to intuitively understand the change of SaO2.', 'The authors present a new method for learning unsupervised embeddings of physiological signals (e.g. time series data) in a healthcare setting. The primary motivation of their paper is transfer learning - the embeddings created by their approach are able to generalize to other hospitals and healthcare settings. \n\nOverall I did like this paper. I found it to be easy to read, well motivated, and addressing an important problem in the healthcare domain. As a researcher in this area, it is very true that we are all using our own ""siloed"" data and do not generally have access to large pre-trained models. I hope that others will produce these kinds of models for the community to use. The authors do not explicitly state that they plan to release their code and pre-trained models, but I sincerely hope that is there intent. If they do not plan to do this, then the impact of this work is dramatically reduced. \n\nHowever, I do have a few concerns about the paper, listed below:\n\n- It might not be fair to truly call this an unsupervised model. The labels used for evaluation are thresholds on the signals themselves (e.g. SaO2 < 92%) , so the ""unsupervised"" model actually receives some form of supervision, at least using the current evaluation method. Using a truly different prediction task not directly based on the physiological signals (e.g. mortality, complication during surgery, etc) would provide a cleaner example of unsupervised embeddings that are useful for transfer learning.\n\n- Differences between PHASE and EMA are statistically significant but unlikely to be clinically meaningful - the largest absolute difference in AP is 0.04, and most are much smaller than this. It\'s unclear if the performance gains enjoyed by PHASE would meaningfully change clinical decision making in any significant way.\n\n- I appreciate the use of XGBoost due to its impressive Kaggle performance, but it strikes me as odd that the authors did not try to fine tune their base model, as that is standard practice for transfer learning. The successes they point to in CV and NLP all use a fine tuning approach, so the evaluation seems incomplete without a performance assessment of fine tuning the base model. \n\n\n']","[-30, -40, 60]","[50, 60, 80]","[""The sentiment score is slightly negative (-30) because while the reviewer finds the problem interesting, they express several concerns and criticisms about the paper's methodology and experimental arguments. They use phrases like 'needs more details', 'needs work', and 'skeptical', indicating areas that require improvement. However, it's not entirely negative as they acknowledge the interesting nature of the problem.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'It would be good to', 'I recommend', and 'Can the authors explain', which are polite ways of suggesting improvements or requesting clarification. The reviewer also acknowledges positive aspects, such as finding a part of the paper 'particularly' interesting. While critical, the language is not harsh or rude, but rather aimed at improving the paper."", ""The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('well-motivated, well-organized and clearly written'), the majority of the review focuses on significant issues and weaknesses in the paper's methodology and claims. The reviewer raises several critical points that undermine the paper's main arguments about transferability and interpretation. However, the score is not extremely negative as the reviewer does recognize some value in the work. The politeness score is 60 because the reviewer uses professional and respectful language throughout, acknowledging positives before discussing issues, and framing criticisms as objective observations rather than personal attacks. The reviewer maintains a constructive tone, even when pointing out significant flaws. The language is more polite than neutral, but not excessively formal or deferential."", ""The sentiment score is 60 (positive) because the reviewer explicitly states 'Overall I did like this paper' and provides several positive comments about the paper being well-motivated, easy to read, and addressing an important problem. However, the score is not higher due to the concerns raised in the latter part of the review. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively (e.g., 'I do have a few concerns'). The reviewer also expresses hope for the authors' future actions, which adds to the polite tone. The language is professional and courteous without being overly formal or deferential.""]"
"['This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. \n\nThe authors suggest that one can predict future frames from a vector comprised of an observation encoding and an action. To train the model, they suggest using a linear combination of three different losses: (1) an adversarial loss that encourages the generated sample to look similarly to training data, (2) an InfoGAN-inspired loss that is supposed to maximise mutual information between the conditioning (e.g. action) and the generated sample, and (3) a content loss, taken to be the mean-squared error of the prediction and ground-truth in the VGG feature space.\n\nThe major contribution of this work seems to be using these three losses in conjunction, while doing conditional frame prediction at the same time. While interesting, there exist very similar approaches that also use adversarial losses [1] as well as approaches using different means to reach the same goal [2, 3]. None of these are mentioned in the text, nor evaluated against. It is true that [1] is not action-conditional, but adding actions as conditioning could be a simple extension.\n\nExperimental section consists of an ablation study, which evaluates importance of different components of the loss, and a qualitative study of model predictions. With no comparison to state of the art (e.g. [1, 3]), it is hard to gauge how valuable this particular approach is. \nThe qualitative evaluation starts with §4.4¶1 “we follow the customary GAN literature to include some qualitative results for illustration”, as if there was no other reason for including samples than to follow the custom. Since the paper is about action-conditional prediction, it would be interesting to see predictions conditioned on the same initial sequence but different actions, which are not present, however. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting.\n\nThe paper is clearly written, but some claims in the text are not supported by any citations (e.g. §1¶2 “More recently, several papers have shown that forward modelling…” without a citation).  Some claims are misleading (e.g. §1¶3 says that by using adversarial training we don’t need to use task-specific losses and it does not put constraints on input modality. While true, using MSE loss is equally general). Some other claims are not supported at all or may not be true (e.g. §3.2¶1 “ResNet … aims at compressing the information in the raw observation” - to the best of my knowledge, there is no evidence for this).\n\nTo conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements. I recommend to reject this paper.\n\n[1] Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., & Levine, S. (2018). Stochastic Adversarial Video Prediction. CoRR, abs/1804.01523.\n[2] Eslami, S.M., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N.C., King, H., Hillier, C., Botvinick, M.M., Wierstra, D., Kavukcuoglu, K., & Hassabis, D. (2018). Neural scene representation and rendering. Science, 360, 1204-1210.\n[3] Denton, E.L., & Fergus, R. (2018). Stochastic Video Generation with a Learned Prior. ICML.\n[4] Buesing, L., Weber, T., Racanière, S., Eslami, S.M., Rezende, D.J., Reichert, D.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., & Wierstra, D. (2018). Learning and Querying Fast Generative Models for Reinforcement Learning. CoRR, abs/1802.03006.', 'This paper proposed to train a forward model used in reinforcement learning (RL) by task-independent losses. The idea is to use the adversarial loss, infoGAN, and perception loss to replace the task-specific losses in RL. \n\nHowever, the experiments did not show any benefits for the RL tasks. While it is possible that the improved prediction in terms of the Euclidean distance could lead to better results for RL, it is better to directly verify it. \n\nMany style transfer methods can be modified to solve the problem considered in the paper. Some works on conditional GAN can also be employed. However, there is no baseline compared in the experiments. \n\nThe notations in Section 3 change from one sub-section to another. It is hard to obtain a coherent understanding about the proposed approach. \n\nOverall, the paper identifies a key component, forward modeling, in RL and aims to improve the solution to that component. However, the proposed approach is a straightforward application of existing techniques to this problem. Both the writing and the experiments could be strengthened, per the suggestions above.', 'Summary: Model-based RL that work on pixel-based environments tend to use forward models trained with pixel-wise loss. Rather than using pixel-wise loss for an action-conditioned video prediction model (""Forward Model""), they use an adversarial loss combined with mutual-information loss (from InfoGAN) and content loss (based on difference in convnet features of VGG network, rather than pixels). They run experiments on video-action sequences collected from an Atari game (Frostbite), and on a Udacity driving dataset.\n\nPros: The introduction and related work section is very well written, and motivation of why one should try adversarial loss for forward models is clear.\n\nWhile I think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what I think the authors need to do to improve the work:\n\n(1) The authors emphasize novelty, and being ""first"" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses. Sure, those works focus on video prediction, while this work focus on building a ""forward model""and is supposed to be for model-based RL, but this work has not performed any model-based RL experiments, so from my point of view, it is a video-prediction model contingent on an action input. Regardless, I believe the approach and results should be compared to existing work on video prediction, and similarities and differences to existing approaches should be highlighted. Adding an action-conditioned element to existing video-prediction techniques is also fairly simple.\n\n(2) From reading the intro/related work section, this work is clearly motivated in the direction of model-based RL, and the authors has already used this model for Frostbite. If this method is useful for model-based RL, I would expect to see experimental results for RL, at least for Frostbite (rather than just the training loss in Table 1). Rather than focusing on saying this method is the first to use triplet loss, or the first to use adversarial loss for forward models, I am much more interested in seeing a forward model that works well for RL tasks, since, that\'s the point right?\n\nAlthough the work is promising, I can only give it a score of 4 at the moment. If the author fixes the writing to include detailed discussion with video prediction literature, with good quantitative and qualitative comparison to existing methods, that is worth 1 extra point. If the author has good results on using this forward model on environments that have previously used older forward models (such as Atari environments in [2] or CarRacing/VizDoom in [3]), and presents those results in a satisfactory way, that may increase my score by another 1-2 points depending on the depth of the experiments. Currently the paper is only < 7 pages, so I believe there is room for more substance.\n\nMinor points:\n- in related work section, should be f_{theta} not f_theta\n\n[1] Denton et al., ""Unsupervised Learning of Disentangled Representations from Video"", (NIPS 2017). https://arxiv.org/abs/1705.10915\n[2] https://arxiv.org/abs/1704.02254\n[3] https://arxiv.org/abs/1803.10122\n']","[-80, -30, -20]","[20, 20, 60]","[""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer recommends rejecting the paper, stating that the approach is not novel, the experimental evaluation is lacking, and the text contains unsupported statements. They criticize the lack of comparison to state-of-the-art methods and point out several weaknesses in the paper's claims and methodology. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'While interesting...' and 'The paper is clearly written, but...' which soften the criticism. However, the overall message is still quite negative, hence the relatively low positive score for politeness."", ""The sentiment score is -30 because the review is generally critical, pointing out several shortcomings of the paper. The reviewer notes that the experiments didn't show benefits for RL tasks, there's a lack of baselines for comparison, and the notation is inconsistent. However, it's not entirely negative as it acknowledges the paper's attempt to address a key component in RL. The politeness score is 20 because the language is professional and constructive, offering specific suggestions for improvement. The reviewer uses phrases like 'it is better to' and 'could be strengthened' rather than harsh criticisms. The overall tone is academic and respectful, even while pointing out flaws."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The introduction and related work section is very well written'), they clearly state that 'this paper is clearly not ready for publication' and provide several major criticisms and suggestions for improvement. The overall tone suggests the paper needs significant work before it can be considered acceptable.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I think this work has potential' and 'I would expect to see' rather than harsh or dismissive language. The suggestions are framed as ways to improve the work rather than outright criticisms. However, the review is not overly effusive or deferential, maintaining a balance between politeness and critical assessment.\n\nThe reviewer provides specific, actionable feedback and even suggests how the authors could improve their score, which is helpful and considerate. The use of phrases like 'I believe' and 'I am much more interested in seeing' personalizes the feedback without being rude.""]"
"['The paper aims at justifying the performance gain that is acquired by the use of ""composite"" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).\n\nI found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.\n\nExample from the Abstract:\n\n""The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from other’s intelligence and diligence, and the other is saving the efforts in data preparation and resources\nand time in training""\n\nThe main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., ""components"") in the input of a network then you have ""more information"", and this cannot be bad. Here are the corresponding claims in the Abstract:\n\n""we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.""\n\n""if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved.""\n\nHowever, this argument seems to be just about expressiveness; adding more features can be statistically problematic. \n\nFurthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.\n\nFinally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.\n\n\nThe motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don\'t see any connection between this and the use of pre-training.\n\nOther examples unclear statements from the intro:\n\n""One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.""\n\n""Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once.""\n\nThere are many typos in the paper including this one about X for the XOR function:\n""Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR""\n\n\n', ""This paper studies composite neural network performance from function composition perspective. In theorems 1, 2 and 3, the authors essentially prove that as the basis functions (pre trained components) increases (satisfying LIC condition), there are more vectors/objects can be represented by the basis. \n\nTo me, this is a very straight forward result. As the basis increases while the LIC condition is satisfied, we can of course represent more objects (the new component is one of them). I don't see any novelties here. The result is straightforward, and this should be a clear rejection.\n\n"", 'The paper considers the problem of building a composite network from several pre-trained networks and whether it is possible to ensure that the final output has better accuracy than any of its components. \n\nThe analysis done in the paper is that of a simple linear mixture of the outputs produced by each component and then by showing that if the output of the components are linearly independent then you can find essentially a better ensemble. This is a natural and straightforward statement with a straightforward proof. It is unclear to me what theoretical value does the analysis of the paper add. Further the linear independence assumption in the paper seems very strong to make the results of value. \n\nFurther the paper seems very hastily written with inconsistent notation throughout making the paper very hard to read. Especially the superscript and the subscript on x have been jumbled up throughout the paper. I recommend rejection and encourage the authors to first clean up notation to make it readable. ']","[-70, -80, -70]","[-20, -20, -20]","[""The sentiment score is -70 because the reviewer expresses significant criticism throughout the review. They describe the paper as 'lacking in terms of writing and in terms of clarity', question the validity of the main arguments, and point out numerous issues with examples and statements. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their criticism is direct and unmitigated. They use phrases like 'lacking', 'invalid', and 'simplistic' without softening the blow. The reviewer also doesn't offer any positive comments or constructive suggestions for improvement, which contributes to the slightly impolite tone. However, they maintain a professional demeanor by focusing on the content rather than making personal attacks, which prevents the score from being even lower."", ""The sentiment score is -80 because the reviewer expresses a strongly negative opinion about the paper, stating that the result is 'very straight forward' and lacks novelty. The reviewer concludes with 'this should be a clear rejection,' indicating a highly negative sentiment towards the paper's publication. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer uses phrases like 'I don't see any novelties here' and 'this is a very straight forward result,' which come across as somewhat impolite in the context of a peer review. The reviewer doesn't offer any constructive feedback or suggestions for improvement, which further contributes to the slightly impolite tone."", ""The sentiment score is -70 because the reviewer expresses significant criticism and recommends rejection. They state that the theoretical value of the paper is unclear, the assumptions are too strong, and the paper is hastily written with inconsistent notation. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite critical and dismissive. They bluntly state the paper's shortcomings without offering much constructive feedback or positive reinforcement. The phrase 'very hastily written' and the recommendation to 'first clean up notation to make it readable' come across as somewhat harsh and impolite.""]"
"['This is a well written paper which proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to existing Bayesian filters, the paper also proposes two different versions of the [differentiable] Unscented Kalman Filter. Performance of the different filters and noise models is evaluated on two real-world robotic problems: Visual Odometry and visual tracking of an object pushed by the robot.\nWhile the general idea of learning the noise variances through backpropagation are straightforward extensions of existing work on differential Bayesian filters, the questions that the paper explores are important to make end-to-end learning of Bayesian filter more common. The results will help future research select the correct differential filter for their use case, and insight in potential benefits (or lack thereof) by learning heteroscedastic or homoscedastic process noise, and/or observation noise.\nA downside is that the paper does not further explore how to weigh different loss terms which are apparently important to successfully train such models. Also unfortunate is the footnote which states that the current results are incomplete and will be updated, hence as a reviewer I am not sure which results and conclusions are valid right now.\n\n\nPros:\n+ clearly written\n+ useful experiments for those seeking to select a differential Bayesian filter, and learning (heteroscedastic) noise from data.\n+ experiments on real-world use cases rather than toy problems\n\nCons:\n- Incomplete experiments according to footnote, thus results and conclusions might change after this review.\n- Unclear what the effect of the selected process / observation model is on the learned noise\n\n\nBelow are more detailed comments and questions:\n* p6. Footnote: ""due to time constraints, ..., results will be updated"" Is this acceptable? I have never seen such a notice when reviewing. So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review?\n* If I understand correctly, the paper \'only\' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. I did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models.\n* I would have liked to see in the paper more explanation on how the process and observations models were selected and validated  in the experiments, since I expect that the validity of these functions affects the learned noise variances. Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data?\n* Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood.\n\n* Section 1:\n\t* ""Our experiments show that ... "" This may be a matter of taste, but I did not expect to see the main conclusions already in the introduction. They should appear in the abstract to help out the quick reader. In the introduction, it appears as if you are talking about some separate preliminary experiments, and which you base some conclusions that will be used in the remainder of this paper.\n\n* Section 3:\n\t* So, mostly empirical study, since heteroscedastic noise models were already used?\n\t* ""Previous work evaluated ... "" please add citations\n\n* Section 4.1:\n\t* ""train a discriminative neural network o with parameters wo to preprocess the raw sensory data D and thus create a more compact representation of the observations z = o(D;wo)."" At this point in the paper, I don\'t understand this. How is z learned, via supervised learning (what is the target value for z)? Or is z some latent representation that is jointly optimized with the filters? This only became somewhat clearer in Sec. 5.2 on p.8 where it states that ""We ... train a neural network to extract the position of the object, the contact point and normal as well as ..."". So if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?).\n\n* Section 4.2:\n\t* ""we predict a separate Qi for every sigma point and then compute Q as the weighted mean"" → So, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points?\n\n* Section 4.3:\n\t* Equation 14: inconsistent use of boldface script: should use bold sigma_t, and bold l_t ?\n\t* ""In practice, we found that during learning ... by only increasing the predicted variance"" →  This is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn\'t we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \\lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason?\n\n* Section 5.1.1\n\t* ""... of varying length (from 270 to over 4500 steps) ..."" it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond.\n\n* Section 5.1.2:\n\t* Table 1: How are the parameters of the filters in the ""no learning"" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the \'no learning\' parameters served as the initial condition for the learning approaches?\n\t* Table 1, first row column Q+R: ""0.2"" → Is there a missing zero here, i.e. ""0.20""? Otherwise, the precision of reported results in this table is not consistent. Hard to say: is the mean of R+Q 0.2, and slightly lower than R+Qh, or could it be as high as 0.24 ?\n\t* ""learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF"". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error).\n\n* Section 6: \n\t* ""Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise."" I don\'t see on what presented results these conclusions were drawn, as this is the first time the word ""outlier"" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.\n', 'This paper presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms.  For observation noise, the approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated (diagonal) covariance.  Similarly for state process noise, a network predicts the (diagonal) covariance of the temporal process given the current state.\n\nThe paper notes that these noise models can be trained end-to-end by instantiating an (approximate) Bayesian filter.  In particular, they explore the use of a Kalman Filteer, Extended Kalman Filter, (Monte Carlo and regular) Unscented Kalman Filter and a Particle Filter.\n\nThe technique is applied to two different tasks, visual odometry on the KITTI dataset and a ""planar pushing"" task.  The results show that the addition of a learned noise model made no significant difference on the KITTI dataset, with the EKF without learning performing as well as any of the other variations.  The planar pushing task has a higher dimensional state space and more challenging noise dynamics.  In that case some gains are seen with learning.\n\nOverall the contribution of this paper seems small and the experimenal results insufficient.  The observation that gradient based training can be done through a Bayesian filter, as the paper pointed out, was developed elsewhere.  Extending that to a more complex noise model seems like a rather small contribution.  Indeed, the observational noise component was not found to have a significant or consistent impact and hence only the process noise is particularly notable.  Further, at least one obvious and important baseline was missing.  Specifically, process noise models could be trained independently by simply maximizing the likelihood of the next predicted state.  It\'s not clear that there\'s a significant benefit to training the model end-to-end in this case.  There may well be, but that is something that should be demonstrated.\n\nA number of other, smaller issues:\n - Eq (4) should be written as a matrix inverse, not a fraction.\n - In the UKF the Julier paper of 1997 also notes a heuristic solution for ensuring positive definiteness of the estimated covarance matrix is lambda is negative.  Was this tried?\n - How was the number of particles selected for the PF at test time?  In particular, how did the computation time between the methods compare?\n', '# Review for ""On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters""\n\nThe method revisits Bayes filters. It evaluates the benefit of training the observation and process noise models, while keeping all other models fixed. Experimentally, a clear performance boost is verified if heteroscedastic noise is used.\n\nFirst, I want to applaud the effort done to do the study. I think it is very beneficial for the community to revisit classic algorithms and evaluate them in a broader and more recent context. I certainly will revisit this article and point colleagues to it. \n\nThe paper is well-written and the experiments seems to be well done. The review of the relevant models is adequate, although space filling, since the methodology  is not at the core of ICLR. I however consider it highly relevant for the future of the field.\n\nHowever, there is a major flaw: the variational state-space model literature is completely ignored. I consider this blank spot unacceptable. Especially, the models proposed have already explored heteroskedastic noise models in contexts where state-space models and posterior approximations were fully trained. It is just that an ablation study was never done.\n\nI am very torn, as I like the paper in general but think that the recognition of the variational SSM literature needs to be added, and not having it in here would foster a separation of two ""micro communities"".\n\nHere is an incomplete list of articles, which can serve as starting points for a more thorough literature review.\n\n- Archer, E., Park, I. M., Buesing, L., Cunningham, J., & Paninski, L.\n(2015). Black box variational inference for state space models. arXiv preprint arXiv:1511.07367.\n- Fraccaro, M., Sønderby, S. K., Paquet, U., & Winther, O. (2016). Sequential neural models with stochastic layers. In Advances in neural information processing systems (pp. 2199-2207).\n- Karl, M., Soelch, M., Bayer, J., & van der Smagt, P. (2016). Deep  variational bayes filters: Unsupervised learning of state space models from rawdata. ICLR 2017.\n']","[60, -50, 50]","[80, 20, 80]","[""The sentiment score is 60 (positive) because the review starts with praise, calling it a 'well written paper' and highlighting its usefulness and real-world applications. The reviewer also lists several pros. However, there are some cons and critical points mentioned, which prevent it from being extremely positive. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames most negative points as suggestions for improvement rather than harsh criticisms. The reviewer also acknowledges that some issues might be matters of personal preference. The use of phrases like 'I would have liked to see' and 'can you clarify' contribute to the polite tone."", ""The sentiment score is -50 because the reviewer expresses a generally negative view of the paper, stating that 'the contribution of this paper seems small and the experimental results insufficient.' They also point out that the main technique was developed elsewhere and that the paper's extension seems like 'a rather small contribution.' However, it's not entirely negative as they do acknowledge some gains in the planar pushing task. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'Overall the contribution of this paper seems small' rather than more harsh language. They also provide specific suggestions for improvement, which is helpful and polite. The language is not overly formal or deferential, but it maintains a respectful tone throughout."", ""The sentiment score is 50 (slightly positive) because the reviewer expresses appreciation for the effort and potential impact of the paper, calling it 'well-written' and 'highly relevant'. However, they also point out a 'major flaw' in ignoring relevant literature, which tempers the positive sentiment. The politeness score is 80 (quite polite) as the reviewer uses respectful language throughout, starting with applauding the authors' efforts and using phrases like 'I want to applaud' and 'I think it is very beneficial'. Even when criticizing, they use diplomatic language like 'I am very torn' and provide constructive feedback with references. The tone remains professional and courteous throughout the review.""]"
"['It has previously been observed that training deep networks using large batch-sizes leads to a larger generalization gap compared to the gap when training with a relatively small batch-size. This paper proposes to add noise sampled from diagonal ""empirical"" Fisher matrix to the large batch gradient as a method for closing the generalization gap. The authors motivate the use of empirical Fisher for sampling noise by arguing that the covariance of gradients from small batch-sizes can be seen as approximately equal to a scaled version of the Fisher matrix. It is then pointed out that using the Fisher matrix directly to sample noise could in principle close the generalization gap but would lead to slow converegence similar to SGD with a small batch-size. The authors then claim that the convergence speed is better when noise is sampled from the diagonal Fisher matrix instead of the full Fisher matrix. This claim is proven in theory for a convex quadratic loss surface and experiments are conducted to empirically verify this claim both in the quadratic setting are for realistic deep networks. Finally an efficient method for sampling noise from the diagonal empirical Fisher matrix is proposed.\n\nComments:\nI think the paper is very well written and the results are presented clearly. In terms of novelty, I found the argument about convergence using diagonal Fisher being faster compared with full Fisher quite interesting, and its application for large batch training to be insightful. \n\nAs a minor comment, for motivating theorem 3.1, it is pointed out by the authors that the diagonal Fisher acts as an approximation of the full Fisher and hence their regularization effects should be similar while convergence should be faster for diagonal Fisher. As a caveat, I think the authors should also point out that the convergence rate would be best when C is set to 0 in the result of the theorem. This implies no noise is used during SGD updates. However, this would imply the regularization effect from the noise will also vanish which would lead to poor generalization. \n\n\nHowever, there is a crucial detail that makes the main argument of the paper weak. In the main experiments in section 4.3, for the proposed large batch training method, the authors mention that they use a small batch-size of 128 for the first 50 epochs similar to Smith et al (2017) and then switch to the large batch-size of 4096, at which point, the learning rate is linearly scaled proportional to the large batch-size with a warmup scheme similar to Goyal et al (2017) and ghost batch normalization is used similar to Hoffer et al (2017). The former two tricks have individually been shown on their own to close the generalization gap for large batch-size training on large datasets like ImageNet. This paper combines these tricks and adds noise sampled from the diagonal Fisher matrix on top when switching to large batch-size after epoch 50 and reports experiments on smaller datasets-- MNIST, Fashion MNIST and the CIFAR datasets. Finally, the accuracy numbers for the proposed method is only marginally better than the baseline where isotropic noise is added to the large batch-size gradient. For these reasons, I do not consider the proposed method a significant improvement over existing techniques for closing the generalization gap for large batch training.\n\nThere is also a statement in the paper that is problematic but can be fixed by re-writing. In the paper, empirical Fisher matrix, as termed by the authors in the paper, refers to the Fisher matrix where the target values in the dataset is used as the output of the model rather than sampling it from the model itself as done for computing the true Fisher matrix. This empirical (diagonal) Fisher matrix is used to sample noise which is added to the large batch gradient in the proposed method. It is mentioned that the covariance of the noise in small batch SGD is exactly same as the empirical Fisher matrix. This claim is premised on the argument that the expected gradient (over dataset) is unconditionally roughly 0, i.e., throughout the training. This is absolutely false. If this was the case, gradient descent (using full dataset) should not be able to find minima and this is far from the truth. Even if we compare the scale of expected gradient to the mini-batch gradient (for small batch-size), the scale of these two gradients at any point during training (using say small batch-size SGD) is of the same order. I am saying the latter statement from my personal experience. The authors can verify this as well.\n\nOverall, while I found the theoretical argument of the paper to be mostly interesting, I was dissapointed by the experimental details as they make the gains from the proposed method questionable when considered in isolation from the existing methods that close the generalization gap for large batch training.', 'Summary: \nThis paper proposes the method which improves the generalization performance of large-batch SGD by adding the diagonal Fisher matrix noise.\nIn the theoretical analysis, it is shown that gradient descent with the diagonal noise is faster than it with the full-matrix noise on positive-quadratic problems.\nMoreover, the effectiveness of the method is verified in several experiments.\n\nComments:\nThe idea of the proposed method is based on the following observations and assumptions:\n\n- Stochastic gradient methods with small-batch can be regarded as a gradient method with Fisher matrix noise.\n- The generalization ability is comparable between diagonal Fisher and full Fisher matrix.\n- Gradient method with diagonal Fisher is faster than that with full Fisher matrix.\nThis conjecture is theoretically validated for the case of quadratic problems.\n\nIn short, the algorithm derivation seems to be reasonable and the derived algorithm is executable.\nMoreover, experiments are well conducted and the results are also good.\n\n\nMinor comment:\n- There is a typo in the next line of Eq. (2):\n\\nabla_{M_L} (\\theta_k)} -> \\nabla_{M_L} L(\\theta_k)}\n\nIn addition, the notation ""l_i"" is not defined at this time.\n', 'In this paper, the authors propose a method to close the generalization gap that arises in training DNNs with large batch. The author reasons about the effectiveness in SGD small batch training by looking at the curvature structure of the noise. Instead of using the naïve empirical fisher matrix, the authors propose to use diagonal fisher noise for large batch SGD training for DNNs. The proposed method is shown empirically to achieve both comparable generalization and the training speedup compared to small batch training. A convergence analysis is provided for the proposed method under convex quadratic setting. \n\nThe idea of exploring the curvature information in the noise in SGD has been studied in (Hoffer et al. 2017). The difference between this approach and the proposed method in the paper is the use of diagonal fisher instead of the empirical fisher. Although there is convergence analysis provided under convex quadratic setting, I feel that the motivation behind using diagonal fisher for faster convergence is not clear to me, although in the experiment part, the comparison of some of the statistics of diagonal fisher appear similar to the small batch SGD. The intuition of using diagonal fisher for faster convergence in generalization performance is still missing from my perspective. \n\nIn the convergence analysis, as there is a difference between the full fisher and diagonal fisher in the Tr(C’AC) term. It would be interesting to see the effect of how this term play on convergence rate, and also how this term scale with batch size. But this is more of a minor issue as we are mostly caring about its generalization performance which is different from optimization error convergence. \n\nIn the experiments section, the authors claim that noise structure is only important for the first 50 epochs. But it would be better if the authors could show experimental results of using the same training method all the way during the experiment. The experiments are conducted on MNIST and CIFAR10 and 100, which I feel is a bit insufficient for a paper that deals with generalization gap in large batch. As in large batch training, we care more about bigger dataset such as ImageNet, and hence I would expect results reported on various models on ImageNet. Another interesting thing to show would be the generalization error over epochs for different methods, which could give a more detailed characterization of the behavior of different methods.\n\nOverall, I feel the motivation and intuition behind the proposed method is not clear enough and experimental studies are not sufficient for understanding the behavior of the proposed method as an empirical paper.\n']","[-50, 80, -50]","[50, 70, 20]","[""The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'very well written', 'results are presented clearly', 'interesting' theoretical arguments), they ultimately express significant disappointment with the experimental details and question the paper's main contributions. The reviewer states that the proposed method is not a 'significant improvement over existing techniques' and that the gains are 'questionable'. This criticism outweighs the initial positive comments, resulting in a moderately negative overall sentiment. The politeness score is 50 because the reviewer uses respectful and professional language throughout, acknowledging positive aspects before presenting criticisms. They use phrases like 'I think', 'As a minor comment', and 'I found' to soften their critiques, and provide detailed explanations for their concerns rather than making blunt dismissals. However, the score is not higher due to the directness of some criticisms, particularly in the final paragraph."", ""The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper. They state that the algorithm derivation seems reasonable, the experiments are well-conducted, and the results are good. The reviewer also notes that the idea is based on sound observations and assumptions. The politeness score is 70 (polite) because the reviewer uses respectful and constructive language throughout. They offer a balanced review, highlighting the strengths of the paper and providing a minor comment for improvement without using harsh or critical language. The use of phrases like 'seems to be reasonable' and 'experiments are well conducted' indicates a polite and professional tone. The reviewer also frames their minor comment as a suggestion rather than a criticism, which contributes to the overall politeness of the review."", ""The sentiment score is -50 because the reviewer expresses several concerns and criticisms about the paper, such as unclear motivation, insufficient experimental studies, and lack of intuition behind the proposed method. However, they do acknowledge some positive aspects, like the convergence analysis provided. The overall tone suggests the paper needs significant improvements. The politeness score is 20 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'I feel' and 'It would be interesting to see' which soften the criticism. However, the review is not overly polite, maintaining a professional and direct tone.""]"
"[""This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution.\n\nStrengths:\n- The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter.\n- The results show the good behavior of the approach.\n\nWeaknesses:\n\nMethod:\n- One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this.\n- In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors.\n- While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \\psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.\n\nExperiments:\n- In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \\Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. \n- Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)).\n- Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult.\n- Many compression methods report results on ImageNet. This would make this paper more convincing.\n- While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression.\n\nRelated work:\n- It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning.\n- The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez & Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates.\n- The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error.\n\nSummary:\nI do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.\n\nAfter Response:\nI appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue.\n"", ""The paper proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. The authors propose two strategies to decide of a compression level, one based on an eigenvalue threshold, the other one based on a heuristic based on the KL divergence between the observed eigenvalue distribution and the uniform one. This is a bit bizarre but does not require searching a parameter. Once one has decided the number of filters to keep, one can either retrain the network from scratch, or iteratively remove the most correlated filters, which, unsurprisingly, work better.\n\nThe authors perform credible experiments on CIFAR-10 and CIFAR-100 that show the results one would expect. They should probably have run ImageNet experiments because many earlier papers on this topic use it as a benchmark and because the ImageNet size often reveals different behaviors.\n\nIn conclusion, this is a very decent paper, but not a very exciting one.\n\n-------------\n\nAfter reading the authors' response and their additional experiments, I still see this work as a very decent paper, but not a very exciting one. This is why I rank this paper somewhat above the acceptance threshold. I could be proven wrong if this approach becomes the method of choice to prune networks, but I would need to see a lot more comparisons to be convinced. \n\n \n"", ""This paper proposes a compression method based on spectral analysis. The basic idea is to analyse correlation between responses of difference layers and select those that are more relevant discarding the others. That, in principle (as mentioned in the paper) differs from other compression methods based on compressing the weights independently of the data being used. Therefore, in theory (nothing shown), different task would provide different outputs while similar works would compress in the same manner. \n\nThen, the paper proposes a greedy algorithm to select those filters to be kept rather than transforming the layer (as it has been usually done in the past [Jaderberg et al]). This is interesting (from a practical point of view) as would lead to direct benefits at inference time. \n\nThis is the second time i review this paper. I appreciate the improvements from the first submission adding some interesting results. \n\nI still miss results in larger systems including imagenet. How all this approach actually scales with the complexity of the network and the task?\n\nThere have been recent approaches incorporating low-rank approximations that would be interesting to couple with this approach. I am surprissed these are not even cited ('Compression aware training' at NIPS 2017 or Coordinating filters at ICCV2017 both with a similar approach (based on weights tho). Pairing with those approaches seems a strong way to improve your results. \n\n\n""]","[-50, 20, 50]","[50, 50, 70]","[""The sentiment score is -50 because while the reviewer acknowledges some strengths of the paper, they ultimately conclude that there are 'too many points to be corrected or clarified and too many missing experiments for this paper to be accepted'. This indicates a generally negative sentiment, though not extremely so. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's strengths and using phrases like 'I appreciate' and 'I would encourage'. However, they also provide direct criticism without excessive softening, keeping the tone professional rather than overly polite. The reviewer maintains a balanced and constructive tone, offering specific suggestions for improvement while clearly stating their concerns."", ""The sentiment score is slightly positive (20) because the reviewer describes the paper as 'very decent' and acknowledges credible experiments, but also mentions it's 'not very exciting' and suggests additional experiments. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, avoiding harsh criticism and acknowledging the paper's merits. The reviewer provides constructive feedback and suggestions for improvement without using negative or rude language."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's interesting approach and appreciates the improvements from the first submission. However, they also express some reservations and suggest areas for improvement. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, acknowledges improvements, and frames suggestions constructively. They use phrases like 'I appreciate' and 'interesting' which contribute to a polite tone. The reviewer provides specific, actionable feedback without harsh criticism, maintaining a professional and courteous demeanor.""]"
"['Thank you for an interesting read.\n\nThe paper proposes an approximate training technique for energy-based models (EBMs). More specifically, the samples used negative phase gradient in EBM training is approximated by samples from another generator. This ""approximate generator"" is a composition of a decoder (which, with a Gaussian prior on latent variable z, is trained to approximate the data distribution) and another EBM in latent space. The authors show connections to WGAN training, thus the name EnGAN. Experiments on natural image generation and anomaly detection show promising improvements, although not very significant.\n\nFrom my understanding of the paper, the main contribution of the paper comes from section 4, which proposes a latent-space MCMC scheme to improve sample quality. I have seen several papers fusing EBMs and GAN training together and to the best of my knowledge section 4 is novel (but with problems, see below). Section 3\'s recipe is quite standard, e.g. as seen in Kim and Bengio (2017), and in principle contrastive divergence also uses the same idea. The idea of estimating of the entropy term for the implicit distribution p_G with adversarial mutual information estimation is something new, although quite straight-forward.\n\nAlthough I do agree that MCMC mixing in x space can be much harder than MCMC mixing in z space, since I don\'t think the proposed latent-space MCMC scheme is exact (apart from finite-time simulation, rejection...), I don\'t see theoretically why the method works.\n\n1. The MCMC method essentially samples z from another EBM, where that EBM(z) has energy function -E_{\\theta}(G(z)), and then generate x = G(z). Note here EBM(z) != p(z). The key issue is, even when p_G(x) = p_{\\theta}(x), there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_{\\theta}(x). You can easily work out a counter example by considering G is an invertible transformation. Therefore I don\'t understand why doing MCMC on this latent-space EBM can help improve sample quality in x space.\n\n2. Continuing point 1, with Algorithm 1 that only fits p_G(x) towards p_{\\theta}(x), I am confident that the negative phase gradient is still quite biased. Why not just use the latent-space MCMC sampler composited with G as the generator, and use these MCMC samples to train both the decoder G and the mutual information estimator?\n\n3. I am not exactly sure why the gradient norm regulariser in (3) make sense here? True that it would be helpful to correct the bias of the negative phase, but why this particular form? We are not doing WGAN here and in general we don\'t usually put a Lipschitz constraint on the energy function. I\'ve seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN, but most of them are just empirical observations...\nAlso the Omega regulariser is computed on which x? On data? Do you know whether the energy is guaranteed to be minimized at data locations? In this is that appropriate to call Omega a regulariser?\n\nThe presentation is overall clear, although I think there are a few typos and confusing equations:\n\n1. There should be a negative sign on the LHS of equation 2.\n2. Equation 3 is inconsistent with the energy update equation in Algorithm 1. The latter one makes more sense.\n3. Where is the ratio between the transition kernels in the acceptance ratio equation? In general for Langevin dynamics the transition kernel is not symmetric.\n', 'It is well known that energy-based model training requires sampling from the current model.\nThis paper aims to develop an energy-based generative model with a generator that produces approximate samples.\nFor this purpose, this paper combines a number of existing techniques, including sampling in latent space, using a GAN-like technique to maximize the entropy of the generator distribution.\nEvaluation experiments are conducted on toy 2D data, unsupervised anomaly detection, image generation.\n\nThe proposed method is interesting, but there are some unclear issues, which hurts the quality of this paper.\n\n1. Correctness\n\nThe justification of adding a gradient norm regularizer in Eq. (3) for turning a GAN discriminator into an energy function is not clear.\n\nSampling in latent space and then converting to data space samples to approximate the sampling from p_theta is operationally possible. There are three distributions - the generator distribution p_G, the distribution p_comp implicitly defined by the latent-space energy obtained by composing the generator and the data-space energy, and the energy-based model p_E.\np_G is trained to approximate p_E, since we minimize KL(p_G||p_E). Does latent space sampling necessarily imply that p_comp leads to be closer to p_E ?\n\n2. Significance\n\nIn my view, the paper is an extension of Kim&Bengio 2016.\nTwo extensions -  providing a new manner to calculate the entropy term, and using sampling in latent space. In this regard, Section 3 is unnecessarily obscure.\n\nThe results of image generation in Table 2 on CIFAR-10 are worse than WGAN-GP, which is now in fact only moderately performed GANs. In a concurrent ICLR submission - ""Learning Neural Random Fields with Inclusive Auxiliary Generators"", energy-based models trained with their method are shown to significantly outperform WGAN-GP.\n', '\nIn this paper, the authors extend the framework proposed by Kim&Bengio 2016 and Dai et.al. 2017, which introduce an extra step to fit a generator to approximate the current model for estimating the deep energy model. Specifically, the generator is fitted by reverse KL divergence. To bypass the difficulty in handling the entropy term, the authors exploit the Deep INFOMAX formulation, which introduces one more discriminator. Finally, to obtain better samples, the authors inject the Metropolis-adjusted Langevin algorithm within the learned generator to generate samples in latent space. They demonstrate the better performances of the proposed algorithms in both synthetic and real-world datasets, and apply the learned model for anomaly detection task.\n\nThe paper is well-written and does a quite good job in combining several existing algorithms to obtain the ultimate algorithm. The algorithm achieves quite good empirical performances. However, the major problem of this paper is the novelty. The algorithm is basically an extension of the Kim&Bengio 2016 and Dai et.al. 2017, with other existing learning technique. Maybe the only novel part is combining the MCMC with the learned generator for generating samples. However, the benefits of such combination is not well justified empirically. Based the figure 4, it seems the MCMC does not provide better samples, comparing to directly generate samples from G_z. It will be better if the authors can justify the motivation of using MCMC step. \n\nSecondly, it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability. However, it will be better if the effect of the regularization for the energy model estimation can be discussed. \n\nMinor:\nThe loss function for potential in Eq(3) is incorrect and inconsistent with the Algorithm 1. I think the formulation in the Algorithm box is correct.  \n\nIn sum, I personally like the paper as a nice combination of recently developed techniques to improve the algorithm for solving the remaining problem in statistics. The paper can be better if the above mentioned issues can be addressed\n\n\n']","[-20, -20, 20]","[60, 50, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting read', 'promising improvements'), they raise several significant concerns and criticisms about the theoretical foundations and methodology. The overall tone suggests skepticism about the paper's contributions. The politeness score is moderately positive (60) as the reviewer uses polite language throughout, starting with a thank you, and phrases criticisms as questions or suggestions rather than harsh statements. They also acknowledge positive aspects before diving into critiques. The reviewer maintains a professional and respectful tone even when pointing out issues."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the proposed method as 'interesting', they also point out 'unclear issues' that 'hurt the quality of this paper'. They raise concerns about correctness and significance, and note that the results are worse than some existing methods. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They avoid harsh criticism and use phrases like 'in my view' to soften their critique. The reviewer also provides specific, constructive feedback, which is a polite way to offer criticism in academic contexts."", ""The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper is well-written and achieves good empirical performance, they express concerns about novelty and request further justification for some aspects. The overall tone is constructive but with reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'well-written', 'quite good job', and 'I personally like the paper' contribute to the polite tone, while still maintaining professional critique.""]"
"['Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers.  While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was.  For example, how the optimization was done is not discussed at all.  At the same time, I\'m not an expert in group theory, so it\'s possible that the paper has technical novelty or significance which I did not appreciate.  \n\nStrengths: \n\n -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.  \n\nWeaknesses: \n  \n-I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3).  \n\n-The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references.  \n\n  -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I\'m missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper).  In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset.  \n\n  -It\'s interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better.  For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it\'s important to confirm).  \n\n\nQuestions: \n\n-Section 3.1 makes reference to ""learning parameters"".  I assume that this is done in the usual way with backpropagation and then SGD/Adam or something?  \n\n-How is it guaranteed that W is orthogonal in the learning procedure?  \n', 'Recently there has been a spate of work on generalized CNNs that are equivariant to various symmetry groups, such a 2D and 3D rotations, the corresponding Euclidean groups (comprising not just rotations but also translations) and so on. The approach taken in most of the recent papers is to explicitly build in these equivariances by using the appropriate generalization of convolution. In the case of nontrivial groups this effectively means working in Fourier space, i.e., transforming to a basis that is adapted to the group action. This requires some considerations from represntation theory. \n\nEarlier, however, there was some less recognized work by Cohen and Welling on actually learning the correct basis itself from data. The present paper takes this second approach, and shows for a simple task like rotated MNIST, the basis can be learned from a remarkably small amount of data, and actually performs even better than some of the fixed basis methods. There is one major caveat: the nonlinearity itself has to be rotation-covariant, and for this purpose they use the recently introduced tensor product nonlinearities. \n\nThe paper is a little rough around the edges. In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose: SO(2) is a really simple commutative group, so the way that ""tensor product"" representations reduce to irreducibles could be summed up in the formula  $e^{-2\\pi i k_1 x}e^{-2\\pi i k_2 x}=e^{-2\\pi i (k_1+k_2) x}$. I am not sure why the authors choose to use real representations (maybe because complex numbers are not supported in PyTorch, but this could easily be hacked) and I find that the real representations make things unnecessarily complicated. I suspect that at the end of the day the algorithm does something very simple (please clarify if working with \nreal representations is somehow crucial). \n\nBut this is exactly the beauty of the approach. The whole algorithm is very rough, there are only two layers (!), no effort to carefully implement nice exact group convolutions, and still the network is as good as the competition. Another significant point is that this network is only equivariant to rotations and not translations. \n\nNaturally, the question arises why one would want to learn the group adapted basis, when one could just compute it explicitly. There are two interesting lessons here that the authors could emphasize more:\n\n1. Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted (Cohen-Welling) basis. This is interesting because Fourier space (""tensor"") nonlinearities are a relatively new idea in the literature. This finding suggests that the nonlinearity might actually be more important than the basis.\n\n2. The images that the authors work on are not functions on R^2, but just on a 28x28 grid. Rotating a rasterized image with eg. scikit-rotate introduces various artifacts. Similarly, going back and forth between a rasterized and polar coordinate based representation (which is effectively what would be required for ""Harmonic Networks"" and other Fourier methods) introduces messy interpolation issues. Not to mention downsampling, which is actually addressed in the paper. If a network can figure out how to best handle these issues from data, that makes things easier.\n\nThe experiments are admittedly very small scale, although some of the other publications in this field also only have small experiments. At the very least it would be nice to have standard deviations on the results and some measure of statistical significance. It would be even nicer to have some visualization of the learned bases/filters, and a bare bones matrix-level very simple description of the algorith. Again, what is impressive here is that such a small network can learn to do this task reasonably well.\n\nSuggestions: \n\n1. Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities.\n\n2. Clean up the formatting. ""This leads us to the following"" in a line by itself looks strange. Similarly ""Classification ising the learned CW-basis"". I think something went wrong with \\itemize in Section 3.1. \n\n', 'This paper proposes autoencoder architectures based on Cohen-Welling bases for learning rotation-equivariant image representations. The models are evaluated by reconstruction error and classification in the space of the resulting basis on rotated-MNIST, showing performance improvements with small numbers of parameters and samples.\n\nI found most of this submission difficult to read and digest. I did not understand much of the exposition. I’ll freely admit I haven’t followed this line of work closely, and have little background in group theory, but I doubt I’m much of an outlier among the ICLR audience in that regard. The “Preliminaries” section is very dense and provides little hand-holding for the reader in the form of context, intuition, or motivation for each definition and remark it enumerates. I can\'t tell how much of the section is connected to the proposed models. (For comparison, I skimmed the prior work that this submission primarily builds upon (Cohen & Welling, 2014) and found it relatively unintimidating. It gently introduces each concept in terms that most readers familiar with common machine learning conventions would be comfortable with. It\'s possible to follow the overall argument and get the ""gist"" of the paper without understanding every detail.)\n\nAll that being said, I don’t doubt this paper makes some interesting and important contributions -- I just don’t understand what they are.\n\nHere are some specific comments and questions, mostly on the proposed approaches and experiments:\n\n* What actually is the “tensor (product) nonlinearity”? Given that this is in the title and is repeatedly emphasized in the text, I expected that it would be presented much more prominently. But after reading the entire paper I’m still not 100% sure what “tensor nonlinearity” refers to.\n\n* Experiments: all models are described in long-form prose. It’s very difficult to read and follow. This could be made much clearer with an algorithm box or similar.\n\n* The motivation for the “Coupled Autoencoder” model isn’t clear. What, intuitively, is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa? The empirical gains are marginal.\n\n* Experiments: the structure of the section is hard to follow. (1) and (2) are descriptions of two different models to do the same thing (autoencoding); then (3) (bootstrapping) is another step done on top of (1), and finally (4) is a classifier, trained on top of (1) or (2). This could benefit from restructuring.\n\n* There are long lists of integer multiplicities a_i and b_i: these seem to come out of nowhere, with no explanation of how or why they were chosen -- just that they result in “learn[ing] a really sharp W_28”. Why not learn them?\n\n* How are the models optimized? (Which optimizer, hyperparameters, etc.?)\n\n* The baseline methods should also be run on the smaller numbers of examples (500 or 12K) that the proposed approach is run on.\n\n* A planar CNN baseline should be considered for the autoencoder experiments.\n\n* Validating on MNIST alone (rotated, spherical, or otherwise) isn’t good enough in 2018. The conclusions section mentions testing the models with deeper nets on CIFAR, but the results are not reported -- only hinting that it doesn’t work well. This doesn’t inspire much confidence.\n\n* Why are Spherical CNNs (Cohen et al., 2018) a good baseline for this dataset? The MNIST-rot data is not spherical.\n\n* Table 1: The method labels (Ours, 28/14 Tensor, and 28/14 Scale) are not very clear (though they are described in the text)\n\n* Table 1: Why not include the classification results for the standard AE? (They are in the Fig. 6 plot, but not the table.)\n\n* Conclusions: “We believe our classifiers built from bases learnt in a CAE architecture should be robust to noise” -- Why? No reasons are given for this belief.\n\n* There are many typos and grammatical errors and odd/inconsistent formatting (e.g., underlined subsection headers) throughout the paper that should be revised.']","[-50, 50, -60]","[20, 70, 20]","[""The sentiment score is -50 because the reviewer expresses several criticisms and weaknesses of the paper, such as finding it 'fairly weak experimentally' and 'difficult to determine what the exact algorithm was'. However, they do acknowledge some strengths and the possibility that they might have missed some technical novelty due to lack of expertise in group theory, which prevents the score from being more negative. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'I found' and 'I had a difficult time understanding' which soften the criticism. They also acknowledge their own potential limitations in understanding the paper. The language is not overtly polite, but it maintains a respectful tone throughout, hence a slightly positive score."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths and innovative approach, despite noting some areas for improvement. They describe the work as 'impressive' and highlight its ability to perform well with a simple approach. However, they also point out areas where the paper could be improved or clarified.\n\nThe politeness score is 70 (fairly polite) because the reviewer maintains a professional and respectful tone throughout. They offer constructive criticism and suggestions for improvement without using harsh language. Phrases like 'please clarify' and 'it would be nice to have' demonstrate a polite approach to feedback. The reviewer also balances critique with praise, acknowledging the paper's strengths alongside its weaknesses."", ""The sentiment score is -60 because the reviewer expresses significant difficulty understanding the paper, criticizes its clarity and readability, and raises many questions and concerns about the methodology and results. However, they do acknowledge that the paper likely makes 'interesting and important contributions', preventing an even lower score. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language like 'I found...', 'I'll freely admit...', and phrase many points as questions rather than accusations. They also acknowledge their own potential lack of background knowledge. The reviewer maintains a professional tone throughout, even while expressing frustration with the paper's clarity.""]"
"['This paper trains an information retrieval (IR) model by contrasting the joint query-document distributions, p(q, d) with negative samples drawn from a resampling of the product of marginals, p(q) x p(d). They use a second discriminator to provide the re-weighting (I believe picking to top negative sample from the other model) and train this other model in a way that mirrors the first. They also attempt to point out some theoretical problems with a competing model, IRGAN, which uses a generator that is trying to model the joint.\n\nWhile I like the proposal idea, I think the paper has too many problems to warrant publication. First, the story is very disappointing. The authors phrase most of the paper as a critique of IRGAN, but this critique falls short. Really this is more of a paper about where to get negative samples when training a model of the joint (or the log-ratio in this case). Using negative samples from real data with noise contrastive estimation [1] is found in numerous works in NLP [2][3], and has gained some recent attention in the context of representation learning [4][5]. The first algorithm proposed is essentially doing a sort of ranking loss on negative samples, which mirrors similar works [6]. In fact, the generator in IRGAN could be viewed as just a parametric / adaptive negative sampling distribution in the context of NCE for the ultimate purpose of learning an estimate of the log-ratio. The most interesting thing I think of this work here is the co-training, i.e., using another model to help re-sample, and I think this idea should be explored in more detail.\n\nSecond, the paper spends far too much time revisiting prior work than addressing their own model, doing more analysis, providing more insight.\n\nThird, the paper is just poorly written. The notation is confusing, some of the equations are unclear (I have no idea how ""r"" is used in any of this), and the arguments of the baseline in IRGAN don\'t really doesn\'t make any sense.\n\nNotes:\nP1\nI don\'t really follow why IRGAN is so central to this work. Good ideas aren\'t difficult to motivate, especially if empirically everything works out.\nP2\nI\'m having trouble with claims, especially more recently, about GAN instability, particularly since numerous approaches [7][8] seem to have more or less solved the problem.\n\nThe use of ""|"" in G is awfully confusing.\nP3\nAlmost 2 pages of unnecessary background\n\nP4\nWhy are we using ""|"" in functions? What\'s wrong with "",""?\ntheta = \\theta\nI don\'t understand the point of the quote (in italics).\nWhat happened to ""r"" in all of this?\nThe last two equations and their relationship could be more clear.\n\nYou use italics, so is this supposed to be a quote? But then you have a section which attempts to show this.\nP5\nI have no idea what\'s supposed to be going on in 5). The samples from the real joint don\'t factor in the generator gradient, or at least it\'s absolutely not clear that this pops out of the baseline? Then you switch from log (1 - x) to - log x and there\'s some claim about this violating the adversarial objective?\n\nIt took me more than a few reads to figure out what the equation at the bottom of P5 is doing: is this resampling? It\'s fairly unclear.\n\n[1] Gutmann, Michael U., and Aapo Hyvärinen. ""Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.""\n[2] Mnih, Andriy, and Koray Kavukcuoglu. ""Learning word embeddings efficiently with noise-contrastive estimation."" \n[3] Mikolov, Tomas, et al. ""Distributed representations of words and phrases and their compositionality.""\n[4] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" \n[5] Hjelm, R. Devon, et al. ""Learning deep representations by mutual information estimation and maximization.""\n[6] Faghri, Fartash, et al. ""VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.""\n[7] Miyato, Takeru, et al. ""Spectral normalization for generative adversarial networks.""\n[8] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. ""Which Training Methods for GANs do actually Converge?."" ', 'This paper is closely related to the SIGIR 2017 paper “IRGAN: A minimax game for unifying generative and discriminative information retrieval models”. The SIGIR paper proposed a Generative Adversarial Nets (GAN) model for Information Retrieval (IRGAN). And in this paper, the authors dissect the IRGAN model and figure out that the discriminator and generator of IRGAN are optimizing directly opposite loss functions. They also provide experimental studies and show that the superiority of IRGAN in the experiments are mainly because of the discriminator maximizing the likelihood of the real data and not because of the generator.  \nStrong Points:\nConsidering that the IRGAN becomes popular after it being published, I should say the analyzing of this paper is important for researchers in the IR domain.\n\nConcerns or Suggestions:\n1.\tExcept the analyzing on IRGAN, the contribution of this paper is limited. Most of the parts of this paper introduce GAN and IRGAN. Only Section 5 focuses on the analyzing. The methods claimed new proposed, Single Discriminator and Co-training, are good for supporting the analyzing but they are not quite novel.\n2.\tIt is strange to introduce the two models, Single Discriminator and Co-training in the experimental setting section. I would suggest to separate them out and introduce them earlier.\n3.\tThe topic of this paper is more related to the IR domain. It will be better to publish it in SIGIR, together with the IRGAN paper.\n4.\tBesides, if it is possible, I would suggest researchers who have direct experiences on the implement and study on IRGAN give more comments on this paper. ', 'This paper tries to argue that the formulation of IRGAN (a method from 2017 that aimed to use GANs for the standard IR task of estimation query-document relevance) is now well-founded and has inherent weaknesses. Specifically the paper claims that (unlike regular GANs and what was likely intended by the authors or IRGAN) the discriminator and generator are working against each other. The paper then aims to show a couple of more well-founded different (generator-free) setups that perform about as well (if not better) as the original IRGAN work.\n\nOverall I found the work to be quite interesting and the findings to be illuminating. That said I think the paper notably lacked rigor and depth which definitely hurt the quality of the paper.\n\nBelow are my thoughts on the different facets as well as more detailed strengths / weaknesses breakdown:\n\nQuality: Above average\nAs mentioned I think some of the findings are illuminating and thus overage the paper scores well on this aspect.\n\nClarity: Slightly above average\nWhile the paper is largely easy to follow, there are certain key sections that are not well explained / have fundamental errors.\n\nOriginality: Strong\n\nSignificance: Little below average\nMy (main) concern here with this work is that the missing rigor and depth of the work is what is needed for readers to have a deeper understanding of the fundamental issue so as to be able to rectify it in future works.\n\n---\n\nStrengths / Things I liked about the work:\n\n+ The topic / theme of the work: I believe as a community we should encourage more such works that take a critical deep dive into recently proposed methods that may have some inherent weaknesses. As the authors noted the IRGAN work has become quite popular despite some of these (previously unknown) issues.\n\n+ The experimental results in general do a fair job illustrating the likely issue (though I would have liked to see more rigor and depth here as well as detailed below)\n\nWeaknesses / Things that concerned me:\n\n- (W1) Lacking rigor / depth: One of my big concerns with this work is that the analysis to demonstrate the inherent flaws of IRGAN if fairly shallow and not detailed enough. For example, Section 5 (which should have been the key section of the work) is quite poorly written and not rigorous enough. Claiming that log(1-z) can be replaced with - log(z) is incorrect -- how can this substitution be made as is?\n\nOverall my sense after reading the work is that I understand that the IRGAN formulation is not completely well-formed in terms of discriminator/generator synergy (the pairwise formulation has the additional issue of separating real pairs rather than higher rank-lower rank pairs). However I do not buy that the generator and discriminator directly oppose each other as is claimed in the work (I believe this arises only due to the incorrect claim that log(1-z) can be replaced with -log(z)).\n\nThus at the end of the day I feel the reader is willing to buy there is a issue with the formulation, but they do not fully understand it not do they understand enough to understand how to rectify the underlying issues. To me that was unfortunate as the paper would have been an excellent work if it had done so.\n\n- (W2) Missing some experimental results / deeper insights : There were some notable empirical results that were missing or not provided, that raised some concerns in my mind. For instance I don\'t see the co-training approach listed for the MovieLens dataset .. Why so? The authors make a secondary claim that they are able to improve upon IRGANs via their proposed approach but then they do not substantiate these on all the datasets which seems like a notable oversight.\n\n - (W3) Missing details: To add to the above I think the authors can clearly be more detailed in describing for instance the models for D, G, p_\\psi etc .. Right now I can speculate what they are but I don\'t think a reader should be expected to speculate in such cases. Likewise empirical details about the datasets and their sizes could easily have been added.\n\nAlso the paper presents the IRGAN pairwise approach and mentions pairs in a couple of places but I don\'t see an approach that can learn from pairs among the ones proposed.\n\nAnother example is the two proposed models Fig 2a (Only discriminator) vs Fig 2b (cotraining). I don\'t see an explanation or intuition for why 2b is expected to be better than 2a. Given the claims of the work I would have wanted to understand this better.\n\n- (W4) Significance testing: This is an important experimental process to understand the validity of some of the claims. While I understand it is not the main claim of the paper, understand the significance of these differences helps put things in perspective. I would strongly urge the authors to add this for all of their experimental results not just the ones the proposed models are outperformed by IRGANs.\n\n- Lastly I would urge the authors to be rigid and clear in their notations. For example in the equation in section 4.5, ""o\'"" occurs out of nowhere.']","[-70, -20, -20]","[-20, 60, 60]","[""The sentiment score is -70 because the reviewer expresses significant criticism and states that the paper has 'too many problems to warrant publication'. They highlight issues with the paper's focus, writing quality, and clarity. The politeness score is -20 because while the reviewer does use some polite language ('I like the proposal idea'), the overall tone is quite critical and direct. The reviewer uses phrases like 'poorly written', 'disappointing', and 'I have no idea what's supposed to be going on', which come across as somewhat harsh. However, they do provide specific feedback and suggestions, which prevents the score from being even lower."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of analyzing IRGAN, they express several concerns about the paper's limited contribution, strange structure, and suggest it might be better suited for a different publication. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as 'concerns or suggestions' rather than outright flaws. They also use phrases like 'I would suggest' which maintains a polite tone while offering feedback."", ""The sentiment score is slightly negative (-20) because while the reviewer found the work 'interesting' and 'illuminating', they also highlighted significant weaknesses and concerns about the paper's rigor, depth, and missing details. The overall tone suggests the reviewer sees potential in the work but is disappointed by its execution. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, balances criticism with praise, and offers constructive feedback. They use phrases like 'I found the work to be quite interesting' and 'I would strongly urge the authors to add this', which maintain a polite and professional tone even when expressing concerns.""]"
"[""This paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node\nis NP-complete. This is an extension of the result of Blum and Rivest'88. The original theorem was proved for \nthreshold activation units and the current paper proves the same result for ReLU activations. The authors do this\nby reducing the 2-affine separability problem to that of fitting a neural network to data. The reduction is well \nwritten and is clever. This is a reasonable contribution although it does not add significantly to the current state of the art. \n  "", ""The main result of this work is to prove that a two-layer neural network with 2 hidden neurons in the first layer and 1 hidden neuron in the second layer is NP-hard to train when all activation functions are ReLU. Similar results with the hard-thresholding activation function were proved by Blum & Rivest in 1988. The main proof consists of a reduction from the 2-hyperplane separability problem which was known to be equivalent to NNs with hard-thresholding activation functions.\n\nQuality: Moderate. Good solid formal results but nothing really surprising. \n\nClarity: The manuscript is mostly well-written, and the authors gave proper credit to prior works. The only minor issue is perhaps the abuse of the variable w in introduction and the same variable w (with an entirely different meaning) in the rest of the paper. It would make more sense to change the w's in introduction to d's.\n\nOriginality: This work is largely inspired by Blum & Rivest's work, and builds heavily on some previous work including Megiddo and Edelsbrunner et al. While there is certainly some novelty in extending prior work to the ReLU activation function, it is perhaps fair to say the originality is moderate.\n\nSignificance: While the technical construction seems plausible and correct, the real impact of the obtained results is perhaps rather limited. This is one of those papers that it is certainly nice to have all details worked out but none of the obtained results is really surprising or unexpected. While I do agree there is value in formally documenting the authors' results, this conference is perhaps not the right venue. \n\nOther comments: The discussion on page 2 (related literature) seems odd. Wouldn't the results of Livni et al and Dasgupta et al already imply the NP-hardness of fully connected ReLU networks, in a way similar to how one obtains Corollary 3.2? If this is correct, then the contribution of this work is basically a refined complexity analysis where the ReLU network is shrunken to 2 layers with 3 nuerons?\n\nI really wish the authors had tried to make their result more general, which in my opinion would make this paper more interesting and novel: can you extend the proof to a family of activation functions? It is certainly daunting to write separate papers to prove such a complexity result for every activation function... The authors also conveniently made the realizability assumption. What about the more interesting non-realizable case?\n\nThe construction to prove Theorem 3.4 bears some similarity to a related result in Zhang et al. The hard sorting part appears to be different."", ""This paper claims results showing ReLU networks (or a particular architecture for that) are NP-hard to learn. The authors claim that results that essentially show this (such as those by Livni et al.) are unsatisfactory as they only show this for ReLU networks that are fully connected. However, the authors fail to criticize their own paper for only showing this result for a network with 3 gates. For the same reason that the Livni et al. results don't imply anything for fully connected networks, these results don't imply anything for larger networks. Conceivably certain gadgets could be created to ensure that the larger networks are essentially forced to ignore the rest of the gates. This line of research isn't terribly interesting and furthermore the paper is not particularly well written. \n\nFor learning ReLUs, it is already known (assuming conjectures based on hardness of improper PAC learning) that functions that can be represented as a single hidden layer ReLU network cannot be learned even using a much larger network in polynomial time (see for instance the Livni et al. paper, etc.). Proving NP-hardness results for proper isn't as useful as they usually are very restricted in terms of architectures the learning algorithm is allowed to use. However, if they do want to show such results, I think the NP-hardness of learning 2-term DNF formulas will be a much easier starting point. \n\nAlso, I think there is a flaw in the proof of Lemma 4.1. The function f *cannot* be represented by the networks the authors claim to use. In particular the 1/\\eta outside the max(0, x) term is not acceptable.""]","[50, -20, -80]","[60, 50, -30]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper as a 'reasonable contribution' and praises the reduction as 'well written and clever'. However, they also note that it 'does not add significantly to the current state of the art', tempering the positive sentiment. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits without harsh criticism. They provide a balanced view, highlighting both strengths and limitations in a professional manner. The tone is constructive and objective, avoiding any rude or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the work as 'solid' and 'correct', they express doubts about its originality, significance, and suitability for the conference. Phrases like 'nothing really surprising', 'originality is moderate', and 'real impact... is perhaps rather limited' indicate a lukewarm reception. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and offering constructive criticism. They use phrases like 'Good solid formal results', 'The manuscript is mostly well-written', and 'I really wish the authors had tried to make their result more general' which maintain a polite and professional tone even while critiquing."", ""The sentiment score is -80 because the reviewer is highly critical of the paper, pointing out multiple flaws and stating that 'This line of research isn't terribly interesting and furthermore the paper is not particularly well written.' The reviewer also dismisses the authors' claims and suggests their work is not significant. The politeness score is -30 because while the reviewer doesn't use explicitly rude language, the tone is quite dismissive and blunt. Phrases like 'the authors fail to criticize their own paper' and 'this line of research isn't terribly interesting' are particularly impolite in academic discourse. The reviewer also doesn't offer any positive feedback or constructive suggestions for improvement, which is generally expected in peer reviews.""]"
"[""Summary: \nThis paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.\n\n\nComments:\n\nThere is a previously known result quite related to this paper: \n\nIshibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. \n\nTheorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. \n\nCombined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. \n\nFor this reason, I am afraid that the main technical result is quite weak.\n\nAfter Rebuttal:\nI read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3.\n"", 'This paper studies the cross-entropy loss for binary classification problems. The authors show that if the norms of samples in two linear separable classes are different, gradient descent based methods minimizing cross-entropy loss may give a linear classifier that gives small margin.\n\nPros\n\n1. The paper is clearly written and very easy to follow. \n\n2. The authors show that for two point classification problems, if the norms of the points are very different then gradient descent will give a very small margin.\n\n3. Further theoretical results are given explaining the relation between cross-entropy loss and SVM.\n\n4. A new loss function called differential training is proposed, which is guaranteed to give SVM solution.\n\nCons\n\n1. My biggest concern is that, the paper, especially the title, may be slightly misleading in my opinion. Although the authors keep claiming that cross-entropy loss can lead to poor margins in certain circumstances (which I agree), in fact Theorem 1 and Theorem 2 have already clearly shown the connection between the cross-entropy solution and the maximum margin direction. For example, Theorem 1 literally proves that when the two points have the same norm (normalized data?), cross-entropy loss leads to maximum margin. Theorem 2 also clearly states that cross-entropy loss and SVM are closely related. Based on these two theorems, perhaps ‘cross-entropy loss is closely related to maximum margin’ is a more convincing statement.\n\n2. The theoretical results given in this paper is slightly incremental. As the authors mentioned, Theorem 1 and Theorem 2 are essentially already proved in previous works. The other results are not very significant either.\n\n3. The authors do not clearly state the advantages of the differential training method compared to SVM. It seems that one can just use SVM if the goal is maximum margin classifier.\n', 'The paper challenges recent claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. Along the road, it presents a couple of nice results that I find quite interesting and I believe they provide useful insights. Finally it presents a simple modification to the cross-entropy loss, which the authors refer to as differential training, that alleviates the problem for the case of linear model and linearly separable data.\n\nCONS:\nI find the paper useful and interesting mainly because of its insightful results rather than the final algorithm. The algorithm is evaluated in a very limited setting (linear model, synthetic data, binary classification); it is not clear if similar benefits would carry over to nonlinear models such as deep networks. In fact, I strongly encourage the authors to do a generalization comparison by comparing the **test accuracy** obtained by their modified cross-entropy against: 1. Vanilla cross-entropy as well as 2. A deep model large margin loss function (e.g. as in ""Large Margin Deep Networks for Classification"" by Elsayed). Of course on a realistic architecture and non-synthetic datasets (e.g. CIFAR-10).\n\nPROS:\nPutting the algorithm aside, I find the theorems interesting. In particular, Theorem 3 shows that some earlier claims about cross-entropy\'s ability to attain large margin (in the linearly separable case) is misleading (due to neglecting a bias term). This is important as it changes the faith of the community in cross-entropy and more importantly creates hope for constructing new loss functions with improved margin.\nI also find the connection between the dimension of the subspace that contains the points and quality of margin obtained by cross-entropy insightful.\n', 'Due to the large variance in reviewer scores, I was asked to give this additional review.\n\nBackground: [Soudry et al. 2018] showed that the iterates of gradient descent, when optimizing logistic regression on separable data, converge to the L2 max-margin (SVM) solution for homogeneous linear separators (without bias). These results were later extended to other models and optimization methods.\n\nThis paper has two main results:\n1)\tIt clarifies that the results of [Soudry et al. 2018] do not apply to logistic regression when the linear separator has a bias term (“b”). This is because the homogenous max margin solution in the extended [x,1] space is not the same as non-homogeneous max margin solution in the original space: the first has a penalty on the size of the bias term, i.e.\nmin_{w,b} ||w||^2 + b^2 s.t. y_n (w’x_n+b) >= 1\n, while the latter does not: \nmin_{w,b} ||w||^2  s.t. y_n(w’x_n+b) >= 1\n2)\tIt suggests using differential training to correct this issue.\n\n\nHowever, I do not believe these contributions are enough for a publication in ICLR. First, (2) is simply a combination of two known results, as mentioned by Reviewer 2. Second, though I commend the authors for pointing out (1), I do not feel this by itself warrants a publication, for the following reasons:\na) It is very simple to explain (1) in only a few lines (as I did above). Therefore, it would be more informative just to write (1) as a comment on the original paper (the ICLR 2018 forum is still open), not as a completely new publication. For me, all the numerical demonstrations and examples of this simple issue did not add much.\nb)\tRegularizing the bias term usually does not make a significant difference to the sample complexity (see the end of section 15.1.1 in the textbook “Understanding Machine Learning: From Theory to Algorithms” by Shai Shalev Shwartz.). Furthermore, the main motivation behind [Soudry et al. 2018] was to explain implicit bias and generalization in deep networks, where there such max-margin results (which penalize all the parameters) could be used to derive generalization bounds (e.g., https://arxiv.org/abs/1810.05369).\nc)\tLastly, the authors here say that “the solution obtained by cross-entropy minimization is different from the SVM solution”. This (as well as the title and abstract) may mislead the readers to think there is something wrong in the proofs of [Soudry et al. 2018] and later papers, and that logistic regression does not converge to the max-margin solution for homogeneous linear separators. However, the max-margin solution for homogeneous linear separators is also called the “max margin” or SVM solution (just for a different family). For example, see the previous paper on the topic [“Margin Maximizing Loss Functions”, Rosset et al. 2004] or section 15.1.1 in the textbook “Understanding Machine Learning: From Theory to Algorithms” by Shai Shalev Shwartz.  As I see it, the only issue in [Soudry et al. 2018] is the sentence “A bias term could be added in the usual way, extending x_n by an additional ’1’ component."" which is confusing since it cannot be applied directly to the SVM solution. The authors should aim to pinpoint this issue, and clarify their phrasing to avoid such confusions.\n\n', 'This paper presents a very specialized example to show that gradient descent on the cross-entropy loss WITHOUT REGULARIZATION leads to poor margin, which is very unrealistic. Moreover, I have the following concerns:\n\n1. In the two points classification example shown in Section 2, I want to see the plot of iteration versus cross-entropy loss during the gradient descent.\n\n2. Whether it makes sense to use cross-entropy loss to quantify loss for two-class classification problem with one point in each class? Statistically, it seems not reasonable at all.\n\n3. In Corollary 1, the authors made a further assumption, x^Ty=1, which is very unnatural.\n\n4. In the numerical results section, I want to see some results on some benchmark dataset. The presented numerical results are too weak to support the proposed differential training.']","[-20, 20, 60, -60, -70]","[40, 60, 70, 20, -20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some value in the paper, they express significant concerns about the novelty and strength of the main technical result. The initial summary is neutral, but the comments section raises doubts about the paper's contribution, stating 'the main technical result is quite weak' and suggesting that the main result can be 'readily derived' from existing work. The 'After Rebuttal' section shows a slight improvement in the reviewer's opinion, but still agrees with another reviewer's concerns.\n\nThe politeness score is moderately positive (40) because the reviewer uses respectful and professional language throughout. They frame their criticisms as personal opinions ('I am afraid that...') rather than harsh statements of fact. The reviewer also shows willingness to consider the authors' rebuttal and adjust their opinion. However, the politeness is not extremely high as the criticism, while politely phrased, is still direct and substantial."", ""The sentiment score is slightly positive (20) because the review begins with a neutral summary and lists both pros and cons. The pros are substantial, praising the paper's clarity and theoretical contributions. However, the cons are also significant, particularly the concern about potentially misleading claims. This balance results in a slightly positive overall sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while constructively presenting concerns. Phrases like 'My biggest concern is' and 'perhaps ... is a more convincing statement' indicate a polite approach to criticism. The reviewer maintains a professional tone without using harsh or dismissive language, even when pointing out limitations."", ""The sentiment score is 60 (positive) because the reviewer starts by acknowledging the paper's challenge to recent claims and presents 'nice results' that provide 'useful insights'. The reviewer finds the paper 'useful and interesting' due to its insightful results. However, it's not a perfect score as there are some criticisms about the limited evaluation of the algorithm. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths while offering constructive criticism. The reviewer uses phrases like 'I find the paper useful and interesting' and 'I strongly encourage the authors', which are polite ways to express opinions and suggestions. The criticism is presented in a balanced way, discussing both pros and cons without using harsh language."", ""The sentiment score is -60 because the reviewer expresses a clear negative opinion about the paper's suitability for publication in ICLR. They state that the contributions are not enough for publication and provide several reasons why. The reviewer acknowledges some positive aspects (e.g., 'I commend the authors for pointing out (1)'), but the overall sentiment is decidedly negative. The politeness score is 20 because the reviewer maintains a professional tone throughout and uses some polite phrases (e.g., 'I commend the authors'). However, the criticism is direct and not overly softened, keeping the score only slightly positive. The reviewer also provides detailed explanations for their views, which is a respectful approach to peer review."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's approach, describing it as 'very unrealistic' and listing multiple specific issues. The lack of positive comments and the use of phrases like 'too weak to support' indicate a strongly negative sentiment. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite direct and critical without any softening language or positive reinforcement. The use of phrases like 'I want to see' and 'it seems not reasonable at all' come across as somewhat demanding and dismissive, which contributes to the slightly impolite tone.""]"
"[""#update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.\n\n------------------ \n\nThe authors provide a study on learning to refer to 3D objects. The authors collect a dataset of referential expressions and train several models by experimenting with a number of architectural choices.\n\nThis is an interesting study reporting results on the effect that several architectural choices have generating referential expressions. Overall, while I appreciate all the experiments and results, I don't really feel I've learned something from this paper. \n\nFirst and foremost, the paper, from the title already starts to build up expectations about the 3d nature of the study, however this is pretty much ignored at the rest of the paper. I would expect the paper to provide  some results and insights regarding the 3D nature of the dataset and how this affects referential expressions, however, there is no experiment that has used this 3d-ness in any way. Even the representations of the objects are stripped down to essentially 2D (a single-view of a 3D object used to derived VGG features is as 3D as any image dataset used for similar studies, right?). \nMy major question is then: why should all this research take place in a 3D dataset? Is it to validate that research like this is at all possible with 3D objects? \n\nMoreover, all interesting aspects of referential expressions are stripped out since the authors experiment only with this geometric visual property (which has again nothing to do with 3d-ness, you could totally get that out of images). An interesting study would be to have all objects in the same image and have referential expressions that have to do with spatial expressions, something that the depth or a different view of the of the object could play a role.\n\nGiven the fact that there are no technical innovations, I can't vouch for accepting this paper, since there has been quite a lot of research on generating  referential expressions on image datasets (e.g., Kazemzadeh., 2014 and related papers). However, learning to refer to 3D objects is a very interesting topic, and of great importance given the growing interest of training agents in 3D virtual environments, and I would really encourage the authors to embrace the 3d-ness of objects and design studies that highlight the challenges and opportunities that the third dimension brings.\n\n\nKazemzadeh et al.: ReferIt Game: Referring to Objects in Photographs of Natural Scenes"", 'The paper investigates how chairs are being described ""in the context of other similar or not-so-similar chairs"", by humans and neural networks. Humans perceive an object\'s structure, and use it to describe the differences to other objects ""in context"". The authors collected a corresponding ""chairs in context"" corpus, and build models that can describe the ""target"" chair, that can be used to retrieve the described object, and that can create more discriminative language if given information about the listener.\n\nThe paper is well written, in particular the appendix is very informative. The work seems novel in combination with the dataset, and an interesting and well executed study with interesting analysis that is very relevant to both situated natural language understanding and language generation. The ""3D"" aspect is a bit weak, given that all chairs seem to have essentially been pictured in very similar positions.', ""Update: I have read author's response (sorry for being super late). The response better indicates and brings out the contributions made in the paper, and in my opinion is a strong application paper. But as before, and in agreement with R1 I still do not see technical novelty in the paper. For an application driven conference, I think this paper will make a great contribution and will have a large impact. I am slightly unsure as to what the impact will be at ICLR. I leave this judgement call to the AC. I won't fight on the paper in either direction.\n\nThe paper studies the problem of how to refer to 3D objects with natural language. It collects a dataset for the same, by setting up a reference game between two people. It then trains speaker and listener models that learn how to describe a shape, and how to identify shapes given a discriminative referring expression. The paper seems to follows state-of-the-art in the design of these models, and investigates different choices for encoding the image / 3D shape, use if attention in the listener, and context and listener aware models.\n\nStrengths:\n1. Overall, I think this is a very well executed paper. It collects a dataset for studying the problem of interest, trains state-of-the-art models for the tasks, and conducts interesting ablations and tests insightful hypothesis.\n\nWeaknesses\n1. I am not sure what is the technical contribution being made in the paper? Contrastive referential expressions have been used to collect datasets for referring to objects in images. Use of listeners and speakers have been used in NLP (Andreas et al.) as well as in vision and language (Fried et al.). Thus, while I like the application, I am not sure if there is any novel contributions being made in the paper.\n\nOverall, this is a well executed paper, however I am not sure about the novelty of contributions made in the paper.""]","[-60, 80, 20]","[20, 70, 60]","[""The sentiment score is -60 because the reviewer expresses significant concerns about the paper's contributions and novelty, stating they 'cannot increase my score' and 'can't vouch for accepting this paper'. They also mention that they haven't really learned something from the paper. However, it's not entirely negative as they do acknowledge it as an 'interesting study' and encourage further research. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I appreciate all the experiments' and 'I would really encourage the authors', which soften the criticism. The reviewer also provides constructive feedback and suggestions for improvement, which is a polite approach to criticism."", ""The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper. They describe it as 'well written,' 'novel,' 'interesting,' and 'well executed.' The reviewer also notes that the study is 'very relevant' to the field. The only slight criticism is about the '3D' aspect being 'a bit weak,' but this doesn't significantly detract from the overall positive sentiment. The politeness score is 70 (polite) because the reviewer uses respectful and professional language throughout. They offer praise without being overly effusive and present their minor criticism in a constructive manner. The tone is objective and focused on the paper's content rather than making personal comments about the authors. The reviewer's language is consistently courteous and academic in nature."", ""The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper as 'well executed' and potentially having 'large impact', they express concerns about its technical novelty and suitability for ICLR. The overall tone is more positive than negative, but with significant reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging strengths and framing weaknesses as questions or uncertainties rather than direct criticisms. They also apologize for being late in reading the author's response, which adds to the politeness. The reviewer maintains a professional and considerate tone while providing honest feedback.""]"
"['This paper proposed a new method for face completion using progressive GANs. The novelty seems very limited compared with previous methods. The results did not significantly outperform previous methods such as CTX in terms of visual quality. In addition, some of the features for the proposed method were not evaluated properly. \n\n1. The frequency attention module is not convincing. The visualization of the attention features look like normal feature in a neural network. Also, in Figure 8, the quality of results with and without FAM look very similar. These 4 images were selected from 3000 test images, but the difference is too small to show the benefit of FAM. \n\n2. In figure 8, it is unclear how the performance changes with each loss. Probably the results without L_bdy, L_rec, L_feat should be analyzed separately. \n\n3.  In figure 6, the results compared to CTX look similar. And the figure is too small to see the details. For example, from row 1, the result by CTX seems even better. \n\n4. How many images were used in the user study? Did each subjects evaluate the entire test set 3009 images? \n', ""The paper proposes a complex generative framework for image completion (particularly human face completion). It aims at solving the following challenges: 1) complete the human face at both low and high resolution; 2) control the attribute of the synthetic content; 3) without the need of complex post-processing. To achieve so, this paper proposes a progressively attentive GAN to complete face image at high resolution with multiple controllable attributes in a single forward pass without post-processing. Particularly it introduces a frequency-oriented attentive module (FAM) to attend on finer details.  \n\nThe method seems interesting, however it seems to make slight change based on ProGAN (ICLR' 18   https://arxiv.org/abs/1710.10196). Also similar idea could be found in many other papers, e.g., Wang et al. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs, CVPR' 18.  \n\nThe authors should \n1) clarify why this paper makes non-incremental contribution? What are the major novelty compared with these existing works? \n2) why the frequency attention module will yield better results?\n3) Improve the experiment, compared with stronger baselines: consider at least one or two of these state-of-the-art approaches. Also in my opinion model size and training time needs to be compared as well.\n\nAlso the experimental results did not demonstrate better performance of the proposed approach. Why is that?\n\n\n"", 'This work uses GANs to recover clean faces from occluded counterparts. The effectiveness of the proposed method is verified qualitatively and quantitatively on CelebA-HQ. The proposed framework can be generalized to several face-related tasks, such as unconstrained face recognition. Although the novelty of the method is not really impressive, the proposed method seems to be useful for face-related applications and the experimental results are convincing to me.\n\nPros:\n- This method is simple, apparently effective and is a nice use of GANs for a practical task. The paper is written clearly and the English is fine.\n\nCons:\n- My main concern with this paper is regarding the novelty. The authors seem to claim a novel GAN architecture by using an adversarial auto-encoder-based architecture. However, it is not clear to me what aspect of their GAN is particularly new.\n\n- Missing experimental comparisons with state-of-the-arts. Detailed experimental comparisons with more state-of-the-arts (e.g., RLA, Zhao et al., TIP 2018, 3D-PIM, Zhao et al., IJCAI 2018) are needed to justify the superiority of the proposed method.\n\n- Missing more in-the-wild comparisons in the Experiment section. This paper mainly performed experiments on CelebA-HQ. More in-the-wild qualitative and quantitative experiments on recent benchmarks with large occlusion variations are needed to verify the efficacy of the proposed method.\n\nAdditional comments:\n- How did authors update each component and ensure stable yet fast convergence while optimising the whole GAN-based framework? \n\n- Can the proposed method solve other challenging in-the-wild facial variations except occlusion? e.g., pose, expression, lighting, noise, etc.']","[-60, -20, 20]","[20, 50, 60]","[""The sentiment score is -60 because the review is predominantly negative. The reviewer states that the novelty is 'very limited', results did not 'significantly outperform' previous methods, and features were 'not evaluated properly'. They also point out several weaknesses in the paper's methodology and presentation. However, it's not entirely negative, as they acknowledge it as a 'new method', hence not the lowest possible score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use neutral language like 'seems', 'unclear', and phrase criticisms as questions or suggestions rather than direct attacks. However, the overall tone is more matter-of-fact than overtly polite, hence a slightly positive but not high score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting approach, they express concerns about its novelty and performance. The reviewer points out similarities to existing works and requests clarification on the paper's non-incremental contributions. They also note that the experimental results did not demonstrate better performance. However, the score is not deeply negative as the reviewer still shows interest in the method.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'The method seems interesting' and frame their criticisms as suggestions or questions rather than harsh statements. The reviewer provides specific recommendations for improvement without using dismissive or rude language. However, the score is not extremely high as the review is direct in its criticisms and doesn't include overtly polite language or praise."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the effectiveness and usefulness of the proposed method, despite concerns about novelty. The review begins with positive aspects and the cons are presented constructively. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions, and acknowledges the paper's strengths. The reviewer uses phrases like 'seems to be useful', 'experimental results are convincing', and frames concerns as 'My main concern' rather than using harsh language. The additional comments are phrased as questions, which is a polite way to address potential issues.""]"
"['This manuscript applies transfer learning for protein surface prediction. The problem is important and  the idea is novel and interesting. However, the  transfer learning model is unclear. \nPros:  interesting and novel idea\nCons:  unclear transfer learning model, insufficient experiments. \n\nDetail: section 4 describes the transfer learning model used in the work, but the description is unclear. It is unknown the used model is a new model or existing model. Besides, in the experiments, the proposed method is not compared to other transfer learning methods.  Thus, the evidence of the experiments is not enough. ', 'Summary:\nThis paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces. They show improvements in predictive performance over some other recent deep learning methods. \nThe work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.\n\nNovelty:\nThe main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN\n\nClarity:\n- The paper is well written, with ample background into the problem.\n\nSignificance:\n- Their method improves over prior deep learning approaches to this problem. However, the results are a bit misleading in their reporting of the std error. They should try different train/test splits and report the performance.\n- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community\n- Protein conformation information is not required by their method\n\nComments:\n- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)\n\n-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred\n\n- The authors use a balanced ratio of positive and negative examples. The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones. Can they show performance at various ratios of positive:negative examples? In case there is a consistent improvement over prior methods, then this would be a clear winner\n', 'For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins’ native structure.\nThe authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.\n\nMy overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario. Also, the compared methods don’t really use the validation set from the complex data for training at all. Thus the experiment comparison is not really fair. 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling. Specifically, the testing dataset is fixed. A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.\n\nSince this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis. Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can. Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods. Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.\n\nOverall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.\n\n\nSome minor issues:\n1)\ton page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?\n\n2)\tIt is not clear what the “replicates” refer to in the experiments.\n\n3)\tSome discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?\n']","[-20, 50, -20]","[50, 75, 60]","['The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the novelty of the idea, they express significant concerns about the clarity of the transfer learning model and insufficient experiments. The pros are outweighed by the cons in this review. The politeness score is moderately positive (50) as the reviewer uses neutral language and balances positive and negative feedback without using harsh or critical tones. They present their concerns as observations rather than direct criticisms, maintaining a professional and respectful tone throughout the review.', ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges improvements in predictive performance and describes the paper as 'interesting' and 'well written'. However, they also suggest areas for improvement and note that the work may be more suitable for a bioinformatics audience. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offers constructive criticism, and frames suggestions as recommendations rather than demands. They use phrases like 'The authors should' instead of more forceful language, and provide specific, actionable feedback without being harsh or dismissive."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is well-written and has potential, they express several significant concerns about the experimental setup and analysis. The reviewer points out that the experiment results don't fully support the authors' claims and suggests that more thorough experimental setup and analysis are needed. However, the score is not deeply negative because the reviewer sees potential in the paper if the issues are addressed.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I would expect,' 'it is still missing,' and 'I do think the paper could be much stronger' rather than using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper, such as it being 'well written,' which contributes to the polite tone. The suggestions for improvement are framed as opportunities to strengthen the paper rather than criticisms.""]"
"['This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments:\n\n1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.  \n\nHoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018\n\n2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. \n\n\nLedig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017.\n\nJohnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016.\n\nKim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016.\n\n3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth? \n', 'Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement.\n\nHowever, the novelty is limited and not well explained.\n1. The idea of adding a task-specific branch has been proposed in Huang et al’s work.\nRui Huang, Shu Zhang, Tianyu Li, Ran He, Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis, ICCV 2017.\n\n2. It is not clear why for task-specific loss authors use mse loss instead of cross-entropy loss.\n3. It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.\n4. The proposed method is not compared with other super-resolution methods.\n5. There are typos with citations. There should be parenthesis around citations.', 'In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse. In general, the paper is addressing an important problem but I still have several concerns as follows:\n1. The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN. \n\n2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. The authors are expected to make comparison with methods not based on GAN framework. \n\n3.  The authors should clarify the details on the Task network since it is non-trivial to model a task. \n\n ']","[-30, -50, -30]","[50, 0, 20]","[""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('shows superior performance'), they express significant criticisms about the paper's novelty and methodology. The overall tone suggests the reviewer is not impressed with the work, citing 'limited novelty' and questioning the baseline comparisons. The politeness score is 50 because the reviewer uses professional language and offers constructive feedback. They use phrases like 'Please see the following comments' and provide specific suggestions for improvement, which is polite. However, they don't use overtly warm or encouraging language, keeping the tone neutral to slightly positive in terms of politeness."", ""The sentiment score is -50 because the review is generally critical, pointing out several limitations and issues with the paper. The reviewer states that 'the novelty is limited and not well explained' and lists multiple concerns. However, it's not entirely negative as it acknowledges the authors' proposal in the first sentence. The politeness score is 0 (neutral) because the language is direct and matter-of-fact without being overtly polite or rude. The reviewer states criticisms plainly without softening language, but also without using harsh or insulting terms. The review focuses on content rather than personal comments, maintaining a professional tone throughout."", ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges the importance of the problem being addressed, they express several concerns about the paper. The phrase 'I still have several concerns' indicates a critical stance. The reviewer points out that the technical contribution is 'rather incremental' and suggests that the authors' chosen method (GAN) may not be the most appropriate for the task. These criticisms outweigh the initial positive acknowledgment.\n\nThe politeness score is slightly positive (20) because the reviewer uses generally respectful language. They begin by summarizing the paper's contribution and acknowledging its importance. The concerns are presented as suggestions or expectations rather than harsh criticisms. Phrases like 'The authors are expected to' and 'The authors should clarify' are polite ways of requesting improvements. The reviewer avoids directly negative or confrontational language, maintaining a professional tone throughout.""]"
"['This paper solves Flappy bird by combining DQN and probabilistic programming. I think this is in general a good avenue to explore.\n\nHowever I found the paper to be poorly written. For example, notation is not properly introduced, there are many mathematical mistakes and typos in the written text and citations. This makes it very hard to understand what is actually going on.\n\nIt is also not clear what is the probabilistic program and what are we conditioning on? What is the inference algorithm? Maybe it\'s useful to expand more on how this ties to the ""RL as inference"" framework (see e.g. Levine, 2018). It seems like we are doing rejection sampling where the condition is ""no collision"". As a result, I\'m not sure whether sampling from prior is a competitive baseline.\n\nFor the DQN experiment, the learning curve seems very noisy in a way that it\'s unclear whether a fair conclusion can be drawn only from one run (as it appears to be done).\n\nThe experiments also feel a bit contrived to make a strong case for probabilistic programming + DQN.', 'This paper combines a model-free approach and a model-based approach for the game of Flappy Bird. The model-free approach is a CNN on the screen snapshots, in the same fashion as DQN was used for Atari games. The model-based approach is a probabilistic model of Newtonian laws of motion. \nThe combination of model-free and model-based approaches is definitely one very relevant issue in machine learning, especially in interactive situations, such as reinforcement learning and robotics. The ideas that this paper combines are state-of-the-art and hence is it informative to see how these two particular techniques from each paradigm work together. The most interesting part is that the probabilistic model restricts the CNN restricts the sample action for the model.\nI’m not sure how novel this particular combination is, but other systems have also used a model (or a solver) to restrict the possibilities of the exploration. For instance, AlphaGo combines the model-free approach with the rules of the game (at least in the first versions of AlphaGo), and others use Montecarlo Tree Search in a similar vain.\nThe paper is generally well-written, with some typos occasionally. However, I think that some parts of the process are no well explained, or explained in the wrong order.\nFor instance, the title and the abstract misled me for quite a while. The title says program induction, and this is then said to be in a fashion similar to Ghahramani 2015, but no further details are given. Is this using Julia? In any case, where is the induction? Later on, it is said that “the model learns the distributions”. but the exact way is completely missing. In other words, the model-based part is not described and encapsulated in a cryptic PHYSICSSIMULATION. In any case, if the model just learn the parameter, I wouldn’t call this “program induction”, at least in the same way as Lake and others use it, or in the way it is used in the area of “inductive programming”. \nThe game should be describe at the start as “unwanted collisions” are meaningless for a reader who doesn’t know the goal of the game (which is explained in the last paragraph of the paper).\nThe main problem is that the physics simulation is not learnt and hence rewritten for other games with other physics. This should be solved, in order to see significant progress for benchmarks such as ALE (and properly compare with DQN and many other variants). Perhaps robotics is a better application area, as the physics are always the same (true physics).\nRegarding the experiments, they are not very conclusive, especially because the difference is not that large and it is only one single game. The application to other games would be needed, especially if the physics is different. Also, the parameters are different (the ms) and I didn’t understand if these are the best choices, or the possible choices given the computational limitations. In other words, I don’t know if all techniques are compared in the same compute and data conditions. In Figure 3, the yaxis should be explained.\n(hum 2017) I imagine that this refers to the authors, otherwise it is a typo.\nI didn’t understand the future work. I didn’t parse the bit: “learn about rewards structures in games of physics intuition”.  Actually, the final paragraph gives hints about how much assistance and specialisation the approach is given and hence the limit of generalisations to other problems. These limitations should be stated from the beginning.\nPros:\n-\tImportant integration of model-free and model-based approaches.\n-\tState-of-the-art techniques\n\nCons:\n-\tI don’t see program induction, despite being in the title.\n-\tExperiments are limited to one game\n-\tThe physics engine is specialised for this game, and hence the approach is difficult to generalise (automatically) for a range of problems where the models should be different.\nTypos: \nGharmani -> Ghahramani\novers -> over\nbird is required to chose -> the bird is required to choose\n', ""The authors present an algorithm that incorporates deep learning and physics simulation, and apply this algorithm to the game Flappy Bird.  The algorithm uses a convolutional network trained on agent play to predict the agent’s own actions given a sequence of frames.  Using this action estimator output as a prior over an action distribution (parameterized by a Dirichlet process), the algorithm iteratively updates the action by rolling out a ground-truth physics simulator of the environment, observing whether this ground-truth simulation yields negative reward, and updating the action accordingly.\n\nWhile I find the authors' introductory philosophy largely compelling (it draws inspiration from developmental psychology, learning to model the physical world, and the synthesis of model-based and model-free learning), I have concerns with most other aspects of the paper.  Specifically, here are a few points:\n\n1)  The authors only apply their algorithm to a single game (Flappy Bird), a game that has no previously established benchmarks.  In fact, while there is no prior work in the literature on this game (perhaps because it is considered very easy), some unofficial results suggest that it is solvable by a straightforward application of existing methods (see this report:  http://cs229.stanford.edu/proj2015/362_report.pdf).  The authors do apply one baseline (out-of-the-box DQN) to this game, but the reported scores are suspiciously low, particularly in light of the report linked above.  No training curves or additional baselines are shown, and no prior work on this game in the literature exists to compare against.\n\n2)  The authors’ algorithm uses privileged information which eliminates the possibility for a fair comparison to baselines.  Specifically, their algorithm uses ground-truth state (not just image input), and a ground-truth physics simulator (which should be an enormous advantage).  Their one baseline (DQN) does not have either of these sources of privileged information, hence cannot be a fair comparison.\n\n3)  The authors’ algorithm is not general-purpose.  Because the algorithm itself uses a ground-truth environment-specific state, a ground-truth environment-specific simulator, and relies on a “crash boolean” (whether the bird hit a tree) specific to this game, it cannot be applied out-of-the-box on a different environment.\n\n4)  The authors make some claims that are too strong in light of the reported results.  For example, they claim that “the performance of the model outperforms all model-free and model-based approaches” (section 1), while they do not even compare against any model-based baselines (and only a single model-free baseline, DQN, which is not state-of-the-art anymore).\n\nOverall, I would recommend the authors choose a game or set of games that has/have established baselines in the literature, come up with a general-purpose algorithm which doesn’t rely on a ground-truth physics simulator, and more rigorously compare to existing methods.\n""]","[-50, -20, -70]","[20, 50, 20]","[""The sentiment score is -50 because while the reviewer acknowledges that the paper explores a 'good avenue,' they express significant concerns about the paper's quality. They describe it as 'poorly written' with 'many mathematical mistakes and typos,' making it 'very hard to understand.' The reviewer also questions the methodology and experimental design. These criticisms outweigh the initial positive comment, resulting in a negative overall sentiment.\n\nThe politeness score is 20 because the reviewer maintains a professional tone throughout, avoiding harsh language or personal attacks. They begin with a positive note and use phrases like 'I think' and 'It seems like' to soften their criticisms. However, the review is direct in its criticisms without excessive hedging or overly polite language, keeping it closer to neutral than highly polite."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance and relevance of the paper's approach, they express several concerns and limitations. The reviewer points out issues with the paper's title, lack of clarity in certain explanations, limited experiments, and difficulties in generalizing the approach. However, they also mention some positive aspects, which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I'm not sure' and 'I didn't understand' instead of more critical language. They also balance their critique by mentioning both pros and cons. The reviewer provides constructive feedback and suggestions for improvement without using harsh or dismissive language."", ""The sentiment score is -70 because the reviewer expresses significant concerns with most aspects of the paper, listing several major issues and recommending substantial changes. The overall tone is critical, though not entirely dismissive. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer constructive feedback. They use phrases like 'I find the authors' introductory philosophy largely compelling' and 'I would recommend' which maintain a level of politeness. However, the critique is still quite pointed, preventing a higher politeness score.""]"
"['Summary.\nThe paper proposes a vehicle’s trajectory planner that iteratively predict next-step (longitudinal and latitudinal) position of an ego-vehicle. Instead of using a raw image, a set of handcrafted features (i.e., the status of traffic lights, route, roadmap, etc) are mapped onto a fixed-size of bird-eye view map, which is then fed into the recurrent neural network. Additional regularizing loss terms are explored for the robustness of the model. The effectiveness of the method is demonstrated in simulation and real-world experiment.\n\nStrengths.\n- Impressive demonstrations in simulation and real-world experiments.\n- The paper is generally well-written and easy to follow.\n\nvs. Existing motion planning approaches.\nThere exists a large volume of papers on vehicle motion planning, which has largely been explored for controlling self-driving vehicles. Some of them successfully demonstrated their effectiveness for navigating a vehicle in typical driving scenarios, including “slowing down for a slow car”.\nA notable survey may include:\n\n[1] Paden et al., “A survey of motion planning and control techniques for self-driving urban vehicles,” IEEE Transactions on intelligent vehicles, 2016. \n\nHowever, the paper provides neither any works of literature on existing motion planners nor any types of comparison with them. This makes hard to judge the proposed learning-based motion planner outperforms others including conventional optimization-based methods. \n\nMissing data collection details.\nThis work depends hugely on its own human-designated oracle-like map, which provides driving-related features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. Generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status). Section 6.2 should be explained more in detail.\n\nA weak novelty of using intermediate-level input/output representation.\nThere exist similar approaches that utilized similar representations to determine a vehicle’s behaviour, examples may include:\n\n[1] Lee et al., “Convolution Neural Network-based Lane Change Intention Prediction of Surrounding Vehicles for ACC,” IEEE ITSC 2017.\n[2] We et al., “Modeling trajectories with recurrent neural networks,” IJCAI, 2017.\n\nMissing evaluation details.\nIn Section 6.2, (though not mentioned) it seems that a training dataset is collected from 60-days of real-world driving (given the context). But, in the testing phase, it seems that the authors used a simulator to evaluate different driving scenarios with various initial condition (i.e., speed, heading angle, position, etc). Can authors clarify details of the evaluation environment?\n\nMinor concerns.\nA paragraph of contribution summary (in Introduction section) will help. \nTypos (e.g., Section 2 line 17: ‘off of’)', 'The authors present a very interesting work on predicting future motion of a self-driving vehicle given image inputs that represent its surrounding and history. The authors use RNNs for this task, and data augmentation to make their model more robust. They also present a number of very interesting videos showcasing the performance.\n- Several key aspects of the work are not well explained. E.g., what are pixel sizes, time resolution, where is the vehicle positioned within the image? All this is missing.\n- Traffic lights are represented as ""a sequence of grayscale images"", how exactly, one for each state? Or some other way.\n- How were videos generated, how were various channels collapsed?\n- Dashed arrows not explained in Fig 2.\n- ""a small regression tower"", this needs to be elaborated. As well as other mentions of ""towers"".\n- In (3), is the sum over all pixels missing?\n- Section 4.1.2 is not clear, this needs to be expanded. It is not well explained how exactly these losses are computed and used.\n- For past-motion dropout, then you simply give blank input?\n- Figure 6 is referenced in the regular text although it is located in the appendix.\n- Orienting vertical axis with delta of +-25deg (as explained in Section 6.2) is not observed in the given videos, seems that there is no delta there. Is that done only during training?\n- What is the exact difference between open- and closed-loop experiments? Given that a number of other key aspects are missing, I am not sure I fully understand a difference here as well.\n- One of major issues in the evaluation section is that other baselines are missing (especially in the context of Fig 5). Even the more obvious ones would help a lot with understanding the performances, such as vehicle continuing to do what it was doing, or baseline predicting the route). This is a major flaw of the paper.\n- Some recent related work missing, see [1], [2], and related work.\n[1] Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net, Luo, Wenjie, Bin Yang, and Raquel Urtasun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[2] Short-Term Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, Djuric, N., Radosavljevic, V., Cui, H., Nguyen, T., Chou, F.-C., Lin, T.-H., Schneider, J., arXiv preprint:1808.05819, 2018.', ""The paper describes a framework for training a self-driving policy by augmenting imitation loss with additional loss terms that penalize undesired behaviors and that encourage progress. The policy takes as input a parsed representation of the scene (rather than raw images) and outputs pose trajectories for a down-stream controller. The method is trained on simulated data that includes perturbations to improve generalizability. The framework is evaluated in simulation through a series of ablations to better understand the contribution of the different loss terms.\n\n\nSTRENGTHS\n\n+ Paper acknowledges the difficulty of end-to-end (pixels-to-torque) learning for autonomous driving and instead reasons over pre-processed inputs in the form of lower-dimensional images (and image sequences) that capture obstacles as bounding boxes and simple lines for routes, grayscale intensities, etc. Similarly, the output is a trajectory that is then fed to a controller responsible for tracking this trajectory.\n\n\nWEAKNESSES\n\n- The insufficiency of behavioral cloning is not surprising, as noted, given the covariate shift. It would be interesting to consider a  no-regret formulation analogous to Ross et al., 2011, even though it would require interaction with a human.\n\n- The limitation of producing paths in this way is that the network does not explicitly reason over the feasibility of the path, which is important for non-holonomic vehicles. Instead, the network must learn the kinematic and dynamic constraints.\n\n- Perturbations of the simulated trajectories are used to expose the model to collisions and other rare events, but is not clear that simple trajectory perturbations such as those used here provide a sufficient exposure to these rare events.\n\n- The fact that the 2D image that expresses the vehicle's position is absolute limits the environment in which the network is valid. The experiments are conducted on images corresponding to an 80m x 80m environment, which is trivially small.\n\n- The proposed framework is highly specific to self-driving and the extent to which it provides insights for other domains is not clear.\n\n- The ablation experiments are not very compelling. In the case of the nudging experiment, all models result in collisions with M4 being the best model with a 10% collision rate. The trajectory perturbation results are better. In the case of the slowing experiment, M3 is the only version to not result in collision, whereas M4 collides 5% of the time. It isn't clear than which model is preferable since, while M3 never collides in the case of the slowing down experiment, it collides 45% of the time in the nudging experiment, almost as frequently as the M0 baseline.\n\n- The paper claims that the model was run on a real robot, but there is no experimental evaluation of the results, only a reference to videos. The results of these experiments should be quantified and discussed or the reference to running on a real vehicle should be toned down, if not removed.\n\n- Equation 3 requires knowledge of the ground-truth distribution. How is this determined?""]","[-20, -20, -50]","[60, 50, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper ('Impressive demonstrations', 'well-written'), they also point out several significant weaknesses. These include lack of comparison with existing methods, missing details on data collection, weak novelty, and unclear evaluation details. The overall tone suggests the paper needs substantial improvements.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They begin by highlighting the paper's strengths, use neutral language to describe shortcomings (e.g., 'This makes hard to judge...', 'details are missing...'), and offer constructive suggestions (e.g., 'Can authors clarify...'). The reviewer also provides helpful references and suggestions for improvement, which is a polite way to guide the authors."", ""The sentiment score is slightly negative (-20) because while the reviewer starts by praising the work as 'very interesting', the majority of the review consists of critical points and areas for improvement. The reviewer points out several missing explanations, unclear sections, and a 'major flaw' in the evaluation section. However, the criticism is constructive and not overly harsh, hence not an extremely negative score. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, starting with praise and framing criticisms as suggestions or questions rather than direct attacks. The use of phrases like 'needs to be elaborated' and 'is not clear' maintains a professional and courteous tone while still conveying the need for improvements."", ""The sentiment score is -50 because while the review acknowledges some strengths of the paper, it lists significantly more weaknesses than strengths. The reviewer points out several limitations and issues with the methodology, experiments, and claims made in the paper. However, it's not entirely negative as it does recognize some positive aspects.\n\nThe politeness score is 50 because the reviewer maintains a professional and objective tone throughout. They use neutral language to describe both strengths and weaknesses, avoiding harsh or personal criticisms. The review is structured and focused on the content of the paper rather than attacking the authors. However, it doesn't go out of its way to be overly polite or complimentary either, maintaining a balanced, academic tone.""]"
"['This paper studied the information bottleneck principle for deep learning. In the paper by (Schwatz-Ziv & Tishby 17\'), it is empirically shown that the mutual information I(X;T) between input X and internal layers T decreases, which is called a compression phase. In this paper, the author found that the compression phase is not always happening and the shape of the curve of I(X;T) highly depends on the ""bining size"" which is used for estimating mutual information by (Schwatz-Ziv & Tishby 17\'). Then the authors proposed to use a noisy DNN to make sure the map X->T is stochastic, then proposed a guaranteed mutual information estimator. Then some empirical results are shown.\n\nI think the problem in (Schwatz-Ziv & Tishby 17\') do exist and their result is highly questionable. However, I have some major question about this paper.\n\n1. In this paper a noisy DNN was proposed. However, how do you choose the noise level \\beta? If I understand correctly, the noise level plays a similar role of the bining size in (Schwatz-Ziv & Tishby 17\'). Noise level goes to zero is similar to bining size goes to zero. I wish to see a figure about how different \\beta affects the curve of I(X;T) (similar to Figure 1 but let \\bet change). \n\n    In Figure 4(d) there is a plot showing different \\beta will affect the mutual information, but the x-axis is ""weight"". I wonder that how the curve of mutual information change w.r.t \\beta, if the x-axis is training epochs. Do your statement stable about \\beta? \n\n2. I think Section 3 and Theorem 1 are interesting and insightful. But I notice that in Section 10 you mentioned that this will be a separate paper. Is it OK to put them together in this paper?\n\n3. The paper by (Schwatz-Ziv & Tishby 17\') has not pass a peer-review process and it is still a preprint. This paper is nothing but only saying some deficiencies of (Schwatz-Ziv & Tishby 17\') (except Section 3 and Theorem 1 which I think should be an independent paper). I think such a paper should not be published as a conference paper before (Schwatz-Ziv & Tishby 17\') pass a peer-review process.\n\nSo totally I think this paper should not be accepted by ICLR at this point. I think Section 3 and Theorem 1 should become an independent paper, and the DNN approach can be an application of the mutual information estimator.', 'Response to author comments:\n\nI would like to thank the authors for answering my questions and addressing the issues in their paper. I believe the edits and newly added comments improve the paper. \n\nI found the response regarding the use of your convergence bound very clear. It is a very reasonable use of the bound and now I see how you take advantage of it in your experimental work. However, I believe the description in the paper, in particular, the last two sentences of Remark 1, could still be improved and better explain how a reasonable and computationally feasible n was chosen.\n\nTo clarify one of my questions, you correctly assumed that I meant to write the true label, and not the output of the network.\n\n\n***********\n\nThe paper revises the techniques used in Tishby’s and Saxe et al. work to measure mutual information between the data and a hidden layer of a neural network. The authors point out that these previous papers’ measures of mutual information are not meaningful due to lack of clear theoretical assumptions on the randomness that arises in DNNs.\n\nThe authors propose to study a perturbed version of a neural network to turn it into a noisy channel making the mutual information estimation meaningful. The perturbed network has isotropic Gaussian noise added to each layer nodes. The authors then propose a method to estimate the mutual information of interest. They suggest that the mutual information describes how distinguishable the hidden representation values are after a Gaussian perturbation (which is equivalent to estimating the means of a mixture of Gaussians). Data clustering per class is identified as the source of compression.\n\nIn addition to proposing a way to estimate a mutual information of a stochastic network, the authors analyze the compression that occurs in stochastic neural networks. \n\nIt seems that the contribution is empirical, rather than theoretical, as the theoretical result cited is going to appear in a different article. After reading that the authors “develop sample propagation (SP) estimator”, I expected to see a novel approach/algorithm. However, unless I missed something, the proposed method for estimating MI for this Gaussian channel is just doing MC estimation (and no guarantees are established in this paper). The convergence bounds for the SP estimator are presented(Theorem 1), however, the result is cited from another article of the authors, so it is not a contribution of this submission. \n\nSince the authors have this convergence  bound stated in Theorem 1, it would be great to see it being used - how many samples are needed/being used in the experiments? What should the error bars be around mutual information estimates in the experiments? If the bound is too loose for a reasonable number of samples, then what’s the use of it?\n\nThe authors perform two types of experiments on MNIST. The first experiment demonstrates that no compression is observed per layer and the mutual information only increases during training (as measured by the binning approach, which is supposed to track the mutual information of the stochastic version of the network). The second experiments demonstrates that deeper layers perform more clustering. \n\nRegarding the first experiment, could the authors clarify how per unit and per entire layer compression estimation differs?\n\nAlso, in my opinion, more clustered representations seem to indicate that the mutual information with the output increases. Could the authors comment on how the noise levels in this particular version of a stochastic network affects the mutual information with the output and the clustering? Do more clustered representations lead to increased mutual information of the layer with the output?\n\nI found it fairly difficult to summarize the experimental contribution after the first read. I think the presentation and summary after each experiment could be improved and made more reader friendly. For example, the authors could include a short section before the experiments stating their hypothesis and pointing to the experiment/figure number supporting their hypothesis.', 'This paper provides a principled way to examine the compression phrase, i.e, I(X;T) in deep neural networks. To achieve this, the authors provides an theoretical sounding entropy estimator to estimate mutual information.  Empirically, the paper did observe this compression phrase across both synthetic and real-world data and relates this compression behavior with geometric clustering. \n\nPros:\n- The paper is well-written and easy to understand.\n- The framework for analyzing the mutual information in DNNs is theoretically sounding and robust.\n- The finding of connecting clustering with compression is novel and inspiring. \n\nQuestions:\n- The main concern of the paper is its conclusion. While the experiments in the paper did show the mutual information goes down as the clustering effect enhanced, it only means `clustering` and `compression` are correlated; but the paper claims `clustering` is the source of `compression`, i.e., `clustering` leads to `compression`. This conclusion is problematic. For example, looking at Figure 5(a), as the mutual information goes down from epoch 28 to epoch 8796, not only the clustering gets enhanced, but also the loss is going down. Thus, alternatively, one can also argue the loss (i.e., `relevance`) is the cause of `compression` instead of `clustering`. From another aspect, the effect of `clustering` is also related to the loss, i.e., it is the loss function that pushes the points of the same class to be closer; then, even if the direct cause of `compression` is `clustering`, the root cause might still be the loss (i.e., `relevance`). \n- In Figure 5(a). Why the mutual information increases from epoch 80 - epoch 541? Also, it seems that the test loss increases as the I(X;T) decreases from epoch 541 to epoch 8796. This seems to be counter-intuitive to the claim that ""lower I(X;T) implies higher generalization ability"". Can you explain this phenomenon?\n\n[UPDATE] the authors address my concerns in a detailed way, and the updated revision is rather robust, therefore, I decide to change my score to accept.']","[-60, 20, 80]","[20, 60, 70]","[""The sentiment score is -60 because the reviewer expresses significant concerns about the paper and ultimately recommends rejection ('this paper should not be accepted by ICLR at this point'). However, they do acknowledge some positive aspects like the 'interesting and insightful' Section 3 and Theorem 1, preventing an extremely negative score. The politeness score is 20 because the reviewer uses generally professional language and phrases criticisms as questions or suggestions ('I wish to see...', 'I wonder that...') rather than harsh statements. They also acknowledge the paper's contributions. However, the overall tone is still critical, preventing a higher politeness score."", ""The sentiment score is slightly positive (20) because the reviewer begins by thanking the authors for addressing their questions and improving the paper. They also acknowledge that the edits and new comments have improved the work. However, the reviewer still has some concerns and suggestions for further improvement, which prevents the score from being higher. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, starting with a thank you and using phrases like 'I believe' and 'Could the authors clarify' when making suggestions or asking questions. The reviewer also acknowledges when they might have misunderstood something ('unless I missed something'). The tone is constructive and professional, without any harsh criticism or rude language."", ""The sentiment score is 80 (positive) because the review starts with acknowledging the paper's contributions and lists several pros. The reviewer also mentions that they changed their score to 'accept' after the authors addressed their concerns. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, frames criticisms as questions, and acknowledges the authors' efforts to address concerns. The reviewer provides constructive feedback and balances positive comments with areas for improvement, maintaining a professional and courteous tone throughout the review.""]"
"[' Summary: \n%%%%%%%%%%%%%%%\nThe paper explores ways to adapt the learning rate rule through a new minimax formulation.\nThe authors provide regret bounds for their method in the online convex optimization setting.\n\nComments:\n%%%%%%%%%%%%%%%\n-I found the motivation of the approach to be very lacking.\nConcretely, it is not clear at all why the minimax formulation even makes sense, and the authors do not explain this issue.\n\n-While the authors provide regret guarantees for their method, the theoretical analysis does not reflect when is their approach  beneficial compared to standard adaptive methods. Concretely, their bounds compare with the well known bounds of AdaGrad. \nIt is nice that their approach enables to extract AdaGrad as a private case. But again, it is not clear what is the benefit of their extension.\n\n-Finally, the experiments do not illustrate almost any benefit of the new approach compared to standard adaptive methods.\n\n\nSummary\n%%%%%%%%%%%%%%%\nThe paper suggests a different approach to adapt the learning rate.\nUnfortunately, the reasoning behind the new approach is not very clear.\nAlso, nor theory neither experiments illustrate the benefit of this new approach over standard methods.\n', 'This paper presents a method for adaptively tuning the learning rate in gradient descent methods. The authors consider the formulation of each gradient descent update as a quadratic minimization problem and they propose adding a phi-divergence between the learning rate that would be used and an auxiliary vector. The authors also propose adding a maximization over all learning rates in the update.  \n\nThe authors study an important problem and propose a novel method. The algorithms suggested by the author are also relatively clear, and it is great that the paper presents both theoretical results as well as numerical experiments.\n\nOn the other hand, I didn\'t find the main idea of hyper-regularization to be well-justified. It is not clear why adding an additional regularization term for the learning rate makes sense , and it is even less clear why this should be presented as a maxmin problem. This can make the update step much more complicated and is probably why the authors also propose a simpler alternating optimization algorithm as an alternative. Unfortunately, the authors do not discuss how this alternating optimization problem relates to the original one, and the theoretical guarantees are only presented for the original algorithm. The authors also do not justify the choice of phi-divergence as the regularizer for the learning rate. The theoretical guarantees in the paper also do not suggest that the algorithm presented in the paper is better than existing state-of-the-art methods, even in specific situations (i.e. the regret bounds don\'t appear better than the AdaGrad regret bounds). Moreover, without tests for statistical significance, I also didn\'t find the experimental results sufficiently compelling.\n\nSpecific comments and questions:\n1) Page 3: Equation (4): The paper would be stronger if the authors motivated why the regularization should be posed as an outer maximization.\n2) Page 3: ""we use the \\phi-divergence as our hyper-regularization"". Why is this a good choice of reuglarizer?\n3) Page 3: ""only a few extra calculations are required for each step"". This is a misleading comment, because the maximization can be hard when phi is complicated, even if the problem splits across dimensions.\n4) Page 4: ""The solution of problem (5) is the same as (7) in unconstrained case"". You should provide a reference for this statement as well as discuss the specific assumptions on the objective that allow you to arrive at this claim.\n5) Page 4: ""while the solution of (7) is more difficult to get. Thus, we choose (5) as our basic problem"". This seems like a very bad motivation for choosing the maxmin formulation. For instance, the problem would be even simpler if  you didn\'t include this extra phi-divergence at all.\n6) Page 4: ""Although setting \\eta-t=\\beta_t is our main focus..."". Why is smoothness in the learning rate a good property? \n7) Page 5: Equation (11). How do these iterates relate to the ones in equation (5) (e.g. when do they coincide, if ever)?\n8) Page 5: ""influence the efficient of our algorithms."" Grammatical error.\n9) Page 6. ""our algorithms are robust to the choice of initial learning rates and do not rely on the Lipschitz constant or smoothness constant"". I\'m not sure why this is a valuable property, since AdaGrad doesn\'t rely on these parameters either.\n10) Page 6: Theorems 6 and 7. How do these results depend on alpha and \\beta_t? This paper would be much stronger if the bounds depend on \\phi more clearly and if the authors were able to show that there exist choices of phi that make this algorithm better than existing methods.\n11) Page 6: Theorem 7: The dependence on G in the regret bound actually makes this worse than the AdaGrad regret bound.\n12) Page 7: ""KL_devergence"". Typo.\n13) Page 7: ""different update rules were compared in advance to select the specific one for any phi divergence in the following experiments."" What does this mean exactly? How much of a difference does the choice of update rule make?\n14) Page 7: ""growth clipping is applied to all algorithms in our framework"". Why is this necessary, and how does it affect the theoretical results?\n14) Page 7-8: Figures 1, 2, and 3. It\'s hard to interpret the significance of these results without error bars.\n\n\n\n\n\n\n\n\n', ""The paper presents a generalization of the Adagrad type methods using a min-max formulation and then presents two alternate algorithms to solve this formulation. \n\nIt is unclear to me that much extra generalization has been achieved over the original AdaGrad paper. That paper simply presents the choice of hyperparameters as an optimal solution to a proximal primal dual formulation. The formulation presented here appears to be another form of the proximal mapping formulation, and so it is unclear what the advance here is. The AdaGrad paper used a particular Bregman divergence, and different divergences yield slightly different methods, as is observed here by the authors when they use different divergence measures.\n\nThe Bregman divergences do make sense from a primal pual proximal formulation point of view, but why do you use a discrepancy function in your min-max formulation that comes from the \\phi - divergence family? Why not consider an L_p normalization of the discrepancy? \n\nThe difference between formulations (5) and (6) is not clearly specified. Did you mean to drop the constraints that \\beta \\in \\cal{B}_t ? Otherwise, why is (6) , which looks to be a re-write of (5), unconstrained and hence separable?\n\nThe authors claim that the method is free of parameter choices, but the initial \\beta_0 seems to be a crucial parameter here since it forms both a target and a lower bound for subsequent \\beta_t's. How is this parameter chosen and what effect does it have on convergence? From the results (Figs in Sec 5), this choice does significantly impact the final test loss obtained. \n    \nI could not find a proof for Thm 6 in the appendix. Did I over look it or is there a typo?""]","[-70, -20, -50]","[-20, 60, 20]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the motivation is 'very lacking', the theoretical analysis doesn't show benefits over existing methods, and the experiments 'do not illustrate almost any benefit' of the new approach. The lack of positive comments and the use of phrases like 'not clear at all' and 'unfortunately' further reinforce the negative sentiment. The politeness score is -20 because while the reviewer isn't overtly rude, the language is quite direct and critical without much attempt to soften the criticism. Phrases like 'very lacking' and 'not clear at all' come across as somewhat blunt. However, the reviewer does use some neutral language and doesn't resort to personal attacks, which prevents the score from being lower."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The authors study an important problem and propose a novel method'), they express significant concerns about the justification, methodology, and results ('I didn't find the main idea of hyper-regularization to be well-justified', 'The theoretical guarantees in the paper also do not suggest that the algorithm presented in the paper is better than existing state-of-the-art methods'). The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms as questions or suggestions rather than direct attacks. They use phrases like 'The paper would be stronger if...' and 'This paper would be much stronger if...', which are constructive in tone. The reviewer also provides specific, detailed feedback, which is a polite and helpful approach in academic review."", ""The sentiment score is -50 because the review expresses several concerns and criticisms about the paper, such as lack of clarity on the advancements made, questioning the choice of divergence measures, and pointing out potential issues with parameter choices. However, it's not entirely negative as it acknowledges some aspects of the work. The politeness score is 20 because while the reviewer raises critical points, they do so in a professional and constructive manner, using phrases like 'It is unclear to me' and asking questions for clarification rather than making blunt criticisms. The language is generally neutral and academic, with a slight lean towards politeness in how concerns are expressed.""]"
"['\nThis paper consider the connections between SGD and distributionally robust optimization. There has long been observed a connection between robust optimization and generalization. Recently, this has been explored through the lens of distributionally robust optimization. e.g., in the papers of Namkoong and Duchi, but also many others, e.g., Farnia and Tse, etc. Primarily, this paper appears to build off the work of Namkoong. \n\nThe key connection this paper tries to make is between SGD and DRO, since SGD in sampling a minibatch, can be considered a small perturbation to the distribution. Therefore the authors use this intuition to propose a weighted version of SGD (WSGD) whereby high variance weights are assigned to mini batch, thus making the training accomplish a higher level of distributional robustness. \n\nThis idea is tested on a few data sets including CIFAR-10 and -100. The results compare WSGD with SGD, and they show that the WSGD-trained models have a lower robust loss, and also have a higher (testing) accuracy. \n\nThis is an interesting paper. There has been much discussion of the role of batch size, and considering it from a different perspective seems to be of interest. But the connection of the empirical results to the theoretical results seems tenuous. It’s not clear how predictions of the theory match up. This would be useful to understand better. More generally, a simpler presentation of the key results would be useful, so as to allow the reader to better appreciate what are the main claims and if they are as substantial as claimed. Overall the writing needs significant polishing, though this is only at a local level, i.e, it doesn’t obscure the flow of the paper. ', 'This paper motivates performing a “robustified” version of SGD. It attempts to formalize this notion, proves some variants of generalization bounds, and proposes an algorithm that claims to implement such a modified SGD.\n\nThe clarity of the paper can be improved. There are several notational and language ambiguities throughout. Most importantly, a couple of paragraphs that are meant to convey key intuitions about the results are very badly written (the one following Theorem 1, the one preceding Theorem 2, the last one in Section 5.1, and the one preceding Theorem 5, more on these later).\n\nApart from these clarity issues, the significance of the results is weak. This is because although the technical statements seem correct, the leap from them to measurable outcomes (such as actual generalization bounds) are missing. Part of this is due to a lack of a good notion of “true” robust risk. Moreover the algorithmic contribution does not connect well with the suggested theory and the experimental results are modest at best. Here is a more detailed breakdown of my objections.\n\n-\tThe notion of distributional robust loss is sound, i.e. R(\\theta, K). Its empirical variant is also good, \\hat{R}(\\theta, K). But the notion of robust loss defined in the paper, \\hat R_{S,P}(\\theta) with the weights on the samples, breaks from this notion. The reason is that in the former case the weights depend on the value of the sample (z) whereas in the latter they depend on the index (i). It is not evident how to map one to the other.\n\n-\tThis makes the question of what is the “true” robust risk unclear. It is tempting to simply say it is its expectation with respect to a generic sample. This is the view taken in Theorem 3, which offers a kind of generalization bound. But if one looks carefully at this, the location of the expectation and supremum should be swapped. Here is an alternative view: if want to think of the infinite sample limit, then we need to have a sequence of robustness classes P_m, that vary with m (say those that put weight only on a q-fraction of the samples, just like in the suggested WSGD). The “true” robust risk would be the limit of the sup of the empirical risk, this keeps the sup and expectation in the right order. Under the right conditions, this limit would indeed exist. And it is difficult to know, for a given m, how *far* the generic-sample expectation of Theorem 3 is from it. Without this knowledge, it is difficult to interpret Theorem 3 as a generalization bound.\n\n-\tTheorem 1 itself is a standard result. The discussion after Theorem 1 is the kind of argument that also explains Fisher information, and can be presented more clearly. I’m not sure whether Theorem 2 exactly proves what the paragraph before it is trying to explain. The fact that SGD converges into the ball seems contrived, since the quantities that we are trying to bound have nothing to do with the optimization method. If the optimum is within the ball (+/- something) then the same result should hold with the step size replaced with the (+/- something). So how does this explain escaping stationary points?\n\n-\tIf we accept Theorem 3 as a generalization bound, alongside with the Rademacher bounds of Theorem 4, I don’t think the paper treats the balance between the various terms adequately enough. In particular we see that the |P|_\\infty term in Theorem 3 has to balance out the (robust) Rademacher bound, and need it to be of the order of (1+RAD_2(P)\\sqrt{\\log N}/m). For P that puts weight k over 1/k points, |P|_\\infty = 1/k. RAD_2(P) is bounded by 1/\\sqrt{k}, so it’s negligible next to 1. But the covering number N can grow exponentially with k (when it’s not too large, and for small \\epsilon, just by counting arguments). So this seems to say that for a good tradeoff in the bounds will lead to k having to be a growing fraction of m. This intuition, if true, is not presented. Not only that, but it also goes against the suggested approach of choosing some constant fraction of m.\n\n-\tTheorem 5 gives a local Rademacher complexity. But again there is a conceptual step missing from this to strong generalization bounds, partly because we are not exactly minimizing the empirical risk within the considered class. Also, the discussion that follows bounding the rad_\\infty with |P|_\\infty is deficient, because it misses again the fact there are two salient terms that need to balance out.\n\n-\tAlgorithm 1 (WSGD) needs to specify (q,r) and which G (G_1 or G_2) as inputs too.\n\n-\tMost importantly, WSGD does not seem to be minimizing the robust risk at all. First, I’m not really sure what the G_1 variant does. If we were to follow the intuition of Theorem 1, we should be looking at the gradients, not the loss values. As for G_2, by sampling we are in fact replacing the sup over p with an average over P. This can have a significantly different behavior, and we could possibly interpret it as a slightly reduced effective batch size, especially in the case of G_2. In fact, in the experiments, when r is set to 0, this is exactly what is happening! At any rate, it is not clear at all how any of the earlier sections connect with sections 6 or 7.\n\n-\tIn the experimental section it is not clarified which of the latter two is used (I assume G_2, the randomized one, given the discussion in the end of Section 6.) When the authors write “accuracy improvement”, they should more clearly say “relative decrease in misclassification error”. That’s the only thing that makes sense with the numbers, and if it does in fact  the authors mistakenly say that the 5-15% improvement is for CIFAR 100 and the 5% is for CIFAR 10, it’s the other way around! And the exception (least) improvement seems to be ResNet-34 on CIFAR-100 (not VGG-16, as they claim, unless the table is wrong.) All in all, these are all pretty weak results, albeit consistent. A better benchmark would have been to compare against various batch sizes, and somehow show that the results do *not* follow from batch size effects.\n', 'The paper aims to connect ""distributionally robust optimization"" (DRO) with stochastic gradient descent. The paper purports to explain how SGD escapes from bad local optima and purports to use (local) Rademacher averages (actually, a  generalization defined for the robust loss) to explain the generalization performance of SGD.\n\nIn fact, the paper proves a number of disjointed theorems and does very little to explain the implications of these theorems, if there are any. The theorem that purports to explain why SGD escapes bad local minima does not do this at all. Instead, it gives a very loose bound on the ""robust loss"" under some assumptions that actually rule out ReLU networks.\n\nThe Rademacher results for robust loss looked promising, but there is zero analysis suggesting why these explain anything. Instead, there is vague conjecture. The same is true for the local Rademacher statements. It is not enough to prove a theorem. One must argue that it bears some relationship to empirical performance and this is COMPLETELY missing.\n\nOther criticisms:\n\n1. One of the first issues to arise is that the definition of ""generalization error"" is not the one typically used in learning theory. Here generalization error is used for what is more generally called the risk.  Generalization error often refers to the difference R(theta) - ^R(theta) between the risk and the empirical risk (i.e., the risk evaluated against the empirical distribution). (Generally this quantity is positive, although sometimes its absolutely values is bounded instead.)  \n\nAnother issue with the framing is that one is typically not interested in small risk in absolute terms, but instead small risk relative to the best risk available in some class (generally the same one that is being used as a source of classifiers). Thus one seeks small excess risk. I\'m sure the authors are aware of these distinctions, but the slightly different nomenclature/terminology may sow some confusion.\n\n2. The unbiased estimate suggested on page 2 is not strictly speaking an estimator because it depends on \\lambda_0, which is not measurable with respect to the data. The definition of K and how it relates to the estimate \\hat \\lambda is vague. Then the robust loss is introduced where the unknown quantity is replaced by a pre-specified collection of weights. If these are pre-specified (and not data-dependent), then it is really not clear how these could be a surrogate for the distribution-dependent weights appearing in the empirical distributionally robust loss.\n\nPerhaps this is all explained clearly in the literature introducing DRO, but this introduction leaves a lot to be desired.\n\n3. ""This interpretation shows a profound connection between SGD and DRO."" This connection does not seem profound to a reader at this stage of the paper.\n\n4. Theorem 2 seems to be far too coarse to explain anything. The step size is very small and so 1/eta^2 is massive. This will never be controlled by 1/mu, and so this term alone means that there is affectively no control on the robust loss in terms of the local minimum value of the empirical risk.\n\n5. There seems to be no argument that robustness leads to any improvement over nonrobust... at least I don\'t see why it must be true looking at the bounds. At best, an upper bound would be shown to be tighter than another upper bound, which is meaningless.\n\n\nCorrections and typographical errors:\n\n1. There are grammatical errors throughout the document. It needs to be given to a copy editor who is an expert in technical documents in English.\n\n2. ""The overwhelming capacity ... of data..."" does not make sense. The excessive complexity of the sentence has led to grammatical errors.\n\n3. The first reference to DRO deserves citation.\n\n4. It seems strange to assume that the data distribution P is a member of the parametric model M. This goes against most of learning theory, which makes no assumption as to the data distribution, other than the examples being i.i.d.\n\n5. You cite Keskar (2016) and Dinh (2017) around sharp minima. You seem to have missed Dziugaite and Roy (2017, UAI) and Neyshabur et al (NIPS 2017), both of which formalize flatness and give actual generalization bounds that side step the issue raised by Dinh.\n\n6. ""not too hard compared"" ... hard?\n\n7. Remove ""Then"" from ""Then the empirical robust Rademacher..."". Also removed ""defined as"" after ""is"".\n\n8. ""Denote ... as an"" should be ""Let ... denote the..."" or ""Denote by ... the upper ...""\n\n9. "" the generalization of robust loss is not too difficult"" ... difficult? \n\n10. ""some sort of “solid,” "" solid?\n\n11. ""Conceivably, when m and c are fixed, increasing the size of P reduces the set Θc"". Conceivably? So it\'s not necessarily true? I don\'t understand the role of conceivably true statements in a paper.\n\n[This review was requested late in the process due to another reviewer dropping out of the process.]\n\n[UPDATE] Authors\' response to my questions did not change my opinion about the overall quality of the paper. Both theory and writing need a major revision. ']","[50, -60, -80]","[75, 20, -20]","[""The sentiment score is 50 (slightly positive) because the reviewer describes the paper as 'interesting' and acknowledges its potential value, but also points out areas for improvement. The reviewer sees merit in the approach but feels the connection between theory and empirical results needs strengthening. The politeness score is 75 (fairly polite) because the reviewer uses respectful language throughout, acknowledging the paper's contributions while offering constructive criticism. Phrases like 'This is an interesting paper' and 'This would be useful to understand better' indicate a polite and professional tone. The reviewer provides specific suggestions for improvement without using harsh or dismissive language."", ""The sentiment score is -60 because the review is predominantly negative. The reviewer points out several issues with the paper, including clarity problems, weak significance of results, disconnection between theory and algorithm, and modest experimental results. Phrases like 'clarity of the paper can be improved', 'significance of the results is weak', and 'experimental results are modest at best' indicate a negative sentiment. However, the reviewer does acknowledge some positive aspects, such as 'technical statements seem correct' and 'notion of distributional robust loss is sound', which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional and academic tone throughout. They use phrases like 'can be improved' instead of more harsh language, and provide detailed explanations for their criticisms. The reviewer also uses hedging language like 'I'm not sure' and 'it is difficult to know', which softens the critique. However, some direct criticisms like 'very badly written' and pointing out mistakes in the authors' claims prevent the score from being higher."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper 'proves a number of disjointed theorems and does very little to explain the implications of these theorems,' and that key claims are not supported. They use strong negative language like 'COMPLETELY missing' and state that the authors' response 'did not change my opinion about the overall quality of the paper.' The politeness score is -20 because while the reviewer provides detailed feedback, the tone is quite harsh and dismissive in places. They use phrases like 'does not make sense,' 'far too coarse to explain anything,' and 'meaningless.' However, they do provide some constructive criticism and specific recommendations for improvement, which prevents the score from being even lower.""]"
"['This paper proposes a distributed policy gradient method for learning policies with large, collaborative, homogeneous swarms of agents. \n\nFormalism / objective: \nThe setting is introduced as a ""collaborative Markov team"", so the objective is to maximise total team reward, as expressed in equation (3). This definition of the objective seems inconsistent with the one provided at line (14): Here the objective is stated as maximising the agent\'s return, L_n, after [k] steps of the agent updating their parameters with respect to L_n, assuming all other agents are static. I think the clearest presentation of the paper is to think about the algorithm in terms of meta-learning, so I will call this part the \'inner loop\' from now on. \nNote (14) is a very different objective: It is maximising the return of an agent optimising \'selfishly\' for [k] steps, rather than the ""collaborative objective"" mentioned above. This seems to break with the entire premise of collaborative optimisation, as it was stated above. \nMy concern is that this also is reflected in the experimental results: In the food gathering game, since killing other agents incurs ""a small negative reward"", it is never in the interest of the team to kill other team-mates. However, when the return of individual agents is maximised both in the inner loop and the outer loop, it is unsurprising that this kind of behaviour can emerge. Please let me know if I am missing something here. \n\nOther comments: \n-The L_n(theta, theta_n) is defined and used inconsistently. Eg. compare line (9), L_n(theta_n, theta), with line below, L_n(theta, theta_n). This is rather confusing \n-In equation (10) please specific which function dependencies are assumed to be kept? My understanding is that \\theata_n is treated as a function of theta including all the dependencies on the policies of other agents in the environment? \n-Related to above, log( pi_\\theta_n ( \\tan_n)) in line 16 is a function of all agents policies through the joint dependency on \\theta. Doesn\'t that make this term extremely expensive to evaluate? \n-Why were the TRPO_kitchensink and A3C_kitchensink set up to operate on the minimum reward rather than the team reward as it is defined in the original objective? It is entirely possible that the minimum reward is much harder to optimise, since feedback will be sparse. \n-The survival game uses a discrete action space. I am entirely missing MARL baseline methods that are tailored to this setting, eg. VDN, QMIX, COMA etc to name a few. Even IQL has not been tried. Note that MADDPG assumes a continuous action space, with the gumble softmax being a common workaround for discrete action spaces which has not been shown to be competitive compared to the algorithms mentioned above. \n-Algorithmically the method looks a lot like ""Learning with Opponent Learning Awareness"", with the caveat that the return is optimised after one step of \'self-learning\' by each agent rather than after a step of \'Opponent-learning\'. Can you please elaborate on the similarity / difference? \n-Equation (6) and C1 are presented as contributions. This is the standard objective that\'s commonly optimised in MARL when using parameter sharing across agents.', 'The introduction of the paper is well-written and the authors quite clearly explain the purpose; however, I believe that the notation should be revisited to further simplify them. The algorithm pretty much is similar to the A2C algorithm (very minor differences) and overall, I don\'t see the contribution of the paper to be significant enough. Also, there are a few other concerns that I summarize next:\n\n1) In example 1, just knowing the relative distance with all other agents is not equivalent to knowing the full state of the environment. This is because the angles with the other agents are important; i.e. you need to know (r,\\theta) Polar Coordinates.\n\n2) I personally don\'t like using the word ""constrained\'\' used in this paper. Going back to constrained RL literature, the purpose of constrained RL is, for example, not entering the hazardous states. At the first time reading the paper, I thought that the constraints are referring to such cases, e.g. make sure that the agents never hit each other. But, the concept of constraint used in this work is totally different and it simply means copying the network weights.\n\n3) In section 3, using the neural networks and averaging of the weight does not make any sense. What does it mean to average weights of several NN? NN is a nonlinear function approximator, and you cannot average weights. Based on your algorithm, I see that you aggregate the gradients which is a correct approach. In fact, the projection step defined in page 5 is never used in your implementation I guess, because otherwise, this algorithm will not work.\n\n4) The distributed model pretty much resembles A2C algorithm where each agent can be considered as a thread. At every time, you only do a gradient step in one of the threads and for the rest, you use the central policy. This way, you stabilize the non-stationarity caused by concurrently learning policy. I do not see any major difference.\n\n5) What is the reason that you do not use the Critic?\n\n6) Having $\\theta_n=\\theta$ implies that $\\pi_n = \\pi$, but the other way around does not hold. Constraints of (8) are not equivalent to (5).\n\n7) Are you using different policies for different agents when using MADDPG or TRPO_Kitchensink? I think for a fair comparison, the agents should also share the policies in these algorithms too. It is very hard to believe that TRPO_Kitchensink and MADDPG almost learn nothing, or even learn in reverse direction (Fig 3).\n\n8) I think that the baseline comparisons for the case of having a small number of agents are necessary.\nMinor:\n* In section 3.1, the notations are over-populated. I would suggest simplifying the notations. \nIn (4), (5),(6), argmax_\\pi\n* (16) is simply the sum of gradients of two consecutive policy gradient steps which can be derived by (sum of grad = grad of sum). You might add this as an intuition beyond this formula.\n', ""## Summary\nThe authors present an approach to training collaborative swarms of agents based around giving all agents identical (or near identical) policies. The training regime involves individual agents rolling out trajectories based on slight perturbations of an agent of focus keeping the policy of other agents fixed. This is repeated for each agent, then these trajectories are used to batch update the joint policy with an average gradient.\n\nOn the whole, I think the paper is well written and the idea novel. There are places where the explanations could be clearer and details more explicit (see below for examples). There are some interesting evaluations but I am not sure these are as rigourous as they could be, in particular (but not limited to) the survival game. I am, however, recommending this for acceptance as on balance the positives outweigh the negatives.\n\n## More detailed comments\nThe authors could make it a bit clearer what existing work on averaging policy gradients exist, and whether their approach is a natural extention of these existing approaches to their swarm domain, or whether there is additional novelty there. It is unclear to me which is the case. They talk about meta-learning in the related work but it is unclear precisely how they relate this to their own work.\n\nThe authors could describe their experiments a little more explicitly. For instance, they say that agents are penalised for getting too close in the navigation task, but do not say how this penalty is constructed. Is it a step function based on distance or something else? Also, they should state what parameters they use for each of the environmental factors, e..g minimum distance etc.\n\nThe survival game is poorly described, as are choices for the evaluation of it. I realise these games are designed elsewhere, but if the exact same parameters are used as in the original papers then this should be stated. Finally, I am a little unclear why the survival game cannot be compared with other algorithms, even if those algorithms fail to learn anything. I realise that the algorithms with decentralised actors won't scale here, but something like the mean field approaches described by the authors in the related work, or even less sophisticated algorithms using some (but not all) features of their own approach would show something interesting. And the choices of 225 and 630 agents needs better justification.""]","[-50, -50, 50]","[20, 20, 70]","[""The sentiment score is -50 because the review is predominantly critical, pointing out several inconsistencies and issues with the paper. The reviewer expresses concerns about the objective function, inconsistent definitions, and missing baseline comparisons. However, it's not entirely negative as the reviewer asks for clarification and suggests the author might be missing something, indicating openness to explanation. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language such as 'Please let me know if I am missing something here' and 'Can you please elaborate on the similarity / difference?'. The tone is professional and constructive, focusing on the content rather than making personal comments. The reviewer also uses phrases like 'My concern is' and 'My understanding is' which soften the critique."", ""The sentiment score is -50 because the review is generally critical, pointing out several significant issues with the paper. The reviewer states that they 'don't see the contribution of the paper to be significant enough' and lists multiple concerns. However, it's not entirely negative as they do acknowledge some positive aspects, such as the well-written introduction. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I believe' and 'I would suggest' which soften the critique. The reviewer also provides constructive feedback and suggestions for improvement, which is a polite approach to criticism. However, some statements are quite blunt, such as 'does not make any sense', which prevents a higher politeness score."", ""The sentiment score is 50 (slightly positive) because the reviewer recommends acceptance and states 'the positives outweigh the negatives', but also points out several areas for improvement. The overall tone is constructive rather than strongly positive or negative. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offering suggestions rather than harsh criticisms. Phrases like 'The authors could make it a bit clearer' and 'I am a little unclear why' indicate a polite approach to feedback. The reviewer also acknowledges positives like 'well written' and 'novel idea' before diving into critiques, which is a polite reviewing practice.""]"
"['This problem of interest in this paper is Curriculum Learning (CL), in the context of deep learning in particular. CL refers to learning a non-random order of presenting the training examples to the learner, typically with easier examples presented before difficult ones, to guide learning more effectively. This has been shown to both speed up learning and lead to better generalization, especially for more challenging problems. In this paper, they claim that their contribution is to decompose the problem of CL into learning two functions: the scoring function and the pacing function, with the role of the former being to estimate the difficulty of each training example and the latter to moderate the schedule of presenting increasingly more challenging examples throughout training.\n\nOverall, I found it hard to understand from reading the paper what exactly is new versus what is borrowed from previous work. In particular, after reading Weinshall et al, I realized that they have already proposed a number of things that are experimented with here: 1) they proposed the approach of transfer learning from a previously-trained network as a means of estimating the ‘scoring function’. 2) they also distinguish between learning to estimate the difficulty of examples, and learning the schedule of decreasing difficulty throughout learning, which is actually stated here as the contribution of this paper. In particular, in Section 3 of Weinshall et al, there is a sub-section named “scheduling the appearance of training examples” where they describe what in the terminology of this paper would be called their pacing function. They experiment with two variants: fixed, and adaptive, which are very similar to two of the pacing functions proposed here.\n\nBootstrapping:\nA component of this work that didn’t appear in Weinshall et al, is the bootstrapping approach to estimating the scoring function. In general, this involves using the same network that is being trained on the task to estimate the difficulty of the training examples. The authors explain that there are two ways to do this: estimate how easy each training example is with respect to the ‘current hypothesis’ (the weights of the network at the current step), and with respect to the ‘final hypothesis’, which they estimate if I understand correctly as the network at the end of training. The latter would necessitate first training the network in the standard way, and then using it to estimate how easy or hard each example is, and using those estimates to re-train the network from scratch using that curriculum. They refer to the former as self-paced learning and to the latter as self-taught learning. I find these names confusing in that they don’t really convey what the difference is between the two. Further, while self-paced learning has been studied before (e.g. Kuman et al), I’m not sure about self-taught learning. Is this a term that the authors here coined? If not, it would be useful to add a reference. \n\nUsing easy / hard examples as judged by the current / final hypothesis:\nWhen using the current hypothesis, under some conditions, Weinshall et al showed that choosing harder examples is actually more beneficial than easy examples, similar in spirit to hard negative mining. On the other hand, when using the final hypothesis to estimate examples’ difficulty, using a schedule of increasing difficulty is beneficial. Based on this, I have two comments: 1) It would therefore be useful to implement a version that uses the current hypothesis to estimate how easy each example is (like the self-paced scoring function) but then invert these estimates, in effect choosing the most challenging instead of the easiest ones as is done for anti-curriculum learning. This would be a hybrid between the current self-paced scoring function and anti-curriculum scoring function that would essentially implement the hard negative mining technique in this context. 2) It would be useful to comment on the differences between the self-paced scoring function used here, and that in Kumar et al. In particular, in this case using a curriculum based on this scoring function seems to harm training but in Kumar et al, they showed it actually increased performance in a number of different cases. Why does one work but the other doesn’t?\n\nExperiments:\nThe experiments are presented in a subset of 5 classes from CIFAR-10 (also used by Weinshall et al.), but also in the full CIFAR-10 and CIFAR-100 datasets. They used both a small CNN (same as in Weinshall et al) as well as a VGG architecture. Overall, their results are comparable to what was previously known: using a curriculum computed by transfer leads to improved learning speed and final performance (though sometimes very slightly) compared to the standard training, and the training with a random curriculum. Further, the benefit is larger when the task is harder (as measured by the final vanilla-trained performance). By computing the distances between the gradients obtained from using a curriculum (via the transfer scoring function) and no curriculum confirms that these two training setups indeed drive the learning in different directions; an analysis similar to Weinshall et al. Also, since, as was previously known and they also observe, the benefit of CL is larger at the beginning of training, they propose a single-step pacing function that performs similarly to other pacing functions while is simpler and more computationally effective. The idea is to decrease only once the proportion of easy examples used in mini-batches, via a step function. Therefore at the start many easy examples are used, and after this threshold is surpassed, few easy examples are used.\n \nOverall, I don’t feel the contribution of this paper is large enough to recommend acceptance. The main points that guided this decision are: \n1) The relationship with previous work is not clear. In particular, Weinshall et al seems to have already proposed a few components that are claimed to be the contribution of this paper, as elaborated on above. The authors should mention that the transfer scoring function was borrowed from Weinshall et al, clarify the differences between their pacing functions from those in Weinshall et al., etc. \n2) The usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently. An additional experiment is proposed above to add another ‘data point’ to this discussion. \n3) self-paced learning is presented as something that doesn’t work and wasn’t expected to work. However, in the past successes were shown with this method, so it would be useful to clarify the difference in setup, and justify this difference.\n4) It seems that the experiments resulted to similar conclusions to what was already known. While it’s useful to confirm these findings on additional datasets, I didn’t feel that there was a significant insight gained from them.\n', 'This paper studies an interesting and meaningful topic that what is the potential of curriculum learning (CL) in training dnn.  The authors decompose CL into two main parts: scoring function and pacing function. Towards both parts, several candidate functions are proposed and verified.  The paper is presented quite clear and gives contribution to better understand CL in the literature of DNN.\n\nHowever, I have several concerns towards the status of this paper.\n\nFirst, quite a few important related works are missing by the authors. Just name a few, [1] studies designing data curriculum by predictive uncertainty. [2,3] studies how to derive data driven curriculum along NN training. In particular, the objective of [2] is exactly “learning the right examples at the right time”. All these three papers focus on, or at least talk about, neural network training. Unfortunately, none of them are compared with, or even referenced. \n\nSecond, although comprehensive study towards different curriculum strategy are given, I found it largely unconvincing. I tried hard to discover a *detailed accuracy number on a benchmark dataset with unchanged setting* but found only case 4. By ‘unchanged’ I mean it is not a subpart of the whole dataset, or using a rarely seen nn architecture.  If it is such `changed’ settings, the results are largely unconvincing since we do not know what the exact baseline is. For the only ‘unchanged’ setting 4 including VGG on CIFAR100, unfortunately the results seem not good (Fig 4a). I understand that some previous work such as the cited [Weinshall et.all 2018] also used the same setting: however it does not mean such settings give *clear and convincing* results of whether CL plays significant role in training DNN. Furthermore, I also expect the results of comparing in terms of wall clock time (including all your bootstrapping training time) but not merely batch numbers. \n\n[1] Chang, Haw-Shiuan, Erik Learned-Miller, and Andrew McCallum. ""Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples."" NIPS. 2017.\n\n[2]  Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018\n\n[3] Jiang, Lu, et al. ""MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels."" ICML. 2018.\n', ""In my opinion this paper is generally of good quality and clarity, modest originality and significance.\n\nStrengths:\n- The experiments are very thorough. Hyperparameters were honestly optimized. The method does show some modest improvements in the experiments provided by the authors.\n- The analysis of the results is quite insightful.\n\nWeaknesses:\n- The experiments are done on CIFAR-10, CIFAR-100 and subsets of CIFAR-100. These were good data sets a few years ago and still are good data sets to test the code and sanity of the idea, but concluding anything strong based on the results obtained with them is not a good idea.\n- The authors claim the formalization of the problem to be one of their contributions. It is difficult for me to accept it. The formalization that the authors proposed is basically the definition of curriculum learning. There is no novelty about this.\n- The proposed method introduces a lot of complexity for very small gains. While these results are scientifically interesting, I don't expect it to be of practical use.\n- The results in Figure 3 are very far from the state of the art. I realize that they were obtained with a simple network, however, showing improvements in this regime is not that convincing.  Even the results with the VGG network are very far from the best available models.\n- I suggest checking the papers citing Bengio et al. (2009) to find lots of closely related papers. \n\nIn summary, it is not a bad paper, but the experimental results are not sufficient to conclude that much. Experiments with ImageNet or some other large data set would be advisable to increase significance of this work. ""]","[-60, -20, 20]","[20, 60, 50]","[""The sentiment score is -60 because the reviewer expresses significant concerns about the paper's contribution and novelty. They state that they 'don't feel the contribution of this paper is large enough to recommend acceptance' and list several major issues. However, it's not entirely negative as they acknowledge some useful aspects of the work. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I found it hard to understand' and 'It would be useful to' rather than harsh criticism. They also acknowledge the positive aspects of the work. However, the review is not overly polite or effusive, maintaining a mostly neutral, academic tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting topic and clear presentation in the first paragraph, they express several significant concerns in the subsequent paragraphs. These concerns include missing important related works and unconvincing results, which outweigh the initial positive comments. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and using phrases like 'I have several concerns' and 'I found it largely unconvincing' rather than harsh criticism. The reviewer also provides specific suggestions and references to improve the paper, which is a polite and constructive approach to peer review."", ""The sentiment score is slightly positive (20) because the reviewer starts by stating the paper is 'generally of good quality and clarity' and lists several strengths. However, they also mention significant weaknesses and areas for improvement, which tempers the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging both strengths and weaknesses without harsh criticism. They offer constructive suggestions and use phrases like 'in my opinion' and 'I suggest', which maintain a polite tone. The review is balanced and professional, avoiding overly negative or confrontational language even when pointing out limitations.""]"
"['This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., “evolutionary layer”. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.\nThe paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.\n-\tIn page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. \n-\tGiven the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.\n-\tBased on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a naïve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. \n-\tThere are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. \n*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. \n*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.\n*** Less Parameter Tuning. Again experiments could clarify this issue.\n*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. \n-\tTo calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 ‘Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis’). \n-\tOne of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 ‘Massive Multimodality, Deception, and Genetic Algorithms’). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.\n', 'This paper proposes to subsample a large network into sub-networks, learn a network model (autoencoder) from each subgraph, perform crossover and mutation operations over the network parameters of different model pairs, and combine the latent representations following the ensemble idea.\n\nThe paper is clearly presented. Originality and significance is limited. Putting the three knowns components - subsampling, generation algorithm and ensembling together seems to be the main contribution of this paper. However, the ways of doing subsampling, performing the crossover and mutation operations and doing the ensembling are relatively straightforward ways of applying them. The fact that combining them to obtain better results is not a surprising result. And according to the experimental results, it is not clear how the gain in performance is resulted and to what extent each of the three components is contributing. For instance, I just guess the combination of subsampling + existing network embedding methods (LINE/DeepWalk/...) + ensembling may also give good results. Currently, the performance comparison is done with the original forms of LINE and DeepWalk. That makes the empirical results not very convincing to explain the key strengths of this work.\n\n+ve:\n`1. The paper is clearly presented.\n2. The design is reasonable one.\n3. A number of benchmark datasets are used for the evaluation.\n\n-ve:\n1. The originality and significance is limited.\n2. The performance comparison should be done with references to more competitive candidates as explained above.\n3. The nodes of different sub-networks are essentially projected to different embedding spaces. The validity and interpretation of performing the crossover operation on two different models (two different embedding spaces) will need more justifications.\n4. The proposed methodology is not an end-to-end. The ensembling being evaluated is just simple addition.\n5. The paper claims that ""The unit learning model, genetic algorithm and ensemble learning can all provide the theoretic foundation for SEGEN, which will lead to sound theoretic explanation of both the learning result and the SEGEN model itself"". Individually being sound does not imply that the way to combine them is sound. Currently, I cannot see the uniqueness of this particular combination.\n', 'The paper introduces Sample-Ensemble Genetic Evolutionary Network, which adopts a genetic-evolutionary learning strategy to build a group of unit models. Explanation on the evolutionary network part is not enough. For example, there is no clear explanation on how chromosomes are defined. Also, detailed analysis on computational aspect is needed.']","[-50, -40, -20]","[50, 20, 0]","[""The sentiment score is -50 because while the reviewer acknowledges that the paper 'touches an interesting topic and proposes a sound method', they also state that 'there are several issues with the paper' and proceed to list multiple criticisms and unaddressed claims. The overall tone is more negative than positive, but not entirely dismissive. The politeness score is 50 because the reviewer uses professional and respectful language throughout, avoiding personal attacks or harsh criticism. They acknowledge positive aspects before presenting their concerns and use phrases like 'It is not clear' or 'There are no discussions/experiments' rather than directly accusing the authors of incompetence. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the polite tone."", ""The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('clearly presented', 'design is reasonable', 'benchmark datasets used'), the overall tone is critical. The reviewer states that 'originality and significance is limited' and raises several concerns about the methodology and results. The politeness score is 20 because the reviewer uses professional language and provides both positive and negative feedback. They offer constructive criticism without using harsh or rude language. The reviewer maintains a respectful tone while expressing their concerns, using phrases like 'will need more justifications' and 'I cannot see the uniqueness' rather than more confrontational language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's introduction of a new method, they point out significant shortcomings in the explanation and analysis. The phrases 'not enough' and 'no clear explanation' indicate dissatisfaction with the paper's content. However, the criticism is not severe, hence only a slightly negative score. The politeness score is neutral (0) as the language used is neither particularly polite nor rude. The reviewer states their observations directly without using overly harsh or particularly courteous language. The tone is professional and matter-of-fact, which is typical for academic peer reviews.""]"
"['In the paper , the authors proposed the use of autoencoder for Model-X knockoffs. The authors proved that, if there exists latent factors, and if the encoders and the decoders can approximate conditional distributions well, the autoencoder can be used for approximating Model-X knockoff random variables: one can find relevant features while controlling FDR (Theorem 2.2).\n\nI think the theoretical part is good, and the experimental results seem to be promising.\n\nMy concern is the gap between theory and practice. In the manuscript, the authors used VAE for approximating conditional distributions. The question is how we can confirm that the trained encoder and decoder satisfy the assumptions in Theorem 2.2. If the trained models violate the assumptions, the control of FDR is no longer guaranteed, which may lead to false discoveries. As long as this gap remains unfilled, we cannot use the procedure reliably: we always need to doubt whether the encoders and decoders are trained appropriately or not. I think this gap is unfavorable for scientific discovery where only rigid procedures are accepted.\nHow we can use the proposed procedure reliably, e.g. for scientific discovery? Is there any way to confirm that the encoders and decoders are appropriate? Or, is there any way to bypass the gap so that we can guarantee the FDR control even for inappropriate models?', 'This manuscript tackles an important problem, namely, generating the knockoff procedure for FDR-controlled feature selection so that it can work on any data, rather than only for data generated from a Gaussian (as in the original work) or several specific other cases (mixtures of Gaussians or HMMs).  The basic idea is to exploit ideas from variational autoencoders to create a generic knockoff generation mechanism. Specifically, the main idea of the paper is to map the input covariates X into latent variable Z using a variational auto-encoder, generate the knockoffs \\tilde{Z} in the latent space, and then map \\tilde{Z} back to the input space to get the knockoffs \\tilde{X}. The authors claim that their contribution in threefold: \n\n(1) Given the assumption is valid that X is mutually independent conditional on Z, the generated knockoffs \\tilde{X} is proved to be valid in terms of satisfying the necessary swap condition. \n\n(2) Given (1) holds, and given that the discrepancy (measured by KL-divergence) between the true conditional probability Q(Z|X) and its estimate using variational auto-encoder is bounded by o((nlogp)^{-1/2}), the FDR is also bounded.\n\n(3) The proposed knockoffs generating procedure can achieve controlled FDR and better power. \n\nIn agreement with the above intuition, I have major concerns about the correctness of the paper.\n\nThe cornerstone of the proof in contribution (1) relies on the assumption that X is mutually independent conditional on Z. However, this assumption is invalid if there are dependencies between x_i and x_j. Therefore, the proposed knockoffs \\tilde{X} cannot be proved valid by Theorem 2.1.\n\nThe erroneous proof in contribution (1) leads to the failure of contribution (2) stated in Theorem 2.2. The FDR is no longer controlled. Intuitively, according to algorithm 1, \\tilde{Z} and \\tilde{X} are expected to have the same distribution as Z and X, respectively; therefore, the FDR cannot be controlled.\n\nThe experimental results are suspicious. In general, it seems fishy that the proposed VAE approach outperforms Model X in the fully Gaussian case.  In this setting, Model X should have an advantage, since it is specifically designed for Gaussian generated data.  Related to this, the text is confusing: ""Since the data were not Gaussian, the second-order matching method has the lowest power. The assumptions of the Fixed knockoff generations holds for the Gaussian cases, …"" In the figure, the second-order matching method has low power even in the Gaussian case.\n\nMinor comments:\n\np. 2: The exposition should explain the motivation for Knockoff+.\n\nThe manuscript contains numerous grammatical errors, a few examples of which are below:\n\np. 1: ""biological linked"" -> ""biologically linked""\n\np. 1: ""associated certain"" -> ""associated with a certain""\n\np. 1: ""showed light"" -> ""shed light""\n\np. 1: ""which has very limited"" -> ""which has limited""\n\np. 1: ""leveraging on the power of of"" -> ""leveraging the power of""\n\n\n', 'This paper proposes a new approach to construct model-X knockoffs based on VAE, which can be used for controlling the false discovery rate. Both numerical simulations and real-data experiments are provided to corroborate the proposed method.  \n\nAlthough the problem of generating knockoffs based on VAE is novel, the paper presentation is not easy to follow and the notation seems confusing. Moreover, the main idea of this paper seems not entirely novel. The proposed method is based on combining the analysis in \'\'Robust inference with knockoffs\'\' by Barber et. al. and the VAE.  \n\nDetailed comments:\n\n1. The presentation of the main results is a bit short. Section 2, the proposed method, only takes 2 pages. It would be better to present the main results with more details. \n\n2. The method works under the assumption that there exists a random variable $Z$ such that $X_j$\'s are mutually independent conditioning on $Z$. Is this a strong assumption? It seems better to illustrate when this assumption holds and fails.\n\n3. The notation of this paper seems confusing. For example, the authors did not introduce what $(X_j, X_{-j}, \\tilde X_j, \\tilde X_{-j} )$ means. Moreover, in Algorithm 1, what is $\\hat \\theta$ and $\\hat f$. \n\n4. I think there might be a typo in the proof of Theorem 2.1. In the main equation, why $\\tilde Z$ and $\\tilde X$ did not appear? They should show up somewhere in the probabilities.\n\n5. In Theorem 2.2, how strong is the assumption that $\\sup_{z,x} | log (density ratio)| $ is smaller than $\\alpha_n$? Usually, we might only achieve nonparametric rate for estimating the likelihood ratios. But here you also take a supremum, which might sacrifice the rate. The paper suggested that $\\alpha_n$ can be o( (n \\log p)^{-1/2}). When can we achieve such a rate?\n\n6. Novelty. Theorem 2.2 seems to be an application of the result in Barber et. al. Compared with that work, this paper seems to use VAE to construct the distribution $ P_{\\tilde X| X}$ and its analysis seems hinges on the assumptions in Theorem 2.2 that might be stringent.\n\n7. In Figure 1 and 2, what is the $x$-axis?\n\n8. A typo: Page 2, last paragraph. ""In this paper, we relaxes the ...""']","[20, -70, -30]","[80, 20, 50]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the good theoretical part and promising experimental results. However, they express significant concerns about the gap between theory and practice, which prevents a higher positive score. The politeness score is high (80) as the reviewer uses respectful language throughout, phrases criticisms as concerns rather than attacks, and asks constructive questions. They use phrases like 'I think' to soften their critique and maintain a professional tone. The reviewer also balances positive comments with their concerns, showing a fair and considerate approach."", ""The sentiment score is -70 because the reviewer expresses 'major concerns about the correctness of the paper' and points out significant flaws in the main contributions. They use phrases like 'erroneous proof', 'failure of contribution', and 'suspicious' results, indicating a strongly negative view of the paper's content. However, the review does begin by acknowledging the importance of the problem tackled, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I have major concerns' rather than more aggressive language. The reviewer also provides constructive feedback and minor comments to improve the paper. However, the use of words like 'fishy' and 'suspicious' slightly reduces the politeness score, as does the directness of some criticisms."", ""The sentiment score is -30 because while the reviewer acknowledges the novelty of the approach, they express several concerns about the paper's presentation, clarity, and overall novelty. The reviewer states that the paper is 'not easy to follow', the notation is 'confusing', and the main idea 'seems not entirely novel'. These criticisms outweigh the initial positive comment, resulting in a slightly negative overall sentiment. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'It would be better to' and 'I think' when offering suggestions, and provide specific, detailed feedback without using harsh language. The reviewer also balances criticism with acknowledgment of the paper's strengths. However, the score is not higher as the review is quite direct in its criticisms, without much softening language.""]"
"[""## Summary ##\n\nThe authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem.\n\n\n## Assessment ##\n\nI don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. \n\n I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.\n\nThey both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n\n\n## Specific Comments and Questions ##\n\n1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$.\n3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this?\n4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently.\n5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. \n6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar."", ""The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process). They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution. They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3.\n\nI think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method. The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult. The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer. I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.\n\nAdditionally, I find the motivation for caring about local optimality unconvincing. I take exception that people care more about local optimality than the actual objective. From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself). This also holds for k-means, which is usually run multiple times with different starting conditions.\n\nSome comments:\n- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample. e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)\n- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).\n- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)\n- I would also encourage the authors to come up with a more descriptive name for the approach.\n"", 'The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure. The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task.\n\nThe proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper. The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution. Can the authors elaborate on this? However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions. The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer. ']","[-50, -50, -20]","[20, 50, 50]","[""The sentiment score is -50 because the reviewer expresses significant doubts about the paper's novelty and methodology. They state 'I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR' and raise several concerns about the experimental setup and assumptions. However, it's not entirely negative as they acknowledge some positive aspects and provide constructive feedback. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I think they should' and 'It would be nice to see' which are polite ways of suggesting improvements. They also provide specific, detailed feedback which is helpful to the authors, even if critical. The language is not overly formal or deferential, but it avoids rudeness or harsh language."", ""The sentiment score is -50 because the reviewer expresses several criticisms and concerns about the paper, such as the lack of strong experimental results, unconvincing motivation, and confounding variables. However, it's not entirely negative as the reviewer also provides constructive feedback and suggestions for improvement. The politeness score is 50 because the reviewer uses respectful language throughout, offering suggestions and expressing their thoughts in a professional manner. They use phrases like 'I think,' 'I would like,' and 'I would encourage,' which maintain a polite tone while still conveying their critiques. The reviewer also acknowledges the authors' arguments and provides specific, constructive feedback, which contributes to the overall politeness of the review."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's proposal, they express several limitations and concerns. They note that the approach is very similar to existing methods, limiting the paper's contributions. They also question the effectiveness of the proposed sampling distribution and point out that the empirical evaluation is limited to only one task. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'Can the authors elaborate on this?' which shows a willingness to engage in dialogue rather than outright dismissal. The reviewer also acknowledges the authors' own observations about potential improvements. The language is neither overly formal nor informal, striking a balance that is appropriate for academic discourse.""]"
"['In the manuscript entitled ""Neural Causal Discovery with Learnable Input Noise"" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).  The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (""learnable noise risk"") with a flexible functional approximation (neural network).  Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.  The simulation and real data experiments are interesting and seem well applied.\n\nA concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn\'t entirely illustrate its relationship to either.  In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.  Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.  In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.  Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).  ', 'This paper aims to estimate time-delayed, nonlinear causal influences from time series under the causal sufficiency assumption. It is easy to follow and contains a lot of empirical results. Thanks for the results, but I have several questions.\n\nFirst, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0. In order to correctly estimate causal relations from data, both cases must be considered.\n\nSecond, the conclusion of Theorem 2 seems to be flawed. Let me try to make it clear with the following example. Suppose x^1_{t-2} directly causes x^2_{t-1} and that x^2_{t-1} directly causes x^3_{t}, without a direct influence from x^1_{t-2}  to x^3_{t}. Then when minimizing (2), we have the following results step by step:\n1) The noise standard deviation in x^2_{t-1}, denoted by \\eta_2, may be non-zero. This is because we minimize a tradeoff of the prediction error (the first term in (2)) and a function of the reciprocal of the noise standard deviation \\eta_2 (the second term in (2)), not only the prediction error.\n2) If \\eta_2 is non-zero, then x^1_{t-2} will be useful for the purpose of predicting x^3_{t}. (Note that if \\eta_2 is zero, then x^1_{t-2} is not useful for predicting x^3_{t).) From the d-separation perspective, this is because x^1_{t-2} and x^3_{t} are not d-separated by x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, although they are d-separated by x^2_{t-1}. Then the causal Markov condition tells use that x^1_{t-2} and x^3_{t} are not independent conditional on x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, which means that x^1_{t-2} is useful for predicting x^3_{t}.\n3) Given that x^1_{t-2} is useful for predicting x^3_{t}, when (2) is minimized, \\eta_1 will not go to infinity, resulting in a non-zero W_{13), which *mistakenly* tells us that X^{1}_{t-1} directly structurally causes x^{(3)}_t.\n\nThis illustrates that the conclusion of Theorem 2 may be wrong.  I believe this is because the proof of Theorem 2 is flawed in lines 5-6 on Page 16. It does not seem sensible to drop X^{j}_{t-1} + \\eta_X \\cdot \\epsilon_X and attain a smaller value of the cost function at the same time. Please carefully check it, especially the argument given in lines 10-13.\n\nThird, it is rather surprising that the authors didn\'t mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993). Such methods are directly applicable to time-delayed causal relations by further considering the constraint that effects temporally follow the causes. \n\nFourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression. For instance, if x^j_{t-1} influences only the variance of x^i_{t}, but not its mean, then the proposed method may not detect such a causal influence, although the constraint-based methods can.\n\nAny response would be highly appreciated.', 'The paper proposes an approach to learn nonlinear causal relationship from time series data that is based on empirical risk minimization regularized by mutual information.  The mutual information at the minimizer of the objective function  is used as causal measure.\nThe paper is well written and the proposed method well motivate and intuitive. \n\nHowever I am concerned by the assumption that the lagged variables X_{t-1}^{(j)} follow a diagonal gaussian distribution. This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.\n\nAnother key concern concerns scalability.  The authors mention gene regulatory networks , neuroscience etc as key applications. Yet the experiments considered in the paper are limited to very few time series. For instance the simulation experiments use  N=30,  which is much smaller than the number of time series usually involved say in gene regulatory network data.  The real data experiments use N= 6 or N=2. This is way to small. \n\nThe real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.  How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?']","[20, -50, -20]","[60, 50, 60]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting aspects of the work and its potential applications, but also expresses concerns about the manuscript's positioning and clarity. The reviewer describes the method as 'novel' and the experiments as 'interesting and seem well applied,' which are positive points. However, the concerns raised about the manuscript's relationship to different fields and the lack of clarity on the value of learnt relationships balance out the positive aspects, resulting in a mildly positive overall sentiment.\n\nThe politeness score is moderately high (60) because the reviewer uses respectful and professional language throughout. They begin by summarizing the work objectively and acknowledge its strengths before presenting their concerns. The concerns are framed as suggestions for improvement rather than harsh criticisms. Phrases like 'A concern I have is...' and 'it would seem valuable to...' indicate a constructive approach to feedback, maintaining a polite tone throughout the review."", ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('It is easy to follow and contains a lot of empirical results'), the majority of the review focuses on critical points and potential flaws in the paper's methodology and conclusions. The reviewer raises several significant concerns, particularly about Theorem 2, which is described as potentially 'flawed' and 'wrong'. This indicates a generally negative sentiment towards the paper's core arguments.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They begin with a positive acknowledgment and use polite phrases like 'Thanks for the results' and 'Any response would be highly appreciated'. The criticisms are presented as questions or observations rather than direct attacks. The reviewer also uses phrases like 'Let me try to make it clear' and 'Please carefully check it', which suggest a constructive approach rather than a harsh critique. However, the score is not higher because the review is still quite critical and doesn't include many overtly polite expressions."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'well motivate and intuitive'), they express significant concerns about key assumptions and limitations of the study. The reviewer points out issues with the assumption of diagonal Gaussian distribution and scalability, which are presented as 'key concerns'. The real data experiments are described as 'not very convincing'. These criticisms outweigh the initial positive comments, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (60) because the reviewer uses respectful and professional language throughout. They begin with positive observations before presenting their concerns, which is a polite approach. The criticisms are presented as concerns rather than harsh judgments, using phrases like 'I am concerned' and 'This appears to be very restrictive' instead of more confrontational language. The reviewer also asks questions at the end, inviting further discussion rather than making definitive negative statements.""]"
"['This paper studies the problem of optimizing non-decomposable metric in classification. This topic has been discussed in several recent works mainly under deterministic classifier context, the authors discuss the possibility of training a neural network and learn the model by gradient-based methods, which could result in randomized classifier; and conducted experiments to compare the performance with other existing methodologies. I have the following concerns after reading it.\n\n1.The main idea of the paper has shown in other related works and the authors didn’t convince me why their work solves something that could not be solved in existing work. The related work section missed some relevant recent work including Ref[1], in which the method is also gradient-based and can be applied to neural networks. The well-behaved notion used in Definition 2 seems much weaker than the assumptions shown in Ref[1,2] to guarantee existence or uniqueness of the Bayes classifier, the authors could spend some effort to discuss why they require less assumptions.\n\n2.For the theory part, all the convergence results are proved in an asymptotic way without further discussion in the sample complexity. This becomes problematic for this work because (as shown in eq (7)) mini batch size goes to infinity is an unrealistic assumption in neural network training. Also when the class is unbalanced, empirical mean converging to population also slows down significantly which is required in Eq (4) and other places. I would like to see more discussion on the sample complexity either theoretically or experimentally.\n\n3.The experiments lack details for reproducing the results or generalizing the gain to other problems. For example, batch size, learning rate or how the size of the network influence the performance metrics. This information will be useful for others who want to apply the proposed method.\n \nThere are some minor formatting issues like the leading space in \\citep. Please fix those.\n\nBased on the above reasons, I’ll give this paper a 5.\n\n[Ref 1] Yan, B., Koyejo, S., Zhong, K. & Ravikumar, P.. (2018). Binary Classification with Karmic, Threshold-Quasi-Concave Metrics. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:5531-5540\n[Ref 2] Narasimhan, H., Kar, P., & Jain, P. (2015, June). Optimizing non-decomposable performance measures: a tale of two classes. In International Conference on Machine Learning (pp. 199-208).', 'Update: I still feel that the paper should have either strong theory, strong experiments, or some of each to be accepted, but that both are lacking. The revisions required would be too great for acceptance at this time. \n\nOriginal review:\nThe paper proposes a general method to optimize for performance metrics which can be written in terms of the entries of the confusion matrix. The idea is to approximate the entries of the confusion matrix using their expected values for a randomized classifier, plug these estimates into the formula for the desired metric, and optimize that quantity. This is a compelling idea but it needs more support than the theoretical or experimental sections give. \n\nThe simplicity and generality of the method are appealing. Smooth surrogates derived from randomized classifiers have been considered in the context of accuracy [1] and other performance measures [2, 3] and the paper should include some discussion of this prior work, but to my knowledge the broad applicability to non-decomposable and non-differentiable metrics expressible in terms of the confusion matrix is new. \n\nThe theoretical sections could use some improvement. It is worth mentioning that the loss obtained with the proposed method is nonconvex. The first equation in theorem 1 is described with “... where convergence in probability is entry-wise”, when the equation refers to almost sure convergence for a scalar, not convergence in probability for entries of a matrix. \n\nNo convergence rates are given, only asymptotic almost sure convergence as the size of the dataset or the minibatch goes to infinity. For finite datasets these statements are obvious, and while convergence is reassuring for infinite datasets, I imagine the rates will look very different for the loss (a scalar) and the gradient (which may have millions of coordinates). Theorem 3 considers the generalization of a single classifier which is independent of the empirical sample, which makes it irrelevant to cases where the model is learned. Theorem 4, which seeks to give a uniform bound over the model class, only shows that generalization occurs in the limit of infinitely much data (which is not surprising or particularly interesting).\n\nThe experimental section compares the algorithm against a well-known and strong baseline, but without any information about the variance of the results and only for a deep network. Several questions remain: Where the proposed method improves over the baseline, is this improvement due to the new method or the interaction between the method and the model? How would the method perform on e.g. a linear model, which is better understood? How do the results depend on batch size, which affects the bias in the gradients? \n\n[1] Roux, Nicolas Le. ""Tighter bounds lead to improved classifiers."" arXiv preprint arXiv:1606.09202 (2016).\n[2] Mozer, Michael C., et al. ""Prodding the ROC curve: Constrained optimization of classifier performance."" Advances in Neural Information Processing Systems. 2002.\n[3] Goh, Gabriel, et al. ""Satisfying real-world goals with dataset constraints."" Advances in Neural Information Processing Systems. 2016.\n', ""This paper proposes a gradient-based learning for F1 measure under the utility maximization framework. F1 is a widely used evaluation metric in information retrieval and machine learning, and it is hard to optimize as it is non-decomposable and non-differentiable. This research direction is hence extremely interesting. \n\nThe paper is well organized and easy to follow. The general methodology seems sound. Below are some detailed comments.\n\n- Page 1, Section 2.1. The notation of the probabilistic classifier is not typed correctly.  \n\n- Page 7. The result strongly depends on how well Eq. (5) holds. Two critical assumptions regarding the data are made here, (1) D -> ∞ and (2) B -> ∞. The first assumption is implicitly confirmed in the experiments, as in Table 1 the proposed method outperforms when the sample size is big. I am a little bit puzzled about the second assumption though. Eq. (7) holds, (and consequently Eq. (5)) when B -> ∞, but it cannot be the case in practice, since B tends to have moderate sizes. I wonder how this impacts the results. Batch size isn't discussed at all in the experiments. The discussion on noise control is nice, but it doesn't contribute to the validation of Eq. (5) or Eq. (7).\n\n- Algorithm 1 & 2. It may be a good idea to be explicit what the outputs of the algorithms are. The algorithms are referenced by their section numbers instead of their algorithm numbers. \n\n- The experimental section can be extended. The paper has extensively discussed other well-behaved metrics and tasks beyond binary classification. None of these are tested empirically. \n\n- If I understand correctly, the GS method, with a much higher computational cost, is near optimal. If so, its results should serve as an empirical upper-bound for F1. Then how come the proposed method outperforms it on 4 over 6 dataset? \n\n- There are additional references on F1 maximization. To name a few: (1) Chai. Expectation of F-measures. SIGIR 2005. (2) Waegeman et al. On the Bayes-Optimality of F-Measure Maximizers. JMLR.""]","[-50, -50, 60]","[20, 50, 80]","[""The sentiment score is -50 because the review expresses several concerns and criticisms about the paper. The reviewer points out missing related work, weak theoretical foundations, and lack of experimental details. The overall score of 5 given by the reviewer also indicates a negative sentiment. However, it's not extremely negative as the reviewer acknowledges some positive aspects like the paper's attempt to apply the method to neural networks. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or rude expressions. They use phrases like 'I have the following concerns' and 'I would like to see more discussion,' which are polite ways to express criticism. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the politeness. However, the score is not extremely high as the review is primarily focused on criticisms rather than praise."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the paper, stating that 'both [theory and experiments] are lacking' and that 'revisions required would be too great for acceptance at this time.' However, the reviewer does acknowledge some positive aspects, such as the 'compelling idea' and 'appealing' simplicity and generality of the method. This mix of criticism and limited praise suggests a moderately negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism and suggestions for improvement. The reviewer acknowledges the paper's strengths while clearly stating its weaknesses, maintaining a polite and academic tone without using harsh or dismissive language."", ""The sentiment score is 60 (positive) because the reviewer starts by highlighting the interesting and important nature of the research direction. They mention that the paper is well-organized and easy to follow, and that the methodology seems sound. These are all positive comments. However, the score is not higher because the reviewer also provides several critiques and suggestions for improvement, indicating that while the overall sentiment is positive, there are still areas that need addressing. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, framing their critiques as suggestions or questions rather than direct criticisms. Phrases like 'I am a little bit puzzled' and 'It may be a good idea' demonstrate a considerate tone. The reviewer also balances positive feedback with constructive criticism, which is a polite approach to peer review.""]"
"['The paper proposes a Siamese network model for mapping the space of natural compounds to a latent representation space; furthermore, it utilizes this representation to compute a similarity score between an already known compound and a newly discovered one. \n\nSome comments:\n- Comparison with LDA requires more details: have you used the same number of topics for both small and large datasets?  How are you training it? \n- Page 4 the definition of micro-averaging is missing: “Micro-averaging for precision and recall is formally defined as: “ \n- Figure 3 does not show a well defined clusters of various compound families because it’s using same color for different families in some cases (according to the caption of Figure 3). I wonder if you can somehow show for which compound families the colors have been repeated or maybe show fewer compound families. \n\nThe problem of finding similar compounds to a novel compound from NMR spectra is an interesting applied problem; however, technical novelty of the paper is not significant. Given the level of technical novelty, I believe the paper is more suitable for a more applied conference/journal in the fields of chemistry or bioengineering.', 'I didn’t worked in the field of structure elucidation from NMR spectroscopy so I might missed something. This paper utilized Siamese neural networks (Bromley 1993) and contrastive loss (Hadsell et al 2006) to learn a latent representation from the NMR spectra, which can be used for similarity search and the most similar compounds will be able to shad some light for the structures of the unknown natural product. The experiments showed significant improvement (AUC under precision-recall curve) over competitors. \n\nMy main concern is the lack of novelty in the methodology. The formulation is the same as Hadsell et al 2006 and the only change is an coefficient added to a loss term which kept to be fixed without explanation. As a result, I don’t feel this is enough to make it publishable at ICLR.  \n\nSome detailed comments below:\n1. In eq(3), why P = 1.5? Some intuition or explanation?\n2. I spent some time understanding the evaluation method in Section 4.2 and still not very sure I understand it. What is the formulae to compute your Tanimoto scores? Is this the same as PubChem Tanimoto scores which depends on fingerprints?\n3. There is a formatting issue for the prediction and recall formulae.\n4. I know in the computational mass spectrometry community there was an recent paper, “Critical Assessment of Small Molecule Identification 2016: automated methods“ where the winners were predicting the molecular fingerprints which directly shad the lights on the compound structures and can also be used for similarity searching, is this an option here for NMR data?', ""This paper identifies structure determination of novel natural products as a bottleneck in the drug discovery pipeline. The authors address this problem by using deep siamese neural networks to learn an representation of 2D NMR spectra that facilitates the rapid comparison of spectra for novel compounds with a database containing the spectra of molecules of known structure. \n\nThe authors have laboriously collected spectra from the published literature, using a protocol that included manual processing and orientation steps, resulting in a set of 1,385 NMR spectra from 104 families after imposing a minimum requirement of 10 compounds per family. They then implement two baseline topic modeling techniques - PLSI and LDA, and compare the performance of these baselines to the deep siamese CNN that they develop. \n\nUsing the random test/val/train split, the deep siamese CNN does show improved performance over the baseline models. However, the interesting and relevant use case is that in which the data is not randomly split, but rather complete families are held out from the training set for use in testing. The authors address this task by building a split in which they hold out four families: the aphanamixoids, teuvissides, tasiamides and macrolactins. \n\nTo evaluate performance, they calculate the averaged Tanimoto score for the top five closest compound families of these probe families. Unfortunately, the authors were not able to carry out this calcuation for two of the four probe families, restricting the evaluation to the aphanamixoids and teuvissides (I am confused as to why they couldn't calculate these scores - they appear to be relying on a PubChem server?). They compare these Tanimoto scores to the similarity score of the NMR spectra generated by their NN model. The similarity score either matches (in one case) or differs (in the other case) from the Tanimoto score, and the authors make the argument that this performance is correct. \n\nI am confused that there is no requirement or criteria for the NN to detect the presence of a new compound family, which seems to be what this tasks calls for. This evaluation is rather limited, given that TC scores could not be computed for half of the probe families. There is also no comparison to the baseline models for this task. \n\nOverall I think that this manuscript does a good job of identifying an interesting question, and makes a start at answering the question. To improve the manuscript I would ask that the authors carry out a more comprehensive evaluation of the performance using splits in which whole families are held out from the training set. Different splits might be evaluated, and in addition the model should be required to 'call' when it believes that a new compound family is present. Furthermore, it would be interesting to compare the siamese CNN approach to other network architectures. In addition, the performance of baseline models on the hold-family-out split should be assessed. ""]","[-20, -50, -20]","[50, 50, 60]","['The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting nature of the problem, they express concerns about the technical novelty and suggest the paper might be more suitable for a different type of conference or journal. This implies the paper may not meet the standards for the current venue. The politeness score is moderately positive (50) as the reviewer uses neutral language and phrases their criticisms as suggestions or questions rather than direct criticisms. They also begin with a neutral summary of the paper and end with a constructive suggestion, which maintains a respectful tone throughout.', ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'significant improvement over competitors'), they express a major concern about the lack of novelty and conclude that it's not publishable at ICLR. This indicates a generally negative sentiment, but not extremely so. The politeness score is 50 because the reviewer uses polite and professional language throughout, acknowledging their own potential limitations ('I might missed something') and providing constructive feedback. They avoid harsh criticism and frame their concerns in a respectful manner. However, the review doesn't go out of its way to be exceptionally polite, maintaining a neutral professional tone overall."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's identification of an interesting question and a good start at answering it, they express confusion about certain aspects and point out several limitations in the evaluation and methodology. The reviewer suggests multiple improvements, indicating that the current work is not yet satisfactory.\n\nThe politeness score is moderately positive (60) as the reviewer uses respectful and constructive language throughout. They acknowledge the authors' efforts ('laboriously collected spectra') and frame their criticisms as suggestions for improvement rather than harsh critiques. Phrases like 'I think that this manuscript does a good job' and 'To improve the manuscript I would ask' demonstrate a polite and collegial tone.\n\nThe reviewer maintains a professional and objective stance, balancing positive observations with areas for improvement, without using any rude or dismissive language.""]"
"['The proposed method is too simplistic, the model being succinctly described in less than one page with many errors in the given math expressions. Only the model is given. The optimization problem, as given in (1) is not explained. the authors need to stud the optimization problem, to derive its resolution, and to describe the obtained algorithm.\n\nThe authors’ main motivation is to “maps the input convolutional features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, …”. However, in the proposed method is operating in this way. The model is a simple transformation, and nothing ensures the compactness of the feature space, neither the separability of the classes.\n\nIt is difficult to understand the “arm CNNs with radial basis feature transformation”. There are two figure in the paper that seek to show this modification of CNN, but this is not enough because nothing is said in the text, which makes these images difficult to understand. Moreover, the figures have notations different than those in the  main body, such as F_{l-1} as opposed to F_{i,j,K}.\n\nWhat is the transformation to be learned ? Is it T as given in the text before (1), or P as given in (3). In (1), it seems that it is a mix of both, namely T* = argmin_P ! Moreover, it is written “To enforce T’s positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T ′T”.  Decomposing T as T’T, means that T is very very special.\n\nEquation (4) is not correct. The summation is on i, which is not in the expressions, but in the result with F_{i,j,K}.\n\nWith the exception of Tables 3 and 4, most experiments are on comparing the conventional versus the proposed method. The authors need to compare to other methods available in the literature on defense against adversarial attacks. Moreover, it is not clear why the author compare the proposed method to ADVT (adversarial training) in Table 4, and not in Table 3.\n\nSome references are incomplete. For example, the second reference is missing the publication type, volume, … \n', 'This paper realizes the radial basis function in deep CNNs by leveraging the Mahalanobis distance between a convolutional feature vector and the corresponding center. The method is implemented for the image classification and segmentation tasks against several types of attack methods and demonstrates good robustness.\n\nAlthough the results against adversarial attacks are promising, the paper is not well written. Especially, the notations in Section 2 are not clearly defined which baffled me a lot on how this method functions. For instance, in Equation 4, what does w_k stand for? Why there are K activation functions in a CNN, are they different? What is the meaning of a dot product between a w_k and an activation function?\n\nAdditionally, there lacks detail on how to train the transformation matrix (P in Equation 3 or T in Equation 1), and the following sentence confused me a lot: ""To enforce T\'s positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T\'T"". I understand why T needs to be PSD matrix, but how can eigenvalue decomposition decompose T into T\'T? And how is this achieved during the training of a CNN? I think the authors should revise this part carefully to demonstrate the proposed methods more clearly.\n', 'The authors propose a new defense against adversarial examples based on radial basis features. Prior work has suggested that the linearity of standard convolutional networks may be a factor contributing to their vulnerability against adversarial examples, and that radial basis functions may help alleviate this weakness. The current paper builds on this idea and proposes a concrete way to add radial basis features to existing convnet architectures.\n\nIn the proposed approach, each layer of the network is augmented with a radial basis feature transform of the features in this layer. The output of this feature transform is then concatenated with the features in this layer. The centers of the radial basis features, the bandwidth, and the distance matrix are trained with the other network parameters. The distance matrix in the feature transform is used to compute the Mahalanobis distance between the features and centers in the radial basis functions.\n\nThe authors evaluate their defense experimentally on the standard MNIST dataset and two medical image datasets: X-chest14 for classification and a 2D RGB skin lesion dataset from the 2017 IEEE ISBI International Skin Imaging Collaboration (ISIC) Challenge for image segmentation. The experiments show that their method improves over an undefended network on MNIST. On X-chest14, their method improves over features squeezing (input quantization) and Gaussian data augmentation. On the image segmentation dataset, the method improves over these baselines as well as adversarial training.\n\nWhile I find the overall idea interesting, I have some doubts about the experimental evaluation. For instance, the authors do not compare their MNIST numbers to the robust optimization results reported in Madry et al. (cited in the paper). Robust optimization achieves higher adversarial accuracy than the numbers reported in Table 1.\n\nMore importantly, it is unclear to what extent unmodified first-order methods are effective for the proposed defense. While the authors investigate whether their networks exhibit gradient masking / obfuscation, the left plot in Figure 3 still leaves some questions. Based on the curves for FGSM and BIM, the proposed defense would still achieve a high accuracy even against attacks with eps = 0.5. However, this would be a clear failure of the first order attacks (but not a sign of true robustness) because an adversary with eps = 0.5 can trivially defend any network by setting every input pixel to 0.5. Hence the authors should investigate what happens in the regime between eps = 0.4 and eps = 0.5.\n\nWhile I support the use of non-standard datasets for evaluation, it would still strengthen the paper if the author also reported accuracy numbers on CIFAR-10. The X-chest14 and the segmentation dataset have not been frequently used in the adversarial robustness literature to the best of my knowledge. Hence it is less clear how well the proposed methods perform on these datasets.\n\nWhile I find the overall idea interesting, with the current experimental evaluation I unfortunately cannot recommend accepting the paper.\n\n\nFurther comments:\n\n- The distinction between ""data-level"" and ""algorithmic-level"" approaches in the introduction is unclear to me. Adversarial training can also be seen as robust optimization, which is arguably an algorithmic approach.\n\n- At the beginning of Section 2, it would be helpful if the authors first introduced the meaning of the variables n, m, and k before using them. In general, it would be helpful if the authors described in more detail how the radial basis features are incorporated into the network.\n\n- How is the adversarial training baseline in Section 4.2 implemented? The choice of adversary in adversarial training / robust optimization can be crucial for the robustness of the resulting model.\n\n- Since the authors refer to the linearity of existing model as a potential weakness: there are also alternative explanations, e.g., see https://arxiv.org/abs/1801.02774 and https://arxiv.org/abs/1804.11285 .\n\n- The test sets used in the evaluation are fairly small (150 and 200 data points). In this regime, 95% confidence intervals can be as large as +/- 8%. Hence I would recommend increasing the size of the test sets to at least 1,000.']","[-70, -20, -50]","[-20, 50, 50]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer criticizes the method as 'too simplistic', points out 'many errors', and states that key aspects are 'not explained' or 'difficult to understand'. They also highlight missing comparisons and incomplete references. The few neutral statements don't offset the overall negative tone. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the feedback. Phrases like 'too simplistic' and 'difficult to understand' are blunt. The reviewer doesn't use polite phrases or acknowledge any positives, which contributes to the slightly impolite tone. However, they do maintain a professional, albeit stern, approach throughout."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the results against adversarial attacks are promising, they express significant concerns about the paper's clarity and presentation. The reviewer states that the paper is 'not well written' and points out several areas of confusion, particularly in the notation and methodology sections. This indicates a generally negative sentiment towards the current state of the paper, despite recognizing its potential value.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They begin by acknowledging the paper's strengths and use phrases like 'I think the authors should' rather than making demands. The criticism is presented as confusion or lack of understanding on the reviewer's part, rather than direct accusations of poor writing. This approach is more polite than neutral, but not excessively formal or deferential."", ""The sentiment score is -50 because while the reviewer finds the overall idea interesting, they ultimately cannot recommend accepting the paper due to concerns about the experimental evaluation. This indicates a negative sentiment, but not extremely negative as they do see some merit in the work. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledges positive aspects, and provides constructive feedback. They phrase criticisms politely (e.g. 'I have some doubts', 'it would strengthen the paper if'). However, it's not extremely polite as it maintains a professional, direct tone rather than being overtly courteous.""]"
"['The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning. My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small. \n\nAlso, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost. \n\nIn chapter 2, related work. The authors state that ""tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data. \n\nTo me these two reasoning statements are not particularly convincing. One could also say:\n\nNN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...\n\nActually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks.', ""This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN). The intended research direction on tabular data is essential and promising. However, the proposed technique does not seem to be handling the problem foundationally well. It seems heavily dependent on GBDT. It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results. Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem. \n\nPros:\n-This is a crucial line of research direction that aims to make DNNs applicable to many real-world problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search.  \n-The starting point of using GBDT seems like a good choice. \n-The Paper is mostly well written except occasional repetitions and missing acronym definitions.\n\nCons:\n-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well. I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented. The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times). This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.   \n-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.\n-In the provided benchmark data sets the depth of the analysis seems to be enough. However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features  (e.g., unstructured raw text for the search queries.) In such scenarios, it had been showed that wide-and-deep NNs perform decently. However such problems are entirely missing in the results section. I also think that this is a lost opportunity for the authors as they could be showing that it is the NN part contributing."", 'Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs.\n\nStrengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;\n\nWeaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it\'s unclear why we wouldn\'t use GBDT.\n\nMinor typos:\n(abstract)\n- ""NN has achieved"" => ""Neural Networks have achieved""\n- ""performances"" => performance\n- ""explicitly leverages"" => ""explicitly leverage""\n\nQuestions:\n- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""\n- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]\n- what is the benefit brought by the \'Structural Knowledge\' transfer? is this quantified anywhere? based on the description, I don\'t understand if this is an add-on to TabNN or whether it is incorporated in TabNN.\n\n\nRecommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?\n']","[-20, -50, -20]","[50, 20, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's 'interesting algorithm and direction', they express concerns about the limited number of datasets used and suggest additional comparisons. The reviewer also challenges some of the authors' statements, indicating a critical stance. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, starting with positive acknowledgment and framing criticisms as suggestions or personal opinions ('My concern is...', 'would be good to...', 'To me...'). The reviewer maintains a professional tone without using harsh or dismissive language, even when disagreeing with the authors' points."", ""The sentiment score is -50 because the review is predominantly negative. While the reviewer acknowledges some positive aspects ('Pros'), the majority of the review focuses on criticisms ('Cons') and expresses significant doubts about the paper's novelty and effectiveness. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'The paper is mostly well written' and 'This is a crucial line of research direction'. However, the criticism is direct and not overly softened, preventing a higher politeness score. The reviewer balances constructive feedback with clear expression of concerns, maintaining a respectful but honest approach."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the performance gains being 'extremely low' and question the practical value of the proposed method compared to existing approaches (GBDT). The reviewer also points out several weaknesses and asks for clarification on multiple points, indicating a somewhat critical stance.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns, offer constructive feedback, and provide specific recommendations for improvement. The reviewer also acknowledges the paper's strengths and novel aspects before delving into criticisms. The use of phrases like 'Would it be possible...' in the recommendations section further contributes to the polite tone.""]"
"['This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.\n\nGenerally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.\n\n1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.\n\n2.  About the \\epsilon_{1,2,3} in formula (3). \\epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \\epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the ""Markov chains are considered"" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.\n\n3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \\Psi and \\Delta? Why \\Delta can be used to predict sentiment intensity in your experiment? It seems that \\Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \\Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.\n\nSo I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.', 'The paper proposes a method to learn the conditional distribution of a random variable in order to minimize and maximize certain mutual information terms.  Interestingly, the proposed method can be applied to sentiment prediction and outperforms a 2018 method based on SVM.\n\nOverall, the ideas seem intriguing, and the results seem promising, but I really cannot understand what the paper is saying, and I think the paper would be much stronger if it was written more clearly (to make individual sentences more clear, but also to make the broader picture more clear). Not only is the writing hard to understand (some sentences lack a verb!), but it is vague, and the notion of a ""complex system"" is never defined.  It seems that the technique can be applied to any (potentially non-stationary) Markov process?\n\nAdditionally, due to the lack of clarity in the writing and lack of mathematical rigor, Theorem 1 does not seem to be true as stated. I think this is an issue of stating the assumptions, and not due to a mistake in the derivation.  Right now, the actual conclusion of theorem 1 is not even clear to me.\n\nQuality: poor/unclear\nClarity: very poor\nOriginality: unclear, perhaps high? Not clear how related it is to the methods of Tishby et al.\nSignificance: unclear, as clarity was poor, and there was minimal discussion of alternative methods.\n\nSpecific points:\n\n- Eq (2), the first term is included because it is for the ""information compression task"", but I do not understand that. Where is the actual compression?  This is not traditional compression (turning a large vector into a smaller vector), but more like turning one PDF into a PDF with lower entropy?\n\n- This paper seems to fall into the subfield of system identification (at which I am not an expert), so I\'d expect to see some related literature in the field. The only compared method was the IF method of Tishby et al. from 18 years ago (and the current work seems to be a generalization of that).\n\n- Equation (4): what exactly is the object being minimized? Is it a PDF/probability measure? Is it an *instance* of a random variable?  If it is a PDF, is it the PDF of B_k | X_{k-1} ?\n\n- The statement of Theorem 1 is either too vague or wrong. To say ""The solution... is given by"" makes it sound like you are giving equations that define a unique solution. Perhaps you mean, ""Any solution ... must necessarily satisfy..."" ? And that is not clearly true without more work. You are basically saying that any minimizer must be a stationary point of the objective (since you are not assuming convexity). It seems everything is differentiable?  How do you know solutions even exist -- what if it is unbounded? In that case, these are not necessary conditions.\n\n- Lemma 1: ""The iterative procedure... is convergent.""  The iterative procedure was never defined, so I don\'t even know what to make of this.\n\n- Section 3.2: ""As proved by prior work, the optimum solution obtained by a stochastic transformation that is jointly Gaussian with bottleneck\'s input.""  I do not know what you are trying to say here. There\'s no predicate.\n\n- Section 4 wasn\'t that interesting to me yet, since it was abstract and it seemed possible that you make a model to fit your framework well. But section 5 is much better, since you apply it to a real problem. However, what you are actually solving in section 5 is unclear. The entire setup is poorly described, so I am very confused.\n\n', 'This paper studies the problem of compactly represent the model of a complex dynamic system while preserving information. The method is based on the information bottleneck method. Basically, for a dynamic system whose states changing from X_{k-1}, X_k to X_{k+1}, the ""information bottleneck hierarchy"" method learns a variable B_k and B_{k+1} such that B_k predicts B_{k+1} well, B_k predicts X_k well, and B_{k+1} predicts X_{k+1} well, while minimizing the information of X_{k-1} contained in B_k. \n\nIn my opinion, this is a very interesting framework for representing and learning a dynamic system. The paper then considers simple examples on a linear model with Gaussian noise and show that the IBH method performs better than the one-BN method. The simulation and the experiments on real data all show very good performance (even with the simple linear Gaussian estimator).\n\nThe reason that I give such a rating is that of the confusing writing.\n* In the abstract, it is unclear what the goal is. For example, the second and third sentence do not explain the first sentence: ""the task is a crucial task"".\n* Introduction is also very confusing. It seems there is not a good logic connecting each sentence.\n* The paper does not give a good survey of other methods performing similar tasks, e.g., the ones the paper are comparing to in the experiment section. Therefore, it is hard to compare or to understand why the previous methods are worse.\n* Figure 2: one-BN is not well defined. How do you design the IB locally? \n']","[50, -50, 50]","[75, 20, 20]","[""The sentiment score is 50 (slightly positive) because the reviewer expresses general approval of the paper's idea and experiment, stating 'Generally I like the idea' and 'I think the experiment is interesting.' However, they also raise 'some major questions' and suggest improvements, indicating a mix of positive and critical feedback. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames their concerns as questions rather than direct criticisms. They also end on a positive note, stating 'the idea in this paper is interesting' and suggesting the paper could be accepted with revisions. The reviewer maintains a professional and courteous tone while providing detailed feedback."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the clarity and quality of the paper, using phrases like 'very poor' for clarity and 'poor/unclear' for quality. However, they do mention some positive aspects like 'intriguing ideas' and 'promising results', which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I think the paper would be much stronger if...' and 'I really cannot understand' rather than using harsh or accusatory language. The reviewer also provides specific, constructive feedback, which is a polite approach to criticism. However, the directness of some criticisms (e.g., 'very poor' clarity) prevents the politeness score from being higher."", ""The sentiment score is 50 (slightly positive) because the reviewer expresses that the paper presents 'a very interesting framework' and mentions that the simulations and experiments show 'very good performance'. However, this positive sentiment is tempered by criticisms about the confusing writing and lack of clarity in certain sections. The politeness score is 20 (slightly polite) because the reviewer maintains a professional tone throughout and uses phrases like 'in my opinion' to soften criticisms. However, the directness of the criticisms, particularly in the bullet points, prevents the review from being overly polite. The reviewer provides constructive feedback without using harsh language, striking a balance between honesty and courtesy.""]"
"['The paper proposes a modification of GAIL (Ho & Ermon, 2016) to make use of non-expert data. The non-expert data is used by training a classifier to classify between roll-outs of the current policy, expert demonstrations and non-expert demonstrations. Similar to GAIL, the policy is iteratively updated using TRPO with a cost that is given by the log probability of predicting the policy. The use of non-expert data acts as regularization in order to learn better features similar to universum prescription (Zhang & LeCun, 2017).  \n\nThe paper is well-written and very clear. The general problem setting is interesting, but I think it is of rather little significance, because I do not see many clear applications. The evaluation focuses on simulated robots, however gathering non-expert data on real robots would be very expensive, so the approach would not make a lot of sense here (even if we replace TRPO a more sample efficient rl method). The paper mentions the game of Go, but learning a policy on such large state spaces is not feasible without major modification and significant computational effort. However, the paper also mentions autonomous driving, which might be a more convincing application, because we can have a lot of demonstrations that we do not want to label as expert trajectories. I think the paper would profit a lot from having an experiment where the importance of making use of non-expert data becomes evident.\n\nThe approach seems sound, although I think that we can not expect much benefit from using the unlabelled data in the proposed way. By not making any assumptions on the non-expert data, they do not carry any information about the objective; the information that they carry about the system dynamics is not exploited for the RL update. Instead, the use of non-expert data is restricted to learning better features for discriminating between the agent and the expert. However, non-expert data is typically not cheaper than policy roll-outs and better features could also be learned by using more samples from the policy. The experiments also show only slight benefits, especially when comparing the final performance (instead of total returns) and accounting for the additional system interactions needed for generating the non-expert data. To make the comparison fairer, we could consider using a few more system interactions (variable K in the paper) per iteration for standard GAIL, so that the total number of function evaluations would match those of M-GAIL after a certain number of iterations. Especially if K is appropriately tuned, it is not clear whether we could still show an advantage of M-GAIL. It would also be interesting to show, whether we can benefit from using the policy roll-outs of previous iterations as non-expert data for the current iteration (in the traditional IL setting where no non-expert data is available a priori).\n\nThe main weakness of the paper is, that the novelty seems marginal. Instead of doing binary classification with cross-entropy loss, we\'re doing three-class classification with cross-entropy loss and use it for binary classification (by throwing away the auxiliary logit for predicting the non-expert class). Did I miss any other difference to GAIL? We can argue whether the policy objective is different (to me, H_\\phi of M-GAIL corresponds to the discriminator of GAIL and the objective is exactly the same), however, even if we call it a minor modification, we would have very little novelty in the approach. As the paper does also not compensate for this with very good results or thorough theoretical analysis, I think that the contribution is too minor.\n\nI do not see the point of section 4.4. and the related appendix A2. For all I understand, it proves that when using lambda=0 (standard GAIL, right?), the proof of Fu et al. (2018) for GAIL is valid (i.e. we learn a completely useless reward function that does not carry any additional information compared to the policy), and when using lambda!=0 we learn something different. I don\'t see the the purpose of this statement and I don\'t think that it needs to be proven. The paper argues, that for small lambda we can treat the discriminator logits as approximations of these (completely useless) reward functions--without providing any bound. As I do not see why this would be useful, I think the section should be removed. \n\nMinor:\nTypo: ""[...]due to its dependent[sic] on the linearity of reward functions and good feature engineering"" \n', ""Summary:\nThis paper is about adversarial imitation learning and how data from non experts can be used to improve representation learning of the discriminator function. They also change the training objective because minmax does not lead to an optimal policy in the proposed setting. The data from non experts is used a separate class (so the discriminator learns to discriminate between expert, agent and non-expert policy). \n\nClarity: well written, with an intro to both relative fields - reinforcement learning and imitation learning. \n\nComments:\nOverall, quite a neat idea - the information from non expert policy can help with representation learning, and the authors show that it is indeed the case via a number of experiments \nAt the same time, i wonder if the third class is required. It seems that gail in the experiments eventually reaches the same performance (figure 1) by looking at more agents trajectories. why can't non expert trajectories be considered as an initial set of agent trajectories? If multiclass is indeed required, it would be nice to see a comparison of what happens when non experts trajectories were just considered agents\nAnother thing i am surprised about is the leel of variance of GAIL in Figure 1. In table 1 we see that standard errors between the new method and gail are comparable, where does such a huge diff in var come from in Figure 1?\nAlso the sensitivity of the algorithm to lambda - how would one go setting it? In the experiments it seems that authors just try two different values (0.1 and 0.5) but i assume this really should be a hyperparameter search for this. Is lambda dependent on the number of expert and non expert demonstrations that are available?\n\n\n\n"", 'This paper proposes M-GAIL, which performs imitation learning from expert as well as sub-optimal demonstrations. This work builds off of GAIL (Ho et. al 2016) but modifies the discriminator with an additional class corresponding to sub-optimal demonstrations. The authors show empirically that including these sub-optimal demonstrations into the training process leads to faster and improved learning.\n\nWhile allowing the use of sub-optimal demonstrations is important and could have useful benefits in practice, I find the new algorithmic formulation unnecessary when compared to previous works. One of the theoretical benefits of MaxCausalEnt IRL (and by extension GAIL) is that because the model is probabilistic, not all demonstrations need to be reward-maximizing. The definition of optimality is relaxed from meaning exact reward maximization to coming from some optimal distribution over trajectories.\n\nCorrect me if I am wrong, but in the case of this work, the proposed algorithm seems equivalent to adding the ""sub-optimal"" demonstrations to the expert demonstrations, but down-weighting each sub-optimal demonstration be a factor of \\lambda. Thus, I don\'t believe we need to introduce an entirely new algorithm, with the concept of a 3rd class, to solve this problem. The existing MaxEnt frameworks seem to handle the notion of sub-optimality proposed in this paper just as well, interpreting the ""sub-optimal"" demonstrations as low-probability expert demonstrations. This feels cleaner and more intuitive than the current proposed explanation as maximizing a mixture of two reward functions (in the IRL view) or minimizing occupancy measure divergence over 3 distributions (in the IL view). If this equivalence is true, I would encourage the authors to include this discussion in the main paper, and if not, discuss the differences and possibly compare against this simple strategy as a baseline.\n\nI find the empirical results quite interesting, even though the gains seem small. Including sub-optimal demonstrations to learn from could be a nice trick to improve the learning of GAIL-like algorithms, which is nice to know. I am curious if the authors tried annealing the \\lambda term from 1.0 to 0.0, as lambda=1.0 is likely easier to learn from, but lambda=0.0 would have better asymptotic performance.', 'Description:\n\nThis paper presents a variant of imitation-based reinforcement learning, when in addition to example trajectories of an expert , example trajectories of non-experts are available.\n\nIn brief, the method is a variant of the GAIL method for adversarial training. In GAIL, the policy is optimized to minimize the ability to discriminate (classify) two classes: trajectories of the expert vs. trajectories from the policy, where the discrimination ability is measured by a neural network discriminator function optimized for maximal discriminative ability. In the proposed  method ""M-GAIL"", the idea is that the discriminator is forced to also discriminate a third class of non-expert demonstrations, but policy optimization is done ignoring the non-expert demonstrations and classifying only the usual two classes with the discriminator.\n\nThe M-GAIL method is compared to GAIL on four control tasks with different amounts of simulated non-expert demonstrations available, and  it outperforms GAIL if the simulated non-expert demonstrations are chosen to be relatively good ones (simulated from a policy having 70% of expert performance).\n\n\nEvaluation:\n\nThe method is described relatively well and the idea of incorporating nonexpert demonstrations seems sound.\n\nIt is not clear to me if the experiment is fair, since M-GAIL learns from more data than GAIL which learns from the expert demonstrations only. It is unclear to me why the experiments did not attempt to supply the nonexpert demonstrations to GAIL too - one could e.g. have naively pooled the nonexpert demonstration into one of GAIL\'s binary classes, ""expert"" or ""policy"". This is especially concerning since bett\n\nThe method also requires the additional parameter lambda which affects performance in the experiments - it is not clear how to set it in practice, does e.g. cross-validation etc. need to be used? It would be useful to know more about sensitivity to lambda, experiments only consider two values.\n\n\nAdditional comments:\n\nIn the methodological derivation, it was unclear to me why only one parameter lambda is used to control class balance, why not two parameters controlling prevalence of the expert class, policy class, and nonexpert class?\n\nIn proposition 1 the fact that the bias vanishes when lambda=0 seems trivial because eq. 9 reduces to the first term on the right hand side.\n\nIn eq. 5 it\'s not quite right to call the right-hand side a loglikelihood. Loglikelihoods should be sums over observations of each class (thus emphasizing classes with more data) whereas here each term is an expectation - or do you assume the number of samples corresponding to each expectation term is equal?\n\nClarify the notation d_phi when you introduce it near eq. 3.\n']","[-40, 60, -20, -20]","[50, 70, 60, 50]","[""The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('well-written and very clear'), they express significant concerns about the paper's novelty, significance, and experimental results. The reviewer states the contribution is 'too minor' and questions the usefulness of certain sections. However, the tone is not entirely negative, hence not a lower score. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They avoid harsh or dismissive language, instead using phrases like 'I think' and 'It would be interesting to show'. The reviewer also acknowledges positive aspects before presenting criticisms, which is a polite approach."", ""The sentiment score is 60 (positive) because the reviewer describes the paper's idea as 'quite neat' and acknowledges that the authors successfully demonstrate their claims through experiments. The overall tone is supportive, although the reviewer does raise some questions and suggestions for improvement. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, framing their comments as questions or suggestions rather than criticisms. They acknowledge the paper's strengths before offering constructive feedback. The reviewer maintains a professional and courteous tone, avoiding any harsh or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting empirical results, they express significant doubts about the necessity and novelty of the proposed algorithm. They suggest that existing frameworks may already handle the problem adequately. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive criticism, and invites the authors to correct them if they're mistaken. They also express curiosity about additional experiments, showing engagement with the work. The reviewer maintains a professional tone, avoiding harsh criticism while clearly stating their concerns."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The method is described relatively well and the idea of incorporating nonexpert demonstrations seems sound'), they raise several significant concerns about the experimental design and methodology. The reviewer questions the fairness of the experiments, the setting of parameters, and points out unclear aspects of the method. These critiques outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They phrase criticisms as questions or suggestions rather than harsh statements (e.g. 'It is not clear to me if...', 'It would be useful to know more about...'). The reviewer also balances critique with positive acknowledgments. While not overly effusive, the tone remains constructive and polite throughout.""]"
"[""The paper proposes a method MixFeat for regularizing deep neural networks models, aiming at avoiding overfitting in training. The MixFeat interpolates, based on a careful selected mixing ratio, the hidden states (feature maps) of two randomly selected examples. Unlike MixUp, the MixFeat does not interpolate the labels of the two selected examples and the feature interpolation processes are conducted in the hidden space. Experiments on both Cifar10 and Cifar100 show that the networks with MixFeat improve their predictive accuracy as well as outperform networks with Mixup as regularizer.   \n\nThe paper is well written and easy to follow, and the experimental results on both Cifar10 and Cifar100 show promising results. Nevertheless, the idea of interpolating pairs of latent features for network regularization is not very novel. Additional, the experimental section is a bit weak in its current form. \n\nMain Remarks:\n\n1.\tMixFeat is very similar to Manifold-Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), where both feature maps and labels of a pair of examples are mixed, so Manifold-Mixup would be a valid comparison baseline to MixFeat. In addition, the proposed method is similar to SMOTE (where features are mixed in the input space). In this sense, performance of SMOTE may be a useful comparison baseline as well.\n2.\tIn the experimental section, the choice of parameter for Mixup seems arbitrary to me and may not be the optimal one. For example, for the Cifar10 and Cifar100 datasets, the original paper highlights that Alpha equals to one is a better choice to obtain better accuracy for ResNet. Also, as highlighted from AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), MixUp is quite sensitive to the choice of Alpha and suboptimal Alpha value easily leads to underfitting. \n3.\tSome claims are not well justified. For example, the authors claim that MixFeat can reduce overfitting even with datasets with small sample size, but did not provide any training cost or errors in Figure6 to support that claim. \n4.\tMixFeat is closely related to MixUp, and I would like to see more experiments with MixUp as baseline in terms of regularization effect. For example, it would be useful to include MixUp in Figures 4 and 6.\n\nMinor remarks: \n\n1.\tWhat were the parameters for MixFeat used for Table 1?\n2.\tIs the proposed method robust to adversarial examples as shown in MixUp and ManiFold-Mixup?\n3.\tHow the incorrect labels are generated in Section 3.2.1 is not very clear to me.\n4.\tSince MixFeat is similar to Mixup, I wonder if MixFeat has the problem of “manifold intrusion” as suggested in AdaMixUp when generating samples from image pairs?  How sensitive is MixFeat to the parameters Theta and Pi? Would learning mixing policies as suggested by AdaMixUp make sense here?\n\n============after rebuttal============\n\nI really appreciate the authors' rebuttal, which has addressed some of my concerns.\nNevertheless, I agree with the other reviewers about the main weakness of the paper. That is, why the proposed method works and what are its advantages over similar strategies, such as Mixup, AdaMixup and Manifold Mixup, are not clear."", 'This paper proposes a method, so-called MixFeat that can mix features and labels. This method is in a similar line of the methods such as mixup and manifold mixup. \n\npros)\n(+) The proposed method looks simple and would have low computation cost at inference phase.\n(+) The experiment of evaluating the possibility of reducing the number of datasets looks good.\n\ncons)\n(-) The advantages of the proposed method are not clarified. There should be at least one insight why this method can outperform others.\n(+) Decomposition of r and theta in eq.(1) looks interesting, but there is no supporting ground to grasp the implicit meaning of this idea. Why the parameters a and b are reparameterized with r and theta?\n(-) Figure 1 is not clearly illustrated and confusing. Just looking at the figure, one can understand mixup is better than others.\n(-) This paper does not contain any results validated on ImageNet dataset. This kind of method should show the effectiveness on a large scale dataset such as ImageNet dataset.\n\ncomments)\n- It would be better to compare with Shake-type method (shake-drop (https://arxiv.org/pdf/1802.02375.pdf), shake-shake) and SwapOut (https://arxiv.org/pdf/1605.06465.pdf). \n- The performance of PyramidNet in Table 1 looks different from the original one in the original paper (https://arxiv.org/pdf/1610.02915.pdf).\n\nThe paper proposes an interesting idea, but it does not provide any insights on why it works or why the authors did like this. Furthermore, the experiments need to contain the results on a large scale dataset, and from the formulation eq.(1), the proposed method looks similar to a single-path shake-drop or shake-shake, so the authors should compare with those methods.', 'This paper follows a recent trend to improve generalization by mixing data from training samples, in this case by mixing feature maps from different samples in the latent space. One of the feature maps is added as a kind of perturbation to the other one, so only the label from the main feature map is used as the learning target. MixFeat, the proposed method of adding ‘noise’ from another learning sample is tested on CIFAR-10 and CIFAR-100 with different architectures. The authors claim that the proposed method makes the latent space more discriminative. Multiple experiments show that it helps to avoid over-fitting. \n\nThe core idea of mixing the latent spaces of two data samples is interesting and the results seem to indicate that it improves generalization, but I have two main criticisms of this work. First, it is unclear as to why this this approach works (or why it works better than similar methods) and the explanations offered are not satisfactory. The phrase “making features discriminative in the latent space” is used repeatedly, but it is not obvious exactly what is meant by this. Design choices are also not clearly motivated, for example what is the advantage of defining a and b as was done? The second criticism is that comparisons to manifold mixup should have been included.\n\nApproach: \n- In “1 Introduction”, the second contribution of presenting “a guideline for judging whether labels should be mixed when mixing features for an individual purpose” is not clearly communicated. \n- Figure 1 is a nice idea to illustrate the types of mixed feature distributions, but is not convincing as a toy example. A visualization of how mixed features are placed in the learned latent space for real data would be more informative. The examples showing 0.4A+0.6B and 0.6A+0.4B are confusing - it’s not clear exactly how it relates to the formulation in (1).  \n- In “2.3 Computation of MixFeat” there is no clear explanation on why the authors chose a and b. Can they just be some other random small values? Is it necessary to have this correlation (cos and sin) between two feature maps we want to mix? Questions like these are not clearly explained. Similar questions can be applied to formula (4) and (6). \n+ Explicitly pointing out how backpropagation works for MixFeat in (2) (5) (7) and Figure 2 is helpful.\n\nExperiments: \n- The authors mentioned important related works in both “1 Introduction” and “4 Relationship with Previous Work”, but in Table 1, they compared the MixFeat with only standard Mixup. Manifold Mixup would be a  better comparison as it has better performance than standard mixup and is more closely related to MixFeat - MixFeat mixes features in every latent space while Manifold Mixup does in a randomly selected space (and standard mixup only mixes the inputs). \n- The method could be described as ""adding some noise along samples\' latent feature directions"". An interesting perspective, and would have been nice to see a comparison of MixFeat vs. perturbing with gaussian noise to see how much the direction towards other examples helps.\n+ The experiments to demonstrate the effectiveness of MixFeat for avoiding over-fitting are strong (aside from the missing baseline). The experiments showing robustness to different incorrect label ratios and with different training data size are convincing.\n- In Figure 6 center, the x-axis is  or ( for original MixFeat and 1D-MixFeat, and  for Inner-MixFeat), but the authors didn’t make a clear distinction in both the figure caption and “3.3.1 Dimensions and Direction of the Distribution”, having it wrong for Inner-MixFeat with “6.94% ( = 0.02)” which should be “( = 0.02)”. \n+ The ablation study motivating the choice of where to apply MixFeat was appreciated.\n\nRelated works\n+ Clearly presented and covered the relevant literature. \n- It would be helpful if the differences between MixFeat and the Mixup family is more clearly stated.\n']","[-20, -20, -20]","[60, 50, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'easy to follow', 'promising results'), they express several criticisms and concerns about the novelty, experimental design, and justification of claims. The overall tone suggests that significant improvements are needed. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms as suggestions or questions rather than harsh judgments. They use phrases like 'I would like to see' and 'I wonder if' which maintain a collegial tone. The reviewer also expresses appreciation for the authors' rebuttal, further contributing to the polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('pros'), there are more significant criticisms ('cons') and areas for improvement mentioned. The reviewer states that the paper lacks insights, has unclear illustrations, and is missing important comparisons and large-scale dataset validations. The politeness score is moderately positive (50) as the reviewer uses professional language, acknowledges positive aspects, and frames criticisms as suggestions for improvement rather than harsh judgments. The reviewer uses phrases like 'It would be better to' and 'The paper proposes an interesting idea, but' which maintain a respectful tone while providing constructive feedback."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper, there are significant criticisms and areas for improvement mentioned. The reviewer states that the core idea is interesting and results indicate improvement, but follows with 'two main criticisms.' They point out unclear explanations, lack of motivation for design choices, and missing comparisons. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, balancing criticisms with positive remarks. They use phrases like 'interesting perspective' and 'appreciated' when discussing positive aspects, and frame criticisms as suggestions for improvement rather than harsh judgments. The reviewer also uses '+ and -' to clearly separate positive and negative points, which is a considerate way to structure feedback.""]"
"['This paper study the model-based approach in deterministic low dimensional continuous control. As far as I am concerned and I understood, the main contribution of this paper is in substituting one-step-ahead prediction model with a multiple-step prediction model, resulting in a more accurate prediction model. I was not able to find points beyond this. I would be happy if the authors could clarify it. ', 'This paper proposes learning a transition model that takes an action sequence as an input (instead of a single action), and performing model-based planning by using the cross-entropy method.\n\nOne obvious concern with this is that this produces a sequence of open-loop plans, rather than a closed-loop policies, with all the inherent limitations. I could see this working well in practice in problems where anticipating how future decisions will react to state changes is not that important, however the authors should discuss the trade-offs more.\n\nA larger concern for me revolves around learning the transition model. Taking the action sequence as an input (which is one of the main novelties in the paper) is likely to require a lot of data, and maybe this is fine on relatively simple Mujoco tasks but I see it as a potential issue when trying to expand this to more realistic problems.\n\nFinally, I suggest that the authors change the title to something more descriptive of the paper’s contents, as there is no analysis of asymptotic performance in the paper (as I would have thought from the title). I also recommend that they look to see if there is any model-based work in the semi-MDP literature, which could be relevant here.\n', ""The authors learn a model that predicts the state R steps in the future, given the current state and intervening actions, instead of the predicting the next time step state. The model is then used for standard model predictive control. The authors find numerically that their method, termed Plan-Conditional Predictor (PCP), performs better over long horizon times (~100 time steps), than other recent model-based and model-free algorithms. This because for long horizon time scales, the model predicting the state for the next time step accumulates error when used recursively.\n\nThe key idea is to use a model that directly predicts multiple time steps into the future. While seemingly an obvious extension, it does not appear to have been used in current algorithms. A main issue that I find with this approach is: since only the state after R steps is predicted, reward r(s_t,a_t) can only be used every R steps, not at every step. The authors gloss over this issue because for both MuJoCo environments that they tested, they only need to consider reward at the end of the planning horizon. Thus to make their algorithm generally applicable, the authors also need to show how or whether their method can deal with rewards that may appear at any time step.\n\nFurther, rather than speculate on the cause of the difference between their PCP and PETS (Chua et al 2018) on half-cheetah to be their different settings for CEM optimization (Fig 7b), the authors should just use the same settings to compare. Possibly the authors ran out of time to do this for the current submission, but should certainly do it for the final version.\n\nWhile the authors have already compared to other algorithms with similar aims, eg Chua et al 2018, they may also wish to compare to a recent preprint Clavera et al Sep 2018, which also aims to combine the sample efficiency of model-based methods while achieving the performance of model-free ones, by using an ensemble of models, over a 200 time step horizon. However, given the recency of this algorithm, I don't consider this essential.\n\nOverall, I feel that the authors idea of an R-step model is worth spreading in the community, if the above two main points are addressed. At the same time, I can only rate it at the border of the cutoff mark."", 'The paper proposes to use a multi-step prediction model in model-based RL. The proposed model maps from current state and a sequence of actions to the state after taking those actions. The paper demonstrates on 2 tasks that in a model-predictive control loop combined with planning by cross-entropy method, this can yield better asymptotic performance than using single-step models.\n\nThe insight of using multi-step prediction models is certainly appealing and makes a lot of sense in deterministic tasks. A systematic empirical comparison of multi-step deep models in RL is of interest, which this paper does provide to some extent.\n\nAn obvious limitation of the proposed deterministic multi-step forward model is the restriction to deterministic systems. One would expect that the performance deteriorates quickly as the system becomes more stochastic. An extension to the stochastic case along the lines of Chua et al, 2018 is non-trivial as capturing the stochasticity is typically more challenging in long-term predictions. Yet, the paper makes an additional assumption that is less clearly communicated: To be able to plan with a R-step model, one needs to be able to evaluate or approximate the sum of R rewards just from the first and last state in that R-long sequence. This work uses simply the reward at the end r(s_{t+R}) as a proxy which works well in these MuJoCo tasks but can fail horribly in others. One can imagine that a model not only outputs s_{t+R} but also the sum of R rewards given s_t and a_{t:t+R} which could work in more general settings but this is not explored in this paper. The contribution in this paper limited as the proposed approach as well as the experimental comparison is restricted to a relatively specific class of problems and no attempts to generalize are made.\n\nThe experiments nicely compare against using single-step dynamics models and the results show that using the multi-step models for MPC performs better in the two considered tasks. However, as fas as I understand both the ACP and Chua et al baseline using the single-step prediction accuracy to train their models. The paper is missing a comparison to single-step models that are trained using multi-step prediction losses (""backprop through time"" as in Learning Nonlinear Dynamic Models by Langford et al 2009). These models should be much more robust to error blow-up for multi-step prediction and do not require the specific reward structure assumed in this paper.\n\nThe proposed R-step model-based RL approach could be connected to the use of options (the planner and model operate on R-step options, but the MPC does update the policy after every time step). It would be interesting to discuss this potential connection in the paper. The paper does a good job of discussing existing recent work in the deep RL literature but it would also be good to also discuss earlier work on multi-step prediction (e.g. in time-series modeling).\n\nAll in all, I think the paper makes a small contribution demonstrating that multi-step models are useful for model-based RL in specific domains -- which is interesting but certainly not surprising. Unfortunately the paper stops somewhat early by not comparing to relevant baselines (single-step models trained with multi-step losses) and by not considering tasks where the benefit of multi-step planning would be less clear.']","[-20, -20, -20, -20]","[50, 50, 50, 60]","[""The sentiment score is slightly negative (-20) because the reviewer expresses some uncertainty about the paper's contributions beyond the main point they identified. The phrase 'I was not able to find points beyond this' suggests a lack of substantial content or novelty in the paper. However, the score is not deeply negative as the reviewer acknowledges a main contribution and seems open to clarification. The politeness score is moderately positive (50) due to the reviewer's use of respectful language. They express their concerns politely, using phrases like 'As far as I am concerned' and 'I would be happy if the authors could clarify'. The tone is constructive rather than critical, inviting the authors to provide more information rather than outright dismissing the work."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's novelty, they express several concerns about the approach. The reviewer points out limitations such as producing open-loop plans and potential data requirements issues. However, the tone is not entirely negative, as they suggest improvements and areas for further discussion.\n\nThe politeness score is moderately positive (50) because the reviewer uses respectful language throughout. They frame their concerns as suggestions ('I suggest', 'I recommend') rather than demands. The reviewer also acknowledges potential strengths of the approach ('I could see this working well in practice') before discussing limitations. The language is professional and constructive, offering specific recommendations for improvement without being harsh or dismissive."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the potential value of the authors' idea, they express significant concerns and state that the paper is 'at the border of the cutoff mark.' The reviewer points out two main issues that need to be addressed and suggests additional comparisons, indicating a cautious and somewhat critical stance. However, the reviewer also recognizes the worth of the core idea, preventing a more negative score. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'The authors may wish to' and 'I feel that,' which are polite ways of offering suggestions. The reviewer also acknowledges potential time constraints for the authors, showing consideration. While critical, the feedback is presented constructively without harsh or rude language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('The insight of using multi-step prediction models is certainly appealing'), they also point out several limitations and missing comparisons. The review suggests that the paper's contribution is 'limited' and 'stops somewhat early'. The overall tone indicates that the reviewer sees potential in the idea but feels the execution and scope are lacking. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before discussing its weaknesses. They use phrases like 'The paper does a good job' and 'it would be interesting to discuss' which maintain a constructive tone. The reviewer also offers specific suggestions for improvement, which is a polite way to address shortcomings.""]"
"['This paper empirically studies the robustness of equivariant CNNs to rotations as well as adversarial perturbations. It also studies their sample efficiency, parameter efficiency, and the effect of rotation- and adversarial augmentation during training and/or testing. \n\nThe main findings are:\n1) Rotation-equivariant networks are robust to small rotations, even if equivariance to small rotations is not directly built into the architecture\n2) Applying rotational data augmentation increases robustness to rotations\n3) Equivariant networks are more sample efficient than CNNs and outperform them for all dataset sizes.\n4) Applying rotational data augmentation decreases robustness to adversarial perturbations, and this effect is more pronounced for GCNNs.\n\nIf true, this is a valuable addition to the literature. It is one of the first independent validations of claims regarding sample complexity and accuracy made by the authors of the various equivariant network papers, performed by a party that does not have their own method to promote. Many of the findings do not have an obvious explanation, so the data from this paper could conceivably prompt new theoretical questions and investigations.\n\nThe authors chose to highlight one finding in particular, namely that GCNNs become more sensitive to adversarial perturbations as they are trained on more heavily rotation-augmented data. However, this appears to be true for both CNNs and GCNNs, the difference being only in degree (see fig 4, 10, 11). This is not apparent from the text though, as e.g. the abstract notes that ""robustness to geometric transformations in these models [equivariant nets] comes at the cost of robustness to small pixel-wise perturbations"".\n\nSince HNets, GCNNs and RotEqNets should be exactly equivariant to 90 degree rotations (and some others, perhaps), it is surprising that figure 1 shows a continuing decline in performance with bigger and bigger random rotations. If the network is made rotation invariant through some pooling layer at the end of the network, one would expect to see a decline in performance up to 45 degrees, followed by an increase back to baseline at 90 degrees, etc. \n\nPolar transformer networks achieve good results in fig. 1, but I wonder if this is still true for rotations around points other than the origin.\n\nSince CNNs and GCNNs differ in terms of the number of channels at a certain number of parameters, and differ in terms of number of parameters at a certain number of channels, it could be that channel count or parameter count is the more relevant factor, rather than equivariance. So it would be good to make a scatterplot where each dot is a network (either CNN or GCNN, at various model sizes), the x-axis is parameter count (or in another plot, 2d channel count), and the y-axis corresponds to the accuracy. This can be done for various choices of augmentation / perturbation. The type of network (CNN or GCNN) could be color coded. If indeed the CNN/GCNN variable is relevant, that should be clearly visible in the plot, and similarly if the parameter count or channel count is relevant. One could also do a linear regression of accuracy or log-accuracy or something using CNN/GCCN, param-count, channel-count as covariates, and report the variance explained by each. \n\nIn several plots, e.g. fig 4, 8, the y-axes do not have the same range, making it hard to compare results between subplots. \n\nThe experiments have some weaknesses. For one thing, it seems like each accuracy value reported comes from a single training run. It would be much preferable to plot mean and standard deviation / error bars. Another weakness is that all experiments are performed on MNIST. Even just a simple validation of the main findings on CIFAR would significantly strengthen the paper.\n\nBecause of the limited scope of the experiments, it is not clear to me how generalizable and robust the experimental results are. With deep network performance it can be hard to know what the relevant hyperparameters are, and so we may well be reading tea leaves here.\n\nIt is also unfortunate that no explanation for the observed phenomena is available. However, it is conceivable that the findings presented in this paper could help researchers who are trying to understand adversarial attacks / robustness, so it is not a fatal flaw. I am certainly glad the authors did not make up some unsupported story to explain the findings, as is all too common in the literature these days.\n\nOverall, I consider this a borderline paper, and am tending towards a reject. My main considerations are:\n1. Uncertainty about generalizability\n2. Uncertainty about usefulness to practitioners or theorists (admittedly, this is hard to predict, but no clear use-case is available at this point)\n3. A lot of data, but no clear central finding of the paper', 'Using the dataset MNIST, the authors empirically studied the robustness of several rotation-equivariant neural network models(GCNN, H-Nets, PTN, et al.) to geometric transformation and small pixel-wise perturbations. Their experiments showed that the equivariant network models(StdCNNs, GCNNs, H-Nets, et al.) are robust to geometric transformation but vulnerable to pixel-wise adversarial perturbations. These findings help us understand the  neural network models better.\nHowever, this paper is not acceptable due to lack of innovation and novelty. ', ""This paper empirically studies various CNN robustifying mechanisms aiming to achieve rotational invariance. The main finding is that such robustifying mechanisms may lead to lack of robustness against pixel-level attacks such as FGSM and its variants. The paper does a comprehensive job in studying relevant robustifying schemes and attacks strategies. However, the paper does not present sufficiently new information worthy of a regular conference paper, it can be a good workshop paper though for the Robust Learning community. Some analytical insights would really strengthen the work. Also, from an empirical standpoint, the authors need to consider other data sets beyond just the MNIST data set.   \n\nxxxxxxxxxxxxxx\n\nWhile I appreciate the authors' rebuttal and revisions, I still do not see sufficient contribution here worthy of a regular ICLR paper.  ""]","[-20, -50, -60]","[60, 20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('valuable addition to the literature', 'independent validations'), they ultimately lean towards rejecting it ('tending towards a reject'). They express several concerns about the paper's limitations and generalizability. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the potential value of the work, and provides constructive feedback. They avoid harsh criticism and use phrases like 'It would be good to...' and 'I am certainly glad...' which contribute to a polite tone. However, the review is not overly effusive in its praise, maintaining a professional and balanced approach."", ""The sentiment score is -50 because while the reviewer acknowledges the empirical study and its findings in a positive light, they ultimately conclude that the paper is not acceptable due to lack of innovation and novelty. This negative conclusion outweighs the initial positive comments. The politeness score is 20 because the language used is generally neutral and professional, with some positive acknowledgment of the work done. However, the final rejection is stated directly without much softening language, which prevents a higher politeness score. The reviewer provides constructive feedback on the study's findings before stating the rejection, which adds to the politeness."", ""The sentiment score is -60 because the reviewer expresses several criticisms and ultimately does not recommend the paper for publication, stating it lacks 'sufficient contribution' for a regular conference paper. However, they do acknowledge some positive aspects like the 'comprehensive job' in studying relevant schemes. The politeness score is 20 because the reviewer uses generally respectful language and acknowledges the authors' efforts ('I appreciate the authors' rebuttal and revisions'), but maintains a critical stance without excessive softening of the criticism. The reviewer provides constructive feedback and suggestions for improvement, which contributes to the slightly positive politeness score.""]"
"[""The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors rather than the dimension of the embedding vectors. \n\nThe list of my concerns or cons of this paper is:\n\n- For the main theorem, i.e., Theorem 1, \na.) why is it intuitive that the size of the training dataset required for learning a norm constrained graph embedding is O(C|A|_2). This is not that intuitive to me. Later, the authors argue that graphs are usually sparse and average node degree is usually smaller than the embedding size, thus it is easily overfitting the training data. However, I would say, in practice, the positive training pairs are not restricted to 1-hop neighbors, but could also be 2 or more hops, in that case, it won't easily overfit. \nb.) the main result from the theorem is that the error gap of norm constrained embeddings scales as O(d^-0.5(lnn)0.5), but I did not see how this is related to the norms of the embedding vectors and how is this evidenced in the empirical studies? It might be better to show a plot of error gap vs. d and/or n. \nc.) how is this analysis related to the later claim that \\lambda_r controls the model capacity of linear graph embedding?\n\n- The linear graph embedding framework considered in this paper assumes that each node only has one set of embeddings, but in practice, one node usually has two sets of embeddings as context node or a center node. How would this affect the whole analysis and claims?\n\n- How would the claims or analysis in this paper be generalized to non-linear graph embedding frameworks?\n\n- For the experiments, \na.) In Figure 3, the y label of (b) is missing, and the Average L2 norm of (c) cannot reflect the Generalization performance \nb.) In Figure 4(a), why after overfitting, we can still observe that the test accuracy increases?\nc.) In Figure 5, why the test precision first increases and then decrease with more regularization?"", 'The manuscript proposes a theoretical bound on the generalization performance of learning graph embeddings. The authors find that the term in the generalization bound that represents the function complexity involves the norm of the learnt coordinates, based on which they argue that it is the norm of the coordinates that determines the success of the learnt representation.\n\nI am not very familiar with the literature on graph embeddings; however, to the extent of my understanding of the paper, I have a number of concerns:\n\n- In a generalization bound like in Theorem 1, it is very typical of the generalization error to include a term that represents the complexity of the hypothesis function class. In the presented result, this would be the second term on the rhs, which involves the spectral norm of the adjacency matrix and the bounds on the norm of the learnt coordinates. This term captures the Rademacher complexity of the hypothesis function class. In my understanding, there is nothing really surprising about this: most of the results in learning theory would include a term directly or indirectly related to some norm on the hypothesis function class. However, it would then require a lot of further justification to conclude that the key factor determining the performance is the norm of the learnt representation based on this.\n\n- I am not sure if the results in Figure 1 provide a really meaningful justification about the importance of norm. It is observed that the norm increases during the epochs, however, shouldn’t we be also checking the evolution of the error at the same time to draw a meaningful conclusion? In particular, the norm seems to increase rather monotonically throughout the epochs, whereas we expect the error to decrease first, reach an optimal, and then start increasing due to overfitting. So can we really say that the error is proportional to the norm?\n\n- Similarly, in the results in Figure 5, we can observe that the regularization coefficient has an optimal value that maximizes the precision. On the other hand, the norm of the learnt coordinates is expected to decrease monotonically with increasing lambda_r. Again, it seems difficult to conclude that the norm is the key factor determining the error.\n\n- Minor comments: \n1. In page 2, in the expression of L, the node u should be in set V, I guess.\n2. Please define the function sigma used in the objective functions. \n3. Typo right under Section 3 title: “grpah”\n4. The definition of matrix A_sigma is not clear to me. What does the “there exists y” expression mean in the first line?\n\n- To sum up, my feeling is that the presence of the term involving the norm in Theorem 1 is rather classical in learning theory, and its importance seems to be over-emphasized in this study. Moreover, I am not fully convinced about the experimental evidence. Therefore, I cannot recommend accepting this paper. ', 'In this paper, the authors proved that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors, rather than the dimensionality constraints. Interestingly, along with the analysis of Levy & Goldberg (2014), they found that linear graph embedding methods are probably computing a low-norm factorization of the PMI matrix. Correspondingly,  experimental results are provided to support their analysis.\nOverall, this work is theoretically complete and experimentally sufficient.\n\n1. it is unclear whether the embedding dimensions of all cases (with varying value of \\lambda_r) are fixed as a constant in Fig. 1 - Fig. 3.\n\n2. Figure 4 shows the impact of embedding dimension on the generalization performance. Are these results obtained after 50 SGD epochs? Comparing Fig.4 (a) with Fig.3 (a), we may infer that the results in Fig.3(a) when \\lambda_r = 0 are obtained by setting the embedded dimension as about 10^2. How about the generalization performance during SGD for \\lambda_r = 0 if the embedded dimension is set to be smaller than 10?\n\n3. In Claim 1, the degree d and the dimension D are mixed. ']","[-30, -70, 80]","[60, 50, 50]","[""The sentiment score is slightly negative (-30) because while the reviewer acknowledges the paper's main contribution, they list several concerns and criticisms without balancing them with positive aspects. The review focuses primarily on limitations and areas needing clarification or improvement. However, it's not entirely negative as the reviewer recognizes the paper's theoretical and empirical contributions. The politeness score is moderately positive (60) because the reviewer uses neutral, professional language throughout. They frame their criticisms as 'concerns' and use phrases like 'It might be better' instead of more direct criticisms. The reviewer also asks questions for clarification rather than making accusatory statements. The tone is constructive and aimed at improving the paper rather than dismissing it outright."", ""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's methodology and conclusions, stating they 'cannot recommend accepting this paper.' The review points out several issues with the authors' interpretations and the experimental evidence, indicating a largely negative sentiment. However, it's not entirely negative as the reviewer acknowledges some aspects of the work and provides constructive feedback. The politeness score is 50 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I am not very familiar with...' and 'to the extent of my understanding,' which show humility. The reviewer also provides specific, constructive feedback and explains their reasoning, which is a polite way to critique. The language is not overly formal or deferential, but it avoids any rudeness or harsh criticism, striking a balance between directness and courtesy."", ""The sentiment score is 80 because the reviewer expresses a very positive overall assessment, stating the work is 'theoretically complete and experimentally sufficient.' This indicates strong approval. However, it's not 100 as there are some questions and suggestions for improvement. The politeness score is 50 because the language is professional and respectful, but not overly formal or deferential. The reviewer directly states their observations and questions without softening language, which is neutral in tone. The critique is constructive and presented objectively, without harsh criticism, maintaining a polite but straightforward approach.""]"
"['The paper studies fatigue monitoring of EEG driving simulator experiments using various EEG analysis algorithms, one also based on ranking. The data used was from a prior experiment. \n\nThe paper is written in a rather confusing manner, which makes the assessment of originality and significance a hard task for the reviewer. A novel algorithm Bdrank (based on raking is defined) and compared to 2 other algorithms; unclear why with these and not with others. The paper ignores a large portion of the literature, starting with Kohlmorgen et al 2007, Blankertz group, Lee group etc. \nThe results  are only somewhat interesting, no understanding of the underlying physiological processes is given. \n\nOverall, I consider the paper somewhat preliminary. ', 'The paper proposes an algorithm for mental fatigue monitoring, relating a subjects\' EEG signals to their reaction time (RT) during a simulated driving task, as an ordinal regression problem. The authors argue that RTs could be heavily skewed and/or non-smooth, making traditional regression approaches unstable due to outlier values. They propose a brain dynamic ranking algorithm,  BDrank, using a generalized EM algorithm to estimate its parameters, and compare it to support vector regression and Logistic Ordinal Regression, where they show improved performance by accuracy and root mean squared error (RMSE) over a database of 44 subjects.\n\nGeneral comments, in no particular order:\n\n1. There are some minor grammatical errors throughout. The paper could benefit from another read-through to correct these errors.\n\n2. It is unclear to me how the model works at test time; as the model is essentially building a relational structure in the data, does the user have to provide multiple EEG trials at time of prediction?\n\n3. There is notation early on in the paper that doesn\'t appear to be appropriately defined. For example, Equation (1) describes two sets of propositions, with M1 and M2 elements, respectively. How is M1 and M2, the total set of propositions, calculated? It appears to be all pair-wise comparisons of RTs but then it\'s unclear why there are two indexes associated with them. Also, what does it mean for the orderings to be significant? (i.e.: that the ""type-1 preference propositions that the orderings between the RTs are significant""). The authors then switch to a new notation x^1 and x^2 without defining them. Notational problems also persist throughout the paper, making it hard to gauge what is being done at each step.\n\n4. The authors describe using an FFT to transform the EEG data into the frequency domain. I\'m assuming they are doing the FFT on the entire 10-s interval but the paper does not make this clear. Also, the authors state using EEG power between 0-30Hz for their analysis; do they further sub-divide this range (for example, to the standard theta/alpha/beta power ranges) or just use the power across the entire 0-30Hz band?\n\n5. I am concerned with the relatively sparse set of comparison algorithms the authors use. The authors only compare to relatively simple approaches (support vector regression and logistic ordinal regression), yet they cite many previous works in this area but do not compare against them, instead just leaving a pretty generic statement of ""The regression assumption of this method between EEG signals and RT is not correct""; they do not elaborate on this aspect.\n\n \nOverall I think there is limited novelty in the approach; the idea to learn the structure of the data relationally instead of absolutely is pretty straight-forward, and is a standard practice for example in non-parametric statistical modeling. I am also not positive that ICLR is the best venue for this work; perhaps a better avenue for this would be in a more BCI/neural engineering-focused venue.\n', 'The mental fatigue is an important factor in road accidents. Finding a direct mapping between EEG features and reaction time is difficult and error-prone, combining the noise measurement of EEG and individual variation of RT.  The authors introduce a measure called BDrank based on partial ordering instead of regression. Formulating the measure as a MAP problem, the authors propose a generalized EM algorithm for prediction. An online extension, relying on iterative L-BFGS optimization over mini-batches. \n\nFigure 3 shows the indegree sequence for 4 selected subjects. What is the criterion to select these subjects? These cases seem interesting, but is it representative for the best/worst case? It could provide some information to show some of the few cases where SVR is more accurate than BDrank.\nRegarding the identification of noisy channels, the 33rd channel is indicated as a non-EEG one. What is it?\n\nSome minor questions and suggestions:\n- It could be interesting to mention the performance of this measure using only a limited set of EEG channels to evaluate its robustness.\n- The introduction indicates that de Naurois et al ., 2017 rely on EEG to estimate the RT, but it is not the case.\n- The formulation of the assumption (2) on page 3 is unclear, as sensors are not supposed to make any emission and there is a high correlation between channels.\n- The model do not consider transition between type-1 and -2 preference, could it be a problem with confidence interval']","[-50, -40, 50]","[-20, 20, 75]","[""The sentiment score is -50 because the review is generally negative, with phrases like 'written in a rather confusing manner', 'ignores a large portion of the literature', and 'somewhat preliminary'. However, it's not entirely negative as it acknowledges some interesting results. The politeness score is -20 because while not overtly rude, the language is quite direct and critical without much softening. Phrases like 'confusing manner' and 'hard task for the reviewer' come across as somewhat impolite. The reviewer doesn't use any particularly polite language or attempt to balance criticism with positive feedback, which contributes to the slightly negative politeness score."", ""The sentiment score is -40 because the review is generally critical, pointing out several issues with the paper such as grammatical errors, unclear notations, and limited novelty. The reviewer also suggests that ICLR might not be the best venue for this work. However, it's not entirely negative as the reviewer acknowledges some positive aspects, like the proposed algorithm's improved performance. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'I think' and 'I am concerned' rather than making blunt statements. The reviewer also provides specific, constructive feedback, which is a polite way to critique. The language is not overly formal or polite, but it's not rude either, hence a slightly positive score."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance of the topic and the novel approach introduced by the authors, while also providing constructive criticism and suggestions for improvement. The review doesn't contain overtly negative comments, but rather focuses on areas for clarification and potential enhancements. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, poses questions and suggestions in a non-confrontational manner, and acknowledges the interesting aspects of the work. The reviewer maintains a professional tone, avoiding harsh criticism and instead offering constructive feedback to improve the paper.""]"
"[""The paper proposes a iterative approach at inference time to improve object detections. The work relies on updating the feature activations and perform new feed forward passes to obtain improved results. \n\nPros:\n(+) The idea of iterative inference is potentially effective\n(+) The paper is well written and clear\n(+) The authors show results on compelling benchmarks\nCons:\n(-) Reported improvements are very small\n(-) Important baselines are missing\n\n\nFirst, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more FLOPs at test time. For N-stages, the approach requires xN more operations for forward passes  and xN for backward passes. This is a serious shortcoming as it adds compute time per image for the inference stage and cannot be parallelized. \n\nThe authors show small improvements for AP on COCO. From their analysis, it seems that the biggest gains come from N=1 stages, while the improvement added for N>1 are miniscule (Table 1). Note that the authors show results on COCO minival (5k images) and from my experience there, it's expected to see a +/- 0.2% AP between different trained models of the same architecture. The authors report a +0.46% gain. \n\nIn addition, the authors do not provide results for other baseline approaches that have similar FLOPs at test time, such as iterative bounding box regression and input scale augmentation. Note that both these approaches do not add any parameters and require no additional memory, but add to the FLOPs at test time. From my personal experience, test time augmentations can add +1.5% to the final performance. Concretely, look at Mask R-CNN arXiv Table 8 last two rows. Test time augmentations add 1.5% on top of an already enhanced model. Empirically, the better the model the harder it is to get gains from inference tricks! And still test time augmentations boost performance significantly.\n\nGiven the small gains and the lack of competing baselines, it is hard to make a case for accepting the paper. "", 'The paper proposes a method called cautious inference to improve inference accuracy for object detection models. The main idea is inspired by the previous work of Guided Perturbations, which is applied to fully convolutional networks to improve the segmentation/accuracy accuracy purely during inference time.  The original idea is to use the predicted labels of the network as pseudo ground truths (after making the predictions to be a one-hot vector), and then back propagate the error signals to the network input to get the gradients. And finally the gradients are added back to the original inputs to perform another round of prediction. Here the inputs can be either the original image, or some intermediate feature maps. Experiments are shown for both 2D and 6D object detections. \n\nComments:\n\n- I think overall it is an interesting idea to directly alter the input of the network in order to fit to the testing distribution. However, the motivation and story told in the introduction is a bit of an oversell compared to the experiment validation section. Most of the results shown are just doing training and testing of images drawn from the *same* distribution. Like coco train and test, or VOC train and test. It would be great to see if the cautious inference would work when the distribution is different. For example ""elephant in the room"" case, or new object categories are added during testing.\n\n- I am actually curious to see this method can be used to improve the AP on the *training* set as well, just to understand it better -- is it trying to recover the generalization error of the network, or it is doing some implicit context reasoning inference that can help training as well. \n\n- It might be better to compare/combine the method to other inference-only improvements for object detection. For example there is soft-NMS, \nBodla, Navaneeth, et al. ""Soft-nms—improving object detection with one line of code."" Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.\n\n - I am not sure I fully understand B-box part: I think it is easy to have B-obj and B-cls as one can just take the max of the class prediction and then use the inferred class label for one-hot vector construction, but I am confused about the box part as no ground-truth is given during testing. In Table 2 I also cannot find BP improving performance by itself in anyway.\n\n- For COCO, please report results on test-dev set, the minival set images are used only for validation. \n\n', 'The goal of the paper clearly motivated and well described. However, the notations and figures are more complicated than necessary; hence, it is a bit hard to follow the paper in detail. There are also some missing related works about domain adaptation for object detectors. For instance,\nChen et al. ""Domain Adaptive Faster R-CNN for Object Detection in the Wild"" In CVPR 2018.\nInoue et al. ""Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation"" In CVPR 2018.\nThe authors should cite these papers and compare with their performance.\nFinally, the proposed method doesn\'t consistently improve the detection accuracy. \nThe proposed method also slows down the frame rate of the detector due to multiple iterations of feedforward/feedback inferences.']","[-60, 20, -20]","[50, 60, 50]","[""The sentiment score is -60 because while the reviewer acknowledges some positive aspects ('pros'), the overall tone is critical and skeptical. The reviewer points out significant shortcomings, such as small improvements, missing baselines, and increased computational cost. The conclusion suggests rejecting the paper, which strongly indicates a negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, acknowledging both pros and cons. They provide constructive criticism and explain their concerns in detail without using harsh or dismissive language. However, the review is not overly polite or praising, maintaining a neutral to slightly positive tone in terms of politeness."", ""The sentiment score is slightly positive (20) because the reviewer describes the paper's idea as 'interesting' and provides constructive feedback. However, they also point out several limitations and areas for improvement, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions (e.g., 'It might be better to...'), and shows genuine interest in understanding the work better (e.g., 'I am actually curious to see...'). The reviewer maintains a professional and courteous tone while providing detailed feedback, avoiding harsh or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper's goal is clear and well-described, they point out several issues: complicated notations and figures, missing related works, inconsistent improvement in detection accuracy, and slower frame rate. These criticisms outweigh the initial positive comment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and specific suggestions for improvement without using harsh or dismissive language. They begin with a positive comment and phrase their criticisms as observations rather than direct attacks, maintaining a professional tone throughout the review.""]"
"[""This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.\n\nIt is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.\n\nThe proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.\n\nOverall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me."", 'This paper considers the problem of recovering the lowest layer of a deep neural network whose architecture is ReLU or sign function followed by a polynomial. This paper relies on three assumptions: 1) the lowest layer has a high threshold (\\Omleg(\\sqrt{d})), 2) the polynomial has 1/poly(d) lower bouned and O(1) upper bounded linear terms and is monotone 3) the input is Gaussian. Under these assumptions, this paper shows it is possible to learn the lowest layer in precision \\eps in poly(1/eps, d) time.\n\nThe proposed algorithm has two steps. The first step is based on the landscape design approach proposed by Ge et al. (2017) and the second step is based on checking the correlation. \n\nProvably learning a neural network is a major problem in theoretical machine learning. The assumptions made in this paper are fine for me and I think this paper indeed has some new interesting observation. My major concern is the writing. There are several components of the algorithm. However, it is hard to digest the intuition behind each component and how the assumptions are used. I suggest authors providing a high-level and non-technical description of the whole algorithm at the beginning. If authors can significantly improve the writing, I am happy to re-evaluate my comments and increase my rating.', 'This paper gives provable recovery guarantees for a class of neural networks which have high-threshold activation in the first layer, followed by a ""well-behaved"" polynomial, under Gaussian input. The algorithm is based on the approach by Ge et al. (2017), as well as an iterative refinement method.\n\nWhile this could be an interesting result, I have several concerns regarding the assumptions, correctness, and writing.\n\n1) It is required that the threshold is at least sqrt{log d} (Thm. 1), where d is the number of hidden neurons in the first layer. It seems that this essentially zeros out almost all the neurons, since the maximum among d Gaussian random variables is roughly sqrt{log d}. The authors should explain what exactly this model is doing, i.e., what kind of functions it can compute, in order to justify why this is an interesting model.\n\nFurthermore, the authors claim that the studied model is a ""deep"" neural network, but I disagree. As I understand, the difference between this model and two-layer networks is that the second layer here is a polynomial instead of a linear function. This doesn\'t make it a deep network since the (polynomial) part above the first layer is not modeled in a layer-wise fashion, not to mention that under the setting considered in the paper the polynomial behaves similar to a linear function.\n\n2) It is stated at the end of Section 2 that the angle can be reduced by a factor of 1-1/d ***with constant probability***. How does this ensure you can succeed after O(d log(1/nu)) iterations? As far as I see you need the success probability in one iteration to be at least something like 1-1/Omega(d) so that you can apply a union bound.\n\n3) Even if the issues of motivation and correctness are clarified, I find it very difficult to understand the overall intuition and main technical contributions in this paper. The writing needs to be significantly improved to reach the level of a top conference.']","[-70, -20, -60]","[20, 50, 20]","[""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's assumptions, describing them as 'unrealistic' and 'too many'. The reviewer states that the assumptions are 'very far from true in any real neural networks', indicating a strong negative sentiment towards the paper's approach. However, the score is not at the extreme negative end because the reviewer acknowledges the difficulty of the problem and shows some understanding for making strong assumptions in general. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'It is unclear' and 'I'm OK with' rather than harsh or rude language. The reviewer also acknowledges the difficulty of the problem, which shows some empathy. However, the score is only slightly positive as the review doesn't go out of its way to be exceptionally polite or encouraging."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('new interesting observation'), they express a 'major concern' about the writing and suggest significant improvements are needed. The overall tone indicates hesitation to fully endorse the paper without revisions. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive feedback, and expresses willingness to re-evaluate their comments. Phrases like 'I suggest' and 'I am happy to re-evaluate' contribute to the polite tone. The reviewer balances criticism with positive remarks, maintaining a professional and courteous approach."", ""The sentiment score is -60 because the review expresses several significant concerns about the paper, including issues with assumptions, correctness, and writing. The reviewer states that while the result 'could be interesting,' they have 'several concerns' and find it 'very difficult to understand' the main contributions. They also disagree with the authors' claim about the model being a 'deep' neural network. These criticisms outweigh the initial positive statement about the paper's potential interest.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone throughout. They use phrases like 'could be an interesting result' and 'The authors should explain' rather than using harsh or dismissive language. However, the review is not overly polite either, as it directly states problems and disagreements without much softening language, hence the modest positive score.""]"
"['The submission explores Breiman\'s dilemma: training margin is not always a good predictor of test error.\n\nIn particular, the authors show that:\n\n- For under-parametrized CNNs, the training prediction margin is a good predictor of the test error.\n- For over-parametrized CNNs, the training prediction margin is not a good predictor of the test error.\n\nThroughout the submission, I suspect that the authors compute the ""functional margin"", that is, the difference between the largest label score and the second largest score, for correctly classified examples. Functional margins ignore the smoothness of the underlying function, a critical factor for generalization. For instance, the function f(x) = 1[x > 0] has large functional margin, but any perturbation around the x-origin would drastically change the prediction. For this reason, I think the authors should consider the ""geometrical margin"" instead, which is unfortunately difficult to compute for general neural networks. Their theory tries to reflect on this issue by using spectrally-normalized bounds, but the practice ignores this issue completely (as far as I can tell).\n\nTherefore, we may be looking at the wrong statistic to predict generalization error. Is Breiman dilemma solved by re-defining margin properly? Geometrical margin can be computed in closed-form for linear classifiers, so perhaps this would be a first step in this investigation.\n\n', ""The authors found that general generalization bounds fail to capture the ramp loss. However, once the network scaled by its Lipschitz constant, it becomes efficient to get an upper bound of generalization error, while also needs to trade-off the constant in the margin error. Due to the limitation of fixing the constant in margin error, the authors  tried to use the quantile margin to change the bound, which is easy to tune the hyper-parameter. They also conducted the experiments that the quantile margin generalization bound could be used to predict the tendency of loss curve both in training and test in some sense.\n\nIt's really an interesting work to provide a way for early stopping and to show the quantile margin maybe a substitution of tendency in training error as well as test error.\n\nQuestions: \n\nIt could be difficult to judge from the phase transition, if exists, in the evolution of normalized margin distributions curve. Maybe  some quantitative descriptions are needed. \n\nBesides, the authors' quantile margin bound (Theorem 2) shows the upper bound of margin (or say margin error). But the bound is not direct to support the powerful experiments results, the relationship between the tendency of quantile margin, training and test error.\n\nTypos:\n In Eqn. (10), the first $f_t$ should be $\\widetilde{f}_t$ .\nIn Eqn. (9) and (11), there is  $1$.\nIn Proof in Lemma A.1, the convolution operator is $x(v)$ not $x(u)$, since Lemma is also true.\nIn Proof in Lemma D.4, though the proof is same in the book `Foundations of machine learning' by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, please check the typo.\nIn Proof of Proposition 1, lack $\\frac{1}{n}$ in Rademacher complexity.\nIn Proof of Theorem 1, maybe you should take $\\mathbb{E}$ not $\\mathbb{P}$ before $\\ell_{\\gamma_1,\\gamma_2}(\\xi(\\widetilde{f}(x,y))))$.\nIn Proof of Theorem 2, page 18 the last line in the equation, why can the second term after divided by $L_f$ bounded by $L$, maybe need some conditions or I missed something.\n\n"", ""Summary: \nThe authors investigate the Breiman’s dilemma in the context of deep learning. They show generalization bounds in terms of the margin distribution. They also perform experiments showing the Breiman’s dilemma.\n\nComments: \nI am afraid the authors miss an important related paper:\n\nLev Reyzin, Robert E. Schapire:\nHow boosting the margin can also boost classifier complexity. ICML 2006: 753-760\n\nReyzin and Schapire explain the Breiman’s dilemma based on base classifiers’ complexity. In particular, their experiments show that arc-gv tends to use more complex decision trees than AdaBoost while it achieves better margin distribution over sample. That is, not only margin distribution, but also the complexity of base classifiers’ class matters. This is already explained by known Rademacher complexity based margin bounds.\n\nAs for quiantile-based analyses on margin bounds the following result is known:\n\nLiwei Wang et al: A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin, \nJournal of Machine Learning Research 12 (2011) 1835-1863.\n\nThey proved a shaper bound using the notion of equibrium margin. The authors should compare the presented results with this. \n\nThe technical results of the paper look quite similar to known margin bounds and I am afraid the contribution is minor or redundant.\n\nAfter the rebuttal:\nI read the authors' comments and understand more the technical results. I raised my score. But I still feel that the techniccal contribution is a bit weak.\n\n""]","[20, 60, -50]","[60, 70, 20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the authors' work and findings, but also expresses concerns and suggests improvements. The review starts by summarizing the authors' contributions, which is positive. However, the reviewer then points out potential issues with the methodology and suggests alternative approaches, indicating a mix of positive and critical feedback. The politeness score is moderately high (60) because the reviewer uses respectful language throughout, avoiding harsh criticism. They use phrases like 'I suspect' and 'I think' to soften their suggestions, and frame their concerns as opportunities for further investigation rather than outright flaws. The reviewer also asks a thought-provoking question at the end, encouraging further exploration of the topic in a constructive manner."", ""The sentiment score is 60 (positive) because the reviewer describes the work as 'really an interesting work' and acknowledges its potential for early stopping and as a substitute for tendency in training and test error. However, it's not extremely positive as the reviewer also raises some questions and points out areas for improvement. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions (e.g., 'Maybe some quantitative descriptions are needed'), and even takes the time to point out typos in a helpful manner. The reviewer maintains a professional and constructive tone throughout, without using any harsh or dismissive language."", ""The sentiment score is -50 because the reviewer expresses several concerns about the paper, including missed related work and a perception that the technical contribution is minor or redundant. However, they do acknowledge some positive aspects after the rebuttal, slightly improving their view. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language such as 'I am afraid' and 'The authors should compare' rather than using harsh or rude phrasing. They also show willingness to reconsider their position after the rebuttal, which indicates a respectful approach.""]"
"['[Summary]\nThis paper proposes an extension of the dual learning framework, with a guider network and multiple languages included: (1) Each language $i$ has a guider network $GN_i$, that can be used to reconstruct the source sentence from either the output of the encoder or the output of the decoder. (2) Multiple languages are used in this framework, where each language also has a $GN_i$  for guiding the training according to the reconstruction error. The authors work on MultiUN dataset to verify their algorithms.\n \n[Clarity]\nThis work is not easy to follow. My suggestions to revise the paper are shown as follows:\n(1) Please use the \\begin{equation}…\\end{equation} environment to clearly describe your framework and training objectives, with each notation, function and hyper-parameter clearly defined. Actually, I do not find the training objective function in this paper.\nBesides, currently, in this paper, there are many undefined notations and typos, for example, (1) in section 3.1, first paragraph, what is the $n$? Then in Eqn.(1) ,what is $N$ and $M$? Also, it is very confusing to use subscripts $i$ and $j$ to distinguish the hidden states from the encoder and decoder. (2) What is the mathematical definition of $ISE_i$? (3) In page 5, 3rd line, “then ISD_i is used to reconstruct Si = GNi(ISE_i , \\theta)…” Should the ISE_i be ISD_i?\n(2) Please use \\begin{algorithm}…\\end{algorithm} to tell the readers how your framework works.\n \n[Details]\n1. The first question is “why this problem”. In the 3rd paragraph of page 1, you mentioned that “However, the best direction to update parameters heavily relies on the quality of sampled translations ... which may be far from real translations Y due to inaccurate translations existing in the sampled ones……” But in practice, dual learning as well as back-translation [ref1] works well for many language pairs. In particular, the dual learning and back-translation works for the unsupervised NMT [ref2], where no labeled data is available. Therefore, I am not fully convinced by this claim and then, the motivation of this work. What’s more, this paper does not work on standard WMT dataset, while previous dual learning and back-translation work on that most commonly used dataset. Therefore, the comparison between the guider network and dual learning are not fair.\n2. I am not sure how the BDE in Eqn. (1) is related to the NMT translation quality. Any reference or theoretical/empirical proofs? \n3. It is hard to reproduce such a complex NMT system with NMT, GN and an RL scheduler. Any open-source code or any simple solutions?\n4. Do you use a single-layer LSTM or a deep LSTM? Transformer [ref3] is the state-of-the-art NMT system. Why don’t you choose this system? Also, you do not work on WMT dataset to verify your GLF-2L (Table 1). Therefore, I cannot justify whether the proposed algorithm is efficient compared to the current NMT algorithms. I am not convinced by the experimental results.\n5. The connection/difference between this work and (Tu et al 2017) should be discussed clearly, and you should implement (Tu et al 2017) as your baseline.  Besides, for the 3-language setting, no multilingual baseline is implemented.\n \n[Pros & Cons]\n(+) This paper tries to extend dual learning from word level to hidden state level;\n(+) Multiple languages are involved in this framework;\n(-) Experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used WMT datasets;\n(-) The paper is not easy to follow. (See [clarify] part for details);\n(-) Training process is a little complex; not easy to implement;\n \nReferences\n[ref1] Edunov, Sergey, et al. ""Understanding back-translation at scale."" EMNLP 2018\n[ref2] Lample, Guillaume, et al. ""Phrase-Based & Neural Unsupervised Machine Translation."" EMNLP 2018\n[ref3] Vaswani, Ashish, et al. ""Attention is all you need."" Advances in Neural Information Processing Systems. 2017.\n ', 'This paper make two contributions: (1) it propose a new framework for semi-supervised training for NMT by introduce constraint of encoder and decoder states. (2) It apply Q-learning to schedule the updates of different components. I personally highly believe find the relation between encoder and decoder hidden states is a very good direction for utilizing pair data. Model scheduling is also an important problem for multilingual-NMT. \n\nHowever,  this paper is very hard to follow. \n1. It has lots of acronyms, e.g. section 3.1. It also try to over-complicated the algorithm and I don\'t think these acronyms are necessarily to be defined.  \n2. It try to link it to information theory but most of study is just empirical (which is fine, but avoid it can simplify the writing and make it more readable), e.g. "" According to information theory and the attention mechanism\n(Bahdanau et al., 2014), it is clear that we.."" I agree with the intuition but how it can be ""if and only if""? \n3. It said Figure 2 shows BDE better aligned with BLUE, is there a quantitative measure, e .g. correlation? Or I missed something.\n4. What is the NMT network structure?\n5. I have trouble to understand ""In this process, one monolingual data Si of language i would first be translated to hidden states (ISD) of deci through NMTi , then ISDi is used to reconstruct..."" ""Guided Dual Learning"" part.\n\nThe experimental results looks good, especially for low-resource case. But addressing of similarity and comparison with some previous methods could be improved. At least there is simply baseline which use pre-training. Adding some published SOTA results in the table can also help to understand how well it is.\n\nIn summary, the paper provide some interesting perspectives. However, it\'s hard to follow on the algorithm part and lack of relevant baseline.', 'The paper proposes a guider network which utilized unlabeled monolingual data as an augmentation to the usual dual learning framework to improve NMT performance. Furthermore, a deep Q-learning style scheduling algorithm is proposed to optimize the overall architecture.\n\nThe writing of the paper needs a major improvement. As a reviewer, I had a very hard time trying to understand the paper, while the proposed idea turns out to be conceptually simple. A few points regarding the writing:\n1) Figure 1 is impossible to understand, especially that zero explanation is given in the caption.\n2) Too many unnecessary definitions and acronyms such as ISE, CISE, GLF, GDL, AE etc. Essentially, only the notion of bi-direction attention entropy is relevant for the purpose of the paper. Much effort should have been dedicated to explaining the idea of the of bi-direction attention entropy instead of irrelevant terminologies.\n3) No objective function or algorithm description is ever shown.\n\nTechnically, I am skeptical about the use of deep Q-learning as a scheduling algorithm. Usually, a Q-net requires training before it can be deployed in an evaluation environment. However, here the paper seems to suggest that the Q-net is trained and deployed together with the NMT architecture in an online fashion. Why use a Q-net in an online setting is beyond my understanding. Ideally, one would choose a truly online algorithm (i.e. UCB for stochastic bandits) in such scenarios, which I believe would work even better than deep Q-learning in practice.']","[-50, -20, -60]","[20, 50, 20]","[""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('This paper tries to extend dual learning from word level to hidden state level' and 'Multiple languages are involved in this framework'), the overall tone is critical. The reviewer expresses significant concerns about the paper's clarity, experimental design, and convincingness of results. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'Please use' and 'My suggestions', and frames criticisms as constructive feedback. However, the directness of some criticisms ('I am not fully convinced', 'It is hard to reproduce') prevents a higher politeness score. The reviewer balances negative feedback with positive points and provides specific suggestions for improvement, which contributes to the slightly positive politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('interesting perspectives', 'experimental results looks good'), they also highlight significant issues with clarity and presentation ('very hard to follow', 'over-complicated', 'hard to understand'). The overall tone suggests more concerns than praise. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and potential ('I personally highly believe...', 'The experimental results looks good'), while offering constructive criticism. They avoid harsh language when pointing out flaws, instead using phrases like 'However,' and 'I have trouble to understand'. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism."", ""The sentiment score is -60 because the review is predominantly critical. The reviewer expresses significant difficulty understanding the paper, points out multiple issues with the writing, and is skeptical about a key technical aspect. However, it's not entirely negative as the reviewer acknowledges the idea is 'conceptually simple'. The politeness score is 20 because while the reviewer is direct in their criticism, they use relatively polite language such as 'needs a major improvement' instead of harsher alternatives. They also frame their comments as personal experiences ('I had a very hard time') rather than direct attacks. The reviewer offers specific, constructive feedback, which is a polite approach to criticism.""]"
"['Strength: \n\nIntuition that multiple sources of uncertainty are relevant to adversarial examples \n\nWeaknesses:\n\nThreat model is unclear\nNo adaptive adversaries are considered\nAttack parameters could be better justified\n\nThe intuition presented at the beginning of Section 4 is interesting. There are indeed multiple sources of uncertainty in machine learning, and the softmax probability only captures confidence partially. In particular, estimating the support of training data for a particular prediction and the density of that support is conceptually relevant to understanding and mitigating outlier test points like adversarial examples. \n\nGiven that the approach is motivated as a defense (see Section 7 for instance), it needs to be evaluated in a realistic adversarial setting. In particular, it would greatly strengthen the paper if a clear threat model was specified. In your rebuttal, would you be able to formulate clearly what adversarial capabilities and goals were assumed when designing this defense? \n\nAll experiments are performed on a binary variant of CIFAR-10. In addition, all pairs chosen for the experiments are well-separated: dogs are semantically further apart from airplanes than they are from horses. Would you be able to clarify in your rebuttal how the approach would generalize to multi-class classification? \n\nPerhaps the strongest limitation of the evaluation is that it does not consider adaptive adversaries. This goes back to the threat model point raised previously. Adaptive strategies will be put forward by adversaries aware of the defense being deployed (security should not be obtained through obscurity). For instance, the adversary could modify their attack to have it minimize the difference between logits on the training and adversarial data. This would help evading detection by the proposed scheme. However, results from Section 6 are shown for attacks that do not attempt to reduce the L1 difference between adversarial and training data. \n\nSome attack parameters could also be better justified. The naming convention for the perturbation sizes reads a bit imprecise and is perhaps more confusing than it is informative. Furthermore, could you explain in your rebuttal why epsilon is larger than 1.0 for the FGSM---when the inputs where normalized between 0 and 1?\n\nDetails: \n\nPage 1: Typo in “defence”\nPage 2: Notation s_i is overloaded multiple times making it difficult to parse expressions\nPage 2: Citation to Kull et al. should use \\citep after “Beta calibration”\nPage 3: Citation to Rozsa et al. should use \\citep after “PASS score”\nPage 5: Generally-speaking, it’s best to compute attacks at the logit layer rather than the probabilities to avoid numerical instabilities, which can then lead to gradient masking. However, the following sentence suggests the opposite: “The attacks were all white-box attacks and performed on the network which included a final softmax layer in its structure.”\n', 'On the positive side, I think it\'s a good idea to experiment with various approaches to defend DNNs against adversarial attacks, like the Background Check approach considered in this manuscript (which hasn\'t gotten a lot of traction in the Machine Learning community so far).\n\nHowever, the manuscript has a number of shortcomings which in my opinion makes it a strong rejection. My main concern is about the experimental evaluation: \n- The authors should test their approach on Carlini & Wagner\'s attack which allows for explicit control of logit differences and thus could entirely defeat the Background Check.\n- Moreover, any paper on this topic should evaluate defenses in a complete white-box setting, i.e. the adversary is aware of the detection method and actively tries to bypass it. \n- A comparison with other detection methods from the literature is missing, too, and the two-class classifier setting is very limited.\n\nBesides that, I find there is a general lack of clarity:\n- It really becomes clear only towards the end of the paper what the Background Check is applied for, namely, the detection of adversarial samples. This should be clearly articulated from the beginning.\n- Notation isn\'t always properly introduced (e.g. in the formula for 3-class average recall on page 6), and the same goes for \nsome acronyms (e.g. what is TPR?).\n- Where does Table 2 show a ""mean reduction in average recall of 11.6"", and what does that mean exactly?', 'This paper addresses adversarial detection through the absolute-value difference between the two logit vector values of a DNN binary classifier, with one class associated with normal data and the other with adversarial data. Assignment of examples to an ""adversarial"" class is problematic in that adversarial examples are typically generated in regions for which training data is very sparse. To cope with this, the authors propose use of the Background Check calibration techniques recently proposed by Perello-Nieto et al. (ICDM 2016).  \n\nHere, BC is used to estimate probabilities in a sparse ""background"" class (here, the adversarial class)  through a form of interpolation based on foreground and background densities. The underlying distributional assumption used for estimating foreground densities was that of a gamma function.  Rather than using BC\'s affine bias for estimating background density from the foreground density, the authors adapt it by raise the weighting for the ""adversarial"" decision to the fifth power. Unfortunately, no justification for this choice is given, other than to say that this was done with ""domain knowledge informing the use of a power value"". \n\nIn their experimentation, the authors generate from CIFAR-10 data four kinds of adversarial attacks: noise alone, images with moderate noise, clear images with noticeable noise, and clear images with imperceptible noise. For a variety of attacks, they showed (in Table 2) differences between the average recall for normal examples vs the average recall for normal plus adversarial images. However, without knowing the proportion of adversarial examples used in testing, the significance of the reported differences cannot be judged. They also list the true positive rates of adversarial examples, which showed much variation from experiment to experiment (trending to rather poor performance for attacks with imperceptible noise). Again, the significance of the results cannot be judged without knowing the false negative rate, true negative rate, etc. Moreover, the results are reported without clearly identifying two of the attacks used (""Mom."" is presumably Dong et al.\'s attack using momentum in gradient descent, and Miyato et al.\'s ""VAT"" is not properly introduced in the related work). Crucially, no evaluation of their method is made with respect to other adversarial detection strategies.\n\nPros:\n* Overall, the calibration approach is well motivated, and likely to be of some benefit.\n* The paper is generally readable and understandable. The issues behind calibration and the use of BC are well explained.\n\nCons:\n* The result is a simple and straightforward application of an existing technique - not greatly original.\n* Many design choices in the model (particularly the raising of one of the weights to a seemingly-arbitrary power) are mysterious. No indication is given as to other alternatives or how they might perform.\n* The experimental results are inadequate to judge the impact of the proposed calibration approach.\n* There is no comparison against other detection methods.']","[-30, -70, -30]","[50, 20, 20]","[""The sentiment score is -30 because while the reviewer acknowledges some strengths ('interesting intuition'), they primarily focus on weaknesses and limitations of the paper. The review points out several major issues like unclear threat model, lack of adaptive adversaries, and unjustified attack parameters. However, it's not entirely negative as it offers constructive feedback and suggestions for improvement. The politeness score is 50 because the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions ('would you be able to clarify...'), and acknowledges positive aspects before discussing weaknesses. The tone is professional and constructive rather than harsh or dismissive, though not overly formal or deferential."", ""The sentiment score is -70 because while the reviewer acknowledges a positive aspect ('good idea to experiment'), the overall tone is highly critical. The phrase 'strong rejection' and the extensive list of shortcomings indicate a predominantly negative sentiment. The politeness score is 20 because the reviewer maintains a professional tone, using phrases like 'On the positive side' and 'in my opinion', which soften the criticism. However, the directness of the criticism and the lack of more polite phrasing keep the score from being higher. The reviewer provides specific, constructive feedback without using harsh language, balancing politeness with honest critique."", ""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('well motivated', 'readable and understandable'), there are significant criticisms. The reviewer points out lack of justification for design choices, inadequate experimental results, and no comparison with other methods. These criticisms outweigh the positives, resulting in a slightly negative overall sentiment. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or personal criticisms. They present both pros and cons objectively. However, the tone is not overly polite or deferential, maintaining a neutral professional tone, hence a slightly positive score.""]"
"['Summary:\nThe paper introduces a new approach for interpreting deep neural networks called step-wise sensitivity analysis. The approach is conceptually quite simple and involves some interesting ideas, but I have some serious concerns whether the output produced by this method carries any meaning at all. If the authors were able to refute my concerns detailed below, I would raise my score substantially.\n\n\nStrengths:\n+ Potentially interesting heuristic to identify groups of feature channels in DNNs that encode image features in a distributed way\n\n\nWeaknesses:\n- Using the magnitude of the gradient in intermediate layers of ReLU networks is not indicative of importance\n- No verification of the method on a simple toy example\n\n\nDetails:\n\n\nMain issue: Magnitude of the gradient as a measure of importance.\n\nI have trouble with the use of the gradient to identify ""outliers,"" which are deemed important. Comparing the magnitude of activations across features does not make sense in a convnet with ReLUs, because the scale of activations in each feature map is arbitrary and meaningless. Consider a feature map h^l[i,x,y,f] (l=layer, i=images, x/y=pixels, f=feature channels), convolution kernels w^l[x,y,k,f] (k=input channels, f=output channels) and biases b^l[f]:\n\nh^l[i,:,:,f] = ReLU(b^l[f] + \\sum_k h^(l-1)[i,:,:,k] * w^l[:,:,k,f])\n\nAssume, without loss of generality, the feature map h^l[:,:,:,f] has mean zero and unit variance, computed over all images (i) in the training set and all pixels (x,y). Let\'s multiply all ""incoming"" convolution kernels w^l[:,:,:,f] and biases b^l[f] by 10. As a result, this feature map will now have a variance of 100 (over images and pixels). Additionally, let\'s divide all ""outgoing"" kernels w^(l+1)[:,:,f,:] by 10.\n\nSimple linear algebra suffices to verify that the next layer\'s features h^(l+1) -- and therefore the entire network output -- are unaffected by this manipulation. However, the gradient of all units in this feature map is 10x as high as that of the original network. Of course the gradient in layer l-1 will be unaltered once we backpropagate through w^l, but because of the authors\' selection of ""outlier"" units, their graph will look vastly different.\n\nIn other words, it is unclear to me how any method based on gradients should be able to meaningfully assign ""importance"" to entire feature maps. One could potentially start with the assumption of equal importance when averaged over all images in the dataset and normalize the activations. For instance, ReLU networks with batch norm and without post-normalization scaling would satisfy this assumption. However, for VGG-16 studied here, this is not the case.\n\nOn a related note, the authors\' observation in Fig. 4b that the same features are both strongly positive and strongly negative outliers for the same class suggests that this feature simply has a higher variance than the others in the same layer and is therefore picked most of the time. Similarly, the fact that vastly different classes such as shark and German Sheppard share the same subgraphs speaks to the same potential issue.\n\n\nSecondary issue: No verification of the method on simple, understandable toy example.\n\nAs shown by Kindermans et al. [1], gradient-based attribution methods fail to produce the correct result even for the simplest possible linear examples. The authors do not seem to be aware of this work (at least it\'s not cited), so I suggest they have a look and discuss the implications w.r.t. their own work. In addition, I think the authors should demonstrate on a simple, controlled (e.g. linear) toy example that their method works as expected before jumping to a deep neural network. I suppose the issue discussed above will also surface in purely linear multi-layer networks, where the intermediate layers (and their gradients) can be rescaled arbitrarily without changing the network\'s function.\n\n\nReferences:\n[1] Kindermans P-J, Schütt KT, Alber M, Müller K-R, Erhan D, Kim B, Dähne S (2017) Learning how to explain neural networks: PatternNet and PatternAttribution. arXiv:170505598. Available at: http://arxiv.org/abs/1705.05598', 'Summary: \nThis paper introduces step-wise sensitivity analysis (SSA), which is a modification of saliency maps (Baehrens et al. 2010, Simonyan et al. 2013) to a per-layer implementation. Instead of only measuring the importance of input nodes (e.g. pixels) to the classification, SSA measures the importance of all nodes at each layer. This allows for a way to find the important sub-nodes for each node in the tree given a particular sample. It is then straightforward to aggregate results across different input samples and output a dependency graph for nodes.\n\nNovelty:\nThe technical contribution is a very simple extension of Simonyan et al. 2013. The main novelty lies within the created dependency graph from the node importance weights, but the usefulness of such graph is unclear. In addition, the claim that this is the first method that aggregates results of an instance-specific method to gain model-centric results is a stretch considering other works have found important nodes or filters for a specific class by aggregating across instance-specific samples (Yosinski et al. 2015).\n\nEvaluation: \nThe idea of producing an interpretable dependency graph for nodes is interesting, and the possible conclusions from such graphs seem promising. However, most of the interesting possible conclusions seem to be put off for future work. I don’t believe the experiments are sufficient to show the significance of SSA. The main hypothesis is that dependency graphs allow for a way to interpret the model across samples, but it doesn’t show any conclusive results about the data or models that wasn’t previously known. The results are mostly speculative, such as the fact that German shepherd and great white shark nodes are clustered together, possibly due to the fact that both of these classes share a PDR encoding sharp teeth, but that is never actually demonstrated.\n', 'The paper proposes a modification of the saliency map/gradient approach to explain neural networks.\n\n# Method summary\n\nThe approach is as follows:\nFor each layer, the gradient w.r.t. it\'s input layer is computed for multiple images concurrently.\nThen for conv layers, the activations are averaged per feature map (over space).\nAs a result, for both fully connected and convolutional layers there is a 3D feature map.\nFrom these at most b positive outliers are selected to be propagated further. \nWhat is a bit strange is that in the results section, guided backpropagation is mentioned and clearly used in the visualizations but not mentioned in the technical description.\n\n# Recommendation\n\nThe current evaluation is definitely not sufficient for acceptance. \nThe evaluation is done in a purely qualitative matter (even in section 4.1 Quantitive justification of outliers as relevant neurons). The results appear to be interesting but there is no effort done to confirm that the neurons considered to be relevant are truly relevant. On top of that, it is also evaluated only on a single network and no theoretical justification is provided.\n\n# Discussion w.r.t. the evaluation\n\nTo improve section 4.1,  the authors could for example drop out the most important neurons and re-evaluate the model to see whether the selected neurons have a larger impact than randomly selected neurons. Since the network is trained with dropout, it should be somewhat robust to this. This would not be a definitive test, but it would be more convincing than the current evaluation. Furthermore high values do not imply importance. \n\nIt might be possible that I misunderstood the experiment in Figure 2. So please correct me if this is the case in the reasoning below. \nIn figure 2, FC2 is analyzed. This is the second to last layer. So I assume that only the back-propagation from logits (I make this assumption since this is what is done commonly and it is not specified in the paper) to FC2 was used. Since we start at the same output neuron for a single class, all visualisations will use the same weight vector that is propagated back. The only difference between images comes from which Relu\'s were active but the amount if variability is probably small since the images were selected to be classified with high confidence. Hence, the outliers originate from a large weight to a specific neuron. \n\nThe interpretation in the second paragraph of section 4.2.1 is not scientific at all. I looked at the German Shepherd images and there are no teeth visible. But again, this is a claim that can be falsified easily. Compare the results when german Shepherds with teeth visible are used and when they are not. The same holds for the hypothesis of the degree of danger w.r.t. the separation. \n\nFinally, there is no proof that the approach works better than using the magnitude of neuron activations themselves, which would be an interesting baseline. \n\nAdditional remarks\n---------------------------\n\nThe following is an odd formulation since it takes a 3D tensor out of a 5D one and mixes these in the explanation:\n""... the result of equation for is a 5D relevance tensor $\\omega^l_{n,i,..} \\in R^{H\\times W\\times K} .....""\n\nThe quality of the figures is particularly poor. \n- Figure 1 b did not help me to understand the concept.\n- Figure 2 The text on the figure is unreadable. \n- Figure 4a is not readable when printed. ']","[-70, -20, -50]","[20, 50, 20]","[""The sentiment score is -70 because the reviewer expresses 'serious concerns' about the paper's methodology and questions whether the output has 'any meaning at all'. The review is predominantly critical, highlighting major weaknesses and issues with the approach. However, it's not entirely negative as the reviewer acknowledges some potential strengths and indicates willingness to raise the score if concerns are addressed. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I have trouble with...' and 'I suggest...' rather than harsh or rude language. The reviewer also offers constructive feedback and suggestions for improvement, which contributes to the slightly positive politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting aspects of the paper, they express significant doubts about its novelty and the sufficiency of the experiments. The reviewer states that the technical contribution is 'a very simple extension' and that the usefulness of the dependency graph is 'unclear'. They also mention that the results are 'mostly speculative' and don't show 'conclusive results'. However, the score is not deeply negative as the reviewer does recognize some potential in the idea.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They provide balanced feedback, acknowledging both strengths and weaknesses of the paper. The language used is neutral and objective, without any harsh or rude comments. The reviewer offers constructive criticism and explains their concerns clearly without being dismissive or overly critical."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the paper's evaluation, stating it is 'definitely not sufficient for acceptance' and pointing out several shortcomings. However, they do mention that the results 'appear to be interesting', which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'please correct me if this is the case' and offer constructive suggestions for improvement. The reviewer also acknowledges potential misunderstandings, which adds to the politeness. However, some statements like 'The interpretation... is not scientific at all' are quite direct, preventing a higher politeness score.""]"
"['The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones. There is a budget issue here and one does not want to just keep the whole training set of all known previously observed classes but rather one wants to consider a maximum memory budget allowed to store what is necessary for an optimal incremental learning (typical examples, statistics etc). There is also a privacy issue preventing from storing original training samples.\n\nThis is a relevant problem that is has gain interest in the last few years. It is related to topics such as few shot learning and meta few shot learning (with respect ti the number of examples per class that are kept, which is limited) and somehow to budget learning . Yet these topics and associated references are surprisingly not evoked in the text.\n\nThe paper is rather well written but it strongly lacks precision about the proposed method. A description of the ICARL state of the art method is missing and would have been mandatory since the proposed work appears to build on iCARL method. Actually the description of the method is very short since the dedicated section (§4) is mainly used to describe a rather standard convolutional auto encoder architecture. At the end one tries to guess what the proposed method consists in. As far as i understand it is based on iCARL method where selected examples of past observed classes are not stored as is but in their encoded form (by the convolutional autoencoder). At the end my understanding of the proposed approach is that it consists in an incremental progress of a state of the art method, then an incremental work with limited innovation.\n\nBy the way i am not sure of the meaning of pseudo exemplar as used in the proposed method. Are these drawn following a distribution computed on training samples ? Or are these pseudo exemplar because you use reconstructed samples from encodings (by the CAE).  \n\nWhen looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing. As written in the paper the proposed system outperforms iCARL in case the examples are encoded in the same dimension as original examples (hence no benefit on the storage side) but reaches similar performance when using less storage capacity. ', 'This paper describes a system for classifying  images  displayed  on  the  websites  by  using an incremental  learning  classifier with Deep Convolutional Neural Network to be used in context aware advertisement.\n\nThis is more of an application paper which is not the focus of a venue like ICLR. Further the paper makes several misleading claims. They claim that their system is inspired by the human brain while providing scant evidence to prove that claim (Unless we take it in a very broad sense to mean that neural networks resemble the human brain). Also they propose a convolutional autoencoder as a kind of an encryption method to store images to alleviate privacy and legal concerns. This is not a good idea because encoding an image using a convnet is not a substitute for encryption. In fact any image hence encoded can be decoded easily to reveal the original contents of the image. Overall this paper is not appropriate for ICLR in its current form.', 'The paper extends an existing incremental learning method, mainly introducing the latent representations of an autoencoder instead of the original images. It includes a lot of hype in that it simulates the human brain -  because it is based on the iCaRL & Fear Net formulation - and that it fulfils the privacy and legal requirements - because it stores and uses the auto-encoder representations instead of the images.\n\nSpecific comments:\n\n- The title of the paper defines its topic to be context aware advertisement, whereas the main results and all comparisons are made on the CIFAR dataset. Only the last Table (5) provides the performance on the IMDB-CAA dataset, without any detailed analysis of the experiments.\n\n- The results in Table 3 are quite strange: the presented approach starts by outperforming iCaRL method, but then deteriorates very fast wrt size and is much lower than the original method, with no justification on this. Some improvement is shown in size 16.6% without, again, any logical explanation provided.\n\n- Section 4.2 does not provide any detail of the integration resulting in the presented system; Fig.2 does not provide a clear description either.\n\n- Language improvement is required in the experimental sections.\n\n-  ']","[-30, -70, -50]","[20, 0, 0]","[""The sentiment score is slightly negative (-30) because while the reviewer acknowledges the relevance of the problem and that the paper is well-written, they express several criticisms. These include the lack of precision about the proposed method, missing description of the ICARL method, limited innovation, and unconvincing experimental results. The reviewer also points out missing references to related topics. However, the tone is not entirely negative, as they recognize some benefits of the proposed method.\n\nThe politeness score is slightly positive (20) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'rather well written' and 'as far as i understand' which soften their criticisms. The reviewer also asks for clarification on certain points instead of making assumptions. However, the language is not overly polite or deferential, maintaining a neutral academic tone for the most part."", ""The sentiment score is -70 because the reviewer expresses a clearly negative opinion about the paper, stating it's not appropriate for the venue and makes misleading claims. They criticize multiple aspects of the paper without offering any positive comments. However, it's not at the extreme negative end as the reviewer does provide some neutral description of the paper's content. The politeness score is 0 (neutral) because the reviewer's language is direct and professional without being overtly polite or rude. They state their criticisms plainly without using harsh language or personal attacks, but also without softening their critique with polite phrases or positive reinforcement."", ""The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper. The reviewer mentions 'hype' in the paper's claims, notes strange results without justification, and calls for language improvements. However, it's not entirely negative as it acknowledges the paper's extension of an existing method. The politeness score is 0 (neutral) because the reviewer maintains a professional tone without being overly polite or rude. They directly state criticisms and recommendations without using harsh language or personal attacks, but also without using particularly courteous phrases.""]"
"['This paper demonstrates that CNNs are more robust to class-relevant label noise. They argue that real-world noise should be class-relevant.\n\nPros:\n\n1. The authors find a new angle to exploit robust learning with noisy labels.\n\n2. The authors perform numerical experiments to demonstrate the effectiveness of their proposal. And their experimental result support their previous claims.\n\nCons:\n\nWe have two questions in the following.\n\n1. Basic definition: in learning with noisy labels, there are two basic models. First, most research focuses on class-conditional noise (CCN) model [1]. Second, recent research explore a bit on instance-dependent noise (IDN) model [2, 3]. As far as I know, there is no class-irrelevant label noise and class-relevevant label noise. In CCN mode, people would like to use symmetric noise and asymmetric noise as a basic benchmark to conduct experiments.\n\n2. Motivation: The authors want to claim CNNs are more robust to such realistic label noise than class-irrelevant label noise. However, they make one mistake. They do not have a clear definition about realistic label noise. In my mind, I believe Clothing1M [4] should be realistic label noise dataset.\n\nBy the way, in learning with noisy labels, there are two kinds of research. First, people propose new robust methods for CCN model. Second, people propose new robust methods for IDN models. Proposing new setting should be encouraged. However, the setting and conclusion should be reasonable.\n\nReferences:\n\n[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 1988.\n\n[2] A. Menon, B. Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. Machine Learning, 2018.\n\n[3] J. Cheng, T. Liu, K. Ramamohanarao, D. Tao. Learning with bounded instance-and label-dependent label noise. arxiv 1709.03768, 2017.\n\n[4] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.', ""This paper attempted to analyse the performance of CNN models when data is mislabelled in different manners, i.e. class dependent labels and class independent labels. It carried out several good experiments as a good start, but several points are not comprehensively studied and analysed. \n1. Try to provide more direct and solid proofs on the relationship between conceptual and visual distances between class dependent labels. \n2. In table 1, model trained with noise on class dependent label has lower fooling rate than model trained with clean data. Is it worth exploiting in a deeper manner?\n3. In figure 5, why the curve appears so different after block 4 only? Would visualising feature maps from different layers help understand this observation?\n4. In figure 6, it seems that the difference between those experiments is marginal, which contribute little to the argument of this paper.\n5. About the discussion on 'cluster', it would be better if sufficient experiments and analysis can be provided.\n\n"", 'The authors challenge the CNNs robustness to label noise, but when the label noise is class dependent, more realistic scenario than class independent noise. \nTo analyse the CNNs behavior in such a scenario, they consider the ImageNet 1k dataset, and change some labels to labels that are close according to the ImageNet 1k tree of WordNet. \nThe authors conduct multiple experiment to compare the effect of class dependent and class independent noise on:\n* the model accuracy\n* the robustness to adversarial perturbation \n* the learned representation\n\nThe paper is generally well written and well structured. The analysis is sound and addresses interesting points, giving insightful results. Nevertheless, the overall conclusion is not very surprising. This work  confirms the commonly admitted fact that CNNs learn features that are visually meaningful.   Moreover, there is no significant novelty in the paper. The paper only analyses the CNNs behavior, without suggesting any new algorithm based on the observations. One specific point that seems under-investigated in my sense is the observation about the robustness to adversarial perturbations. The model with the class dependent noisy labels is in average less sensitive to the perturbations, even if this is not significant for the tested noise level. Did the authors test with different noise levels? This calls for a further analysis. It has the potential to give more insights, and probably inspire new methods to improve training robustness.    ']","[-20, -20, 20]","[60, 50, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some pros of the paper, they raise significant concerns and questions about the basic definitions and motivation of the study. The reviewer points out potential mistakes and suggests that the conclusions may not be reasonable. However, the score is not deeply negative as the reviewer does recognize some positive aspects.\n\nThe politeness score is moderately positive (60) because the reviewer uses respectful language throughout. They frame their criticisms as questions rather than direct accusations, and use phrases like 'As far as I know' to soften their statements. The reviewer also acknowledges the pros of the paper before moving on to the cons, which is a polite approach. The tone is professional and constructive, encouraging improvement rather than being dismissive."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('carried out several good experiments as a good start'), they also point out that 'several points are not comprehensively studied and analysed'. The overall tone suggests that the paper needs significant improvements. The politeness score is moderately positive (50) as the reviewer uses neutral language and provides constructive feedback without harsh criticism. They use phrases like 'Try to provide', 'Is it worth', and 'it would be better if', which are polite ways of suggesting improvements. The reviewer also balances criticism with acknowledgment of the paper's strengths, which contributes to the polite tone."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges that the paper is well-written, well-structured, and provides insightful results. However, they also mention that the conclusion is not very surprising and there's no significant novelty, which tempers the positive sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering both praise and constructive criticism without harsh or rude phrasing. They use phrases like 'well written,' 'sound analysis,' and 'insightful results,' while also politely suggesting areas for improvement or further investigation. The tone is professional and balanced, maintaining courtesy while providing honest feedback.""]"
"[""The paper presents some interesting observations related to the connection between the universal adversarial attacks on CNNs and spectral properties. While most of the results are empirical, the authors present two theorems to justify some of the observations. However, the paper is poorly written and very hard to read. Rather than providing too many plots/results in the main paper (maybe use supplementary matl.), the empirical results should be better explained to help the readers. Similarly, the implications of the theorems are not really clear and bit hand-wavy.    \n\nxxxxxxxxxxxxxx\n\nIt seems that the authors provided a generic response to all the reviewers and I am not sure if they acknowledge the lack of clarity and lot of hand-wavy explanations in the paper. This issue has been raised by other reviewers too and is quite critical for becoming a good paper worthy for ICLR. Therefore, I am unable to update my score for this paper. However, I do appreciate the comparison with Moosavi-Dezfooli et al. (CVPR'17), this is a good addition as suggested by another reviewer. "", ""The authors made an interesting observation: There's an important common subspace of Gradient/FGSM/Deepfool attacks among all examples. Therefore, they propose to use top SVD components of the directions to conduct universal attack. This is an interesting finding but also not surprising; we know the gradient of loss function w.r.t input can be used for interpretability, and in MNIST examples they usually reveals some rough shape of the class. This is also observed in Figure 8-13 in this paper, and thus it makes sense that the gradient directions share a common subspace. Therefore I think this observation itself is not significant enough. \n\nUsing this for universal attack is interesting, however the experiments are not that convincing: \n\n1. To show this is a good way for universal attack, I think the authors should compare with previous work in (Moosavi-Dezfooli et al). \n\n2. All the experiments are on MNIST. How about cifar/ImageNet? \n"", 'This paper studies the problem of computing non-data specific perturbations, also known as universal perturbations, to attack neural networks and take profit of their inherent vulnerability. Compared to previous works in the domain, the authors look specifically at equivariant networks, and derive geometric insights and methods to compute universal perturbations for these networks. \n\nThe paper starts by analysing the main/principal directions of set of perturbations that are able to change the decisions in different forms of equivariant neural networks. With this heuristic study, a few main directions are shown to be shared by most adversarial perturbations. The authors then propose to construct universal perturbations built on the insights given by the principle directions of perturbations, which is an interesting an effective method. In addition, it is shown that a few adversarial samples are sufficient to identify pretty  accurately the principle directions. The fooling rates achieved by this method is pretty good, which demonstrates that the proposed strategy is reasonable.\n\nThe key idea in this paper (using principal shared directions of perturbations, computed on a small subset of data points) has unfortunately already been proposed and tested in classical (non-equivariant) neural networks - see for example Fig 9 in Moosavi-Dezfooli, 2017, cited in the paper, and published in CVPR 2017. The present paper proposes however a few additional bits of information with a nice theoretical analysis, while the previous works were mostly based on heuristics. It is probably not sufficient however to pass the cut in ICLR. \n\nThe interesting additional novelty here is the study of equivariant networks. However, this ends up falling sort of initial expectations - there seems to be nothing specific to equivariant networks in the proposed study, and the solution and algorithm is actually applicable to any neural network architectures (?). Also, no specific insights are derived for equivariant networks, which could be potentially very interesting to make progress in understanding better equivariant representations, which still consist in a widely open research problem. \n\nIn general, the paper has a non-classical organisation, with a lot of heuristics that are not discussed in depth - that gives a sort of high-level impression that the proposed idea is potentially nice, but that but superficially addressed. It should probably be improved in the next versions of this work. ']","[-50, -20, -20]","[0, 50, 50]","[""The sentiment score is -50 because while the reviewer acknowledges some interesting observations and theoretical contributions, they express significant concerns about the paper's clarity, organization, and explanations. The phrase 'poorly written and very hard to read' indicates a strong negative sentiment. However, it's not entirely negative as they do recognize some value in the work. The politeness score is 0 (neutral) because the reviewer's language is direct and critical but not overtly rude. They provide constructive feedback without using inflammatory language. The reviewer maintains a professional tone throughout, balancing criticism with acknowledgment of positive aspects, such as appreciating the comparison with previous work."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the work as 'interesting', they also state that the observation is 'not surprising' and 'not significant enough'. The reviewer also points out that the experiments are 'not that convincing' and suggests additional comparisons and datasets. This indicates a generally critical stance, albeit with some positive aspects. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the authors' work as 'interesting' and providing constructive feedback. The tone is professional and the criticisms are presented as suggestions for improvement rather than harsh judgments. The reviewer maintains a balanced and courteous approach, even when pointing out limitations in the study."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'interesting and effective method', 'pretty good' fooling rates), they also point out significant limitations. The reviewer suggests that the key idea has already been proposed, the paper falls short of initial expectations, and the organization needs improvement. These criticisms outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'interesting and effective method' and 'nice theoretical analysis', which maintain a polite tone. Even when pointing out limitations, the language remains professional and constructive, avoiding harsh or dismissive statements.""]"
"['This paper presents a method for predicting future frames of a video (or unseen views of a 3D scene) in a ""jumpy"" way (you can query arbitrary viewpoints or timesteps) and ""consistent"" way (when you sample different views, the scene will be consistent). They use a VAE that encodes the input video in a permutation invariant way, which is achieved by summing the per-frame latent vectors. Then, they sample a latent vector using a DRAW prior. This latent vector can then be used to render the video/scene from different times/viewpoints via an LSTM decoder. They test the model on several toy datasets: they compare to video prediction methods on a dataset of moving shapes, and 3D viewpoint prediction on a 3D MNIST ""dice"" dataset.\n\nPros:\n- The idea of developing new methods for viewpoint and video synthesis that allow for ""jumpy"" and ""consistent"" predictions is an important problem.\n\n- The paper is fairly well written.\n\n- The design of the model is reasonable (it is a natural extension of VAE viewpoint/future prediction methods).\n\nCons:\n- All of the experiments were done on toy datasets. These are also not well-established toy datasets, and seem tailored to debugging the model, so it is not particularly surprising that the method worked. Since the main contribution is not very novel from a technical perspective (it is more about designing a model from existing, well-established components), this is a significant limitation. \n\n- The paper suggests with experiments that GQN generates predictions that are less consistent across samples, but it is not clear exactly which design decisions lead to this difference. Why is this model more jumpy and consistent than GQN?\n\n- The paper claims that JUMP trains more reliably than several video prediction methods in Figure 5. Yet, in the 3D viewpoint synthesis task, they suggest that JUMP had trouble with convergence, i.e.: ""We ran 7 runs for each model, and picked the best 6/7 runs for each model (1 run for JUMP failed to converge)."" This is confusing for two reasons. First, why was this evaluation protocol chosen (i.e. running 7 times and picking the 6 best)? If it was a post-hoc decision to remove one training run, then this should be clarified, and the experiment should be redesigned and rerun. Second, is the implication that JUMP is more stable than video prediction methods, but not necessarily more stable than GQN for viewpoint prediction?\n\n- The paper should consider citing older representation learning work that deals with synthesizing images from multiple viewpoints. For example: \n\nM. Tatarchenko, A. Dosovitskiy, T. Brox. ""Multi-view 3D Models from Single Images with a Convolutional Network"". ECCV 2016.\n\n- There is insufficient explanation of the BADJ baseline. What architectural changes are different?\n\n- The decision to use DRAW, instead of a normal VAE prior, is unusual and not explained in much detail. Why does this improve the visual fidelity of the samples?\n\nOverall:\nThe paper does not present enough evidence that this model is better at jumpy/consistent predictions than other approaches. It is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be OK, but it seems to be a fairly straightforward extension of existing models. ', 'This paper proposes a general method for indexed data modeling by encoding index information together with observation into a neural network, and then decode the observation condition on the target index. I have several concerns regarding the way the paper using indices, and the experimental result.\n\nThe strategy this paper use for indexed data is to encode all data in a black-box, which can be inefficient since the order of temporal data or the geometric structure of spatial data is not handled in the model. These orders can be essential to make reasonable predictions, since they may encode causal relations among those observations, and certainly cannot be ignored. Another critical problem for this paper is that the relative time scale are not explicitly modeled in the context. My worry is that when putting all those informative data into a black-box may not be the most efficient way to use them.\n\nOn the other hand, experiments in this paper look quite artificial. Since sequential and spatial modeling have multiple real-life applications. It would be great if this method can be tested on more real dataset.\n\nThis paper does show some promise on sequence prediction task in a long range, especially when the moving trace is non-linear. A reasonable uncertainty level can be seen in the toy experiments. And the sample quality has some improvement over competitors. For example, JUMP does not suffer from those multi-mode issues. These experiments can be further strengthened with additional numerical results.\n\nFor now, this paper does not convince me about its method for modeling general indexed data, both in their modeling assumption, and their empirical results. In my opinion, there is still a long way to go for challenging tasks such as video prediction. This paper proposes an extreme way to use indices, but it is still far from mature. ', 'The paper motivates and provides a model to generate video frames and reconstructions from non-sequential data by encoding time/camera position into the model training. The idea is to allow the model to interpolate, and more importantly, extrapolate from frames and learn the latent state for multiple frames together. The same techniques are also applicable to 3d-reconstruction.  JUMP is very closely related to GQN with the main difference being that the randomness in JUMP is learned better using a ""global"" prior. The evaluation is reasonable on multiple synthetic experiments including a 3d-scene reconstruction specially created to showcase the consistency capabilities in a stochastic generation. Paper is mostly clear but more priority should be given to the discussion around convergence and the latent state.\n\nTo me, the 3d-reconstruction use-case and experiments are more convincing than the video generation. Interpolation between frames seems like an easier problem when specifically trained for. On the other hand, video algorithms trained on sequential prediction should be able to go forward or backward in time. Moreover, jumpy prediction throws away information (the middle frames) that might lead to a better latent state. The experiments also show certain frames where there seems to be a superposition of two frames. In this aspect, sSSM is better despite having worse video quality.\n\nFor video experiments, prediction of more complex video, with far-away frame predictions would solidify the experiments. The narratives seem somewhat limited to show what kind of advantage non-sequential context gives you.\n\nReliable convergence (less variance of training progress) of the method seems to be the strongest argument in favor of the JUMP. It is also unclear whether having a global latent variable is why it happens. More discussion about this should probably be included considering that JUMPy prediction seems to be the cause of this.\n\nBetter evaluation of the latent state might have presented a better understanding of what the model is really doing with different samples. For example, what is the model causes some frames to look like superpositions??']","[-40, -50, 20]","[50, 20, 50]","[""The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('Pros'), the overall tone is critical. The reviewer points out several significant limitations ('Cons') and expresses skepticism about the paper's contributions and experimental design. The politeness score is 50 because the reviewer uses professional and respectful language throughout, avoiding harsh or personal criticisms. They present their concerns as objective observations and suggestions for improvement, maintaining a constructive tone. The reviewer balances positive and negative feedback, which contributes to the politeness, but the predominance of critical points prevents a higher politeness score."", ""The sentiment score is -50 because the review expresses several concerns and criticisms about the paper's approach and experimental results. The reviewer states that they are not convinced by the method and that 'there is still a long way to go' for challenging tasks. However, the score is not extremely negative as the reviewer acknowledges some promising aspects of the work, such as improvements in long-range sequence prediction and sample quality. The politeness score is 20 because the reviewer uses generally neutral and professional language, avoiding harsh or rude expressions. They offer constructive criticism and suggestions for improvement, such as testing on more real datasets. The tone is academic and objective, though not overtly polite or complimentary."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contributions and finds some aspects convincing, particularly the 3D reconstruction use-case. However, they also express several concerns and suggest areas for improvement, which tempers the overall positivity. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout, offering constructive criticism without using harsh language. They use phrases like 'to me' and 'seems to be' which soften their critiques. The reviewer also balances positive and negative feedback, showing consideration for the authors' work while still providing honest assessment.""]"
"['The paper uses a number of deep learning approaches to analyse sets of Traffic data. However, as these sets of traffic data are never explained it is difficult to follow or understand what is going on here.\n\nSome major comments:\n1) Many of the key concepts in the paper are not discussed. The primary one would be that of what the two data sets contain. Without knowledge of this it is difficult to ascertain what is going on. \n\n2) Many of the processes used are not described in enough detail to either understand what is going on or to re-produce the work. Without this it is difficult to make headway wit the work.\n\n3) It is not clearly articulated what the experiments performed are doing. For example, how have you applied the other techniques to this data?\n\n4) Key terms are not defined. Such as Traffic Flow.\n\n5) The English structure of the paper is poor with many mistakes. A thorough proof-reading is essential.\n\nSome more specific points:\n- ""with the larger road network, the difficulty of flow forecasting grows."" - This seems to be a consequence of the other ones not a challenge in it\'s own right.\n\n- What is ""superiority""?\n\n- ""Spatiotemporal traffic flow forecasting task is currently under a heated discussion and has attracted a large research population."" - evidence to back up this statement.\n\n- Your contributions aren\'t contributions, but rather a list of what you have done.\n\n- How does your related work relate to what you have done?\n\n- Hard to parse ""To extract temporal relationships within the history traffic flows, we model this process as a layering structure with autoencoder as cell""\n\n- Appendices B and C should be in the main paper.\n\n- What is in x^{(1)}?\n\n- ""When take the sparsity constrains into consideration"" - what are the sparsity constraints?\n\n- How do you obtain the weights?\n\n- Figure 2 should come much sooner as it relates a lot of the concepts together.\n\n- ""On both datasets, we slice traffic flow information into 15 minutes windows, where 70% of data is for training, 10% for validation and remaining 20% for testing."" - Is that each 15 mins is split 70:10:20?\n\n- Proof by example is not a proof.\n', ""This paper has potential, but I do not think it is ready for publication. I will ask some questions / make some suggestions:\n\n1) Your first sentence makes a claim about there being a large body of research on traffic flow forecasting. I don't doubt this, but you should cite some papers, please.\n\n2) Your contributions raise the following questions for me: \n\n- Contribution 1 is that you use a very large dataset (for training? you don't say.) and a small dataset (for testing), thus proving that your method works and generalizes. Your method may be effective, but compared to what? Your method may generalize, but how do we know that if you've only tested it on one small dataset?\n\n- Contribution 2 says that you creatively used lagged data in a time series model. This is probably a good idea, but it does not sound all that creative to me, compare with, e.g. an AR model.\n\n- Contribution 3 says that you use driving distance to model spatial correlation. Again, this is probably a good idea, and when we get further we learn that you applied a Graph Convolution Network. Were these the choices that you claim are novel? Are they novel? What other choices might be reasonable and how would they compare?\n\n3) Section 3 immediately jumps into the use of autoencoders. But I think you need to justify why we care about using autoencoders in the first place. If the problem is traffic forecasting, why don't you tackle that problem head on?\n\n4) Section 3 mentions sparsity without justifying why I care about sparsity. This might be an important tool for regularization in a deep neural network. Or it might not be--given enough data and other regularization techniques (weight decay, early stopping, dropout).\n\n5) Is the spatial dependency that you end up learning qualitatively different than the spatial dependency you would get by instead assuming a particular parametric form as is done in kernel methods / Gaussian processes, e.g. the Gaussian kernel or the Matern kernel parameterizes the covariance between observations at two spatial locations?\n\n6) In your experiment I believe you randomly split 15 minute blocks into train/test/validate. I think this evaluation will be over-optimistic insofar as if 10:30-10:45 and 11:00-11:15 are in the train set, but 10:45-11:00 is in the test set, it will be relatively easy to predict 10:45-11:00. I would suggest considering train/test/validate splits based on larger chunks, e.g. leave the data in 15 minute blocks, but randomly select hours (4 blocks) to put in train/test/validate."", 'I am sorry but I am super confused with this paper. There is no clarity and about half of the sentences are written with broken english. \n\nThe model (as far as I can understand from the partial explanations and Figure 2) looks like a kitchen sink -- a combination of pieces from previously explored methods in the context of traffic flow estimation. This might be fine, but there is no motivation provided for this. \n\nRather than spending the method section with repeating well known Loss equations, KL-divergence, convolution, etc... Please focus on the architecture provided in the paper and the motivations behind it. More importantly, how it differs from previous approaches and why these choices have been made. \n\nThis paper is not ready for publication. It needs a re-write at least, preferably working out the original motivations behind architectural choices. ']","[-60, -50, -80]","[20, 50, -20]","[""The sentiment score is -60 because the review is predominantly critical, pointing out numerous issues with the paper. The reviewer states that it's 'difficult to follow or understand what is going on' and lists several major concerns about lack of explanation, insufficient detail, and poor English structure. However, it's not entirely negative as the reviewer does provide constructive feedback for improvement. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'Some major comments:' and 'Some more specific points:' to organize their feedback, and provide specific suggestions for improvement rather than just criticism. The language is not overtly polite, but it's not rude either, maintaining a neutral to slightly polite professional tone."", ""The sentiment score is -50 because the reviewer starts by stating that the paper is not ready for publication, which is a negative assessment. However, they do mention that the paper has potential, which prevents the score from being more negative. The review then lists several critical points and questions, indicating a generally negative sentiment towards the current state of the paper. The politeness score is 50 because the reviewer uses polite language throughout, framing their criticisms as questions or suggestions rather than direct criticisms. They use phrases like 'I will ask some questions / make some suggestions' and 'please' when requesting citations. The tone is professional and constructive, avoiding harsh or rude language, but it's not overly formal or excessively polite either."", ""The sentiment score is -80 because the reviewer expresses strong confusion and dissatisfaction with the paper. They state that the paper is 'not ready for publication' and needs a 're-write at least', indicating a very negative view. The reviewer also mentions 'broken english' and lack of clarity, further emphasizing their negative sentiment. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite blunt and critical. The opening line 'I am sorry but I am super confused' and phrases like 'kitchen sink' are somewhat impolite. However, they do offer specific recommendations for improvement, which prevents the score from being lower. The reviewer doesn't use overtly polite language or soften their criticisms, leading to a slightly negative politeness score.""]"
"['This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE).  By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy.\n\nPros:\n- The paper carefully constructs a method to estimate the mutual information between high dimensional variables and address the infinite mutual information issue by adding noise to the output. This is novel and theoretical sounding. \n- The paper connects the IB theory of DNN with weight decay, which is a novel founding.\n\nCons:\n- The paper claims no literature has been doing IB functional on a layer-by-layer objective, however, see [1, 2] for the total correlation explanation work which is closely related to IB functional and they have also verified the effectiveness of layer-by-layer objective. \n- The scope of the paper is unclear. It seems that the paper is trying to convince two things to the readers: 1) The compression phase in DNN does exist 2) Layer-wise training helps to improve the accuracy. Although these two things are close related to each other (because they all requires to estimate the IB functional), it seems that neither these two conclusions are convincing. First, the compression phase is achieved only through weight decay; without weight decay, as shown in the paper, the compression phase is gone. Does that verify the incorrectness of IB theory of deep nets? Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. But if we really want to show the `effectiveness` of `layer-wise` training, one should compare the `layer-wise` training with `end-to-end` training while keeping the objective itself fixed. Otherwise, it is really difficult to draw conclusions about why the accuracy is improving, it is because of the objective changes or because of the `layer-wise` training.\n- How does the beta (in IB objective) selected in the experiments for comparison? Do you use a validation dataset, and what is the final beta? If the paper fine-tune beta on the validation dateset, then the comparison of ""IB functional, only the first term"" and ""IB functional"" is unfair. \n\n[1]  Ver Steeg et al. Maximally Informative Hierarchical Representations of High-Dimensional Data. AISTATS 2015\n[2]  Gao et al. Auto-Encoding Total Correlation Explanation. Arxiv 1802.05822.\n\n[update] After carefully reading the response (also from other reviewers), I decide not to change my rating.', 'While overall the writing quality of the paper is high, the paper itself is a strong rejection.  I believe the analysis of the paper is at points flawed, and the experiments are minimal.  \n\nThis work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017.  Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone.  To combat this they use the noise-regularized mutual information estimator (I(X; L+eps)).  To actually estimate the mutual information the authors use the MINE estimator of Belghazi (2018).  Here they suggest using the neural network itself as a structural element in the form of the discriminator to take advantage of the specific circumstances in this case.  Doing this ensured that their estimator diverged in the zero noise limit as expected.  From here they show some experimental results of the effect of their objective on an MNIST / CIFAR10 classification task.\n\nThis paper fits into what is an increasingly large discussion in the literature, surrounding Information Bottleneck.  The paper itself does a very good job of citing recent relevant work.  Technically however I take issue with the framing of previous work in the last paragraph of the ""Deep neural nets"" subsection of Section 2.  Technically Achille & Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a ""single bottleneck layer"" as stated in the paper.  More generally at the end of that paragraph it is implied that the single bottleneck layer scheme ""deviates from the original theory"".  This is a misleading characterization of the original information bottleneck (Tishby et al 1999) in which there was a single random variable, a representation of the data (Z) satisfying the Markov conditions Z <- X -> Y.   I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017).  In general the paper does a poor job of distinguishing between the Shwartz-Ziv & Tishby paper and the rest, but this is a distinction that should be maintained.  The original information bottleneck may and has demonstrated utility regardless of whether the information bottleneck generally can help explain why ordinary deterministic feed forward networks trained with cross entropy and sgd generalize well.  \n\nThis also raises one of the main problems with the current work. The title, abstract and especially the conclusion (""This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning"") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks.  Here the authors modify the ordinary cross entropy objective, and so their networks are necessarily not ordinary and so they cannot claim they have helped clarify our understanding of the vast majority of neural networks currently being trained.  Again, this is distinct and should be kept distinct from the utility of their proposed objective, itself inspired by the information bottleneck.  Here too the paper falls flat.  If instead of attempting to comment on networks as they are designed today they aim to proposed a new information bottleneck inspired objective they really ought to directly compare other attempts along those lines (such as the ones they themselves cite  Alemi et al. 2018, Kolchinsky et al. 2017, Chalk et al. 2016, Achile & Soatto 2018, Belghazi et al. 2018) but there are no comparative studies.\n\nThe experiments are extremely lacking, not only are any of their cited alternatives compared, they don\'t compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.  Their reported numbers are not very impressive with their top MNIST number at 98.09 and their baseline at 97.73. These numbers are worse than many of the papers they themselves cite.  Only a single comparative results for both a limited training set run and the full one are shown, as well as only a single choice of beta.  The CIFAR10 numbers are not very good either.  There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.\n\nTechnically a deterministic function need not have infinite mutual information, if it is non-invertible, i.e. the sign function, or just floating point discretization. \n\nTheir own results in Figure 2 and the main body of the text highlight that the authors believe the true mutual information between the activations of the intermediate layers and the input is infinite.  If the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?\n\nJust plugging in the Discriminator for the objective (equation (7)) is flawed.  The discriminator, if optimal would learn to approximate the density ratio 1 + log p(x,y)/(p(x) p(y)) .   ( see f-GAN, Norowin et al. 2016).  How does this justify using the individual elements of the discriminator in the functional form of the IB objective?  \n\nAt the bottom of page 6 they rightfully say that mutual information is invariant to reparameterizations, but their noise regularized mutual information estimator is not (by their own reference (Saxe et al 2017).\n\nThe discussion at the center of page 8 is confusing.   They claim that Figure 5 (a) is more \'quantized\' than (b) and ""has reduced entropy"".  I think it should be the other way.  More clusters should translate to a higher KL divergence, or higher entropy.  If you need only identify which cluster an activation is in, that should require log K nats where K is the number of clusters.  (a) shows more clusters and so seems like it should cost more and have a higher entropy not a lower one.\n\nDespite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well.', '\nThis work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent ""compression"" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). As a consequence, no p.d.f. exists (with respect to the Lebesgue measure of R^{p+q}). Thus, defining mutual information is cumbersome. The paper attempts to overcome this problem by using a noisy version of the latent compression, i.e. L\' = L + \\epsilon, which can be seen as an ""ad hoc"" fix of this problem. Not too surprising, this additive noise works as a ridge-type (or weight-decay) regularizer, just as a Gaussian prior in regression.\n\nOn one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Such results are certainly interesting, both from a theoretical and from a practical point of view. On the other hand, I honestly think that on the conceptual side, this work does not make that many really interesting contributions. The observation that additive noise works as a weight-decay regularizer is in my opinion almost trivial, and any claims about experimental results ""validating(!) the IB theory"" seem to contain some degree of over-selling. In summary, I think that this is a paper that certainly contains some interesting ideas, but on the other hand I am not fully convinced about the significance and relevance of the findings.       ']","[-20, -80, -20]","[50, 20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also list several significant concerns ('Cons') that outweigh the positives. The reviewer points out issues with the paper's scope, methodology, and comparisons, and ultimately decides not to change their rating after reading responses. This suggests an overall negative sentiment, though not extremely so.\n\nThe politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They begin by acknowledging the paper's contributions and use phrases like 'carefully constructs' and 'novel founding'. Even when expressing criticisms, the reviewer maintains a constructive tone, asking questions and suggesting improvements rather than making harsh judgments. The use of 'Pros' and 'Cons' also provides a balanced structure to the feedback. However, the score is not extremely high as the review doesn't go out of its way to be overtly polite or complimentary."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states it's a 'strong rejection' and identifies numerous flaws in the analysis, experimental design, and conclusions. They criticize the paper's framing, lack of comparative studies, and poor experimental results. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They acknowledge the paper's 'high writing quality' and use phrases like 'I believe' and 'I take issue with' rather than making blunt accusations. However, the politeness is limited by the directness of the criticism and the lack of positive feedback beyond the initial comment on writing quality."", ""The sentiment score is slightly negative (-20) because while the reviewer finds the paper interesting in some aspects, they express significant doubts about the conceptual contributions and relevance of the findings. The reviewer uses phrases like 'not fully convinced about the significance and relevance' and suggests some claims are 'over-selling'. However, they do acknowledge some positive aspects, preventing a more negative score. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'I find this paper interesting' and 'certainly contains some interesting ideas'. They express criticisms in a relatively gentle manner, using phrases like 'I honestly think' and 'in my opinion', which softens the negative feedback. The language is not overly formal or polite, but it avoids rudeness or harsh criticism, maintaining a respectful tone.""]"
"['This paper proposes a joint embedding model that aligns video sequences with sentences describing the context (caption) in a shared embedding space. With the space, various tasks such as zero-shot activity recognition and unseen video captioning can be performed. The problem tackled in this paper is interesting. However, the approach proposed is limited in novelty and there are some serious flaws in the experimental settings. So overall, this paper is not yet ready for publication. \n\nPros:\n\n•\tThe overall bidirectional encoder-decoder architecture for learning a shared embedding space is sensible. It is also interesting that adversarial training is introduced so that unlabelled data can be utilized. \n•\tAdditional annotations are provided to two activity recognition datasets, creating new benchmarks.\nCons\n•\tMy first concern is the limited novelty of the work. Although I am not aware of a joint embedding learning model that has exactly the same architecture and formulation, the model is closely related to many existing ones both in zero-shot learning and beyond. More specifically,\no\tThe overall framework is similar to “correlational neural networks”, Neural Computation, 2016 by Chandar et al. This should be acknowledged.\no\tThe connections to CyclyGan and its variants for image-to-image style transfer is obvious, as pointed out by the authors.\no\tMore importantly, there are quite a few closely related zero-shot learning (ZSL) papers published recently. Although they focus on static images and class name, rather that image sequences and sentences, I don’t see any reason why these models cannot be applied to solve the same problem tackled in this paper. In particular, the autoencoder architecture was first used in ZSL in E. Kodirov, T. Xiang and S. Gong, ""Semantic Autoencoder for Zero-Shot Learning"", in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, Hawaii, July 2017. This work is further extended in Chen et al, “Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network”, cvpr18, now with adversarial learning. Similarly, variational autoencoder is adopted in Wang et al, Zero-Shot Learning via Class-Conditioned Deep Generative Models, AAAI 2018.  Note that the joint embedding spaces in these studies are the semantic spaces – attribute or word vector spaces representing the classes. In contrast, since the semantic modality is a variable-length word sequences, this is not possible, so a third space (other than the visual feature space or semantic space) is used as the embedding space. Beyond these autoencoder based models, there are also a number of recent ZSL works that use a conditional generative model with adversarial loss. Instead of learning a joint embedding space where the visual and text modalities are aligned and compared for recognition, these works use the text modality as condition to the generative model to synthesize visual features for the unseen classes followed by a conventional supervised classifier. The representative one of this line of work is Xian et al, “Feature Generating Networks for Zero-Shot Learning, cvpr18”.\no\tIn summary, there are too many existing works that are similar to the proposed one in one or more aspects. The authors failed to acknowledge most of them; moreover, it is not argued theoretically or demonstrated empirically, why combining different approaches together is necessary/making fundamental differences.\n\n•\tMy second main concern is the experiment setting. This paper adopts a conventional ZSL setting in two aspects: (1) the visual features are obtained by a video CNN, I3D, which is pretrained on the large (400 or 600 classes depending on which version is used) Kinetics dataset. This dataset have classes overlapping with those in ActivityNet, HMDB and UCF101. So if these overlapped classes are used in the unseen class partition, then the ZSL assumption (the target classes are ‘unseen’) is violated. (2): The test data only contains unseen class samples. In practice, one will face a test set composed of a mix of seen and unseen classes. Under this more realistic setting (termed generalized ZSL in the ZSL community), a ZSL must avoid the bias towards the seen classes which provide the only visual data available during training. The two problems have been identified in the ZSl community when static images are considered. As a result, the conventional setting has been largely abandoned in the last two years and the ‘pure’ and ‘generalized’ settings become the norm; that is, there is no overlapping classes between the test classes and classes used to pretrain the visual feature extraction network; and both seen and unseen class samples are used for testing. The ZSL evaluation is only meaningful under this more rigorous and realistic setting. In summary, the experimental results presented in this paper are obtained under the wrong setting and the proposed model is not compared with a number of closely related ZSL models listed above, so it is not possible to judge how effective the proposed model is. \n', 'The paper attempts multimodal representation of video and text through an attention layer that allows weighted temporal pooling. The approach was tested on a collection of datasets including a newly introduced dataset, with the embedding and evaluated on three tasks: zero-shot classification, activity clustering and captioning.\n\nThe paper is easy to read in general and the approach is scientifically sound. The need for an autoencoder in multimodal embedding has been proven for a variety of modalities including image-text, video-text, image-audio and video-audio. The contribution here is thus focused on temporal pooling through a learnt attention layer.\n\nHowever, the paper has a mix of tasks (3 tasks tested), without a conclusive understanding of the effect of the various loss functions on the learnt space. As the importance of various losses changes per task and dataset, the take-away message from the work is not obvious. Additionally, using unpaired data, proposed through a large-scale dataset is not obvious. The paper concludes that related data is required but how related data can be collected remains unexplored.\n\nThe evaluation for the unsupervised discovery seems biased – 1NearestNeighbour is used as opposed to the more balanced mAP on ranking all test sequences as opposed to top-1. \n\nThe collected dataset, which is a contribution of the paper is also poorly explained. The authors collect ‘dense annotations’ but it is not clear how many annotators were used, and what instructions they were given. The paper does not give examples of the collected annotations and how these differ from previous annotations available with the dataset (Fig 4).\n\nAppendix 1 concludes with sentences proposed to annotate UCF. These seem to apply per action and it’s not clear how they scale to the different instances, e.g. Action Surfing (85) is assigned to a male caption ‘a man is’, action 100 to a woman and action 96 to groups of people ‘people are riding’. This distinction is not obvious in all the instances of the dataset and such captioning might have significantly biased the results.\n\nOverall, there is little explanation of the decisions made to produce the comparative results. The novelty is limited to the attention pooling, which is not evaluated on all the three tasks. ', 'Summary:\nThe paper aims to learn a common embedding space for video appearance and text caption features. The learned shared embedding space then allows multiple applications of zero-shot activity classification, unsupervised activity discovery and unseen activity captioning.\n\nThe method is based on two autoencoders which have a common intermediate space. The losses optimized encourage the standard unimodal reconstructions in the AEs, along with joint embedding distances (appearance and text of the same video are encoded close by) as well as cross domain mapping (video encoding generates text and vice-versa), and cycle consistency. Apart from these additional supervised losses, unsupervised losses are added with adversarial learning which aim to bring the video and text encoding distributions in the common space close, as well as the standard real and generated video/text distributions close by adding corresponding discriminators (like in GANs). The whole system is learned end-to-end in two phases, first with supervised paired data and then with all paired and unpaired data.\n\nThe experiments are shown on four datasets: ActivityNet, HMDB, UCF101, MLB-YouTube\n\nPositives:\t\n- The problem of multimodal learning is an interesting and challenging problem\n- The paper is novel; while the idea of a joint shared embedding space is not new this paper adds new losses as summarized above and shows reasonably convincing empirical results\n- The results are shown for diverse applications which highlight the generality of the method\n- The use of unpaired/unsupervised data is also relatively less explored which this paper incorporates as well\n- The empirical results given are convincing, eg Tab1 gives a good ablation study showing how the different components affect the performance. SoA comparison are given on a standard task (however see below)\n\nNegatives:\n- Comparison with state of the art result Tab2 should also contain the features used. The performances might just be higher due to the better features used (Kinetics pretrained I3D). Please give a self implemented baseline method with same features but some standard loss in the shared space to give an idea of the strength of the features.\n- Ideally features provided by previous papers’ authors should be used if available and it should be shown that the proposed method improves results.\n\nOverall the paper is well written and had novel aspects which are convincingly evaluated on challenging and diverse tasks.\n']","[-60, -20, 70]","[20, 50, 80]","[""The sentiment score is -60 because the review is predominantly negative. The reviewer states the paper is 'not yet ready for publication' and lists several major concerns, including 'limited novelty' and 'serious flaws in the experimental settings'. While some pros are mentioned, they are outweighed by the cons. The politeness score is 20 because the reviewer uses professional language and acknowledges some positive aspects, but is also direct in criticism. Phrases like 'interesting' and 'sensible' are used alongside more critical statements. The reviewer avoids harsh language but clearly expresses concerns, maintaining a generally polite but firm tone throughout the review."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('easy to read', 'scientifically sound'), they raise several significant concerns and criticisms. These include the lack of a conclusive understanding of loss functions, issues with the evaluation method, poor explanation of the dataset, and limited novelty. The overall tone suggests more concerns than praise.\n\nThe politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They begin with positive comments and use neutral language to express criticisms ('However...', 'It is not clear...') rather than harsh or dismissive statements. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism. While not overly effusive, the language is consistently courteous and constructive."", ""The sentiment score is 70 (positive) because the review begins with a detailed summary of the paper's content, followed by a list of positives that outweigh the negatives. The reviewer acknowledges the paper's novelty, convincing empirical results, and diverse applications. The negatives mentioned are constructive suggestions rather than severe criticisms. The concluding statement is notably positive, stating that the paper is 'well written and had novel aspects which are convincingly evaluated on challenging and diverse tasks.' The politeness score is 80 (polite) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to describe the paper's content and provide balanced feedback. The criticisms are framed as suggestions for improvement rather than harsh judgments. Phrases like 'Please give' and 'Ideally' when suggesting changes indicate a polite approach to offering feedback. The overall tone is constructive and encouraging, which is characteristic of a polite academic review.""]"
"[""This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling.  The goal is to reduce the memory access and make efficient computation, in the meantime, to sustain good ASR performance.  Overall, the paper is well motivated and well written.  However, I have following concerns.\n\n1. It is not clear from the paper whether both the training and inference are conducted on embedded devices or only the inference?  I assume it is the latter but can't find it explicitly mentioned in the paper.  \n\n2. The exploration carried out in the paper is more on the system level and the novelty is not overwhelmingly significant.\n\n3. My major concern is that the reported WERs on WSJ and phoneme classification accuracy are quite off.  20%-30% WERs for WSJ  do not seem to be usable in real applications.  Honestly, I don't even think this performance is better than well-trained GMM-HMM acoustic models using a Viterbi decoder.  Furthermore, there is no clear winners across the investigated architectures  in terms of performance.  One question is if one wants to deploy such an on-device system, which architecture shall be chosen?  \n\n4. A more general comment on the work explored  in the paper.  First of all, the on-device memory issue puts a heavy constraint on the capacity of acoustic models, which will significantly hurt the modeling capability for the DNN-based acoustic models.  Deep learning acoustic models can outperform GMM-HMM because they can use large model capacity with very deep and complex architectures when a large amount of training data is available.  Second, for CTC, when the training data is limited,  its performance is far worse than the hybrid DNN-HMM model, let alone a pure end-to-end fashion without using external LM and dictionary.  If WFST-based decoders (composition of WFSTs of LM, dictionary and deblank/repetition) are used, then the memory issue will surface again. \n"", 'This paper present a study on efficient acoustic modeling using neural networks-based model. Four approaches are presented and evaluated: diag LSTM, QRNN, Gated ConvNet and adding a 1D convolution layer. The evaluation is done on ASR task using WSJ and in phoneme classification task using the TIMIT corpus. The study show that the inference speed is improved with comparable of better performance than the standard LSTM model.\n\nThe findings presented in this paper are interesting and quite useful when one wants to implement a LSTM-based acoustic model on mobile devices. The paper is well written and easy to ready. \n\nThe main issue of this paper is the lack of novelty: the three evaluated approaches (Diag LSTM, QRNN and Gated ConvNet) are not novel, the only novelty is the addition of a 1D convolution, which is not enough for a conference like ICLR. \n\nMinor comments on the experiments:\n* The network quantization approach has been shown to lead to efficient neural networks, could the authors provide a comparison between their approach and the quantization approach ?\n* On the TIMIT experiment, the authors could add a decoder and use the PER metric instead of the frame accuracy, so they could provide comparison with the literature. \n* WSJ and TIMIT are quite small corpora compared to the available corpora, maybe the authors should consider using large corpora like Librispeech. It could be interesting to see the performance of the presented approaches.\n\nOverall, this paper feels more like a technical report: the findings could be useful, but its novelty is too limited for ICLR. Hence I argue for rejection, and suggest that the authors consider submitting the paper to a speech conference like ICASSP.', 'This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. The paper well describes the problem of the current LSTM, especially focusing on the recurrent connection matrix operations, which is a bottle neck in this scenario, and introduces variants of RNNs (e.g., QRNN). Also these variants may not yield enough performance compared with LSTM, but 1-D convolution and/or deep structure helps to avoid the degradation. One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. Therefore, the paper\'s claim on CTC is not along with the current application trends. (It may be changed near future, but still hybrid systems are dominant). For example, the WSJ WER performance listed in Table 3 is easily obtained by a simple feed-forward DNN in the hybrid system. The latest Lattice free MMI with TDNN can achieve better performance (~2.X% WER), and this decoding is quite fast compared with LSTM. The authors should consider this current situation of state-of-the-art speech recognition. Also, the techniques described in the paper are all based on existing techniques, and the paper lacks the technical novelty.\n\nOther comments:\n- in Abstract and the first part of Introduction: as I mentioned above, CTC based character-prediction modeling is not a major acoustic model.\n- The paper needs some discussions about TDNN, which is a major acoustic modeling (fast and accurate) in Kaldi\n- p.4 first line ""and \x0c represents element-wise multiplication"": The element-wise multiplication operation was first appeared in Eq. (1), and it should be explained there.\n- Section 3.2: I actually don\'t fully understand the claims of this experiment based on TIMIT, as it is phoneme recognition, and not directly related to the real application, which is the main target of this paper I think. My suggestion is to place these TIMIT based experiments as a preliminary experiment to investigate the variants of RNN or gated CNN before the WSJ experiments. (I did not say that Section 3.2 is useless. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.)\n\n\n']","[-30, -20, -30]","[50, 60, 50]","[""The sentiment score is -30 because while the reviewer starts with some positive comments ('well motivated and well written'), they express several significant concerns about the paper's methodology, results, and overall contribution. The concerns outweigh the initial positive remarks, leading to a slightly negative overall sentiment. The politeness score is 50 because the reviewer uses polite and professional language throughout, even when expressing concerns. They use phrases like 'I have following concerns' and 'My major concern is' rather than using harsh or dismissive language. The reviewer also offers constructive feedback and suggestions, which contributes to the polite tone. However, the score is not higher as the review is primarily focused on critiquing the paper rather than offering extensive praise or encouragement."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting and quite useful', 'well written and easy to read'), they ultimately recommend rejection due to lack of novelty. The main criticism is that the paper's contribution is not significant enough for the ICLR conference. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive feedback. They use phrases like 'could the authors provide', 'the authors could add', which are polite ways of suggesting improvements. The reviewer also softens the rejection by suggesting an alternative venue for submission, which is a considerate approach."", ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'The paper well describes the problem'), they also point out significant issues. The reviewer criticizes the paper's focus on CTC-based models as not aligning with current trends, mentions a lack of technical novelty, and suggests major revisions. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'The authors should consider' and 'My suggestion is' rather than harsh criticisms. They also acknowledge the value of some parts of the paper, such as the TIMIT experiments. The language is constructive and aimed at improving the paper rather than dismissing it entirely.""]"
"['The paper introduces a relatively simple method for imitation learning that seems to be successful despite its simplicity. The method, SQIL, assigns a constant positive reward (r) to the demonstrations and zero reward to generated trajectories. While I like the connections between SQIL and SQL and the simplicity of the idea, I think there are several issues which connections with GAIL that are not discussed; some ""standard"" environments (such as Mujoco) that SQIL has not compared against the baselines. I believe the paper would have a bigger impact after some addressing some of the issues.\n\n(\nUpdate: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.\n\nThe Pong case is also quite interesting, although it seems slightly ""unfair"" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards.\n)\n\n1. The first issue is the similarity with GAIL in the ""realistic"" setting. Since we cannot have infinite expert demonstrations, there would always be some large enough network that could perfectly distinguish the demonstrations (assign reward to 1) and the generated policies (assign reward to 0). Therefore, it would seem to me that from this perspective SQIL is an instance of GAIL where the discriminator is powerful and expert demos are finite (and disjoint from generated trajectories, which is almost always the case for continuous control). In the finite capacity case, I am unsure whether the V and Q networks in SQIL does a similar job as the discriminator in GAIL / AIRL type algorithms, since both seem to extrapolate between demonstrations and generations?\n\n2. Moreover, I don\'t think SQIL would always recover the expert policy even with infinite demonstrations. For example, lets think about the Reacher environment, where the agent controls a robotic arm to reach a target location. The expert demonstration is the fastest way of reaching the target (move minimum distance between joints). If we consider the MDP to have possibly very large / infinite horizon (only stops when it reaches the target), I could construct a hack policy that produces larger episodic reward compared to the expert. The policy would simply move back and forth between two expert demonstrated states, where it would receive 1 reward in the states for odd time and 0 reward for the states for even time. The reward would be something like 1 / (1 - \\gamma^2) compared to the experts\' reward which is \\sum_{i=0..T} \\gamma^{i} = (1 - \\gamma^{T+1}) / (1 - \\gamma). \n\nSome fix would be to set the reward for generated policies to be negative, or introduce some absorbing state where the expert will still receive the positive reward even after reaching the target (but that is not included in demonstrations). Nevertheless, a suitable reward prior seems to be crucial to the success of this SQIL, as with GAIL requiring reward augmentation.\n\n3. Despite the above issues, I think this could be a very practical method due to its (perhaps surprising) simplicity compared to GAIL. However, the experiments only considered two environments that are not typically considered by GAIL; I believe SQIL would make a bigger impact if it is compared with GAIL in Mujoco environments -- seems not very implementation heavy because your code is based on OpenAI baselines anyway. Mujoco with image inputs would also be relevant (see ACKTR paper).\n\nMinor points:\n- What is the underlying RL algorithm for GAIL? It would seem weird if you use Q-learning for SQIL and TRPO for GAIL, which makes it impossible to identify whether Q-learning or SQIL contributed more to the performance. While GAIL used TRPO in the original paper, it would be relatively straightforward to come up with some version of GAIL that uses Q-learning. \n- Some more details in background for MaxEnt RL to make the paper more self contained.\n- More details about the hyperparameters of SQIL in experiments -- e.g. what is \\lambda?\n- Did you pretrain the SQIL / GAIL policies? Either case, it would be important to mention that and be fair in terms of the comparison.\n- Why does SQIL-11 perform worse than SQIL even though it is a tighter bound?\n- wrt. math -- I think the anonymous comment addressed some of my concerns, but I have not read the updated version so cannot be sure.', 'This paper proposes an imitation learning algorithm framed as an off-policy RL scenario. They introduce all demonstrations into a replay buffer, with positive reward r=1. Subsequent states derived from the agent will be also introduced in the replay buffer but with reward r=0. There is no additional reward involved. The hope is that agents will learn to match the expert in states appearing in the replay buffer and not try to do much else.\n\nOverall, I am not convinced that the paper accurately supports its claims.\n\n1.\tThe authors support their method by saying that extending GAIL and other imitation learning algorithms to support pixel observations has failed. However, there are actually some papers showing quite successful applications of doing that: e.g. see Li et al. 2017 in challenging driving domain in TORCS simulator. \n\n2.\tMore problematic, I think there is a clear flaw with this algorithm: imagine having trained successfully and experiencing many trajectories that are accurately reproducing the behaviour of the expert. Given their method, all these new trajectories will be introduced into the replay buffer with a reward of 0, given they come from the agent. What will the gradients be when provided with state-action pairs with both r=0 and r=1? These situations will have high variance (even though they should be clear r=1 situations) and this will hinder learning, which will tend to  decay the behaviour as time goes on.\nThis actually seems to be happening, see Figure 1 at the end: both SQIL curves appear to slowly starting to decay.\nThis is why GAIL is training its discriminator further, you want to keep updating the distribution of “agent” vs “expert”, I’m not sure how this step can be bypassed?\n\n3.\tHow would you make sure that the agent even starts encountering rewarding states? Do you need deterministic environments where this is more likely to happen? Do you need conditions on how much of the space is spanned by the expert demonstrations?\n\n4.\tAdditionally, Figure 1 indicates BC and GAIL as regions, without learning curves, is that after training? \n\n5.\tI am not convinced of the usefulness of the lengthy derivation, although I could not follow it deeply. Especially given that the lower bound they arrive at does not seem to accurately reflect the mismatch in distributions as explained above. \n\n6.\tThere are no details about the network architecture used, about the size of the replay buffer, about how to insert/delete experience into the replay buffer, how the baselines were set up, etc. There are so few details I cannot trust their comparison is fair. The only detail provided is in Footnote 5, indicating that they sample 50% expert and 50% agent in their mini-batches.\n\nOverall, I think the current work does not offer enough evidence and details to support its claims, and I cannot recommend its publication in this current form\n\n', 'In this paper, the author derived a unified approach to utilize demonstration data and the data collected from the interaction between the current policy and the environment, inspired from soft Bellman equation. Different from previous methods such as DQfD (Hester et al., 2017), SQIL does not require the reward signal of the expert data, which is more general and natural for real-world applications such as demonstration from the human.  The author verified SQIL on a toy Lunar Lander environment and a high-dimension image based observation environment, which demonstrate its advantages over behavior cloning and GAIL. Besides the advantages, I have serval concern which may help the author to further improve the paper.\n\n- The core algorithm is simple but I found the derivation is hard to read, which is a little messy from equation (5) to (7), and also the final loss (14) seems to be unrelated to the previous derivations( from equation (11) to (13)).  Also, can you add the gradient updates for $\\theta$ with equation (11) (denoted by SQIL-11 in your paper)? I am looking forward to reading the revised version.\n\n- To demonstrate the advantages of SQIL in high-dimension observations, the author only conducts one simple environment Car Racing, which is not enough to demonstrate its advantages. I wonder if it is possible to run more benchmark environments such as Atari game or Minecraft.\n\n- In the related work, the author argues that methods such as DQfD require reward signal, but it would be great to demonstrate the advantages of SQIL over these methods (including DQfD and NAC (Gao et.al, 2018)).\n\n- In previous imitation methods such as GAIL, they studied the effect of the amount of the demonstration data, which the paper should also conduct similar experiments to verify the advantage of SQIL.\n\n\nHester, Todd, et al. ""Deep Q-learning from Demonstrations."" arXiv preprint arXiv:1704.03732 (2017).\nGao, Yang, et al. ""Reinforcement learning from imperfect demonstrations."" arXiv preprint arXiv:1802.05313 (2018).']","[-20, -70, 50]","[60, 20, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('I like the connections between SQIL and SQL and the simplicity of the idea'), they also point out several issues and areas for improvement. The reviewer suggests that the paper would have a bigger impact after addressing these issues, indicating that the current version is not entirely satisfactory. The update section shows some improvement in the reviewer's opinion, but still maintains some reservations.\n\nThe politeness score is moderately positive (60) because the reviewer uses respectful and constructive language throughout. They acknowledge positive aspects, use phrases like 'I believe' and 'I think' to soften criticisms, and provide specific suggestions for improvement rather than just pointing out flaws. The reviewer also shows flexibility by updating their opinion based on the authors' revisions. However, the score is not extremely high as the review maintains a professional, rather than overly friendly, tone."", ""The sentiment score is -70 because the reviewer expresses significant doubts about the paper's claims and methodology. They use phrases like 'I am not convinced,' 'there is a clear flaw,' and 'I cannot recommend its publication in this current form,' which indicate a strongly negative sentiment. However, it's not at the extreme negative end as they do acknowledge some aspects of the work and provide constructive feedback. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I think' and 'I am not convinced' rather than making blunt statements, and they provide detailed explanations for their concerns. The language is not overtly polite, but it avoids rudeness and maintains a respectful, academic tone."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and advantages, but also expresses several concerns and suggestions for improvement. The review begins with a positive summary of the paper's approach and its advantages over previous methods. However, the reviewer then lists multiple areas for improvement, indicating a balanced perspective.\n\nThe politeness score is 80 (quite polite) because the reviewer uses respectful and constructive language throughout. They begin by highlighting the paper's strengths and use phrases like 'I found,' 'I wonder if,' and 'it would be great to' when offering suggestions. The reviewer also expresses anticipation for a revised version, which is encouraging. The tone is professional and helpful, without any harsh or dismissive language.""]"
"[""TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.\n\n* Summary\n\nThe manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\\lambda\\in (0,1)$ (sampled from a $\\mathrm{Beta}(\\alpha,\\alpha)$ distribution).\n\nA salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.\n\nA sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.\n\nI found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.\n\n* Major remarks\n\n- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.\n- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.\n- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.\n\n* Minor issues\n\n- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.\n- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability."", 'The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. \n\nThe paper is well written and easy to follow. Various experiments are conducted to support the contributions of the paper.  Nevertheless, the technical novelty seems a bit weak to me. The method basically moves the interpolating process from input space as in MixUp to randomly selected hidden states. More importantly, some of the paper’s claims are not very convincing to me in its current form.\n\nMajor remarks:\n\n1.\tThe authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. \nThe authors theoretically prove that with the proposed training cost in Manifold Mixup, the representation for each class will lie on a subspace of dimension dim (h) –d +1 (h and d are the hidden dimension and number of classes, respectively). I did not get the idea of how such dimension reduction relates to the ‘’flattening’’ of the manifold and in particular how such representations (representations for each class “concentrating into local regions”) can avoid the class collision issues as that in Mixup.\nExperimentally, from Figures 3 and 4, it seems the class collision issue could be worse than that of Mixup. For example, for mixing ratio of 0.6 (meaning the created image has almost half labels from the two original images), MixUp clearly shows, for instance in the second row, that there are two overlapped images (Horse and Plane), but Manifold Mixup seems to have only the Plane in the mixed image with a soft label. \n\n2.\tThe observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning. In this way, the paper would be much more convincing because the proposed method is so close to MixUp and the observation here is contradictive.\n3.\tI wonder how sensitive is the parameter Alpha in Manifold Mixup. For example, how the mixing rate Alpha impacts the results for NLL and Semi-supervised learning in section 5.2? \n4.\tIt would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper.\n\nMinor remarks:\n\n1.\tIn Table2, the result from AdaMix seems missed.\n2.\tWhy not using Cifar100, but with a new dataset SVHN for the semi-supervised learning in section 5.2?\n3.\tIn related work, regarding regularizing deep networks by perturbing the hidden states, the proposed method may relate to AgrLearn (Guo et al., Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks) as well.\n', ""The tone of the paper is notably scientific, as the authors clearly state the assumptions and all observations, whether positive or negative. That said, the approach itself can be seen as a direct extension of the earlier advanced 'mixup' scheme. In addition to performing data augmentation solely in the input space, their method proposes to train the networks on the convex combinations of the hidden state representations by learning to map them to the convex combinations of their one-hot ground truth encodings. \n\nThe results are competitive, in most cases exceeding the current state-of-art. However, the scheme has only been tested on low-res datasets such as MNIST, CIFAR and SVHN while the predecessor (plain 'mixup') also demonstrated improvement over the much larger and high-res ImageNet dataset.\n\nAlthough their work is not extremely novel, the experiments and observations could serve as a useful extension to this line of research. \n\nSuggestions:\n1.  The results on ImageNet would be a useful add-on to really drive home the benefit of their method when we talk of real-world large-scale datasets. \n2. The associated functions represented by 'f',  'g' and 'h' change meaning between sec. 2 and sec. 3. It would be more smooth if some consistency in notations was maintained.""]","[80, -20, 20]","[70, 50, 50]","[""The sentiment score is 80 (positive) because the reviewer expresses a very favorable opinion of the manuscript, stating 'In my opinion, a clear accept' and praising several aspects of the work. They found 'no flaw in the (two) proofs' and described the work as having a 'salient asset'. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive feedback and suggestions rather than harsh criticism. They use phrases like 'I would suggest' and 'I would like to read the authors' opinion', which are polite ways of offering recommendations. The reviewer also acknowledges the authors' work positively, which contributes to the polite tone. However, it's not extremely polite (100) as it maintains a professional, slightly formal tone rather than being overly deferential."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'easy to follow', 'various experiments conducted'), they express significant concerns about the paper's technical novelty and the convincingness of its claims. The reviewer states that 'the technical novelty seems a bit weak' and that 'some of the paper's claims are not very convincing'. These criticisms outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) because the reviewer uses respectful and professional language throughout. They offer constructive criticism and suggestions for improvement rather than harsh criticism. Phrases like 'I did not get the idea' and 'it would be very useful if' indicate a polite approach to expressing concerns and suggesting improvements. The reviewer also balances criticism with positive feedback, which contributes to the polite tone."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's scientific approach, competitive results, and potential usefulness as an extension to existing research. However, they also note that the approach is not extremely novel and has limitations in terms of datasets used. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive suggestions, and balances positive observations with areas for improvement without being harsh or dismissive. The tone is professional and objective, providing a fair assessment of the paper's strengths and weaknesses.""]"
"['This paper introduces a ""behavior module"" (BM) which is a small network that encodes preferences over actions and runs parallel to the fully connected layers of a policy. The paper shows this architecture working in Atari games, where the same policy can be used to achieve different action preferences over a game while still playing well. It also includes a thorough recap of past modular approaches. \n\nThe motivation for the BM is that we may want deep networks to be able to decompose ""strategy"" and ""behavior"", where behavior may influence decisions without affecting the performance. In this framework, the BM is trained on a reward of correctness + personalized “satisfaction”.\n\nThe experiments model behavior as preferences over how many actions to play simultaneously. The trained BMs can be transferred to new tasks without finetuning. The ideas here also have some similarity to the few shot learning literature.\n\nComments on the experiments:\n1. The Table 2  do not show a smooth interpolation between reward scaling and AMSR vs BD. This is surprising because the performance on the game should be highest went it is weighted to the most. This indicates to me that the results are actually high variance, the 0.8 vs 0.88 in stage 2 of 0.25r vs 0.5r means that is probably at least +/- 0.08 standard deviation. Adding standard deviations to these numbers is important for scientific interpretability.\n2. I expect some BMs should perform much better than others (as they have been defined by number of actions to play at once). I would like to see (maybe in the appendix) a table similar to table 2 for for individual BMs. I currently assume the numbers are averaged over all BMs.\n3. Similarly, I would like to see the BD for BM0 (e.g., if a policy is not optimized for any behavior, how close does it get to the other behaviors on average). This is an important lower bound that we can compare the other BD to. \n4. An obvious baseline missing is to directly weight the Q values of the action outputs  (instead of having an additional network)  by the designed behavior rewards. There is an optimal way to do this because of experimental choices. \n\nQuestions:\n1.For BM2, you write "" Up and Down (or Right and Left)"" did you mean ""Up and Right""? How can Up and Down be played at the same time?\n\nOverall, this paper uses neuroscience to motivate a behavior module. However, the particular application and problem settings falls short of these abstract ""behaviors"". Currently, the results are just showing that RL optimizes whatever reward function is provided, and that architectural decomposition allows for transfer, which was already showed in (Devin 2017). An experiment which would better highlight the behavior part of the BM architecture is the following:\n1. Collect datasets of demonstrations (e.g. on atari) from different humans.\n2. Train a policy to accomplish the task (with RL)\n3. Train BMs on each human to accomplish the task in the style of each human.\nThis would show that the BMs can capture actual behavior. \n\nThe dialog examples discussed in the abstract would also be very exciting.\n\nIn conclusion, I find the idea interesting, but the experiments do not show that this architecture can do anything new. The abstract and introduction discuss applications that would be much more convincing. I hope to see experiments with a more complex definition of ""behavior"" that cannot be handcoded into the Q function.', 'The authors try to build a deep neural network model based on observations from the human brain Pre-Frontal Cortex connectivity. Based on a DQN network, the authors add additional fully connected layers as Behavior Module to encode the agent behavior and add the Discriminator to transfer information between behavior modules. The authors experiment on four different games and evaluate based on metrics game scores and behavior distance.\n\nOverall the quality of the paper is low and I recommend to reject it.\n\n[Weakness in Details]\n1. I am not convinced that the proposed algorithm actually solves/works as described in the motivation. Moreover, the whole framework just adopts existing algorithms(like DQN and adversarial training) which provides little technical contribution.\n\n2. I am skeptical about the motivation whether mimicking the human brain Pre-Frontal Cortex connectivity can really result in a better neural network model. The poor execution and insufficient evaluation of this work prevent me from getting a clear answer.\n\n3. It is very strange that the authors emphasize that ""This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states."" while in the end doing experiments with video games playing. There are tons of public recommendation data sets out there, why not experiment on recommendation, which has much clear(well-established) evaluation metrics and public-domain datasets that can make it easier for others to repeat the experiments.\n\n4. The experiments are insufficient and the baselines are weak. Lots of state of artworks are left out.\n\n5. The writing of this paper needs further improvement and parts of this paper is not clearly written which makes it challenging for readers to follow the authors\' ideas.', '# Summary\nThis paper proposes to learn behaviors independently from the main task. The main idea is to train a behavior classifier and use domain-adversarial training idea to make the features invariant to sources of behaviors for transfer learning to new behaviors/tasks. The results on Atari games show that the proposed idea learns new behavior more quickly than the baseline approaches. \n\n[Cons]\n- Some descriptions are ambiguous, which makes it hard to understand the core idea and goal of this paper. \n- The experimental setup is not well-designed to show the benefit of the idea. \n\n# Comments\n- This overall idea is a straightforward extension from domain-adversarial learning except that this paper considers transfer learning in RL.\n- The goal/motivation of this paper is not very clearly described. It seems like there is a ""main task"" (e.g., maximizing scores in Atari games) and ""behavior modules"" (e.g., specific action sequences). It is unclear whether the goal of this paper is to learn 1) the main task, 2) learning new behavior modules quickly, or 3) learning new (main) tasks quickly. In the abstract/introduction, the paper seems to address 3), whereas the actual experimental result aims to solve 2). The term ""task"" in this paper often refers to ""main task"" or ""behavior"" interchangeably, which makes it hard to understand what the paper is trying to do. \n- The experiment is not well-designed. If the main focus of the paper is ""transfer to new tasks"", Atari is a not a good domain because the main task is fixed. Also, behavior modules are just ""hand-crafted"" sequences of actions. Transfer learning across different behaviors are not interesting unless they are ""discovered"" in an unsupervised fashion. \n- The paper claims that ""zero-shot"" transfer is one of the main contributions. Zero-shot learning by definition does not require any additional learning. However, they ""trained"" the network on the new behavior modules (only the main network is fixed), which is no longer ""zero-shot"" learning. ']","[-20, -80, -50]","[60, -20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer finds the idea interesting, they express significant concerns about the experiments and results. They state that 'the experiments do not show that this architecture can do anything new' and suggest that the paper falls short of its abstract goals. However, it's not entirely negative as they do acknowledge the interesting concept and potential for future work.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They offer specific suggestions for improvement, ask clarifying questions, and provide detailed feedback without using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper, such as the 'thorough recap of past modular approaches' and the potential of the idea, which contributes to the polite tone.\n\nOverall, the review balances critique with constructive feedback in a respectful manner, resulting in a slightly negative sentiment but a notably polite tone."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer explicitly recommends rejection and lists multiple weaknesses without mentioning any strengths. The phrase 'Overall the quality of the paper is low' is particularly damning. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite harsh and dismissive. Phrases like 'I am skeptical,' 'It is very strange,' and 'The poor execution' contribute to a somewhat impolite tone. The reviewer doesn't soften criticisms or offer encouragement, which would be expected in a more polite review."", ""The sentiment score is -50 because the review is predominantly critical, highlighting several cons and limitations of the paper. The reviewer points out ambiguities in descriptions, poorly designed experimental setup, and issues with the paper's goals and claims. However, it's not entirely negative as it acknowledges some positive aspects like the paper's results showing quicker learning of new behaviors.\n\nThe politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional and objective tone throughout. They use neutral language like 'Some descriptions are ambiguous' and 'The experiment is not well-designed' rather than harsh or rude phrasing. The reviewer also begins with a neutral summary of the paper's content before moving into criticisms, which is a polite approach in academic reviews.""]"
"['The paper aims to address issues with Domain Adaptation by using RL approaches. Domain Adaptation is an actively studied area in NLP research and so this paper is relevant and timely. This paper proposes and algorithm that is in line with work that aims at selecting data smartly when performing Domain Adaptation. The proposed algorithm learns representations for text in the source and target domains jointly. The proposed algorithm has two components i) a selection distribution generator (SDG) and ii) a task specific prediction for tasks being POS tagging, Dependency parsing and Sentiment Analysis.\n\nWhile the proposed algorithm is interesting from a RL perspective and make sense, there is no explanation provided as to why this algorithm should do better over non RL based approaches for tasks such as Sentiment Analysis.\n\nDomain Adaptation is widely studied for Sentiment Analysis and a lot of current research focuses on the various aspects of domain data, such as word and sentence level semantics, when developing algorithms. For example the following papers all (saving the third) address the problem of Domain Adaptation for Sentiment Analysis through various approaches fairly similar to the authors\' algorithm, that provide similar if not better results than those provided in the paper,\n\n[1]Barnes, Jeremy, Roman Klinger, and Sabine Schulte im Walde. ""Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains."" arXiv preprint arXiv:1806.04381 (2018).\n[2] Ziser, Yftah, and Roi Reichart. ""Pivot Based Language Modeling for Improved Neural Domain Adaptation."" In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), vol. 1, pp. 1241-1251. 2018.\n[3]An, Jisun, Haewoon Kwak, and Yong-Yeol Ahn. ""SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment."" arXiv preprint arXiv:1806.05521 (2018).\n\nParticularly the second paper is a clear improvement over SCL (the earlier pivot based approach), a baseline that is considered by the authors in this work. There are no comparisons against this work in this paper, yet the authors compare against SCL alone.\n\nDue to lack of comparisons against state-of-the-art in Sentiment Analysis/Domain Adaptation for Sentiment Analysis it is hard to accept the claims made by the authors on the superiority of their algorithm. Had their paper aimed at improving over other RL based approaches for Domain Adaptation for Sentiment Analysis, some experiments could be over looked. \n\nBut, when making a claim that addresses the problem of Sentiment Analysis, comparisons against the state-of-the-art non RL based approaches is extremely important. Particularly, given the size of the data sets used, one could use lexical/dictionary based approaches [3] and improve upon the classification accuracies without having to train such an involved algorithm.\n\nFurthermore there is no qualitative analysis provided to gain insights into the behavior of the embeddings spaces of the target and source domains that are learned jointly via the proposed algorithm. At least such an analysis would have provided some insight into why the authors\' RL based solution is better than a non RL based solution.\n\nThe lack of reference or comparisons against relevant literature is future highlighted by the seemingly relevant, yet largely dated related works section.\n', ""== Originality ==\nThe idea of matching features/representations across the source domain and target domain is an old idea, but it is executed in an interesting new way in this paper.\n\nIn this approach, feature representations are learned by training a neural classifier on the source domain, and an RL agent influences the feature representation by iteratively adding/removing examples from the source training data. The RL agent receives reward when the resulting feature representation causes the source domain data and target domain data to look more similar in distribution in feature space. To efficiently estimate the improvement in feature matching, a nice data bucketing strategy is used.\n\nThe novelty of the approach is the main strength of this paper.\n\n== Quality of results ==\nThe experimental results seemed overall positive, but I felt that they could have been stronger.\n\nFor POS tagging, the authors don't compare against the domain adaptation methods mentioned in their related work section. Instead, they compare against Bayesian optimization using several heuristic criteria, and it was unclear where this baseline comes from. This made it hard to see whether the new approach represents a true improvement over existing techniques.\n\nFor dependency parsing, it appears that the proposed approach is outperformed by simply training on all of the source domain data.  It would be interesting to know whether this is because feature-matching is not a good proxy for target domain performance (objective mismatch) or whether the RL system converged to a poor local optima (optimization failure).\n\n== Clarity ==\nI felt that the abstract and introduction were vague in describing the main conceptual contribution.\n\nHowever, Section 2 (The Approach) was clearly written and I came away understanding exactly what the authors are doing.\n\n== Minor comments ==\n- Algorithm 1 seems to have a typo: the definition of \\nabla \\tilde{J}(W) on the second to last line seems to be missing \\nabla \\log \\pi\n- Many citations throughout the paper need to be wrapped in parentheses\n\n== Conclusion ==\nThis paper presents an interesting new approach for dataset selection and learning domain-invariant representations.\n\nPros:\n- originality of the approach\n\nCons:\n- Experiments could have been more convincing:\n    - should compare against at least one other state-of-the-art domain adaptation method\n    - results on dependency parsing (the most challenging task they consider) were mostly negative\n    - evaluation on other more recent multi-domain NLP tasks would have been nice (e.g. MultiNLI)\n- Abstract and intro could provide better description of the conceptual contribution, as well as motivation"", 'Response to author comments:\n\nUnfortunately I am still significantly unclear on why RL is useful here.  The author response attempts to clarify that by pointing me to paragraph 2 of the intro, which states that RL has been used for data selection in other settings in the past.  What would help me (and I believe, the paper) more is a reason why greedy selection isn\'t sufficient for this particular problem.  Even just a single motivating example would be extremely helpful.  R3 mentioned similar concerns in their review, saying that the paper lacks explanation for why RL would win over non-RL for e.g. sentiment analysis.\n\nLikewise, while I appreciate the authors comparing against a stronger baseline in Figure 3, I don\'t know how to interpret the figure.  Why is Figure 3(b) better than Figure 3(c), and why does using RL cause that difference to arise?\n\nOriginal review:\n\nDomain adaptation is an interesting task, and new methods for it would be welcome.  This paper appears to have technical depth and the experimental results are promising.  However, the presented approach is complex, and I found it very hard to understand -- both in terms of how exactly it works, and in terms of why the chosen techniques were chosen.  More detail on my questions and confusions follows.\n\nFirst, I never understood the motivation for using RL here.  If minimizing the distance between selected data from the source domain and data in the target domain is the objective (equation 1), how does RL help?  The reward seems like it is immediate in each time step.  How does the *order* in which we add source examples to our collection matter?  I never understood the crucial difference that made the RL approach outperform the baselines that just select examples that minimize e.g. JS divergence.  Neither the paper\'s discussion of motivation nor the experimental analysis clarifies this.\n\nThe paper says in Section 2.1 that a formal description of the representations is to follow.  I didn\'t see this description (I do not see a formal definition of how the feature extractor works, and e.g. how it produces vectors that are *distributions* that can be used within e.g. JS divergence).\n\nThe paper also says it follows (Ruder and Plank, 2017) in using JS as a baseline, but as I understand that work the JS baseline is computed over words, not learned representations.  What is done in the submission, is the JS baseline over words in the instances, or the representations from the feature extractor?\n\nWhat is the reason for partitioning the source data into disjoint ""data bags""?  Why not just select the best source domain examples (from among all the source data) using RL?\n\nThe experiments are generally over enough tasks and compare against several baselines, and although the empirical wins are not that large I feel that they would be sufficient for publication if not for my other concerns.  The analysis (sec 4) did not make it clear to me why the RL approach works.  The visualization in Figure 3 only contrasts the proposed approach with a weak baseline of selecting all source data -- what we really need to see is an analysis that reveals why the learning of a policy with RL is better than simply greedily minimizing JS for each source data selection, for up to some limit of n selections.\n\nMinor\nThe paper has a number of typos\nThe citations in the paper are mis-formatted -- seem to use shortcite where they shouldn\'t (e.g. ""scenarios Akopyan and Khashba (2017)"" should be ""scenarios (Akopyan and Khashba, 2017)"").\nWhen the policy \\pi_w(a | s) is introduced at the start of Sec 2.2, it uses symbols (a, s) that have not been defined, also that policy variable is not really utilized in the text so it could be deleted.']","[-50, 20, -50]","[20, 60, 20]","[""The sentiment score is -50 because while the reviewer acknowledges the relevance and timeliness of the paper's topic, they express significant concerns about the lack of comparisons with state-of-the-art methods, insufficient explanations, and missing qualitative analysis. The review is more negative than positive, but not entirely dismissive. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'interesting from a RL perspective' and 'the proposed algorithm is interesting,' while also clearly stating their concerns. The language is not overly formal or polite, but it avoids rudeness and presents criticisms in a constructive manner."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the originality and interesting approach of the paper, calling it the 'main strength'. However, they also express concerns about the experimental results and clarity of the abstract/introduction, balancing out some of the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging positives before mentioning negatives, and framing criticisms as suggestions for improvement rather than harsh judgments. They use phrases like 'I felt that...' and 'It would be interesting to know...' which maintain a polite tone. The reviewer also provides a balanced conclusion with both pros and cons."", ""The sentiment score is -50 because the reviewer expresses significant concerns and confusion about the paper's approach and motivation, stating they are 'still significantly unclear' on key aspects and found it 'very hard to understand'. However, they do acknowledge some positives like 'technical depth' and 'promising' results, preventing an extremely negative score. The politeness score is 20 because the reviewer uses generally polite language like 'unfortunately' and 'I appreciate', and frames criticisms as their own confusion rather than attacking the authors. However, the overall tone is quite critical, limiting how high the politeness score can be.""]"
"['Edited: I raised the score by 1 point after the authors revised the paper significantly.\n\n--------------------------------------------\n\nThis paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.\n\nThe approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.\n\n1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.\n\n2. In Section 2.2 the authors write ""... framework is shown in Figure X"" without even showing the figure.\n\n3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.\n\n4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: ""vertices which satisfies"", ""makes W be affected"", ""the some strong baseline methods"", ""a set research papers"", and ""in align with"". The authors are suggested to do a thorough proofreading.\n\n', 'I appreciate the author response and additional effort to provide comparison with MoNet. I have raised my rating by 1 point. It should be noted that the edits to the revision are quite substantial and more in line of a journal revision. My understanding is that only moderate changes to the initial submission are acceptable.\n\n-----------------------------------------------\n\nThe paper introduces a new regularization approach for graph convolutional networks. A transposed GCN is appended to a regular GCN, resulting in a trainable, graph specific regularization term modelled as an additional neural network.\n\nExperiments demonstrate performance en par with previous work in the case where sufficient labelled data is available. The SRGCNs seem to shine when only few labelled data is available (few shot setting).\n\nThe method is appealing as the regularization adapts to the underlying graph structure, unlike structure-agnostic regularization such as L1.\n\nUnclear why the results are not compared to MoNet (Monti et al. 2017) which seems to be the current state-of-the-art for semi-supervised classification of graph nodes.\n\nOverall, well written paper with an interesting extension to GCN. The paper is lacking a comprehensive evaluation and comparison to latest work on graph neural networks. The results in the few shot setting are compelling.', 'This paper proposes to regularize the training of graph convolutional neural networks by adding a reconstruction loss to the supervised loss. Results are reported on citation benchmarks and compared for increasing number of labeled data.\n\nThe presentation of the paper could be significantly improved. Details of the proposed model are missing and the effects of the proposed regularization w.r.t. other regularizations are not analyzed.\n\nMy main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.\n\nDetails / references of the transposed convolution operation are missing (see e.g. https://ieeexplore.ieee.org/document/7742951). It is not clear what the role of the transposed convolution is in that case. It seems that the encoder does not change the nodes nor the edges of the graph, only the features, and the filters of the transposed convolution are learnt. If the operation is analogous to the transposed convolution on images, then given that the number of nodes in the graph does not change in the encoder layers (no graph coarsening operations are applied), then learning an additional convolution should be analogous (see e.g. https://arxiv.org/pdf/1603.07285.pdf Figure 4.3.). Could the authors comment on that?\n\nDetails on the pooling operation performed after the transposed convolution are missing (see e.g. https://arxiv.org/pdf/1805.00165.pdf, https://arxiv.org/pdf/1606.09375.pdf). Does the pooling operation coarsen the graph? if so, how is it then upsampled to match the input graph?\n\nFigure X in section 2.1. does no exist.\n\nSupervised loss in section 2.2.1 seems to disregard the sum over the nodes which have labels.\n\n\\hat A is not defined when it is introduced (in section 2.2.2), it appears later in section 2.3.\n\nSection 2.2.2 suggests that additional regularization (such as L2) is still required (note that the introduction outlines the proposed loss as a combination of reconstruction loss and supervised loss). An ablation study using either one of both regularizers should be performed to better understand their impact. Note that hyper-parameters chosen give higher weight to L2 regularizer.\n\nSection 3 introduces a bunch of definitions to presumably compare GCN against SRGCN, but those measures of influence are not reported for any model.\n\nExperimental validation raises some concerns. It is not clear whether standard splits for the reported datasets are used. It is not clear whether hyper-parameter tuning has been performed for baselines. Authors state ""the parameters of GCN and GAT and SRGCN are the same following (Kipf et al; Velickovic et al.)"". Note that SRGCN probably has additional parameters, due to the decoder stacked on top of the GCN. Reporting the number of parameters that each model has would provide more insights. Results are not reported following standards of running the models N times and providing mean and std. Moreover, there are no results using the full training set.']","[-50, 50, -60]","[20, 75, 20]","[""The sentiment score is -50 because while the reviewer acknowledges the approach as 'reasonable', they state that 'more work is needed before it reaches an acceptable level' and list several significant issues with the paper. The reviewer's tone is critical overall, pointing out dubious theorems, missing figures, and numerous grammatical errors. However, they do offer some positive feedback initially. The politeness score is 20 because the reviewer uses relatively polite language like 'the authors are suggested to' and 'may be strengthened', avoiding harsh direct criticism. They also acknowledge the paper's potential and offer specific recommendations for improvement. However, some phrases like 'dubious' and 'handwavy' slightly reduce the politeness score."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the authors' efforts and improvements, noting that they have raised their rating. They also praise the paper as 'well written' and the method as 'appealing'. However, they also point out some limitations, such as lacking comprehensive evaluation and comparison to latest work. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, starting with 'I appreciate' and using phrases like 'It should be noted' instead of more direct criticisms. They also balance critiques with positive comments, showing consideration for the authors' work while maintaining professional objectivity."", ""The sentiment score is -60 because the review is predominantly critical. The reviewer expresses several concerns about the paper's presentation, model design, novelty, and experimental evaluation. Phrases like 'could be significantly improved,' 'main concerns,' and 'raises some concerns' indicate a negative sentiment. However, it's not entirely negative as the reviewer does provide constructive feedback and asks for clarifications, hence not reaching the lowest possible score. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use polite phrases like 'Could the authors comment on that?' and provide specific suggestions for improvement. The language is not overtly rude, but it's also not excessively polite, leaning slightly towards the polite side of neutral.""]"
"[""This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning: \n1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states. \n2) the policy network which takes in the state and the predicted value function of the opponent. \nThe main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes. \n\nTechnical detail and concerns: \nThe paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns. \n\nThe authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent. \nMy expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA. \n\nFurthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant. \n\nWhile the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.\n\n\n"", 'The paper presents an approach to multi-agent learning based on the framework of model-agnostic meta learning. The originality of the approach lies in the decomposition of the policy in two terms, with applications to opponent modeling: the first part of the policy tries to predict some important characteristic of the agent (the characteristic itself is prior knowledge, the value it takes for a particular opponent is learnt from observations). The second part of the policy takes the estimated characteristic of the opponent as input, the current state and produces the action. All networks are trained within the MAML framework. The overall approach is motivated by the task of opponent modeling for multi-agent RL.\n\nThe approach makes sense overall -- the ""value"" of the opponent is valuable prior knowledge. The originality is limited though. In this kind of paper, I would expect the experiments to make a strong case for the approach. Unfortunately, the experiments are extremely toyish and admittedly not really ""multi-agent"": the ""opponent"" has a fixed strategy that does not depend on what the current is doing (it is therefore not really an opponent). The experimental protocol is more akin to multitask RL than multi-agent RL, and it is unclear whether the approach could/should work for opponent modeling even on tasks of low complexity. In other words, the experimental section does not address the problem that is supposed to be addressed (opponent modeling).\n\nother comments:\n- ""The opponent in our game is considered as some player that won’t adapt its policy to our agent."" -> in the experiments it is worse than that: the opponents actions do not even depend on what the agent is doing... So admittedly the experiments are not really ""multi-agent"" (or ""multi-agent"" where the ""opponent"" is totally independent of what the agent is currently doing).\n\n- ""Each method trains 800 iterations to get the meta learners and use them to initialize their networks. Then 10 new opponents are sampled as testing tasks. Four methods all train 4000 games for each testing task."" -> what does 800 iterations mean? Does it mean 800 episodes (it would seem strange for a ""fast adaptation task"" to have fewer episodes for training than for testing).\n\n- ""Notice that the reward trend for MOA first drops and then raises as the testing process goes on. This shows the process that the meta-learner adapt to the current task."" -> the adaptation to the new opponent does not really explain the drop?\n\n- Figure 3(c): the MA baseline has a reward of ~-10, which is worse than random (a uniform random placement at the 5 strategic positions would get 10*1/5-10*4/5 = -6). On the other hand, MOA achieves very high rewards, which indicates that the ""opponents"" strategies have low entropy. What is the best achievable reward on the blocking game?\n', 'This paper focuses on fast adaptation to new behaviour of the other agents of the environment, be it opponents or allies. To achieve this, a method based on MAML is proposed, with two main components:\n1) Learn a model of some characteristics of the opponent, such as ""the final goal, next action, or any other character we wish to predict""\n2) Learn a policy that takes as input the output of the model and the state, and that outputs the action of the agent.\n\nThe goal is that after a phase of meta learning, where the agents learns how to play against some new agents sampled from the distribution of opponents, it can quickly adapt to a new unseen agent. (""Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent"")\n\nWhile the motivation of this work is clear and the goal important for the RL community, the experiments fail to support the claim above.\n\nThe first task they demonstrate their approach on is a chasing game, where the opponent has a private goal cell it tries to reach, and the agent has to chase it. At the end of the game, it gets a reward of 10 if it is on the same cell, 5 if in an adjacent cell, and 0 otherwise. The exact details of the dynamic are not really clear, for example what happens in the event of a collision is not mentionned, and the termination condition is not mentionned either. (the text reads ""One game lasts for at least 15 steps"", maybe it was meant to be ""at most 15 steps"" ?).\nThe first incoherent aspect of this experiment is that they use 800 iterations of meta-learning, and then, when testing, they fine-tune their networks against each test opponent during 4000 games. That is, they use 5 times more game when fine-tuning as opposed to when pre-training, which contradicts the claim ""the agent can adapt to the new opponent with a small number of interactions with the opponent"" (this is not really few-shot learning anymore).\nFurther more, they compare their approach with various ablations of it: they either remove the meta-learning for the model (MA), for the policy (MO), or both (NM). The description of the NM baseline is not very precise, but it seems that it simply boils down to a regular (dueling) DQN: In this setting, since the opponent appears to have a fixed goal, finetuning against a single opponent simply boils down to learning a policy that reaches a specific cell of the grid, which we can expect DQN to solve perfectly on a 8x8 grid with 4000 training games. And yet, the curves for NM in graph 2c is not only really noisy, but also falls far from the optimum, which the authors don\'t discuss. There might be a problem with the hyperparameters used or the training loop.\n\nThe second task is a blocking game: the opponent has to choose amongst 5 paths to get to the top, and the agent has to choose the same path in order to block it. The action space should be precisely described, as it stands it is difficult to understand the dynamic. There are at least two possible ways to parametrize the actions:\n1) Similarly to the blocking game, the agents could move in the 8 directions. In that case, based on the picture 3a, it seems that the agent can just mirrors the move of the opponent: since the moves are simultaneous, that would mean that the agent is always one step late, but each path is long enough for the agent to reach the exit before its opponent (it explicitly stated that the agent needs to block the exit, and that the opponent will not change path during one game). That would imply that perfect play is possible without any meta-learning or oponent modeling, and once again the NM baseline (or any vanilly DQN/Policy gradient method) should perform much better.\n2) One other alternative is to have an action space of 5 actions, which correspond to the 5 paths. In that case the game boils down to a bandit, since both agents only take one action. Note that under this assumption, the random policy would get the right path (and reward +10) with probability 1/5 and a wrong one (reward -10) with probability 4/5, which leads to an expectated reward of -10*4/5 + 10/5 = -6. This is not consistent with the graph 3c, since at the beginning of the training, the NM agent should have a random policy, and yet the graph reports an average reward of -10 (the -6 mark seems to be reached after ~1000 episodes)\n\nThe last task boils down to one opponent that reaches one cell on the right, and the agent must reach the matching cell on the left. In this setting, the same discussion on the action space as the second task can be made. We note that the episode for 16 steps, and the distance from the center to any cell is at most 4 steps: an optimal policy would be to wait for 4 steps in the middle, and as soon as the opponent has reached its goal, use the remaining 12 steps to get to the mirror one. Once again, this policy doesn\'t require any prediction on the opponent\'s goal, and it\'s hard to believe that DQN (possibly with an lstm) is not able to learn that near perfectly.\n\n\nIn a last test the authors compare the performance of their algorithms in a one shot transfer setting: they sample 100 opponents for each task and play only one game against it (no fine-tuning). It is not clear whether special care has been taken to ensure that none of the sampled opponents has already been seen during training.\nWe note that the rewards reported for MO and MA (resp 0.0 and -0.08) are not consistent with the description of the reward function: on the worst case, the opponent chooses a goal on one extreme (say y1 = 1) and the agent chooses an object on the other end (say y2 = 7). In that case, the reward obtained is sampled from a gaussian with mean \\mu = 10 - 3/2 * |y1 - y2| (which in this case evalutes to 1), and variance 1. This is highly unlikely to give such a low average reward over 100 episodes (note that this is worst case, if the opponent\'s goal is not on the extreme, the expected reward is necessarily higher). One possibility is that the agent never reaches an object, but in that case it would imply the that the meta-learning phase was problematic.\nWe also note that it is explicited that the MOA, MO and MA methods are tested after meta-training, but nothing is precised for NM. Has it been trained at all? Against which opponents? Is it just a random policy? There are too many missing details for the results to be interpretable.\n\n\nApart from that, the paper contains a significant amount of typos and gramatical mistakes please proof-read carefully. Some of them are:\n""To demonstrate that meta-learning can do take""\n""player 1 is the red grid and player 1 is the green one""\n""we further assume that there exist a distribution""\n"" the goal’s location over the map is visualize in figure""\n""Both players takes actions simultaneously""']","[-70, -50, -70]","[-20, 20, 20]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's technical details, missing baselines, and clarity issues. They conclude that the paper is 'unsuitable for publication in its current form', which is a strong negative sentiment. However, they do acknowledge that 'the idea itself is interesting', preventing the score from being even lower. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite direct and critical. They use phrases like 'The authors entirely miss this and fail to address possible concerns' and 'the above concerns render the paper unsuitable for publication', which come across as somewhat harsh. The reviewer doesn't use many polite phrases or soften their criticisms, leading to a slightly negative politeness score."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the paper, particularly regarding the experimental section. While they acknowledge that the approach 'makes sense overall', they criticize the originality as 'limited' and the experiments as 'extremely toyish' and not truly multi-agent. The reviewer states that the experimental section 'does not address the problem that is supposed to be addressed'. These criticisms outweigh the initial positive comments, resulting in a negative overall sentiment.\n\nThe politeness score is 20 because the reviewer maintains a professional tone throughout and uses relatively neutral language. They begin with positive aspects before moving to criticisms, which is a polite approach. Phrases like 'The approach makes sense overall' and 'Unfortunately, the experiments are...' soften the criticism. However, the use of terms like 'extremely toyish' and the direct questioning of the approach's validity prevent the score from being higher. The reviewer also provides specific, constructive feedback, which is considered polite in academic contexts."", ""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's methodology, experimental design, and results interpretation. They point out multiple inconsistencies, unclear descriptions, and potential flaws in the experiments. The reviewer states that 'the experiments fail to support the claim' made by the authors, which is a strong negative sentiment. However, the score is not at the extreme negative end because the reviewer acknowledges the clear motivation and important goal of the work.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone throughout. They use phrases like 'While the motivation of this work is clear and the goal important' and 'There might be a problem with', which soften the criticism. The reviewer also offers specific suggestions for improvement and points out areas that need clarification, which is constructive. However, the score is not higher because the criticism is quite direct and the tone, while not rude, is not overtly polite either.""]"
"['This paper is about using ""neural stethoscopes"", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process. The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks. Moreover it can promote or suppress the correlation of such features to the tasks related to the main one. The framework is applied to the task of visual stability prediction of block towers. The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes. Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task. Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.\n\nStrengths:\n+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.\n+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.\n+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.\n\nWeaknesses:\n- The novelty is limited related to multitask learning, thus it is an incremental paper.\n', ""This paper combines the global and local stability prediction and tries to get interpretable results using the stethoscope design, which is actually a weighted subbranch for the main branch. There are several concerns regarding the proposed framework. \n\n1) How to choose \\lambda? A better design could be a learnable \\lambda. Instead of just one scalar value, it could be better to learn a map of \\lambdas, which indicates the distribution of local stability and how it is related to global stability. The visualization of the \\lambda map might be more interpretable for understanding the stability prediction. \n\n2) The global stability prediction does not have a consistent correlation with the local stability prediction, as shown by the easy and hard examples. This complex relationship will confuse the network during the training. That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up. This is hard to provide a meaningful interpretation of the task. \n"", 'The paper focuses on the stability prediction task on the ShapeStacks dataset. Specifically, the paper creates a new extension to the dataset, and it proposes the use of ""Neural Stethoscopes"" framework to analyze deep neural nets\' physical reasoning of local stability v.s. global stability. It is shown in the paper neural nets tend to be misled by local stability when the task is to predict global stability. Then the paper utilizes the proposed framework to de-bias the misleading correlation to achieve a state-of-the-art on the dataset.\n\nThe paper is very well-written and easy to follow. The main idea is simple and the experiments are detailed. Specifically on the task of stability prediction, it is quite interesting to know that neural nets can be misled by visual cues (local stability).\n\nHowever, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper\'s implication is.']","[70, -50, 50]","[80, 20, 80]","[""The sentiment score is 70 (positive) because the review begins with a detailed, appreciative summary of the paper's content and methodology. The reviewer lists several strengths, describing the paper as 'very nice', 'well written', and 'easy to read'. They also note that the experiments are 'convincing' and 'interesting'. The only weakness mentioned is the limited novelty, which slightly reduces the overall positive sentiment. The politeness score is 80 (quite polite) due to the respectful and constructive tone throughout. The reviewer uses positive language like 'very nice paper' and 'interesting', and frames the weakness in a neutral, non-confrontational manner. The review maintains a professional and courteous tone, offering balanced feedback without harsh criticism."", ""The sentiment score is -50 because the review expresses several concerns about the proposed framework, indicating a somewhat negative sentiment. The reviewer points out issues with the design and suggests improvements, which shows that they are not fully satisfied with the current approach. However, it's not entirely negative as they acknowledge the paper's attempt to combine global and local stability prediction.\n\nThe politeness score is 20 because the language used is professional and constructive. The reviewer presents their concerns in a respectful manner, using phrases like 'There are several concerns' and 'A better design could be' instead of using harsh or dismissive language. They offer suggestions for improvement, which is a polite way to critique. However, the score is not higher because the review doesn't include any explicitly positive or encouraging statements, maintaining a more neutral tone overall."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths, such as being well-written and easy to follow, and finds the main idea interesting. However, they also express concerns about the limited application domain and the lack of broader implications. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offering both praise and constructive criticism. They use phrases like 'very well-written' and 'quite interesting' to commend aspects of the paper, while politely expressing concerns using phrases like 'my concern is' rather than using harsh or dismissive language.""]"
"['The paper explores unsupervised deep learning model for extractive telegraphic summaries, which extracts text fragments (e.g., fragments of a sentence) as summaries. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.\n\nFirst, in term of methodology (as described in Section 3), the paper has little novelty. There has been intensive study using various deep learning models on summarization. The models described in the paper contain little novelty compared with previous work using autoencoder and LSTM for both extractive and abstractive summarization. \n\nSecond, the paper claims contributions on using deep learning models on telegraphic summarization, but the advantage is not well demonstrated. For example, the advantage of the resulting summary is not compared with state-of-the-art sentence compression models with intrinsic evaluation or (probably better) with extrinsic evaluation. (By the way, it is interesting that the paper argues the advantage of using telegraphic summaries for fictional stories but actually gives an example which looks also very typical in news articles (the “earthquake Tokyo 12 dead” example).)\n\nThird, there has been much work on speech summarization that summarizes with the “telegraphic” style (this is natural, considering speech transcripts are often non-grammatical, and “telegraphic” style summaries focusing on choosing informative fragments actually result in usable summaries.) The author(s) may consider discussing such work and compare the proposed methods to it.\n', 'The authors consider the problem of telegraphic sentence compression: they train a system in an unsupervised fashion to predict which words can be dropped from a sentence without drastic loss of information. To that end, they propose a new auto-encoding type architecture which uses the extracted words as latent code, and, most importantly, a linkage loss which relates a word\'s perplexity given the summary of its left context to its likelihood of being retained. The model itself is sober and well motivated, and the linkage loss is, to the best of my knowledge, original. The authors show that their method outperforms some simple baselines in terms of ROUGE and compression on a small human-annotated test set.\n\nThe paper is generally well written, although the initial presentation of the model could be made a little clearer (it is not obvious from the text that the Decoder takes the text as input -- Figure 2 helps, but comes a couple pages later). However, the authors fail to appropriately justify the choice of their hyper-parameters (e.g. ""The optimum value of r for our experiments was found to be 0.65"", ""the best value of b was found to be 5"", ""The weights λ1, λ2, λ3, and λ4 have been set to 3, 2, 50 and 3 respectively for our experiments"" -> how is ""best"" measured on the validation set, which does not have gold references?). The choice of the specific sparsity constraint (one could as well imagine using a simpe L1 regularization for the Binarization loss) and of \\Chi_i (why not simply use the likelihood?) could also be better motivated.\n\nThe model also relies on a hand-crafted rules (Section 3.3) whose effect needs to be made more evident. What weights are used in practice? How were they chosen (""We observed that..."" needs to be further developed)? The authors claim that ""the quantitative scores are not affected significantly"", but that is presumably only the ROUGE score, what about annotator\'s preferences?\n\nMost importantly, however, the task of telegraphic sentence compression, whose usefulness is not a priori obvious, is barely motivated.  The author refer to ""Malireddy et al. (2018)"" for a justification, but it is important to note that the latter provides a telegraphic summary of a whole document, with a compression factor of 0.37. The claim is that the concatenation of the telegraphic sentence compression can act as a summary of a whole document, but given the fact that compression for individual sentences is closer to 0.69, this is yet to be demonstrated. And even if that were true, it is unclear whether the cognitive load of reading a sequence of telegraphic sentences would be that much lower than that of reading the original text.\n\nThis paper presents some interesting ideas and is well written, but the content is not quite sufficient for publication. In addition to the clarifications and justifications requested above, the authors are encouraged to apply there methods to full lengths documents, which would make for a more substantial contribution. ', 'The authors introduce the problem of telegraphic summarization: given a sentence, we want to reduce its size while retaining its meaning, with no penalty for grammatical mistakes. The main application presented by the author is that of summarizing fictional stories and plays.\n\nThe setting proposed by the author prescribes that the summarized sentence can be obtained by the input sentence by dropping some words. So, for example, the simplest baseline for this problem would consist of simply dropping stop words.\n\nThe approach proposed is basically an auto-encoder, consisting of a 2-step encoder-decoder network: in the first step, the sentence is encoded into a vector which is in turn decoded to a (smooth) indicator vector to mask words in the sentence; in the second step, the masked sentence is encoded into a vector, which is in turn decoded into the output (summarized) sentence. \n\nThe optimization is a tradeoff between recoverability of the input sentence and norm of the indicator vector (how many words are dropped). In order for the network not to learn repetitive masking patterns (eg, drop first half of the sentence, or drop every other word), an additional loss is introduced, that penalizes keeping easily inferable words or dropping hard-to-infer words.\n\nConcerns:\n- the problem doesn\'t seem to be well-motivated. Also, the length of the obtained summarized sentences is ~70% that of the original sentences, which makes the summaries seem not very useful.\n- the proposed complex architecture seems not to justify the goal, especially considering that simply dropping stop words works already quite well. \n- In order for the presented architecture to beat the simple stop-words baseline, an additional loss (L4, linkage loss) with ""retention weights"" which need to be tuned manually (as hyper-parameters) is required. \n- there\'s not enough discussion about the related work by Malireddy et al, which is extremely similar to this paper. A good part of that work overlaps with this paper.\n- comparison with literature about abstractive summarization is completely missing.\n\nMinor comments:\n- Figure 1: Indicator Encoder should be Indicator Decoder.\n- Are negations part of your stop words? From your discussion, you should make sure that ""not"", ""don\'t"", ""doesn\'t"", ... do not belong to your stop word set.\n- How did you optimize the hyper-parameters r (desired compression), the regularization weights, and the retention weights?\n- Were pre-trained word embeddings used as initialization?\n- What\'s the average compression of golden sentences?\n\n']","[-50, -20, -50]","[50, 50, 20]","[""The sentiment score is -50 because while the reviewer acknowledges that the paper is well-structured and easy to follow, they express significant concerns about the paper's novelty, methodology, and lack of comparative analysis. The reviewer states that the submission 'does not have enough content to be accepted to the conference,' which is a clear negative sentiment. However, the criticism is balanced with some positive remarks, preventing the score from being extremely negative.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I think' to soften criticism and provide constructive feedback with specific suggestions for improvement. The reviewer also acknowledges positive aspects of the paper before presenting criticisms. While not overly formal or excessively polite, the language is appropriate and courteous for a peer review, avoiding any rudeness or harsh language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting ideas', 'well written'), they ultimately conclude that 'the content is not quite sufficient for publication' and request significant additional work. The reviewer points out several areas needing improvement or justification, which contributes to the negative sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive feedback. They use phrases like 'The authors are encouraged to...' and 'could be better motivated' rather than harsh criticisms. The tone is professional and aimed at improving the paper rather than dismissing it outright."", ""The sentiment score is -50 because the review expresses several concerns about the paper, including lack of motivation, complexity of the architecture, and missing comparisons with related work. However, it's not entirely negative as it acknowledges some aspects of the work. The politeness score is 20 because the reviewer uses neutral language and frames criticisms as 'concerns' rather than direct attacks. They also provide constructive feedback and minor comments to improve the paper. The tone is professional and not overtly rude, but also not excessively polite.""]"
"['This paper provides a theoretical study of GANs in the following setting:\n\n- The target distribution has a locally smooth density on a compact set [0, 1]^D. It might be supported only on M disjoint components, each of which has a smooth boundary, within that compact set.\n- The latent noise dimension (inputs to the generator) is of the same dimension as the data.\n- An IPM loss (1) is used.\n- The discriminator functions of the IPM and the generator networks are both ReLU networks of at most L layers and at most S total nonzero weights, with all weights having magnitude at most B.\n- The discriminator functions are Lipschitz continuous; this is implied by the previous assumption, but the bound is tighter if we have a tighter constraint here.\n- We obtain the generator which exactly minimizes the IPM between the empirical distributions of m samples from the model and n samples from the target.\n- Maybe: there is some g such that P* is produced by g. It\'s not clear in your statement of Theorem 1 whether this is necessary, or exactly what it means when you say that in Lemma 3, and I haven\'t fully checked what\'s used yet; it would help to explicitly say this is not assumed if it\'s not.\n\nPointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest.\n\nThe full-dimensional support assumption is not ideal, but of course just because the analysis doesn\'t apply to most practical GAN settings doesn\'t mean it\'s not an important step towards one that does.\n\nAlso note that the assumption about the structure of the networks eliminates the MMD GANs that you use in experiments -- which have a kernel function at the top of the critic network -- though it does allow for most GAN variants. Maybe the most interesting algorithm for this setting is the Coulomb GAN (ICLR 2018, https://arxiv.org/abs/1708.08819 ), which uses a neural network critic of the kind you study but estimates a distance which (unlike the Wasserstein and most other GAN objective functions) has good statistical convergence properties (kappa=2 as you mention).\n\nMy biggest concern by far, though, is Proposition 1. You present it as if it\'s a lower bound: establishing that there is some class of distributions for which Theorem 1 shows that GANs can account for local smoothness and standard methods are shown not to be able to. This isn\'t what you do; instead you exhibit a class of distributions for which Theorem 1 shows that GANs can account for local smoothness, and previous analyses of standard methods do not show that they are able to take advantage of it. This is not the same thing at all! Although the previous upper bounds have matching lower bounds, you don\'t demonstrate (and it is likely not the case) that the distribution you show fits into the class of distributions used by the previous lower bounds, and so it remains very possible that other methods are able to take advantage of local smoothness as well as GANs do.\n\nGiven that Proposition 1 is hence a very weak statement, your main contribution in terms of disconnected support becomes ""we can show that GANs can adapt to disconnected supports (under these other assumptions) that has not previously been shown for other methods."" Not only is this a weaker result, but the degree to which you show GANs can take advantage of local smoothness is somewhat limited: at least with the parameter choices in Corollary 1, the smoothness only improves m dependence, not n dependence.\n\nBut in the GAN setting, m is essentially a question of how long you optimize for (and the relative rates between generator and discriminator updates, and various other questions like that out of scope for this paper), not any kind of externally fixed limitation like n. It\'s perhaps not too surprising that you don\'t show that optimizing a WGAN is statistically easier than estimating Wasserstein the distance, but given that estimating Wasserstein is so hard in high dimensions, it\'s a little disappointing. (Maybe it\'s easier, so that kappa is smaller, with locally smooth densities as here, though I don\'t know of any results like that offhand.)\n\n(It\'s interesting, then, that in Corollary 1 the generator complexity depends only on m, basically the amount of optimization you\'re willing to do, while the discriminator complexity depends only on the available number of target samples n.)\n\nAnother concern is that to me, it is not very clear exactly what the statements of the assumptions mean. For example, does Theorem 1 apply only if I search over all generators in the class \\mathcal G = \\Xi(S_g, B_g, L_g)? In particular, does this mean that I have to consider all possible architectures matching those constraints, including allowing for all possible depths up to L_g, and all possible ways of allocating widths of the various interior layers / which weight entries are fixed at zero? It seems so, but this could be more explicit in the statements, even just by replacing ""an existing \\mathcal G"" with ""\\mathcal G = \\Xi(S_g, B_g, L_g)"" in the statement of (e.g.) Theorem 1.\n\nIn your numerical experiments: you don\'t make it at all clear enough that you\'re plotting *different loss functions* for the GANs and the other methods! (You say this, but only in the text where it should definitely also be in the figure caption.) What happens if you plot the L2 difference for all methods, and the MMD/Wasserstein for all methods? Looking at Figure 6, it\'s not obvious that the GAN would do so substantially better. (It does seem to perhaps have the overall scale of the two components better than the other methods, but it doesn\'t look like as enormous a difference as it seems from Figure 5.)\n\nOverall: Theorem 1 is of interest, but the results and especially the comparison to classic methods are not as resounding as they\'re presented here.\n\n(Note that I have not (yet) verified or even really read most of the proofs; I might come back and do that later.)\n\nSmaller points:\n\n- I don\'t think that f-GAN actually fits in the framework (1) as you claim, since it needs to use the conjugate of f on the samples from Q. Also, the original GAN does fit into (1) but not your assumptions about the network form of f, since it needs a log in its activations.\n\n- Another class of generative models where disconnected supports are really important is normalizing flows, which often build ""bridges"" between separated modes because (like your Lemma 2) their generators are constrained to be smooth and invertible. See e.g. Figure 2 of https://arxiv.org/abs/1810.01367 (who propose a new normalizing flow less susceptible to these problems). \n\n- Remark 1 seems so obvious that it need not even be stated, since beta-smooth implies beta\'-smooth for beta\' < beta. It would only be interesting if you could actually take advantage of the smoother components somehow.\n\n- Many papers in your bibliography are cited only as arXiv preprints when they were actually published in various places. For example, the first four papers were published at ICLR 2017, ICML 2017, ICML 2017, and ICLR 2018, respectively.\n\n- There are many small typos and grammatical errors in the draft, including some that would be caught by a spell-checker (""methdos"" on page 8), and an undefined LaTeX reference at the top of page 2. It would benefit from a thorough proofread.\n', 'The paper provides some bounds on the generalization performance of GANs for approximating distributions with discontinuous support. This work relies heavily on the results shown in [1] and [2] on the approximation power of Deep networks for non-smooth functions. The paper is globally well written and the proof seems sound. However, the experiments could be more convincing and the relevance of the result is questionable:\n\n- By choosing the function class F to the be L_1-lipschitz, the resulting error bound loses it’s dependence on the smoothness beta and becomes slightly worse than the classical methods (equation 7 with kappa = 2+2D). Is this an artifact of the proof? if that is the case, it would be good to have a tighter bound: [3] might be a good starting point. \n- Neural networks used in practice are continuous usually, but it seems that all the analysis is all based on the fact that distributions with disjoint support require discontinuous networks. Can similar results be obtained in the more realistic case of continuous networks? Also what network architecture was used in the experiments?\n- Although the bound in eq (5) clearly shows a tradeoff for S_g it only says that S_f should be as small as possible. Of course, if S_f =0 there is no discriminative power, but it’s unclear to me how the expression for S_f in eq (6)  can be obtained from (5) and why it would keep the discriminative power (in what sense?). Again, this tradeoff was discussed in prior work [3], so it might be worth looking into that direction.\n- The discussion right after lemma 1 doesn\'t seem to be true: a distribution might have disjoint support and still have a density (i.e.: absolutely continuous with respect to the Lebesgue measure). It can even have a smooth density.\n- The experiment doesn’t use the same metric to compare GANs method with other methods, so it is unclear how these methods compare. Moreover, figure 6 seems to show that other methods are also able to get the support right (Kernel E). Based on what could we claim that one method is better than the other?\n\nRevision:\nThank you for your response. \n> In fact, our estimator in the theoretical and experimental analysis employs a continuous (ReLU) network. Though discontinuous networks are necessary for our setting (Lemma 2), we show that (continuous) ReLU networks can approximate the discontinuous network effectively (Lemma 3), hence the effectiveness of GANs is proved (Theorem 1).\n\n-That clarifies things, however I find that the discussion after lemma 2 rather missleading, if in the end the result ends up using continuous generator:\n""Because of the discontinuity, generative models with smooth functions, such as an\nadversarial generative model with kernel generators (Sinn & Rawat, 2018), cannot work well with\ndisconnected supports.""  \n\n- It is still unclear to me how the optimal value of S_f is obtained from eq (5). The author points out the work by Zhang+ (2018), but this should be clarified in the current version of the paper: What result in Zhang+(2018) do you use to get this value?\n\n- I find the experiments  not very convincing. I understand that the point is not to show that GANs are better than  other methods but it is important to be make meaningful compairisons (use comparable scores) otherwise there is little scientific value in figure 5 especially.  \n\n- As reviewer 1 mentions, lemma 3 is supposed to be one of  the main theoretical contributions of the paper, however, the proof seems very similar to the one in ([2], appendix B.1). Although the authors mention lemma 1 of [2] in the proof of lemma 3, it seems like the whole section in ([2] appendix B.1) is dedicated to show the very same result.\n\nFor all these reasons I still wouldn\'t recommend accepting this paper. \n\n\n\n\n\n\n[1]: Yarotsky. Error bounds for approximation with deep relu networks.\n[2]: Massaki Imaizumi, Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively.\n[3]: P. Zhang, Q. Liu, D. Zhou, T. Xu, and X. He. On the Discrimination-Generalization Tradeoff in GANs. \n', 'This paper claims three contributions. \n1. We show that GANs perform better than other standard methods of estimating probability\nmeasures when the measure satisfies the disconnected support property.\n\nI prefer to state the contribution as ""We show that deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property."" This claim is essentially from Lemma 3, and it is a property of deep generative networks instead of GANs. If we have another way to train deep generative networks (say, variational auto-encode), we still get the same good approximation error, just linearly dependent of the number of disconnected pieces. The proof of Lemma 3 is mainly from the definition of locally smoothness and the results in Petersen & Voigtlaender 2017. It\'s a nice effort to leverage the result in Petersen & Voigtlaender 2017 to prove approximation properties of deep generative models. The flaw of this part is that the claims of other standard methods (Proposition 1) is very hand-wavy and floppy. The proof of Proposition 1 has lots of typos. For example, the definition of S_1 and S_2. And this sentence ""Then, by the proof of Lemma 1, p2(x) on S2 is a quadratic function with respect to z1 is 1-times differentiable but not twice-differentiable at the boundary ..."" I guess that the authors want to argue that traditional function approximation methods (like Kernel methods, polynomial approximation) all have rate n^{-1/(2+D)} approximation rate in the L^2 norm when the function to approximate has discontinuities... However, the authors fail to make this point clear. Moreover, if we use a mixture model of traditional approximations, the rate will not be deteriorated.  And we will get similar results in Lemma 3. Then, even the claim ""deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property"" is not that grounded. \n\n\n2. We provide a new generalization error bound under a general formulation of GANs by analyzing an approximation error. The result is thus applicable to a wide range of variations of GANs.\n\nThis corresponding to the results in Theorem 1. The authors may want to write the assumption ""all the discriminators are L_1 Lipschitz continuous"" in the Theorem 1 explicitly, because this is an important assumption to get the results. The analysis in i & iii is standard. The analysis is the main contribution of this paper, and is from Lemma 3. \n\n3. Based on the generalization bound, we provide a theoretical guideline for selecting architectures\nof generators and discriminators. \n\nI think the authors mean Equation (6) in this claim. However, this practical guidance is not practical, because (1) both \\beta and \\kappa are unknown in practice, especially \\beta, (2) I can hardly image the number of connections (non-zero weights) will be my model design guidance instead of the model architecture. \n\nIn the numerical experiment, ""We use d_F to evaluate errors by GANs, and a root of the expected squared errors with the L2-norm for the other methods."" With different metrics, is this a fair comparison?\n\nFinally, there are lots of typos in the paper and appendix. ""refers to a probability measures"", ""The property makes a probability measure be complex"", ""with disconnected support"", ""Theorem 1, Corollary 1 and ??"", \n\nAmong others, the original GAN(Goodfellow et al., 2014) is realized if F contains a logarithm of density ratio. The f-GAN (Nowozin et al., 2016) also belongs to this class. Equation (1) and (2) only includes Integral Probability Metrics, not divergences in Goodfellow et al., 2014 or Nowozin et al., 2016.\n\n""smoothness and a dimension of data are sufficient to characterize an optimal convergence of generalization errors."" If we allow mixture models, that\'s a different story.\n\n""A boundary of S is J combination of \x0b-smooth hyper surfaces"" Definition of S_{\\alpha, J} is not clear. The exact definition in appendix is based on the definition of the horizon function, which include ""x_d \\pm h"". Really confused about this \\pm. The original definition in Petersen & Voigtlaender 2017 does not have this \\pm. \n\n“an ordinary density function cannot be defined.” Can an ordinary density function be defined by setting the value outside its support to be 0?\n\n“an empirical norm \\| \\|” What is the empirical norm?\n\nIn Proof outline of Theorem 1, we have the decomposition of i, ii and iii. Should P_0 in iii be P^{*}?\n\n""We compare the numerical performance of GANs and the other methods with toy data with.""\n\nMore typos in the proofs, especially in proof of Lemma 3.\n']","[-30, -60, -40]","[50, 20, 20]","[""The sentiment score is -30 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'Pointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest'), they express significant concerns about key aspects of the work. The reviewer's biggest concern is with Proposition 1, which they argue is much weaker than presented. They also point out limitations in the comparison to classic methods and issues with the numerical experiments. These criticisms outweigh the positive elements, resulting in a somewhat negative overall sentiment. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'My biggest concern' and 'It would benefit from' rather than harsh language. They also acknowledge positive aspects of the work. However, the review is not overly effusive or deferential, maintaining a balanced and objective tone, which prevents it from scoring higher on politeness."", ""The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges that the paper is well-written and the proof seems sound, they express significant concerns about the experiments, the relevance of the results, and several technical aspects of the paper. The reviewer ultimately recommends not accepting the paper, which strongly indicates a negative sentiment. The politeness score is 20 because the reviewer uses generally polite language, starting with some positive comments and using phrases like 'Thank you for your response.' However, the criticism is direct and the tone becomes more critical as the review progresses, which prevents a higher politeness score. The reviewer maintains a professional tone throughout, avoiding rudeness while clearly expressing their concerns."", ""The sentiment score is -40 because the review is predominantly critical, pointing out several flaws and limitations in the paper's claims and methodology. The reviewer challenges the main contributions, highlights issues with proofs and definitions, and notes numerous typos. However, some positive aspects are acknowledged, such as the 'nice effort' in leveraging previous results, which prevents the score from being more negative. The politeness score is 20 because while the reviewer is direct in their criticisms, they use professional language and offer constructive feedback. Phrases like 'I prefer to state' and 'The authors may want to' indicate a polite approach to suggesting improvements. The reviewer also balances criticism with acknowledgment of positive aspects. However, the overall tone is more matter-of-fact than overtly polite, hence the relatively low positive score.""]"
"['Overview:\nThe authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set.  \nInstead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding \'accuracy vs. randomization‘ curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. \nI have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect \'accuracy over randomization‘ curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a ’steep decrease’?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature.  \nOverall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested.\n\n\nDetailed remarks:\n\nGeneral:\nA proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. \n\nYou mention complexity of data and model several times in the paper but never define what you mean by that.\n\n\nDetailed:\nPage 3, last paragraph: Why did you not use bias terms in your model?\n\nPage 4, Assumption. \n- What do you mean by the data being independent? Independent and identically distributed?  \n- ""As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn.“ What do you mean by ""easier to learn""? Better generalization? Better training error? \n- I don’t understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later?\n- What does ""similar scale“ mean? \n\nPage 4, Monotony. \n- You state two assumptions or claims, \'the accuracy curve is strictly monotonically decreasing for increasing randomness‘ and \'we also expect that accuracy drops if the regularization of the model is increased’, and then state that \'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization.‘ Although you didn’t show anything but only state assumptions or claims (which may be reasonable but are not backed up here). \nI actually don’t understand the purpose of this paragraph.\n\n- Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didn’t show yet.\n\nMy main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as \'above the diagonal line‘ and underfitting as ‚below the line‘, which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of ’sharp drops’ and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). \n\nCriterion 2 (b) is not clear.  \n- I neither understand ""As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)"" \nnor do I understand ""accuracy over regularization curve (plotted in log-log space) is constant""?\nDoes that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit?\n\nDue to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \\lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit.\n\n\n\n\n', 'This paper proposed criteria to measure the capacity of a neural network by injecting perturbation (randomized training data). The paper attempted to show that $l_1$-regularization of the kernel weights is a good measure to control the capacity of a network which contradicts the previous finding by Zhang et al (2017) on regularization which claimed that regularization is neither necessary nor by itself sufficient for controlling generalization error.\n \n\nThe proposed method does not require a held out data to check overfitting, which is an interesting direction to explore. The theoretical analysis is seeming to be correct, however, I don’t have strong expertise in theory, therefore, can not assure the correctness.  The experiments, however, are limited. The experiment was done on cifar-10 and the analysis is based on the early stopping, regularization factor and network depth. \n\nThere is only one dataset that was used for the experiments, more dataset should be explored for robust evaluation. \n\nThe assumptions should be clarified and write clearly. For example, “Thus we also expect that accuracy drops if the regularization of the model is increased.”, which accuracy (training?) and what exactly means by increased regularization (value of $\\lambda$)?\n\n', 'This paper is about detecting overfitting of deep neural networks without using a validation set. This is an interesting research problem. However, it is not clear how this paper contributes to solve the problem. My understanding is that this is a preliminary work, put into a paper in haste. There are more research efforts required to turn it into a good paper, theoretically as well empirically.\n\nOne of the key ideas proposed is to obtain multiple instances of neural network models with each one from training on a dataset that is a noisier version of the original dataset; noise is added by permuting lables for a fraction of the original dataset. Then, one can plot training error w.r.t. the level of noise so as to see if the neural model is overfitting. \n\nAuthors present their intuitions on what what patterns for the curves (concave curves) would correspond to overfitting. While the arguments seem convincing, one can not be sure unless there is some solid experimental evaluation across multiple datasets or a good theoretical basis. \n\nHere it is also worth noting that the proposed method is not compared w.r.t. any other baseline methods. Basically, in their empirical evaluation, the authors use the existing techniques for regularization to build a variety of neural network models, and then manually analyze the generalization gap for a given model by looking upon the aforementioned curve on training error w.r.t. noise. \n\nDoes it mean that the method is just for tuning the values of the parameters related to regularization (like l1 regularization constant, number of iterations, etc)? If so, is there an algorithm to do the fine tuning rather than doing manual analysis of the curves with each one representing a configuration of the regularization parameter values. What would be compute complexity of such an algorithm considering the fact that producing a single curve requires training the neural network multiple times.\n']","[-70, 20, -50]","[20, 50, 20]","[""The sentiment score is -70 because the reviewer expresses several significant issues with the paper and concludes that it is 'not fit for publication'. The reviewer criticizes the vagueness of the criteria, lack of thorough testing, and limited applicability. However, it's not entirely negative as the reviewer acknowledges some potentially reasonable assumptions and provides detailed feedback for improvement. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I have several issues' and 'I think' rather than making blunt statements. The reviewer also provides detailed explanations for their concerns, which is a courteous way to give feedback. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score."", 'The sentiment score is slightly positive (20) because the reviewer acknowledges some positive aspects of the paper, such as the interesting direction of not requiring held-out data and the seemingly correct theoretical analysis. However, they also point out limitations in the experiments and the need for clarification, which tempers the overall positivity. The politeness score is moderately positive (50) as the reviewer uses neutral and professional language throughout, avoiding harsh criticism and instead offering constructive feedback. They acknowledge their own limitations in expertise regarding the theoretical aspects, which shows humility. The reviewer provides specific suggestions for improvement without using confrontational or rude language.', ""The sentiment score is -50 because the reviewer expresses significant concerns about the paper's contribution and completeness, describing it as 'preliminary work, put into a paper in haste.' They also point out several limitations and areas needing improvement. However, they do acknowledge that it's an 'interesting research problem,' which prevents the score from being even lower. The politeness score is 20 because the reviewer uses generally respectful language and phrases criticisms as questions or suggestions rather than direct attacks. They use phrases like 'it is not clear' and 'one can not be sure' instead of more confrontational language. The reviewer also acknowledges the potential of the work, which adds to the politeness. However, the overall critical nature of the review prevents the politeness score from being higher.""]"
"[""This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario.\n\nPros:\n-Paper was easy to follow\n-Using orthogonal encodings to decorrelate gradients is an interesting idea\n-Benchmark results appear promising compared to prior works\n\nCons:\n-This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme?\n-Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? \n-Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations.\n-It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet\n\nOverall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels."", 'This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. The reasoning is as follows:\n\n1. different models that share the same final softmax layer will have highly correlated gradients in this final layer\n2. this correlation can be carried all the way back to the input pertubations\n3. the use of a multi-way encoding results in a weaker correlation in gradients between models\n\nI found (2) to be a surprising assumption, but it does seem to be supported by the experiments. These show a lower correlation in input gradients between models when using the proposed RO encoding. They also show an increased resiliance to attack in a number of different settings. \n\nOverall, the results seem to be impressive. However, I think the paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer. The correlation numbers in Table 2 are unexpected and interesting. I would like to see a deeper investigation of these correlations.\n\nI am not familiar with the broader literature in this area, so giving myself low confidence.\n', 'Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. Authors supply extensive experimental evidence to support their method. Those experiments shows significant amount of gains compared to baselines. Although proposed method is neat, I believe it has room to be improved. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? One may use evolutionary computing to empirically analyse such a encoding or one may come up an existence/non-existence proof (I am not expert in the field however I guess ecoc field should have investigates similar problems) of such encoding. ', ""This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance of the classifier output from the encoding of the correct class. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient. Hence I consider it below the ICLR bar.\n\nI find the approach weakly motivated. The argument in Figure 1 is very hand-wavy with no clear experimental or theoretical support. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture.\nMoreover, the approach is not fundamentally different from standard networks with cross-entropy training. One can consider adding an extra layer (with number of neurons equal to the encoding vector dimensions) and keeping the weights of these neurons fixed (the output weights are essentially the encoding dictionary). Then training with cross-entropy is increasing the inner product with these vectors. This is qualitatively very similar to the proposed approach of this paper. Is there a benefit from explicitly considering the encoding vectors? \n\nMoreover, why is the length of the encoding vectors important from a conceptual point of view? As far as I can tell, this is simply encouraging the output of the network to be large in norm. This could be leading to gradient masking, similar to the phenomena observed for defensive distillation.\n\nI find the proposed approach to watermark evasion interesting. However I consider it orthogonal to the rest of the results so it is hard to consider it as a contribution to the main point of the paper.\n\nFigure 3 is missing white-box evaluation of RO classifiers. Is this on purpose? It is important to understand if the claimed improvement in robustness actually stems from RO rather than mostly from combining it with adversarial training.\n\nThe authors report an increase in white-box adversarial robustness. However, I don't believe that the evaluation of their method is thorough. There are plenty of examples by now where PGD has not been sufficient to evaluate the ground-truth robustness of a model. This can distort the relative robustness of different approaches. Given that the increase from baselines in white-box robustness is relatively small (<10% for most datasets) a much more thorough evaluation is required to conclusively demonstrate the benefit of this method. For instance, applying the SPSA attack from Uesato et al. (2018, https://arxiv.org/abs/1802.05666) or a variant of the CW (https://arxiv.org/abs/1608.04644) attack adapted to the particular method used.\nAs an additional point of concern emphasizing this issue, the authors present the results of Kannan et al. (2018) as state-of-the-art. So far, there is no conclusive evidence about ALP improving the robustness of neural networks beyond adversarial training. The original paper was found to be not as robust as claimed and retracted from NIPS. A similar paper reporting ALP to improve robustness in smaller datasets (CIFAR10) was submitted to ICLR (https://openreview.net/forum?id=Bylj6oC5K7) but was withdrawn after the authors performed additional experiments. The fact that the authors find the approach of Kannan et al. (2018) to offer an increase over the robustness of Madry et al. (2017) thus raises concerns about the reliability of the evaluation.\n\nOther comments:\n-- When the authors perform PGD, what is exactly the loss it is applied on? Is it clear that this is the optimal loss to use when attacking RO classifiers?""]","[20, 50, 60, -60]","[60, 75, 70, 20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as it being easy to follow and having an interesting idea. They also mention promising benchmark results. However, the reviewer expresses several concerns and is not fully convinced by the method, which prevents a higher positive score. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, presenting their concerns as questions or suggestions rather than harsh criticisms. They acknowledge the paper's strengths before discussing potential issues, which is a polite approach. The reviewer also uses phrases like 'I believe' and 'I think' to soften their critiques, maintaining a professional and courteous tone."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's impressive results and interesting findings, while also suggesting areas for improvement. The reviewer expresses surprise at some assumptions but notes they are supported by experiments. The politeness score is 75 (quite polite) due to the reviewer's constructive tone, use of phrases like 'I found' and 'I think,' and acknowledgment of their own limited expertise in the field. The reviewer offers suggestions for improvement without harsh criticism and maintains a respectful tone throughout."", ""The sentiment score is 60 (positive) because the reviewer acknowledges that the paper is well-organized, easy to follow, and provides extensive experimental evidence. They describe the authors' method as 'neat' and note 'significant amount of gains compared to baselines'. However, it's not extremely positive as the reviewer suggests room for improvement and poses additional questions. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively. They frame their suggestions for improvement as questions rather than direct criticisms, which is a polite approach. The phrase 'I am not expert in the field' also shows humility, which contributes to the polite tone."", ""The sentiment score is -60 because the reviewer states that the paper is 'below the ICLR bar' and points out several significant weaknesses, including insufficient motivation and experimental evaluation. However, they do acknowledge some positive aspects like the paper being well-written and the approach being interesting. The politeness score is 20 because the reviewer uses generally professional language and offers constructive criticism. They acknowledge positive aspects before diving into criticisms and use phrases like 'I find' to soften their critiques. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.""]"
"['This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.\n\nI like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.\n\nBut, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and I’m not happy with lack of baselines in the experimental comparison (again, see below).\n\nThe math problems\n1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesn’t depend on r and the right-hand side does.\n2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).\n3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesn’t form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.\n4) The function f() minimized in (5) is not defined (it should be!), but if it doesn’t have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I don’t get how the automatic rank reduction can be done).\n5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I don’t even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).\n6) I don’t get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.\n7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).\n8) Also, I don’t get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).\n9) Finally, I don’t get the proposition 1 nor it’s proof: how can it be that rounding to a fixed r_max won’t change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.\n10) I didn’t get this line: “From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = ∇f(x) and Exp−1 x (y) = −η∇xf(x), and thus Theorem 3 can be derived.” What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?\n11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?\n12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?\n13) What is E in Theorem 3? Expected value w.r.t. something? Since I don’t understand the statement, I was not able to check the proof.\n\nThe experimental problems:\n1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).\n2) The convergence plots are shown only in iteration (not in wall clock time) and it’s not-obvious how much overhead the Riemannian machinery impose.\n3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.\n\nAnd finally some typos / minor concerns:\n1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesn’t allow for Riemannian optimization and can be harder to work with numerically).\n2) It’s very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.\n3) In Def 2 “different function” should probably be “differentiable function”.\n4) How is W_c represented in eq (25), as TT or not? It doesn’t follow the notation of the rest of the paper. How is a_t used?\n5) What is “score” in eq (27)?\n6) Do you include bias parameters into the total number of parameters in figures?\n7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).\n8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).\n\n\nPlease, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns!\n', 'In this paper, the authors proposed a new method to update the weights in RNN by SGD on Riemannian manifold.  Due to the properties of manifold learning, the updated weights in each iteration are contracted with a low-rank structure, such that the number of the parameters of TT can be automatically decreased during the training procedure. By using the new algorithm, the authors modified two types of sophisticated RNNs, i.e., bi-directional GRU/LSTM and Encoder-Decoder RNN. The experimental results validate effectiveness of the proposed method. How to determine the rank of the tensor networks in weight compression problem is indeed an important and urgent task, this paper does not clearly illustrate how RSGD can efficiently solve this problem.\n\n1. Compared to the conventional SGD, not only the convergence rate of the proposed method seems slower (mentioned in the paper,), but also additional computational operations should be done in each iteration like exponential mapping (with multiple QR and SVD). I’m worried about the computational efficiency of this method, but  this paper neither discusss the  computational complexity nor illustrate the results in the experimental section.\n\n2. In proof of proposition 1, I’m confused why the input tensor X should belong to M, and why the eq. (8) holds?\n\n3. In the convergence analysis, I don’t know why the eq.  $Exp^{-1}(y)=-\\eta….$ holds even though the authors claims the it is not hard to find. So that, I cannot find the relationship between Theorem 3 and the proposed method.  Furthermore, can Theorem 3 be used to prove the convergence of the proposed method?\n\n4. Eq. (16) would make no sense because the denominator might be very small. \n\n5. In the experiment, please compare with other existing (tensor decomposition based) compression methods to demonstrate how the proposed method makes sense in this task.\n\nMinior:\n1. By the definition in Oseledets’ paper, the tensor decomposition model used in this paper should be called TT-matrix rather than TT.\n2. 9 ->(9) in Definition 2, and 15->(15) in the proof of Theorem 3.', '\nSummary: \nThe paper proposes to use Riemannian stochastic gradient algorithm for low-rank tensor train learning in deep networks. \n\nComments:\nThe paper is easy to follow. \n\nC1.\nThe novelty of the paper is rather limited, both in terms of the convergence analysis and exploiting the low-rank structure in tensor trains. It misses the important reference [1], where low-rank tensor trains have been already discussed. Section 3 is also not novel to the paper. Consequently, Sections 2 and 3 have to be toned down. \n\nSection 4 is interesting but is not properly written. There is no discussion on how the paper comes about those modifications. It seems that the paper blindly tries to apply the low-rank constraint to the works of Chung et al. (2014) and Luong et al. (2015).  \n\n[1] https://epubs.siam.org/doi/abs/10.1137/15M1010506\nSteinlechner, Michael. ""Riemannian optimization for high-dimensional tensor completion."" SIAM Journal on Scientific Computing 38.5 (2016): S461-S484.\n\nC2.\nThe constraint tt_rank(W) \\leq r in (5) is not a manifold. The equality is needed for the constraint to be a manifold.\n\nC3.\nUse \\langle and \\rangle for inner products. \n']","[-50, -20, -30]","[50, 50, 20]","[""The sentiment score is -50 because while the reviewer expresses initial interest in the topic ('I like the topic chosen by the authors'), they quickly follow with significant criticisms. The review lists numerous mathematical errors, experimental problems, and other concerns, indicating that the paper needs substantial improvement. The overall tone suggests the paper is not ready for publication in its current state.\n\nThe politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'Please, do take this as a constructive criticism' and 'I would be happy to see you resubmitting the paper after fixing the raised concerns!' which demonstrate a polite and encouraging attitude. The reviewer also frames their criticisms as areas for improvement rather than outright dismissals. However, the score is not higher because the review is quite direct in pointing out numerous flaws, which could be perceived as somewhat harsh despite the polite framing."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions and validates its effectiveness, they express several concerns and criticisms. The reviewer points out issues with computational efficiency, unclear proofs, and the need for more comparisons. However, it's not entirely negative as they recognize the importance of the topic and some positive aspects of the work. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They phrase criticisms as questions or concerns rather than direct attacks, and use polite language such as 'I'm confused' or 'I don't know' when pointing out issues. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the polite tone."", ""The sentiment score is -30 because the review is generally critical, pointing out limited novelty and issues with the paper's content. However, it's not entirely negative as it acknowledges that the paper is easy to follow and that Section 4 is interesting. The politeness score is 20 because the language is professional and constructive, offering specific recommendations for improvement. The reviewer uses phrases like 'The paper is easy to follow' and 'Section 4 is interesting' which add a polite tone. However, some critiques are direct without softening language, preventing a higher politeness score.""]"
"['\nThis paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation. To summarize my understanding of the proposed objective, in the bi-modal case, it combines both objectives of TELBO [1] and JMVAE-kl [2] with some hyperparameters to learn the uni-modal encoders. The terms of Eqns 7,8, and 9 are equivalent to TELBO and Eqns 9 and 10 are JMVAE-kl. It would be very beneficial for the readers if you could more clearly contrast your objective with the related work given how similar they are. \n\nGiven these similarities between objectives, its unclear why JMVAE-Zero was chosen over JMVAE-kl as a baseline. Furthermore, the reasoning for the improvement of the ELBO of M^2VAE over the baselines in Section 5.3 is unclear, given the similarities between the objectives. \n\nThe qualitative figures throughout the paper are hard to interpret. By looking at Fig 4., I cannot tell which latent space is best. \n“one can see from Fig. 4 that the most coherent latent space distribution was learned by the proposed M^2VAE” \nWhat is meant by ‘coherent latent space’? \n\nThis paper was hard to follow and there are a number of typos throughout the paper. For instance, the labels within Fig 4 and the caption contradict themselves. If the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.  \n\n[1] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative Models of Visually GroundedImagination. ArXiv e-prints, May 2017.\n[2] M. Suzuki, K. Nakayama, and Y. Matsuo. Improving Bi-directional Generation betweenDifferent Modalities with Variational Autoencoders. ArXiv e-prints, January 2018\n\n', 'The paper proposes a multi-modal VAE with a variational bound derived from chain rule. \n\nPros:\nIt is an interesting and important research direction. \nThe presentation is in general clear. \n\nCons:\n1. The re-visit of JMVAE seems not precise. The JMVAE should bound the joint p(a, b) not log p(a|b)p(b|a).\n2. Due to the potential misunderstanding of JMVAE, the paper uses the JMVAE bound for log p(a|b) + log p(b|a) in equation (5), which seems wrong. \nEquation (4) &(5) itself seems confusing alone. It says L_m = log p(a,b) in (4) then L_m = log p(a|b) + log p(b|a) in (5).\n3. If I am not mistaken the error above, the proposed bound is in fact wrong. \n4. Assume that the method is correct, with a massive amount of beta:s, I doubt the method would be very sensitive to beta tuning. The experiments just presented some examples of different betas. Quantitive evaluation of beta and performance is needed. \n5. To generate multi-modal data, other methods such as VAE-CCA or JMVAE are able to that as well. It is not unique to the proposed method. \n6. The experiments are very toyish. The multi-modal data were generated. The method should be evaluated with a real-world benchmarking multi-modal dataset. ', ""This paper introduces a new VAE model (JMVAE) for multi-modal data with a\nshared latent representation. An method is also introduced to synthetically\ncreated bi-modal datasets with correlated latent representations.\n\nThe writing was a little awkward to follow at times, and I'm still not\nsure what Ι am suppose to take away from the figures plotting the latent\nrepresentation. The evaluation is fairly qualitative and it's difficult to\nunderstand what we achieving from using JMVAE.\n\nI'm not clear what the contribution of this work provides, as there is already\nplenty done on learning multi-modal representations.\n\nOne weakness with this work is all the examples are fairly toy\nproblems. The article motivates the work as combining raw multi-modal\nsensor datasets, but no real tasks are shown.\n""]","[-40, -50, -50]","[20, 20, 0]","[""The sentiment score is -40 because the review is generally critical, pointing out several issues with the paper such as lack of clarity, hard-to-interpret figures, and typos. However, it's not entirely negative as it suggests ways to improve and acknowledges potential contributions. The politeness score is 20 because while the reviewer is direct in their criticism, they use respectful language and offer constructive feedback. Phrases like 'It would be very beneficial' and 'If the clarity and quality of the writing could be improved' indicate a polite tone. The reviewer also asks questions for clarification rather than making outright negative statements."", ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('interesting and important research direction', 'presentation is in general clear'), the majority of the review focuses on significant criticisms and concerns about the paper's methodology, correctness, and experimental rigor. The cons outweigh the pros, indicating a generally negative sentiment.\n\nThe politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language to express criticisms. They begin with positive points and use phrases like 'If I am not mistaken' which shows a degree of politeness. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a mostly neutral, matter-of-fact tone when discussing the paper's shortcomings."", ""The sentiment score is -50 because the review is generally negative, pointing out several weaknesses and expressing skepticism about the paper's contribution. However, it's not entirely negative as it does acknowledge the introduction of a new model and method. The politeness score is 0 (neutral) because the reviewer uses direct language without being overtly polite or rude. They express criticisms straightforwardly (e.g., 'The writing was a little awkward to follow', 'I'm not clear what the contribution of this work provides') without using harsh language or personal attacks, but also without softening their critique with polite phrases.""]"
"['This paper proposes a new meta-learner for few-shot learning that conditions the parameters of the model on the given query image. The authors argue that this allows the model to focus on features particular to the query, thereby facilitating classification. The paper introduces a kernel generator as a meta-learner and report performance on two standard benchmarks, Omniglot and miniImagenet.\n\nSeveral methods propose meta-learners that adapt the learner’s parameters to the task or each class in the task. This paper adapts to the query itself, which may provide other benefits, and provides a useful complement to prior work on parameter adaptation in few-shot classification.\n\nWhile the core idea itself is clearly articulated, the reading is dense and many of the finer points are vaguely presented. This makes the paper hard to read and its contribution unclear. In particular, the meta objective itself is not defined, the second loss function contains an undefined (learnable?) functions whose role is not entirely clear. In the experimental section, the authors mention that they use Prototypical Networks (Snell et al., 2017) on top of their kernel generator. This puts their contribution in a different light, now as an extension of Snell et al., (2017). I’m also unclear about the novelty of kernel generator the authors supposedly introduce. The kernel generator appears identical to that of Han et at. (2018), in which case the contribution is its application to few-shot learning, not the kernel generator itself.\n\nSince the main contribution of this paper is to condition the learner’s parameters on the query, as opposed to the task or the classes in the task, the relevant comparison is with respect to such alternative methods. Several such benchmarks are missing (below), and when considered, the reported results are relatively weak. \n\nFor an up-to-date collection of benchmarks on miniImagenet, see Rusu et. al., (2018, https://arxiv.org/abs/1807.05960).\n\n===\n\n[1] Gidari and Komodakis. Dynamic few-shot visual learning without forgetting. 2018.\n[2] Oreshkin et al.. TADAM: Task dependent adaptive metric for improved\nfew-shot learning. 2018.\n[3] Qiao et al.. Few-shot image recognition by predicting\nparameters from activations. 2017.\n[4] Bauer et al.. Discriminative k-shot learning using probabilistic models. 2017.', 'Summary:\nOne of popular approach to few-shot classification is to learn an embedding function to a common feature space where the similarity between two examples is expected to be well determined. The current work claims that query-dependent feature space (referred to as individualized feature space) gains over the common feature space, in the task of few-shot classification. To this end, the paper employed a technique \'kernel generator\' which has been recently proposed in [Han et al., 2018]. Few-shot classification is done using distance (e.g. Euclidean) in the query-dependent feature space. \nThe paper evaluates this method using Omniglot and miniImagenet.\n\nStrengths:\n- Constructing individualized feature space tailored to each query is a novel idea.\n- The paper shows strong quantitative results.\n\nWeaknesses:\n- The clarity is a big obstacle in this paper. Section 3 contains the main idea on \'kernel generator\' which is the critical technique to map input images to individualized feature spaces. Unfortunately Section 3 is hard to follow. \n- Moreover, the idea of kernel generator is the one used in [Han et al., 2018], so the contribution of this paper is very limited.\n- In a nutshell, the current work can be considered as a mix of matching network and kernel generator. \n\nSpecific comments:\n- Regarding terminology, authors state that ""there are three sets of examples in a few-shot classification task: a training set, a support set, and a testing set. The training set and the support set have disjoint label spaces with each other while the testing set shares the same label space with the support set."" I am very confused with what authors mean by support set. In general, each episode has a support set as well as queries in both meta-training and meta-test phases. Meta-training and meta-test has disjoint label spaces. \n- It is not clear to me what the problem setting is here. Queries in training and test phases have different label spaces. So, I am wondering feature space tailored to queries in the training phase can be well generalized to the test phase. Or you assumes that both cases have the same label space?\n-Fig 2: The kernels and the conv features interact in a node which says “X”, making it seem like we are either pointwise multiplying or taking an outer product. The figure would be clearer if it somehow expressed that the two interact via convolution. (To add to this confusion, the kernels are thin which makes them look like vectors)\n-eq(6): the index i is used to denote two things at once (g_i, c_ij). This notation should be different.\n-eq(6): it says g_i is a fully-connected layer, but P_q^ij is a 3d tensor. Is g_i a 1x1 convolution, or do you flatten P?\n-eq(9): what is H? Does it mean entropy? How is the set K_q a distribution of kernels? How does this loss relate to capturing the intrinsic characteristics of an object? This whole part should be clearer.\n\n', '# Summary\nThis work deals with few-shot learning and classification by means of similarity learning. The authors propose a method for generating a set of convolutional kernels, i.e. a mini-CNN, for a query image given a set of support samples (with samples from the same class and some other classes). Kernels are generated for each query and are adapted to the specific visual content found in the query image, thus a new embedding space is identified. The difficulty of the task is constrained by using a common base CNN for feature extraction, making the task resolution more feasible in the few shot regime. The method is evaluated on standard benchmarks Omniglot and miniImagenet with competitive results. \n\n\n# Paper strengths\n- The paper has a good coverage of related work\n\n- The proposed method is interesting and the results are encouraging\n\n- The authors argue and study the influence of multiple elements over their contribution: number of sub-generators, distance metric, choice of architecture\n\n# Paper weaknesses\n- My main concern with this work is the incremental contribution with respect to the work by Han et al. (2018), ""Face recognition with contrastive convolution"". In that work the authors proposed a convolutional kernel generator for every pair of images to be compared/matched, while here the principle is simplified to re-use the same convolutional kernels for the a query image. The loss functions are nearly identical, both works use a classification loss and a loss ensuring kernels at different images with the same object should be similar. The visualizations of the feature maps are similar as well, though these would have been necessary any way for this type of contribution.\n\n- The architecture of the kernel generator is not clear to the reader. Is it similar with the one from Han et al.?\n\n- The related and relevant work from Gidaris and Komodakis (2018), Dynamic few-shot visual learning without forgetting, is not included as baseline in the evaluation. Their method is superior when using C4 and similar (while still keeping performance levels on previous tasks).\n\n- Given that the evaluation for few-shot classification takes random samples of query and support samples and that we\'re dealing with stochastic models, it\'s common and encouraged to include error-bars/standard deviations in the results to get a better idea on the performances. I encourage the authors to do the same.\n\n- The visualizations from Figure 3 would need some additional clarifications from the authors in the text. It\'s not clear what does the colormap refer to, red is for high activation and blue for low activation (as typical for jet colormap) or the other way around? If blue is highly active, it\'s worrying that most dogs (Q1,Q2,Q3) are active on the white dog in S3. As said, it would be useful to have some comments from the authors in the text to better explain the visualizations\n\n- Minor remarks:\n    + The evaluation protocol from Omniglot should be specified as there are 2 ways of doing it: 1) using characters from different alphabets at test time (easier); 2) using characters from the same alphabet (more difficult)\n    + There are some other works dealing with weight generation or with adaptive embedding that would be worth mentioning: \n        * Y.X. Wang et al., Learning to model the tail, NIPS 2017\n        * A. Veit and S. Belongie, Conditional similarity networks, CVPR 2017\n    \n\n# Conclusion\nThis paper advances an interesting idea for few-shot classification and gets competitive results. As mentioned in the section above, I\'m worried about the incremental contribution on top of the work by Han et al.. In addition results are outperforming state of the art works, while requiring generating kernels and features for each query. My current rating is between Weak Reject and Borderline.']","[-30, -20, -20]","[50, 50, 60]","[""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects of the paper ('core idea itself is clearly articulated', 'provides a useful complement to prior work'), they express several significant concerns. These include the paper being dense and vague in parts, unclear contribution, missing comparisons, and relatively weak results. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They offer constructive feedback and suggestions for improvement rather than outright dismissal. The reviewer also acknowledges the potential merits of the work alongside the criticisms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths ('novel idea', 'strong quantitative results'), they also point out significant weaknesses. The reviewer states that 'clarity is a big obstacle', the contribution is 'very limited', and expresses confusion about terminology and problem setting. These criticisms outweigh the positive aspects. The politeness score is moderately positive (50) because the reviewer uses neutral language and frames criticisms as observations rather than attacks. They use phrases like 'I am very confused' and 'It is not clear to me' instead of more aggressive language. The reviewer also balances criticism with acknowledgment of strengths, which is a polite approach to reviewing."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the incremental contribution and some methodological issues. The overall recommendation is between 'Weak Reject and Borderline', indicating a leaning towards rejection. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and phrases criticisms constructively as suggestions for improvement rather than harsh judgments. They use phrases like 'I encourage the authors' and 'it would be useful', maintaining a professional and courteous tone even while expressing concerns.""]"
"['This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:\n\n1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.\n\n2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).\n\n3. It demonstrates the value of the proposed approach through an extensive experimental setup.\n\nAt the same time, I see two major limitations to the paper:\n\n1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.\n\n2. The authors claim that: ""To our best knowledge, this work is the first to propose\nan unsupervised CLTL framework without depending on any cross-lingual resource""\n\nThis is, unfortunately, not true. I refer the authors to the paper:\n\nDeep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.\n\nIn their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.', 'My main reservation with this paper is the limited novelty. The approach seems to be a rather direct application of a subset of the sluice network architecture in [0]\xa0- which has been available on ArXiV since 2017 - with MUSE pre-trained embeddings. In particular, I don’t think the claim that the authors “propose the first zero-resource multilingual transfer learning model” is necessary - and I think it is way too strong a claim. Training an LSTM on English data with MUSE/vecmap embeddings is pretty standard by now, and this does not require any target language training data or cross-lingual supervision either. See zero-shot scenarios in [1-2], for example.\n\nApart from that, I think the write-up is nice, the approach makes a lot of sense, and results are impressive. I would have liked to see a bit more analysis. In particular, the fact that you learn gate values, makes it easy to analyze/visualize what and how your networks learn to share. \n\nI think there’s a few baselines in between BWE and MAN, e.g., simple adversarial training and adversarial training with GradNorm [3], that would put your results in perspective. Finally, I would like to encourage the authors to run experiments with actual low-resource languages: A literature on cross-lingual transfer experimenting with German, Spanish, and Japanese, could end up being very heavily biased. For tasks with data in more languages, consider, for example, POS tagging [4], morphological analysis [5], or machine translation [6]. \n\n[0] https://arxiv.org/abs/1705.08142\n[1] http://aclweb.org/anthology/P18-1074\t\n[2]\xa0http://aclweb.org/anthology/P18-2063\n[3] https://arxiv.org/abs/1711.02257\n[4] http://universaldependencies.org/\n[5] http://unimorph.org/\n[6]\xa0http://christos-c.com/bible/', 'This paper presents a multilingual NLP model which performs very well on a target language with any leveraging labeled data. The authors evaluated their framework on there different tasks: slot filling, named entity recognition and text classification. Overall, the results look very promising.\n- Strengthens:\n+ The proposed idea is novel.\n+ The results are very good for all three tasks.\n- Weaknesses:\n+ The authors claimed that their model knows what to share. However, they did not provide any evidence proving this hypothesis. Only the experimental results are not enough.\n+ The paper also lacks an analysis to show to some extent what the model learned, e.g. the attention weights or the value of the gate. Is there any correlation between the similarity among languages (source and target) and the attention weights.\n- What are not clear:\n+ It is not clear to me what exactly has been done with the CharCNN embeddings in Section 4.2? How did the authors train the embeddings (only with the source languages or also with the target language)? It seems to me that the proposed model did not work well in this case. ']","[50, -20, 50]","[75, 60, 75]","[""The sentiment score is 50 (slightly positive) because the reviewer begins by highlighting several positive aspects of the paper, including its well-written nature and multiple contributions. However, the reviewer also points out two major limitations, which balances out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The reviewer also uses phrases like 'IMO' (in my opinion) to soften subjective statements. The criticisms are presented constructively and backed with specific examples and references, maintaining a professional and courteous tone."", ""The sentiment score is slightly negative (-20) because the reviewer expresses a 'main reservation' about limited novelty and challenges a key claim of the paper. However, they also note some positives like impressive results and a nice write-up. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive feedback, and encourages the authors to expand their work. They balance criticism with praise and provide specific suggestions for improvement, maintaining a professional and courteous tone."", ""The sentiment score is 50 (slightly positive) because the reviewer starts by highlighting the paper's strengths, noting that the results are 'very promising' and the idea is 'novel'. However, they also point out significant weaknesses and unclear aspects, balancing the positive with critical feedback. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions or questions rather than direct attacks. They acknowledge the paper's strengths before discussing its weaknesses, and use phrases like 'It is not clear to me' instead of more accusatory language. The reviewer maintains a professional and constructive tone throughout the review.""]"
"['This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling\'s original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work.\n\nThe paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline.  The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed.\n\nThe experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper\'s thread.\n\n- Please explain why tuning the HMC algo by maximizing eq 16 should work. I don\'t think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this?\n\n- The authors write ""Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution π as L increases.""\n\nThis is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don\'t care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me.\n\nIn practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> ""we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate"". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)', ""This paper presents an approximate bayesian inference method based on chaining measure preserving transformation with trainable parameters and optimizing for those using an ad-hoc objective based on a lower-bound on the likelihood.\n\nThe paper is clearly written and easy to follow. The proofs seem correct.\n\nIn terms of methods, I still have major questions:\n- The whole premise of the paper is based on chaining transformations that preserve the target density. However, in practice, you use a leapfrog operator without the Metropolis-Hastings step --what happens to the theoretical guarantees in that case? I'm guessing Eq (8), (9) don't hold anymore and neither does Theorem 1.\n- When swapping L for F, could you provide more justifications? You use the argument that p(z|x) ~= q_L so the effect of the entropy term will be negligible. It seems that if they are so similar for large L, why even train the \\phi? It also comes back to my first point that in your experiment, the transformations *do not* preserve the target density. \n- Regarding the use of measure preserving flow, I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. \n\nI think this paper also lacks recent references on training parameters for MCMC algorithms, most notably Song et al. (2017) and Levy et al. (2018). Both of these work seem quite related and should be mentioned and compared to. I would have also liked to see the authors contrast their work with Salimans (2015), especially the HVI part; is the main difference the reverse model?\n\nIn terms of evaluation, the toy distributions show that the method seems to converge to the right target but does not compare to either vanilla HMC, A-NICE-MC or L2HMC --which all guarantee asymptotic convergence. There should probably also be a mention of one of ESS/Auto-correlation/ESS per sec to get a sense of how helpful the method could be.\n\nFor the generative model experiments, I agree with the comments of AnonReviewer3 in that evaluating HMPF-VAE with AIS while evaluating HVI with IWAE is somewhat unfair as the latter can happen to be much looser. I also think a natural baseline to compare to would be Hoffman (2017) or Levy et al. (2018) where after obtaining an approximate posterior sample, these works run an MCMC algorithm before updating the decoder. The algorithms seem to be related (albeit the objectives are slightly different) and should be talked about I think.\n\nReferences:\n\nHoffman, Matt. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo, ICML 2017.\n\nSong, Jiaming et al. A-NICE-MC: Adversarial Training for MCMC, NIPS 2017.\n\nLevy, Daniel et al. Generalizing Hamiltonian Monte Carlo with Neural Networks, ICLR 2018."", 'This paper proposes a simple heuristic for tuning HMC\'s parameters: just optimize the expected log-density of the Lth sample. It seems to work reasonably well on the problems the authors evaluate on.\n\nThis heuristic is arrived at by a somewhat roundabout derivation, which I found interesting (although many of the same ideas are implicit in Salimans et al. (2014; ""MCMC & VI: Bridging the Gap"")). But ultimately this derivation comes to a head at this very heuristic argument:\n\n“…since qL(zL; φ) converges to pθ(z|x) as L increases, we expect the effect of H[qL(zL; φ)] on φ to be small and that most of the similarity of qL(zL; φ) to pθ(z|x) will be captured by the first term in the RHS of (15). Therefore, we propose to tune φ by optimizing the tractable objective given by the first term…”: \n\nI don’t see why this argument applies to the entropy term and not to the log-joint term. In particular, If q_L has really converged to p(z|x), then there’s no point optimizing φ either, right?\n\nHere’s a concrete example of how I could imagine this procedure going wrong: make q(z0) a delta at the latent vector z* that maximizes the log-joint, and set the step size of the Hamiltonian simulation to 0. This will make the entropy term (which is ignored) -∞, maximize the log-joint term, and I think it even makes D^L_{KL}=0.\n\nIt seems like this isn’t what actually happens experimentally, though—perhaps I’m missing something?\n\nRegarding the experiments, a natural baseline would be something akin to the approach of Hoffman (2017; ""Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo”), simply initializing the HMC sampler with a mean-field Gaussian. I would expect this to produce worse results for small numbers of steps, since the variational Gaussian would choose a single mode, but I’m curious how the quantitative metrics would compare.\n\nSome more minor points:\n\n* “Variational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field\nVI with a Gaussian parametric distribuion and amortization.” I disagree with this terminology—DGMs trained with, say, IAF are routinely called VAEs.\n\n* Section 3.1: It might be good to clarify that you’re describing exact Hamiltonian integration, whereas in practice one always uses a discretized numerical integrator. (The leapfrog integrator is reversible and preserves volume, but doesn’t conserve energy, so this does make the results a bit more complicated.)\n']","[-50, -20, 20]","[20, 60, 60]","[""The sentiment score is -50 because the reviewer expresses significant criticisms of the paper, including that it's unnecessarily complicated, lacks clear demonstration of improvement over previous methods, and makes claims that may not be justified. However, they do acknowledge some potential value in the method, preventing a more negative score. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout, use phrases like 'Please explain' and 'Fair enough', and offer constructive feedback. The language is not overtly polite, but it avoids rudeness and maintains respectful academic discourse."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positives ('clearly written', 'easy to follow', 'proofs seem correct'), they raise 'major questions' and point out several limitations and missing comparisons. The overall tone suggests more concerns than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions, and acknowledges positive aspects. They use phrases like 'I think', 'I would have liked to see', and 'could you provide more justifications?' which maintain a polite tone while expressing concerns."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges that the proposed heuristic 'seems to work reasonably well' and finds the derivation 'interesting'. However, they also raise several questions and concerns, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions (e.g., 'I don't see why...', 'It might be good to clarify...'), and acknowledges potential misunderstandings on their part ('perhaps I'm missing something?'). The reviewer also offers constructive suggestions for improvement and additional experiments, which contributes to the polite tone.""]"
"['The paper present interactive parallel exploration (IPE), a reinforcement learning method based on an ensemble of policies and a shared experience pool. Periodically, the highest-return achieving policy is selected, towards which the other policies are updated in a sense of some distance metric. IPE is applicable to any off-policy reinforcement learning algorithm. The experiments demonstrate some improvement over TD3 on four MuJoCo benchmark tasks.\n\nThe method is motivated heuristically, and and it provides some benefits in terms of sample efficiency and lower variance between training trials. However, it is hard to justify the increased algorithmic complexity and additional hyperparameters just based on the presented results. The paper motivates IPE as an add-on that can increase the performance of any off-policy RL algorithm. As such, I would like to see IPE being applied to other algorithms (e.g., SAC or DQN) as a proof of generalizability, and compared to other similar ensemble based algorithms (e.g., bootstrapped DQN).\n\nWhile the improvement in the sample complexity is quite marginal, what I find the most interesting is how IPE-TD3 reduces variance between training trials compared to vanilla TD3. Convergence to bad local optimum can be a big problem, and IPE could help mitigate it. I would suggest including environments where local optima can be a big problem, for example HumanoidStandup, or any sparse reward task. Also the paper does not include ablations, which, given the heuristic nature of the proposed method, seems important.', 'This paper describes a new architecture for parallelizing off-policy reinforcement learning systems with a pool of independent learners trained on identical, but independent instances of the environment with a scheme for periodically synchronizing the the policy knowledge across the pool. The paper provides demonstrations in several continuous control domains.\n\nI think this paper should be rejected because: 1) the approach is not well justified or placed within the large literature on parallel training architectures and population-based training methods, (2) the results are competitive with the best in each domain, but there are many missing details. Since the contribution is entirely support by empirical evidence, these issues need to be clarified. I look forward to the author response, as I will pose several questions below and my final score will carefully take the answers into account.\n\nJustification of decision. There are numerous papers on parallel architectures for training deep RL systems [1,2, 6] and you cited a few, and while not all of them focus on continuous control there are design decisions and insights in those works must be relevant to your efforts. You should make those connections clear in the paper. One line of the paper is not nearly enough. The stated focus of the paper is exploration-exploitation yet there is little to no discussion of other ideas including noisy networks, intrinsic motivation, or count-based exploration methods. The paper is missing a lot of key connections to the literature.\n\nI am certainly not a fan of architectures that assume access to many instances of the environment. In this case that assumption seems worse because of the target application: continuous control domains. These domains are simulations of physical control systems; on a robot the agent receives only one stream of experience and thus these architectures would not work well. Though there is some work on applying these multi-environment architectures to farms of robot arms; the reality of the real-world is that the arms end up being very different due to wear and tear, and engineers must constantly fix the hardware because these multi-environment architectures do not work when the environments are different. We cannot loose sight of the goal here—maximizing these simulation environments is not of interest itself, its a stepping stone—architectures that only work on simulations that afford multiple identical environments but fail in the real world have very limited application. I think this paper needs to motivate why parallel training in this way in these robotics inspired domains is interesting and worthwhile.    \n\nThe main area of concern with this paper is the experiment section. There are several issues/questions I would like the authors to address:\n1) You built on top of TD3, however you just used the parameter settings of TD3 as published and didn’t tune them. This is a problem because it could just be that the existing parameter choices for TD3 were just better for the new approach. You have to take additional effort in this case to ensure your method is actually better than just using TD3. Additional parameter tuning of TD3 is required here.\n2) I think its an odd choice for TD3 to have an infinite buffer, as recent work has show at least for DQN that large buffers can hurt performance [7].  Can you justify this choice beyond “the authors of TD3 did it that way”?\n3) Why is R_eval different for each method?\n4) Why did you not compare to TD3 on the same set of domains as used in the TD3 paper? Why a subset? Why these particular domains?\n5) In 2 of the 4 domains the proposed method ties or is worse than the baselines. In half-cheetah it looks close to significant, and in the ant domain the result is unlikely to be significant because the error-bars overlap and the error-bars of TD3 are wider than the other methods so a simple visual inspection is not enough. There does not seem to be a strong case for the new method here. I may be misunderstanding the results. Help me see the significance. \n6) The paper claims improvement in variance, but this requires additional analysis in the form of an F-test of better.\n7) Why these baselines (e.g., SAC) and not others? Why did you not include D4PG [6]? Soft Q-learning? A population-based training method [3,4,5,8] to name a few?\n\n[1] GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning\n[2] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\n[3] Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\n[4] Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\n[5] Structured Evolution with Compact Architectures for Scalable Policy Optimization\n[6] Distributed Distributional Deterministic Policy Gradients\n[7] A Deeper Look at Experience Replay\n[8] Evolution Strategies as a Scalable Alternative to Reinforcement Learning\n \n\nSmall things that did not impact the score:\n1) references to “search interval” in the abstract are confusing because the reader has not read the paper yet\n2) Description of the method in abstract is too specific\n3) P1 intro, not a topic sentence for what follows\n4) “performs an action to its environment” >> grammar\n5) “One way to parallel learning…” >> grammar\n6) “that the value parameter and” >> grammar\n7) “pitfalls” >> minima \n8) Did you try combining you method with other base off-policy methods? how did it work?\n9) GAE undefined?\n10) “among the baseline and”>>grammar…there are many grammar errors', ""Revision: The authors added many references to prior work to the paper and did some additional experiments that certainly improved the quality. However, the additional results also show that the shared experience buffer doesn't have that much influence and that for the original tasks (the humanoid results in the appendix look more promising but inconclusive) the reloading variant seems to catch up relatively quickly. Reloading and distributed learning seem to lead to the largest gains but those methods already existed. That said, the IPE method does give a clear early boost. It's not clear yet whether the method can also lead to better end results. I improved my score because I think that the idea and the results are worth sharing but I'm still not very convinced of their true impact yet.\n\nThe paper proposes a scheme for training multiple RL agents in parallel using a shared replay buffer and an objective that pulls the policies towards the best performing policy as determined by the last comparison event. The method is combined with the TD3 continuous control learning algorithm and evaluated on Mujoco tasks from OpenAI Gym.\n\nThe experiments in the paper seem correctly executed and it is nice that there are multiple baselines but I'm not convinced that the comparison is very insightful. It is somewhat odd that the architectures for the different methods differ quite a bit sometimes. The experiments are already hard to compare due to the very different natures of the optimization algorithms (distributed or not, asynchronous or not). It would be nice to also see plots of the results as a function of the number of learner steps and wall time if these can be obtained from the already executed experiments. \n\nThe paper doesn’t include many references and fails to mention research about parallel (hyperparameter) optimization methods that seems very related, even if the goal of those methods is not always behavioral exploration. Especially Population Based Training (PBT; Jaderberg et al., 2017) is very similar in spirit in that a population of parallel learners occasionally exchange information to benefit from the findings of the best performing individuals. The method is also similar to knowledge distillation (Hinton et al. 2015), which has also been used to speed up multi-task RL (Teh et al., 2017). It would also be nice to see an ablation of some of the different components of the algorithm. For example, it would be interesting to know how important the following of the best policy is in comparison to the gains that are obtained from simply using a shared replay buffer. \n\nThe paper is easy to follow and seems to describe the methods in enough detail to allow for a replication of the experiments. The terminology is not always precise and I’m a bit confused about whether the distance between policies is measured between their actions or their parameter vectors. Equation 8 suggests the former (as I'm assuming is what is also meant in the paper) but the text often speaks about the search radius in parameter space.\n\nExploration is a big problem in reinforcement learning and while parallelization of environment simulations helps to speed up training, additional computational effort typically provides diminishing returns. Methods for coordinating parallel exploration could have a severe impact. Since many RL setups are already distributed, the novelty of the paper mainly comes from sharing a replay buffer (I haven't seen this before but it seems like such an obvious thing to try that I wouldn't be surprised if it has been done) and the way in which learners are forced to follow the best individual. It is promising that the method provides the largest gains for the environment which seems to be the most challenging but it’s hard to draw conclusions from these results. It would be more insightful to see how the method performs on more challenging tasks where exploration is more important, but I understand that these experiments are computationally demanding. \n\nAll in all, the paper presents a method that is simple while having potential for impact but needs to frame it more in the context of previous work. The empirical evaluation is a bit limited and would be more impressive with some additional tasks or at least benefit from a more thorough analysis of the settings and relative contributions of the shared replay buffer and following of the best policy. \n\nReferences\nJaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... & Fernando, C. (2017). Population based training of neural networks. arXiv preprint arXiv:1711.09846.\nHinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\nTeh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., ... & Pascanu, R. (2017). Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems (pp. 4496-4506).\n""]","[-20, -70, -20]","[50, 20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'demonstrates some improvement', 'provides some benefits'), they express significant reservations. The reviewer suggests that the method's benefits may not justify its complexity and additional hyperparameters, and requests more extensive experiments and comparisons. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive phrasing. They acknowledge the paper's contributions while politely expressing their concerns and recommendations for enhancing the work."", ""The sentiment score is -70 because the reviewer explicitly states 'I think this paper should be rejected' and provides several critical points about the paper's shortcomings. The reviewer expresses concerns about the approach not being well-justified, missing connections to existing literature, and issues with the experimental section. However, it's not entirely negative as the reviewer does mention some positive aspects ('results are competitive') and shows willingness to consider the authors' responses. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I look forward to the author response' and 'Help me see the significance,' which indicate a willingness to engage in constructive dialogue. The reviewer also provides detailed feedback and suggestions for improvement, which is helpful and courteous. However, the directness of some criticisms and the overall recommendation for rejection prevent a higher politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges improvements and potential value in the paper, they express significant reservations about the impact and novelty of the work. The reviewer states they are 'still not very convinced of their true impact yet' and notes that some of the most effective components 'already existed'. However, they do recognize that 'the idea and the results are worth sharing', which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They offer specific suggestions for improvement, acknowledge the authors' efforts ('The authors added many references...'), and use phrases like 'It would be nice to see...' rather than making demands. The reviewer also balances criticism with positive remarks, such as 'The paper is easy to follow' and 'It is promising that...', which contributes to the polite tone.""]"
"['This paper attempts to speed up convergence of deep neural networks by intelligently selecting batches. The experiments show this method works moderately well.\n\nThis paper appears quite similar to the recent work ""Active Bias"" [1].\nThe motivation for the technique and setting appear very similar, while the details of the techniques are different. Unfortunately, this is not mentioned in the related work, or even cited.\n\nWhen introducing a new method, it is important that design choices are principled, have theoretical guidance, or are experimentally verified against similar design choices. Without one of these, the methods become arbitrary and it is unclear what causes better performance. Unfortunately, this paper makes several choices, about an uncertainty function, the probability distribution, the discretization, and the algorithm (when to update) that appear rather arbitrary. For instance, the uncertainty function is a signed standard deviation of the softmax output. While there are a variety of uncertainty functions, such as entropy and margin, a new seemingly arbitrary uncertainty function is introduced.\n\nThe experiments are good but could be designed a bit better. For instance, it is unclear if the gains are because of lower asymptotic error or because of faster convergence. The learning curves are stopped too early, while the test error is still dropping quickly.\n\nIn summary, it is not clear if this paper adds any insight beyond ""Active Bias"".\n\n[1] Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. 2017. Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum.', 'The paper introduces an adaptive importance sampling strategy, as opposed to uniform sampling, for batch normalization. The key idea is to assign higher importance to those correctly classified training samples with relatively smaller soft-max prediction variance, hopefully to push the deep nets to learn faster from uncertain samples near the decision boundary. Experimental results on several benchmark datasets (MNIST, CIFAR-10) and commonly used deep nets (LeNet, ResNet) are reported to show the power of boundary batch selection in improving the overall training efficiency.\n\nThe paper is clearly presented and the numerical results are mostly easy to access. My main concern is about the novelty of technical contribution which is mainly composed by two: 1) a prediction variance based importance sampling strategy for batch selection and 2) an empirical study the show the merits of approach. Concerning the first contribution, the idea of defining boundary samples according to prediction variance looks fairly common, if not superficial, in modern machine learning. The way of defining the sampling probability (see Eq. 4 & 5) follows largely the rank-based method (Loshchilov and Hutter 2016) with slight modifications. The numerical study shows some promise of the proposal on several relatively easy data sets. However, as a practical paper, the numerical results could be much more supportive if more challenging data sets (e.g., ImageNet) are included for evaluation. \n\nPros: \n\n-The method is well motivated and clearly presented. \n- The paper is easy to follow. \n\n\nCons:\n\n-  The overall contribution is incremental with limited novelty.  \n- As a practical paper, the numerical study falls short in evaluation on large-scale data. \n', 'The paper describes a sampling distribution construction over examples from which to draw mini-batches to train multi-classification models. A distance function on examples is described wherein an example\'s current (softmax) label probabilities and correctness are taken into account. The bounded distance function supports quantization of example distances and then subsequent sampling from an exponentially decaying probability mass function defined over the binned examples. Results from experiments implementing the proposed method and some baselines on three image classification datasets are provided.\n\nClearly, any generic improvement to training DNN\'s has the potential for far-reaching impact. I thought the exposition was fairly clear and appreciated how the introductory sections provided an intuitive understanding of e.g., the differences between the proposed method and the method of Loshchilov and Hutter (2016). The relative conceptual simplicity of the proposed method is a clear positive. The experimental methodology and results are my biggest issues with the paper. The experimental evaluation suggests the proposed method was run 3 times, one for each value of the selection pressure parameter. Then, the best run was selected for comparison. This suggests the proposal is not practical. For results, the benefit of the proposed method is only clearly apparent in one of the three experiments (Fashion MNIST). In the MNIST case, the proposed method does not seem to improve upon the online batch method. For CIFAR-10, where a good case for the proposed method could have been made since the architecture is more complex and potentially more difficult to train, the improvement seems slight. Moreover, it isn\'t clear whether a relevant baseline was included (see second question below). Also, at least some discussion of computational cost incurred by the method should have been provided. Even better would be to include results wrt/ wall clock training time.\n\nQuestions/Comments:\n\nWhy not set \\gamma = 1? Having a larger value seems to run counter to making training faster. Technically, to use the proposed method, all of the examples need to be processed only once before the distance-based sampling distribution can be utilized. \n\nDoes the random method of the paper denote uniformly at random from the entire dataset per batch or sequential batches from a pre-shuffled dataset per epoch? The distinction is important as Loshchilov and Hutter (2016) report that the latter method performs better than their online batch method on CIFAR-10.\n\nADA-easy seems to be an irrelevant baseline given the context of the paper.\n\nThe phrase ""learner\'s level"" is used multiple times, but not defined.\n\nThe average is reported in the convergence curves, but shouldn\'t the variance be reported as well?\n\nPerhaps the selection pressure parameter can be annealed as performed in Loshchilov and Hutter (2016)?']","[-50, -20, -20]","[20, 60, 60]","[""The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper such as similarity to previous work, arbitrary design choices, and experiments that could be improved. However, it's not entirely negative as it acknowledges some positive aspects like 'good' experiments. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism, avoiding harsh or rude phrasing. They use phrases like 'appears quite similar' and 'it is unclear' rather than more accusatory language. The reviewer also provides specific suggestions for improvement, which is a polite approach to criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('clearly presented', 'easy to follow'), they express significant concerns about the novelty and scope of the work ('main concern is about the novelty of technical contribution', 'overall contribution is incremental with limited novelty'). The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, balancing criticism with praise ('The paper is clearly presented', 'The method is well motivated'), and frames concerns as constructive feedback rather than harsh criticism. The reviewer also uses polite phrases like 'My main concern is' instead of more direct or negative statements."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Clearly, any generic improvement to training DNN's has the potential for far-reaching impact', 'The relative conceptual simplicity of the proposed method is a clear positive'), they express significant concerns about the experimental methodology and results ('The experimental methodology and results are my biggest issues with the paper'). The reviewer points out several limitations and areas where the proposed method doesn't show clear improvements. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms as questions or suggestions rather than harsh statements. They use phrases like 'I thought', 'I appreciated', and 'Perhaps' which maintain a collegial tone. The reviewer also provides specific, constructive feedback for improvement, which is a polite way to address shortcomings.""]"
"['This paper proposes a measure (“effective path”) of which units and weights were most important for classification of a particular input or input class. Using the effective path, the authors analyze the overlap between paths across classes for CNNs and between adversarially modified and unmodified images. Finally, the paper proposes an adversarial defense method based on effective path which detects adversarially manipulated images with high accuracy and generality to a variety of settings. \n\nOverall, this paper is interesting and provides several novel observations. The clarity of the exposition is generally good, but can be improved in several places (mentioned below). As for significance, effective path is likely to inform future analyses of neural networks, and the adversarial defense may prove impactful, though ultimately, its impact will depend on if and when the defense is broken. \n\nHowever, there are several important controls missing from the analysis, several claims which are unsubstantiated, and experimental details are lacking in a few places. As such, in its current form, I can only weakly recommend this paper for acceptance. If in the revision the controls requested below are included, additional evidence is provided for the unsubstantiated claims (or if those claims are toned down), and exposition of missing experimental details is included, I’d be happy to raise my score. \n\nMajor points:\n\n1) While the observation regarding path specialization is very interesting, one cannot gauge whether or not the degree of overlap observed between class-specific paths signals path specialization or simply high input-to-input path variance (which is similar both within and across classes). In order to distinguish between these possibilities, a measure of intra-class path similarity is necessary. In addition, an experiment similar to that in Figure 2 with CIFAR-10 would be quite helpful in evaluating whether this phenomenon exists in more natural datasets (the ImageNet results are difficult to interpret due to the large number of classes).\n\n2) Several claims in the path specialization section are unsubstantiated. \n\n2a) In particular, the claim that ‘1’ has the highest degree of specialization “because of its unique shape” is made without evidence as is the similarity between ‘5’ and ‘8’. ‘6’ is also similar to ‘8’ and yet does not show the same similarity in the path specialization. These differences may very well simply be due to chance.\n\n2b) The claim that the path specialization in ImageNet matches the class hierarchy is made only based on the rough non-linearity of Figure 3. Please either measure the overlap within and across class categories or soften this claim.\n\n3) The similarity analysis for adversarial images is also very interesting, but a comparison between unmodified and randomly perturbed images with matched norms to the adversarially perturbed images is necessary to establish whether this effect is due to noise generally or adversarial noise.\nIt’s unclear how the effective path is calculated when negative weights are involved. Further exposition of this aspect would be helpful.\n\nMinor points/typos: \n\n1) There are several places where confusing concepts are introduced in one paragraph but explained several paragraphs later. In particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though they’re introduced on page 3.\n\n2) Typos: \n\n2a) Section 1, fourth paragraph: “...and adversarial images, we uncover...” should be “...and adversarial images, and we uncover...”\n\n2b) Section 1, fourth paragraph: “...by small perturbation, the network…” should be “...by small perturbations, the network…”\n\n2c) Section 2, first paragraph: “...the black-boxed neural…” should be “...the black-box neural…”\n\n2d) Section 2, first paragraph: “In the high level…” should be “At a high level…”\n\n2e) Section 4, first paragraph: “...as it does no modify…” should be “...as it does not modify…”\n\n2f) Title, should be ""Neural Network""? \n', 'The authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.\nOverall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.\nThe experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesn’t really show how the proposed work is adding value to the field.\nIt lacks the experimental comparison with previous methods but only include discussion in texts.\nThis paper could turn out to be a stronger paper but it is not ready yet.\n\nBelow are some more detailed comments.\n1) The authors motivate by stating that the vulnerability of NN to input perturbations is due to the lack of interpretability (Section Introduction & Abstract). I can understand that we want more interpretability, and we want less vulnerability, but I can’t agree that vulnerability is caused by lack of interpretability. Also, the authors are trying to accomplish both tasks, interpretability and adversary detection, by showing data analysis of how the findings coincide with prior knowledge (eg. Class of digit 1 is the most different from other classes in MNIST task), and by showing detecting adversary images. However, neither has valid quantitative comparison with previous work; actually for the interpretability topic, the authors didn’t really provide a tool or a generalizable method. Thus, I would suggest to choose one of the two topics (ie. adversarial image detection) and focus on it by adding thorough comparison with other methods; in the discussion and result section, include the interpretability analysis to justify why the proposed adversary detection method is behaving in certain ways.\n\n2) One topic that is missing from the paper is the time complexity of the proposed method. At a naïve estimate, it would require tracking and finding the minimum set of effective neurons with threshold \\theta and thus per instance, at least O(m log m) is required at prediction phase, where m is the number of features; for n instances, the asymptotic complexity is O(nm log m) How does it compare to the other adversary detection methods?\n\n3) Page 3 mentions that the work for critical routing path (Wang et al. 2018) requires re-training for every image; this statement is not really true without more context. Also authors discuss this work again very briefly in Page 8 due to the high similarity in methods and motivation with the proposed method, but the authors don’t show any quantitative comparison. After all, both methods are trying to identify neurons that contribute the most to the prediction, some more concrete comparison would be nice.\n\n4) Page 3 mentions that the derived overall effective path is highly sparse compared to the original network and the effective path density for five trained models ranges from 13% to 42% which conforms with the “80%” claim from another paper. Together with the other similar statements, it would be really nice to note what \\theta is used for such statements; how does such statement change with different \\theta. Also some discussion would be nice about what such sparsity implies. Specifically, does the sparsity suggest the opportunity for feature selection, or does it suggest a way for detecting overfitting?\n\n5) Page 5 shows the path similarity between the normal and the adversary examples; from the figure 5a and 5b, we can see the on the first layer, the mean deviate between normal and others but why the last layer they almost reach to the same point? It seems it is the middle layer that distinguish the normal from the adversary examples the most. Some more discussion would be good.\n\n6) Some justification of why \\theta=0.5 is chosen would be good on Page 6.\n\n7) On Page 7, the authors are discussing the performance of the proposed method, however, there is no really comparison with other methods. But rather, the authors stated “better accuracy”, “AUC… is better…” by comparing different evaluation scenarios. I don’t find such discussion helpful in showing the contribution of the proposed method. Also in the parameter sensitivity, it would be nice to add the analysis for the effective path density and see if it still conforms with the “80%” claim with different \\theta.\n\n8) Page 1, need to add citations for the statement “… and even outperformed human beings.”\n\n9) Minor issue: Page 1 “such computer vision…” should be “such as computer vision…”.\n', 'This paper proposes a method for the detection of adversarial examples based on identification of critical paths (called ""effective paths"") in DNN classifiers. Borrowing from the analysis of execution paths of control-flow programs, the authors use back-propagation from the neuron associated from the final class decision to identify a minimal subset of input synapses accounting for more than a threshold proportion (""theta"") of the total input weight. The identification process is then recursively applied at the preceding layer for those neurons associated with the selected minimal subset of synapses, forming a tree of synapses (the ""effective path""). The authors then propose to compare the effective paths (actually, unions of paths) of different examples using simple structural dissimilarity measures, which they extend to allow comparison to a typical (aggregated) path for multiple examples drawn from a common class.\n\nIn their experimentation with their measure, they noted that examples generated by a number of adversarial attacks tend to be less similar to their first-ranked estimated class than normal examples are to their own first-ranked classes. Similarly, they note that these same adversarial attacks tend to be *more* similar to their second-ranked classes than normal examples are to their own second-ranked classes (as the authors point out, this is likely due to the increased likelihood of the second-ranked class of adversarial examples being the true class for the original example from which it was perturbed). The authors then propose the difference between these two similarities (that is, first-ranked dissimilarity minus second-ranked dissimilarity) as a characterization of adversarial examples.\n\nThe idea of using critical paths in the DNN to detect adversarial examples is interesting, and the authors deserve credit for showing that these critical paths (as defined in this paper) do show differences from those of normal examples. However, the originality of the approach is undercut by the recent work of Wang et al. (CVPR, 2018), which the authors acknowledge only in the discussion of experimental results. Although the details are different as to how critical paths are identified, and how adversarial examples can be detected using them, the strategies are definitely related - a more detailed explanation of this should have been given in the introduction of the paper. More troubling is the fact that a head-to-head experimental comparison is not provided, neither with Wang et al. nor with other state of the art detectors, other than a qualitative assessment of the capabilities of some detectors in Table 1. Note that even this qualitative discussion does not include some of the recent detection approaches, such as BPDA (Athalye et al., ICML 2018) or LID (Ma et al., ICLR 2018).\n\nThe question of how best to define critical paths and their similarities is still very much open - the authors\' approach is rather simplistic and straightforward. For example, is their similarity measure biased towards the contributions from early layers? Can a layer-by-layer weighting of contributions improve the performance?\n\nThe authors do not always interpret their own experimental results correctly. For example, their results in Figures 7i and 7j don\'t really support their conclusion that performance ""remains almost unchanged"" when theta is in the range 0.5 - 1.0. Also, Figure 4 does not show that their effective path similarity is not *directly* ""a great metric to distinguish between normal and adversarial"" examples, because a large proportion of adversarial examples have scores that fall in the typical range for normal examples (however, there are differences in tendency which can be exploited, as the authors do show).\n\nThe organization of the paper is in some need of improvement. For example, the discussion of densities of ""effective paths"" (Section 2) comes well before the details of the choice of threshold value theta used to generate them (Section 4.1).\n\nTo summarize:\n\nPros:\n* A good case is made for the use of critical paths as a way of differentiating adversarial examples from normal examples.\n* The reported improvement in similarity of adversarial examples with respect to their second-ranked classes is particularly intriguing.\n* The paper is generally well written and easy to follow.\n\nCons:\n* The experimental treatment is insufficient; in particular, a more carefully considered experimental justification is needed with respect to other detection strategies.\n* The question of how best to define critical paths and their similarities is still very much open.\n* The authors do not always interpret their own experimental results correctly.\n* The organization of the paper is in some need of improvement.']","[20, -40, -20]","[70, 50, 50]","[""The sentiment score is 20 (slightly positive) because the reviewer finds the paper 'interesting' and 'novel', but also points out several important issues and can only 'weakly recommend' it in its current form. The overall tone is constructive, suggesting improvements that could lead to a higher recommendation. The politeness score is 70 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'I'd be happy to raise my score' and the use of 'please' in requests for changes indicate a courteous approach. The reviewer also balances critique with praise, maintaining a professional and considerate tone throughout the review."", ""The sentiment score is -40 because the review is generally critical, stating the paper 'is not ready yet' and 'lacks experimental comparison'. However, it's not entirely negative as it acknowledges the paper is 'well written' and 'could turn out to be a stronger paper'. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'I would suggest' and 'it would be nice to', which are polite ways of giving feedback. The reviewer also acknowledges positive aspects before critiquing, which is a polite approach. However, the score isn't higher as the review is still quite direct in its criticisms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting idea', 'good case made', 'well written'), there are significant criticisms. These include insufficient experimental treatment, lack of comparison with state-of-the-art methods, and issues with result interpretation and paper organization. The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the authors' contributions, and frames criticisms constructively. They use phrases like 'the authors deserve credit' and provide a balanced view with both pros and cons. However, the score is not higher due to the directness of some criticisms.""]"
"['This paper uses pruning and model distillation iteratively to reduce the model sizes. The pruning step is based on Molchanov et al. (2017). This is followed by a hints step that minimizes the feature map difference between student and teacher. Finally, a reconstruction step is used to restore original weights. Results are shown on CIFAR-10, Imagenet and COCO datasets for classification and pose estimation tasks where PWH reduces model costs with a small loss in accuracy.\n\nThe paper is interesting that it proposes a unique combination of existing methods iteratively to improve the compression rates in modern CNNs. However, given that these methods already exist, the novelty aspect of this paper is low. Furthermore, it is also hard to rebut these methods, since they have been published and extensively evaluated in the respective papers. Nevertheless, it is interesting to note that both methods assist one another in your set of experiments.\n\nIn order to improve the paper and for the reviewers to judge the papers more favorably, the authors can compute the time for reconstruction/hints and demonstrate that it is clearly superior to fine-tuning/re-training and offers a clear advantage. This should be emphasized and articulated in related work and introduction and will help the paper.\n\nHow did you arrive at the magic number of pruning just 256 channels in every step?', 'In this paper, the authors propose a pruning-with-hints framework for model acceleration.  Via performing pruning and hints iteratively, one can leverage the complementary characteristics of these two approaches to boost performance. \n\nHere are the comments:\n1 The framework seems to be technically sound. However, the novelty is limited. Most techniques (e.g., pruning, hints) have been widely investigated in the literature. Reconstruction can be treated as another type of hints. Furthermore, the integration of these strategies is standard.\n2 In the experiment, the pruning rate is relatively low for larger models and data sets.  Hence, the effectiveness should be further investigated.  Additionally, please compare with the state-of-the-art approaches such as light-weight design (e.g., MobileNet, ShuffleNet).  This can further enhance the motivation of choosing the proposed framework for real-life applications. \n', 'This paper proposes a new framework which combines pruning and model distillation techniques for model acceleration. Though the ``pruning” (Molchanov et al. (2017)) and hint components already exists, the authors claim to be the first to combine them, and experimentally show the benefit of jointly and iteratively applying the two techniques. The authors show better performance of their new framework over baseline, pruning only method and hint only method on a few standard Vision data set.\n\nThe motivation is clearly conveyed in the paper. As a framework of combining two existing techniques, I expect the framework can stably improve its two components without too much additional time cost. I have some small questions.\n\n--What is the ``additional cost” of your proposed framework. For example, how many iterations do you typically use. For each data set, what time delta you spent to get the performance improvement comparing to pruning only or hint only models.\n--In your iterative algorithm (pseudo code in appendix), the teacher model is only used in the very beginning and final step, though richest information is hidden in the original teacher model. In the intermediate steps, you are fine tuning iteratively without accessing the original teacher model.\n--In your reconstruction step, you said due to the randomness, you do not always use the learned new W. How much your algorithm benefit from this selection strategy?\n']","[20, -20, 50]","[60, 50, 75]","[""The sentiment score is slightly positive (20) because the reviewer finds the paper 'interesting' and acknowledges its unique combination of methods, despite noting low novelty. They offer constructive suggestions for improvement, indicating a generally positive but cautious attitude. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while offering criticism in a constructive manner. They use phrases like 'interesting to note' and offer specific suggestions for improvement, maintaining a professional and courteous tone. The reviewer also asks a question at the end, inviting further explanation rather than dismissing the approach outright."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the framework is technically sound, they express concerns about limited novelty and the need for further investigation of effectiveness. The reviewer also suggests additional comparisons, indicating that the current work may be lacking in some areas. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive feedback without harsh criticism. They use phrases like 'seems to be' and 'please compare' which maintain a respectful tone. The reviewer also balances critique with acknowledgment of the work's technical soundness, contributing to the overall polite tone."", ""The sentiment score is 50 (moderately positive) because the reviewer acknowledges the paper's novel contribution in combining existing techniques and shows interest in the results. The reviewer states that the motivation is clearly conveyed and expects the framework to improve upon its components. However, the score is not higher as the reviewer also raises some questions and concerns. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledges the authors' claims, and frames their questions as 'small questions' rather than criticisms. The reviewer also uses phrases like 'I expect' and 'For example,' which maintain a collegial tone. The questions are presented as requests for clarification rather than demands, further contributing to the polite tone.""]"
"['The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The cost function of the minimax GAN optimization problem is changed accordingly. Experimental results suggest that this approach leads to improved results, both visually and w.r.t. FID metric.\n\nImproving on the well known problem of mode collapse in GAN training scenarios is without a doubt an important endeavor. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission.\n\nWith respect to the proposed method, I am not completely clear on how it can increase sample variety, if it does. In understand the arguments brought forward in Appendix B, but:\nConsider a minimax game involving one generator G and one discriminator D, where each batch of generated samples from G(Z), is split up into two parts, A and B, via selection without replacement using uniform sampling. Z is a matrix of noise inputs, where each column corresponds to one item of the batch. D is now tasked to differentiate whether a sample came from A or B. It seems intuitive to say that in this case, D can neither win, nor provide any useful signal to G, since the sets A and B were split randomly, and there is no influence on G during training. The variety of samples in A as well as in B will be identical to the variety in the set (A and B).\n\nYet this random microbatch splitting is what seems to be happening here, if I understood Section 3 correctly; just with an ensemble of discriminators, and not just with one.\nWhile it is thus not completely clear to me *why* the proposed additional term seems to bring increased variety, experiments strongly suggest that it does.\nAs described in Section 3.1, choice of the weighting parameter alpha seems crucial, and additionally alpha needs to depend on the iteration index. Different schedules are demonstrated, but optimality of either is not guaranteed. This makes the actual influence of the additional loss term even harder to judge and evaluate.\n\nSection 4.1 seems to confirm the increase in variety via the self-defined ""Intra FID"" measure. I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5.\nIn Table 1, both min and mean FID are given over 50k iterations. Instead of reporting the minimum, it might be fairer to compare FIDs after a fixed number of iterations (i.e. 50k in this case).\n\nThe method comparison in Section 5 is generally appreciated, but I think some of its flaws are:\n- Datasets, including ImageNet, are all downsampled to 32x32 pixels. We have seen generators in recent work that produce interesting high-resolution output in even megapixel size; the tiny size seems like a pessimization of overall approaches.\n- The proposed method is compared to other methods using only 2 discriminators, although Section 4.3 suggest a larger number is better.\n- MicroGAN does not compare favorably to many of the compared to methods in Table 2. This may not necessarily by a flaw of the MicroGAN contributions, but is rather a problem of an apples-to-oranges comparison, as the authors readily admit (""the use of more powerful architectures [...] plays a big role""). I question the value of such a comparison, if not only the method differ, but also implementation details such as network architectures.\n\nOverall the submission is quite interesting, but not without the above-mentioned flaws.', 'The paper proposes a multi-discriminator based extension to GAN training. Specifically, it proposes to split a minibatch of samples into further smaller minibatches (microbatches) and train different discriminators on each. The authors state that ""since each D is trained with different fake and real samples, we encourage them to focus on different data properties"". This seems incorrect. Random samples drawn from a distribution do not change various statistics of the distribution in expectation (such as means). It only introduces differences due to noise in the sampling process and this noise is not correlated across training iterations or consistent within a microbatch. Without a meaningful/consistent change in the distributions between microbatches there should be no different data properties for the various discriminators to focus in on. As a consequence, a discriminator which evaluates samples independently should not be able to perform this task. Using batchnorm in the discriminator introduces some batch level interactions via the mean and variance statistics but this appears to be serendipity that the authors do not highlight explicitly. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? What happens if you remove batchnorm so that the samples are processed independently? Is there an additional detail to your paper/method that I missed or misunderstood that addresses the issue raised here? Can you better articulate your intuition on how randomly assigning data to different microbatches results in different data properties? Does the discriminator make use of batch level statistics in some more advanced way beyond just batchnorm such as the minibatch features in Improved GAN (Salimans 2016)? \n\nA baseline of always having alpha set to zero in order to tease out the potential improvements of the proposed approach from the potential benefits of having multiple discriminators would increase confidence in the approach.\n\nPros: \n+ The proposed IntraFID is interesting but is missing two baselines (IntraFID for two batches of real data and IntraFID for two batches of a baseline GAN without the proposed technique) which would help calibrate and contextualize the newly introduced metric.\n\nCons:\n- The paper seems to have a flaw which calls into question whether it is well motivated (see main text).\n- The paper does not have any direct/controlled comparisons with other methods that utilize multiple discriminators or batch based discrimination.\n- The paper mis-states the Inception Score of Improved GAN. The best result from the Improved GAN paper achieves 6.86 in an unlabeled setting (see -L+HA in the ablation study in Table 3) but is listed as 4.36. \n- The paper misses relevant literature - CatGAN (Springenberg 2015) which trains a discriminator to minimize entropy over a categorical distribution assigned to the generator\'s samples while the generator is trained to maximize entropy of discriminator in this space.  It also misses PACGan (Lin 2017) which also augments a discriminator to look at multiple samples to improve diversity.', 'The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different ""micro-batch"" of examples. The diversity is controlled by a hyperparameter \\alpha, and some strategies to set \\alpha are studied empirically. \n\nI don\'t fully understand how the proposed objective can promote diversity. Under what situation will the discriminator fail to discriminate different fake samples? \n\nMany questions are unanswered in the paper. \n1. What is the equilibrium of the proposed objective? Does the proposed objective has an equilibrium?\n2. How to choose the number of discriminators?\n3. How does the proposed approach compare to other papers trying to prevent mode collapse?\n\nThe experiment Table 2 and 3 doesn\'t look convincing to me. The inception score and FID of the proposed approach clearly lack behind state-of-the-art approaches. I don\'t see any evidence showing that FID of microGAN can be better than, for example, WGAN.']","[-20, -60, -50]","[60, 20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the topic and the interesting aspects of the submission, they express several concerns and criticisms. They question the method's effectiveness, point out flaws in the comparison, and express skepticism about some of the results. However, the tone is not entirely negative, as they do recognize some positive aspects.\n\nThe politeness score is moderately positive (60) because the reviewer uses respectful and professional language throughout. They phrase criticisms constructively, using phrases like 'I would have liked to see' and 'I am not completely clear on' rather than making blunt negative statements. The reviewer also acknowledges the value of the work, calling it 'quite interesting' despite the flaws. The language is consistently polite and academic, avoiding any harsh or rude expressions."", ""The sentiment score is -60 because the review is predominantly critical. The reviewer points out several flaws in the paper's methodology and reasoning, questions the fundamental premise of the work, and highlights missing comparisons and baselines. While there are some positive aspects mentioned (e.g., the 'interesting' IntraFID), these are outweighed by the criticisms. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They ask questions for clarification rather than making outright dismissals, and use phrases like 'seems incorrect' instead of more harsh language. The reviewer also acknowledges potential misunderstandings on their part, which adds to the politeness. However, the overall critical nature of the review prevents a higher politeness score."", ""The sentiment score is -50 because the reviewer expresses significant doubts and criticisms about the paper. They state that they don't fully understand the proposed method, raise several unanswered questions, and find the experimental results unconvincing. However, it's not entirely negative as they do acknowledge the authors' proposal and efforts. The politeness score is 20 because while the reviewer is direct in their criticisms, they phrase their concerns as questions and observations rather than harsh statements. They use phrases like 'I don't fully understand' and 'doesn't look convincing to me' which soften the critique. The language is professional and not personally attacking, but also not overtly polite or complimentary.""]"
"['Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\\alpha$ weights (see page 4).These weights give ""how much"" this word belongs to a certain topic. For every topic we compute the $\\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   \n\nQuestions and remarks:\n     1) How sensitive is the method to a change in the number of topics (k)?\n    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful\n \nPoints in favor: \n   1) Good results and thorough tests \n    2) Paper is easy to read and follow \n\nPoints against:\nA very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. \n\n\nThe novelty of the paper is not enough to justify its acceptance at the conference.', '\nPros:\nThe paper shows that we could have a better document/sentence embedding by partitioning the word embedding space based on a topic model and summing the embedding within each partition. The writing and presentation of the paper are clear. The method is simple, intuitive, and the experiments show that this type of method seems to achieve state-of-the-art results on predicting semantic similarity between sentences, especially for longer sentences. \n\nCons:\nThe main concern is the novelty of this work. The method is very similar to SCDV (Mekala et al., 2017). The high-level flow figure in appendix H is nearly identical as the Figure 1 and 2 in Mekala et al., 2017. The main difference seems to be that this paper advocates K-SVD (extensively studies in Arora et al. 2016) as their topic model and SCDV (Mekala et al., 2017) uses GMM. \nHowever, in the semantic similarity experiments (STS12-16 and Twitter15), the results actually use GMM. So I suppose the results tell us that we can achieve state-of-the-art performances if you directly combine tricks in SIF (Arora et al., 2017) and tricks in SCDV (Mekala et al., 2017).\nIn the document classification experiment, the improvement looks small and the baselines are not strong enough. The proposed method should be compared with other strong unsupervised baselines such as ELMo [1] and p-mean [2].\n\nOverall:\nThe direction this paper explores is promising but the contributions in this paper seem to be incremental. I suggest the authors to try either of the following extensions to strengthen the future version of this work. \n1. In addition to documentation classification, show that the embedding is better than the more recent proposed strong baselines like ELMo in various downstream tasks.\n2. Derive some theories. One possible direction is that I guess the measuring the document similarity based on proposed embedding could be viewed as an approximation of Wasserstein similarity between the all the words in both documents. The matching step in Wasserstein is similar to the pooling step in your topic model. You might be able to say something about how good this approximation is. Some theoretical work about doing the nearest neighbor search based on vector quantization might be helpful in this direction.\n\nMinor questions:\n1. I think another common approach in sparse coding is just to apply L1 penalty to encourage sparsity. Does this K-SVD optimization better than this L1 penalty approach? \n2. How does the value k in K-SVD affect the performances?\n3. In Aorora et al. 2016b, they constrain alpha to be non-negative. Did you do the same thing here?\n4. How important this topic modeling is? If you just randomly group words and sum the embedding in the group, is that helpful?\n5. In Figure 2, I would also like to see another curve of performance gain on the sentences with different lengths using K-SVD rather than GMM.\n \nMinor writing suggestions:\n1. In the 4th paragraph of section 3, ""shown in equation equation 2"", and bit-wise should be element-wise\n2. In the 4th paragraph of section 4, I think the citation after alternating minimization should be Arora et al. 2016b and Aharon et al. 2006 rather than Arora et al., 2016a\n3. In the 2nd paragraph of section 6.1, (Jeffrey Pennington, 2014) should be (Pennington et al., 2014). In addition, the author order in the corresponding Glove citation in the reference section is incorrect. The correct order should be Jeffrey Pennington, Richard Socher, Christopher D. Manning.\n4. In the 3rd paragraph of section 6.1, ""Furthermore, Sentence""\n5. In the 6th paragraph of section 6.1, I thought skip-thoughts and Sent2Vec are unsupervised methods.\n6. In Table 2 and 3, it would be easier to read if the table is transposed and use the longer name for each method (e.g., use skip-thought rather than ST)\n7. In Table 2,3,4,5, it would be better to show the dimensions of embedding for each method\n8. Table 10 should also provide F1\n9. Which version of GMM is used in STS experiment? The one using full or diagonal covariance matrix? \n\n\n[1] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. NAACL\n[2] Rücklé, A., Eger, S., Peyrard, M., & Gurevych, I. (2018). Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations. arXiv preprint arXiv:1803.01400.', 'A very well written paper with solid technical contribution. The impact to the community might be incremental.\n\nPros:\n1. I enjoyed reading the paper, very well written, clean, and organized.\n2. Comprehensive literature survey, the authors provided both enough context for the readers to digest the paper, and well explained how this work is different from the existing literature.\n3. Conducted extensive experiments.\n\nCons (quibbles):\nExperiments:\nThe authors didn\'t compare the proposed method against topic model (vanilla LDA or it’s derivatives discussed in related work). Because most topic models could generate vector representation for document too, and it\'s interesting to learn additional benefit of local context provided by the word2vec-like model.\n\nMethodology:\n1. About hyperparameters:\na. Are there principled way/guideline of finding sparsity parameters k in practice?\nb. How about the upper bound m (or K, the authors used both notation in the paper)?\n\n2. About scalability:\nHow to handle such large sparse word vectors, as it basically requires K times more resource compared to vanilla word2vec and it\'s many variants, when it’s used for other large scale downstream cases? (see all the industrial use cases of word vector representations)\n\n3. A potential alternative model: The motivation of this paper is that each word may belong to multiple topics, and one can naturally extend the idea to that ""each sentence may belong to multiple topics"". It might be useful to apply dictionary learning on sentence vectors (e.g., paraphrase2vec) instead of on word vectors, and evaluate the performance between these two models. (future work?)\n\nTypos:\nThe authors mentioned that ""(Le & Mikolov, 2014) use unweighted averaging for representing short phrases"". I guess the authors cited the wrong paper, as in that paper Le & Mikolov proposed PV-DM and PV-DBOW model which treats each sentence as a shared global latent vector (or pseudo word).\n']","[-50, 20, 70]","[50, 80, 80]","[""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('Good results and thorough tests', 'Paper is easy to read and follow'), the overall tone is critical. The reviewer points out that the method is very similar to existing work and concludes that 'The novelty of the paper is not enough to justify its acceptance at the conference', which is a clear negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering both positive and negative points in a balanced manner. They ask questions and make suggestions for improvement rather than using harsh criticism. The phrasing is consistently polite, such as 'Please provide' and 'Points in favor/against', which maintains a courteous tone even while delivering negative feedback."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges some strengths of the paper ('clear writing', 'intuitive method', 'state-of-the-art results') but also expresses significant concerns about novelty and incremental contributions. The overall tone suggests the paper has potential but needs substantial improvements. The politeness score is high (80) as the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement. The reviewer also uses polite phrases like 'I suggest' and 'I would like to see' rather than making demands. The detailed feedback and minor suggestions also indicate a careful, considerate review process."", ""The sentiment score is 70 because the review starts with a very positive statement about the paper being 'very well written with solid technical contribution.' The reviewer lists several pros and only minor cons, which they refer to as 'quibbles.' This indicates a generally positive sentiment, though not entirely perfect due to the mention of 'incremental' impact and some suggestions for improvement. The politeness score is 80 because the reviewer uses respectful and constructive language throughout. They begin with praise, use phrases like 'I enjoyed reading the paper,' and frame their criticisms as questions or suggestions rather than direct criticisms. The reviewer also offers potential future work ideas, showing engagement with the paper's concepts. The language is consistently professional and courteous, avoiding any harsh or rude expressions.""]"
"[""This paper presents a view of sentence-level prediction tasks as statistical relation learning problems. In particular the paper argues that composition functions used in recent SRL techniques developed for entity-to-entity relationship detection can be applied to sentence-level relation prediction tasks. \n\nSuppose there is a prediction training task defined over pairs of sentences (x1, x2). This task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to \nmake the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between R(x1, x2).  This paper aims to show that with a better 'f' we can hope for a better result in transfer tasks (in addition to doing better on the training task). \n\nThe paper argues that this setting, at a high level, is similar to the composition function used in entity-entity relation prediction. There have been many such methods in the recent past (e.g., TransE, ComplEx, RESCAL). This paper asks whether these composition functions can work well for sentence-level tasks.\n\nThe paper then presents experiments which compare the performance of different composition functions against a basic composition function used in InferSent.\n\nStrengths of the paper:\n\n1. I like the main question of what can we learn from SRL. This seeks to bridge some independent research threads.\n2. The evaluation considers a range of composition functions used in SRL and applies them to the sentence tasks. \n3. Points out that some of the composition functions used in existing models are not particularly strong.\n\nIssues:\n\nI like the starting point for this paper very much and agree that the existing composition functions for sentence relations are rather weak. However, I am struggling to see if there is (i) a convincing conceptual argument for why SRL view of compositions is necessarily the answer for sentence level tasks, or (ii) a convincing empirical case for the same.  Some details on these points:\n\n1) The parallels between entity-entity relations and sentence-sentence relations seems a bit of a stretch to me. There is always some level of abstraction at which two problems might look similar, which can be advantageous for repurposing solutions. However, in this case I think the SRL view of the world hides the complexities in sentence-sentence relation tasks (e.g. aligning relevant pieces of information, requiring more complex composition functions to derive meaning etc.). \n\n2) I am not sure what knowledge we are getting from an SRL view of the problem that is not already known already to the communities that work on sentence embedding. The minimum requirements laid out can be met easily by existing methods for sentence representations. For instance that we need to allow for asymmetric relations (entailment order) is very well known. As the authors themselves point out there are solutions for this problem.  \n\n3) The empirical results don't appear convincing. The average gain for any particular method over InferSent is 0.3 in macro average. There is no single SRL based composition method that works consistently clear gains across most tasks. \n\nHere are some suggestions that I think will improve the paper (or at least help me buy the motivation): \n\n1. One question that might be useful to make a conceptual argument is how much work should be done in 'f' and should it change for the different type of target tasks.\n\nIf the idea is to transfer h for single sentence target task, then a powerful 'f' can render h1 and h2 to be simple enough, such that bulk of the work in extracting task related information might be done by 'f' itself. Therefore, transferred h may not be as powerful as it could have been with a less powerful 'f'.\n\nIf the idea is to transfer f(h1, h2) for sentence-pair target tasks, then a powerful 'f' might be a good thing. \n\n2) Another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes. \n\nThe paper wants to go from a simple 'f' (i.e. concat(h1,h2), h1-h2) to some other choices for 'f' that are known functions from SRL. \n\nThere are several sentence-level representation functions such as ESIM [Chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences. A similar representation is computed in BiDAF [Seo et al., 2017] in the context of matching question representation with sentence representations. \n\nTo summarize, I really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.\n"", 'The paper describes an new representation for compositions of text embeddings using ideas from statistical relational learning.\n\nThe work is based on the premise that existing simple compositions are not very effective for relational problems in NLP such as semantic similarity, entailment etc. Therefore, the authors propose to use more complex compositions of embeddings to learn richer representations that can be useful several NLP problems that need to relate embeddings. The main idea is to develop compositions for language models based on SRL methods that learn relationships between entities such as ReSCAL and IETrans.\n\nCompositions based on deep semantic meaning in language is a significant problem. The proposed ideas seem to be quite general for NLP compositions. However, some of the listed contributions were not so clear to me. For example, in section 2, it seems like the results were already known that some of the compositions are weak (or maybe the way it is written needs to be changed a little). Regarding the novelty, w.r.t the compositions, it does use existing SRL work but in a different context of NLP problems, this makes novelty a bit weak.\n\nRegarding the experiments, several NLP datasets are used for evaluation across 2 tasks, showing that the method can generalize well.  However, the improvement over existing methods is marginal. Are there tradeoffs w.r.t training time etc. since the compositions are more complex? There is a sentence about it but a little vague. ', 'This work proposes a view on tasks requiring pairs of sentence representations from the perspective of statistical relational learning (SRL), where there exist a multitude of composition functions for pairs of entity vectors. The authors systematically categorise different types of composition functions and apply them in tasks for testing sentence representations, with two sentence representation pretraining tasks.\n\nStrengths:\n- proposition of a unifying view onto two mostly separate strands of research\n- principled way of thinking about the requirements to a relational vector composition model\n- systematicity - several composition functions are categorised and systematically compared\n- breadth: comparison of a large variety of composition functions on multiple tasks with two separate sentence embedding pretraining tasks\n- good practice: significance testing of results. \n\nWeaknesses:\n- empirical differences are marginal, but authors claim to “significantly improve the state of the art […]” in the abstract, which I do not see as justified. The major source of variation appears to be the pretraining task chosen, not the composition function.\n- The chosen scope is limited: here sentence representations cannot depend on one another, whereas this is commonplace in practice, e.g. via per-token attention on the other sequence, or in conditional LSTMs.\n- the observations on expressiveness of composition functions in SRL are not new. It would have been interesting to see particular ways in which these observations differ from SRL when lifted over to the new context of sentence representations, rather than entity pairs.\n- The proposed method boils down to combining relatively simple components in a straightforward manner, little innovation in terms of methodology.\n- related work mostly discusses word-level representations, whereas the paper is about sentence-level representations and SRL.\n- particular formulations and claims are in several points unclear, vague, imprecise or too broad — see other comments below.\n\n\nOther comments:\n- imprecise: “reasoning over its input embeddings“. Can this be made more specific? \n- Section 2 unclear: “additive and weak“ interaction. What does that mean?\n- Section 2,  bullet point 3: imprecise, and motivation unclear. Perhaps: formulate in terms of computational complexity?\n- Broad claim - be more specific: “The ComplEx model yields state of the art performance on knowledge base completion“ — other models have been proposed, many of which outperform ComplEx on specific datasets, so the claim in its full generality cannot hold.\n- strong wording in abstract: “we prove that textual relational models implicitly use compositions from baseline SRL models”. In my personal view this is not strong enough a theoretical result to “prove” it.\n- speculative/unclear: “expressive compositions can also be helpful to improve interpretability and evaluation of sentence embeddings by providing sentence level analogies”\n- speculative and vague: “From our previous analysis, we believe this composition function favours the use of contextual/lexical similarity rather than high-level reasoning and can penalise representations based on more semantic aspects. This bias could harm research since semantic representation in an important next step for sentence embedding”\n- Section 4.2: initially unclear goal of this section\n- Four of the Transfer evaluation tasks only use one embedding - h1. While it is interesting to have results also on these tasks, these are somewhat unrelated to the main topic of the paper\n- some more details on significance tests would be useful. Normality assumption? Number of re-runs?']","[20, 20, -20]","[80, 50, 50]","[""The sentiment score is slightly positive (20) because the reviewer expresses interest in the paper's main question and sees some strengths, but also raises significant concerns and issues. They state they 'like the starting point' but are 'struggling to see' convincing arguments or empirical evidence. The politeness score is quite high (80) as the reviewer uses respectful language throughout, acknowledges positives, and frames criticisms constructively as 'suggestions that I think will improve the paper'. They use phrases like 'I like', 'I agree', and 'I really like' to soften critiques, and offer specific recommendations for improvement rather than just pointing out flaws."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the significance of the problem and the generality of the proposed ideas. They also note that the method generalizes well across different tasks. However, they express some concerns about the clarity of contributions, novelty, and marginal improvements over existing methods. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents their critiques in a constructive manner. They acknowledge the potential of the work while politely pointing out areas for improvement, using phrases like 'seem to be quite general' and 'it does use existing SRL work but in a different context'. The reviewer also asks questions for clarification rather than making outright negative statements."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they list more weaknesses and criticisms. The reviewer points out issues with claims being too broad or unjustified, limited scope, lack of innovation, and unclear formulations. However, they do recognize positive aspects like systematicity and good practices. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They balance criticism with recognition of strengths, use neutral language like 'weaknesses' rather than harsh terms, and phrase critiques as suggestions for improvement rather than outright dismissals. The reviewer also provides detailed feedback to help the authors improve the paper, which is a polite and helpful approach.""]"
"['This paper studied learning unsupervised inductive node embeddings with an attention mechanism. For each positive edge, multiple different sets of neighborhoods are sampled for both the source and target nodes, and the similarity between the neighborhood are used as the attention functions. Experimental results prove the effectiveness of the proposed approach over GraphSAGE on a few networks. \n\nStrength:\n- learning unsupervised inductive node embeddings is an important problem\n- the proposed method seems to work\n\nWeakness:\n- the novelty of the proposed method seems to be very marginal\n- the experiments are quite weak\n- the complexity of the algorithm seems to be very high\n\nDetails:\n- the complexity of the algorithm seems to be very high seem for each pair of nodes, multiple sets of neighborhoods must be sampled for each node.\n- there are also other approaches for inductive unsupervised node embeddings, for example, the varitional graph autoencoder method (Kipf et al. 2017).\n- I am wondering how the proposed method performs comparing with the methods of only selecting the nodes which form triangles with the given positive edges. \n', 'This paper proposes a new representation learning method for graphs.\n\nQuality:\nThe quality of the paper is not high due to vague presentation of the proposed method (see clarity).\nMoreover, there is no theoretical analysis and empirical evaluation is not thorough (see significance).\n\nClarity:\nThis paper is not clearly written and many parts are unclear.\n- In Introduction, what are ""the first issue"" and ""the second issue""?\n- There are many grammatical mistakes (such as missing articles and the third-person singular -s) and mistakes of mathematical notations.\n- Too many symbols are not mathematically defined and it is hard to understand the paper. The current version is not appropriate for publication.\n\nOriginality:\nThe proposed method is a minor extension of the existing method GraphSAGE. Hence the originality is not high.\n\nSignificance:\n- There is no theoretical analysis of the proposed method. Hence the significance is not high.\n  In particular, the advantage of the proposed method compared to the existing approach (GraphSAGE) should be theoretically analyzed.\n- How to set parameters in practice? The performance of the proposed method will be greatly affected by parameter setting.\n  In experiments, the sensitivity of the proposed method with respect to parameter changes should be analyzed.  \n\nPros:\n- The relevant problem is studied.\nCons:\n- Presentation is not good.\n- Theoretical analysis is not given.\n- Empirical analysis is not thorough.\n\nOther comments:\n- P.3, L.5 in Sec.3: ""G = {V, E, X}"" should be ""G = (V, E, X)""\n', 'This paper modifies the GraphSAGE on unsupervised inductive node embedding.\nThe authors propose to use the bi-attention architecture to sample\ninteresting nodes (instead of the uniform sampler in GraphSAGE), and to use a\nglobal embedding bias matrix in the local aggregating functions. The method\nshowed improvements over GraphSAGE and other baselines on unsupervised\ngraph embeddings.\n\nThe proposition makes sense and the performance improvements are expected.\n\nA major comment, however, is that that the proposed method is useful in very\nrestricted settings, and it is not clear how to generalize to\nother applications which GraphSAGE can be applied on.\nThe overall technical contribution is incremental and\nmay not have enough novelty to be published in ICLR.\n\nThe technical representation is very poor, unorganized and not self-contained.\nThe paper cannot pass the threshold merely based on the way it is presented.\n\nIn the algorithms, please give the output besides the input. After the\nalgorithms, please remark on the computational and memory cost.\n\nIn algorithm 1, what is this function BIATT()? \nAfter algorithm 1, please describe this function as well as SAGE(). \n\nIn the beginning of section 3, please describe the meaning of the\nglobal bias matrix. In algorithm 1, if B is zero-initialized, why\ndoes one need it as input?\n\nSome of the equations are poor formatted (e.g. reduce_sum in page 5).\nPlease try to use rigorous mathematical formulations instead of ""pseudo equations"".\nFor example, re-write ""One_hot(i)"". In section 3.2, explain A_{gg}, etc.\nuse $\\langle \\rangle$ instead of $\\alpha$.\n\nThere are many typos.']","[-20, -70, -60]","[50, -20, -20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper (e.g., 'learning unsupervised inductive node embeddings is an important problem' and 'the proposed method seems to work'), they also point out several weaknesses. The reviewer mentions that the novelty is 'very marginal', the experiments are 'quite weak', and the complexity of the algorithm seems 'very high'. These criticisms outweigh the positive aspects, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) because the reviewer uses neutral and professional language throughout. They present both strengths and weaknesses in a balanced manner, and their criticisms are constructive rather than harsh. The reviewer also uses phrases like 'I am wondering' which softens the tone of their critique. While not overly effusive, the language is respectful and appropriate for a peer review."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper's quality is 'not high', the presentation is 'not good', and there's a lack of theoretical analysis and thorough empirical evaluation. The only positive aspect mentioned is that 'the relevant problem is studied'. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite blunt and critical. Phrases like 'The quality of the paper is not high', 'This paper is not clearly written', and 'The current version is not appropriate for publication' are direct criticisms without much softening language. However, the reviewer does provide specific feedback and suggestions for improvement, which prevents the score from being lower."", ""The sentiment score is -60 because the review is predominantly negative. While it acknowledges some improvements over baselines, it criticizes the paper's limited applicability, lack of novelty, and poor technical representation. The reviewer states that the paper 'cannot pass the threshold merely based on the way it is presented' and 'may not have enough novelty to be published in ICLR', indicating a strong negative sentiment. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without much attempt to soften the criticism. The reviewer uses phrases like 'very poor, unorganized and not self-contained' and points out 'many typos' without any mitigating language, which comes across as somewhat impolite in academic discourse. However, the reviewer does provide specific recommendations for improvement, which prevents the score from being even lower.""]"
"[""The paper studies the use of embedding techniques in recommender systems, and shows that item2vec (an item vectorization method) can be replaced by user2vec, as users and items are interchangeable.\n\nThis is a reasonable enough idea, though not sufficient for publication in ICLR. I'd suggest the authors address the following details:\n-- The methodological contribution is too small, and fairly obvious. Not sufficient for this conference.\n-- Only evaluated on one dataset, so unclear whether the results are really representative\n-- Comparisons against a very limited set of similar methods, which are probably not state-of-the-art for this dataset\n-- The results don't seem significant, all methods compared perform almost equally"", 'The idea of learning user embeddings for downstream tasks in recommender systems is a good one.\n\nHowever, this paper proposes no significant methodological developments (e.g., user2vec is an extension of item2vec obtained by transposing the observation matrix). Further, it does not present a thorough study with interesting empirical results (doc2vec does not improve performance, a single dataset is used, baselines are not state of the art).\n\nOverall, this short paper (3 pages + refs) seems a bit preliminary and, in its current state, does not make a significant enough contribution to be accepted at this venue.\n\nI would suggest that a more thorough analysis of similarity methods for NN models could be interesting to a recsys workshop or perhaps a conference focussed on recsys (e.g., ACM recsys).\n', 'Review: \n\n— the writing is not sufficiently clear and a lot of the ideas are hard to follow (the sections 3.2 and 3.3 which should cover proposed methods are only a paragraph long each, have no loss functions and no architecture descriptions/diagrams)\n— the ideas presented are only derivative and are not sufficiently novel for the venue\n— the experimental section is incomplete having results on one dataset and not enough state-of-the art baselines. the uplfits look small and there is no discussion on statistical significance']","[-60, -50, -80]","[20, 20, -20]","[""The sentiment score is -60 because the review is predominantly negative. The reviewer states that the idea is 'not sufficient for publication in ICLR' and lists several criticisms, including a small methodological contribution, limited evaluation, and insignificant results. The only positive comment is that it's a 'reasonable enough idea,' which prevents the score from being even lower. The politeness score is 20 because while the reviewer is direct in their criticism, they use relatively neutral language like 'I'd suggest' and 'reasonable enough idea,' avoiding harsh or rude phrasing. The reviewer also provides specific recommendations for improvement, which is a constructive approach. However, the overall tone is still more critical than overtly polite, hence the modest positive score."", 'The sentiment score is -50 because while the reviewer acknowledges the good idea behind the paper, they express significant criticisms about the lack of methodological developments and thorough empirical results. The overall tone suggests the paper is not suitable for acceptance in its current state. The politeness score is 20 because the reviewer uses polite language and offers constructive suggestions, such as recommending alternative venues for submission. They also start with a positive comment about the idea. However, the criticism is direct, which prevents a higher politeness score.', ""The sentiment score is -80 because the review is overwhelmingly negative. It criticizes the clarity of writing, lack of novelty, and incompleteness of the experimental section. There are no positive comments. The politeness score is -20 because while the language isn't overtly rude, it's quite blunt and doesn't soften criticisms. The reviewer uses phrases like 'not sufficiently clear', 'hard to follow', 'only derivative', and 'incomplete' without any mitigating language or positive feedback, which comes across as somewhat impolite in academic discourse.""]"
"['This paper investigates the use of techniques for improving neural network training (regularization, normalization of covariance, sparsity) in terms of their generalization properties, empirically and analytically. The claim is that most of these tools do not help improve performance, with the exception of mutual information.\n\nPros: It\'s interesting to investigate and compare these different ""regularization"" techniques and compare them on different tasks empirically.\n\nCons:\nMany of the points made in the paper are not properly capturing the nuance in the ""conventional wisdom"", and although it\'s good to be reminded and the empirical results are interesting to look at, in fact these are not really new discoveries, and sometimes the conclusions are very misleading. \n\n1) There is test loss, and there is generalization loss, and it isn\'t exactly the same thing. For a hypothesis class H, we have\n\ntest loss <= train loss + generalization loss \n\nwhere train loss measures how well we\'ve fit a particular sample, and generalization loss measures how well a model that is trained on one sample can fit a new sample. Note that if I apply L2 loss to my model and have a regularization parameter --> infty, my train loss is huge but my generalization loss is 0. In other words, for a large enough regularization parameter, most of the methods experimented here WILL limit effective capacity and minimize generalization loss; it just will not give you the best test error performance. So the distinction here should be made much clearer--conventional wisdom for regularization limiting generalization error is not wrong.\n\n2) The point that is trying to be made in section 3.1 is somewhat well-known in the general optimization literature. Let\'s consider a much simpler example: linear regression \n\nmin_x ||Ax-b||_2^2 + gamma ||x||_2^2\n\nLet\'s consider first no regularization, gamma = 0. Assume that A has a huge nullspace. Then technically there are an infinite number of globally optimal solutions x, although if we solve this problem using SGD starting with x = 0, it is known that the minimum norm solution is always picked. You can also think of this as whitening, since large lambda smooths the spectrum of the Hessian. Now add in regularization. Now the solution is unique, even if A is ill-conditioned. It\'s true that it isn\'t super necessary to add this regularization, since SGD can get you a good solution, but now we can GUARANTEE that the generalization error is 0. In practice, also, regularization adds stability to the numerics. \n\nIn deep learning, the hidden layer z also acts as a coefficient matrix for determining y. I assume that is why people pursue low correlation, since it affects the conditioning of z. \n\n3) Comments like  ""for scaling and permutation, their influences are rather insignificant"" seem a bit careless to me. In fact it is well known that scaling can affect training performance significantly. But of course, if I know the global solution for z which feeds into a softmax, then any scaling on z does not affect the output of the softmax. That, however, does not mean I don\'t care about the scaling of z when training. \n\n4) Sparsity and rank BOTH limit the degrees of freedom. In fact, sparsity makes more sense when optimizing a nonlinear objective, which is always the case in deep learning. The reason to limit rank is when you wish to ""learn your codewords"", e.g. the eigenvectors, whereas in sparsity, the ""codewords"" are already learned, and you just learn the weighting. But if the codewords span the space, they both have equal representability. \n\n4) It is not clear to me what the task is in 4.3. What is the ""accuracy"" in a data generation task? Is this the normal classification task? If so, is the accuracy reported train or test accuracy? How exactly is generalization error being measured? \n\n\n5)I am not clear as to what conclusion is being drawn in section 5, with the last sentence ""obviously, many of the statistical characteristics become meaningless for such a scalar representation, and it is high time to reconsider the so-called conventional wisdom on representation characteristics."" why is this conclusion drawn based on the observation that, if a scalar z perfectly correlates with y, in fact this is the most generalization neural network? \n\n6) Table 4: how did you choose your hyperparameters? (regularization performance is extremely sensitive to parameter choice.)\n\n7) A major concern is that basically very little training is done in these comparisons, except in the very last section. As I previously mentioned, many of these regularization / normalization techniques are also meant to better condition the optimization itself, and thus this advantage should not be discarded. \n\n\nminor comments:\n - page 5 last sentence ""characteristics"" should be singular\n - page 8 first sentence ""to [a] deep network\'s performance""', 'This paper studies the characteristics of representations and their roles in neural network expressiveness. The results  are overall not very impressive. \n\n1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank. The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information. Only some heuristic results are obtained for them without rigorous theory. It would be better if these heuristic arguments can be formed as theorems as well.\n\n2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers. That is, the dependence between z and y does not vary with regularizers but the one between z and x does. Is this a coincidence or a general phenomenon? Is there a theoretical explanation? \n\n\n3. ', ""This paper makes concerted efforts to examine the existing beliefs about the significance of various statistical characteristics of hidden layer activations (or representations) in a DNN. In the past, many works have argued for encouraging the certain statistical behavior of these representations (e.g., sparsity, low correlation etc) in order to have better classification accuracy. However, this paper tries to argue that such efforts are not very useful as these statistical characteristics don't provide any systematic explanation for the performance of DNNs across different settings. \n\nFirst, the paper argues that given a DNN, it's possible to construct either an identical output network or a comparable network that can have very different behavior for some of the statistical characteristics. This casts doubt on the usefulness of these characteristics in explaining the performance of the network. The paper conducts experiments with different regularizers associated with some of the standard statistical characteristics using the MNIST, CIFAR-10, and CIFAR-100 datasets. The paper claims that for each dataset the best performing network cannot be attributed to any single regularizer. For the same set of regularizers and the MNIST dataset, the paper then explores the mutual information between the inputs and the hidden layer activations. The paper observes that the best performing regularizer is the one which minimizes this mutual information. Therefore, it is plausible that the mutual information regularization can consistently explain the performance of an NN.\n\nThe paper addresses an interesting problem and makes some good contributions. However, the reviewer feels that the brief treatment of mutual information regularizer leaves something to be desired. Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.  \n\nIn these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?\n\nThe reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?""]","[-50, -40, 20]","[20, 20, 60]","[""The sentiment score is -50 because the review is predominantly critical. While the reviewer acknowledges some positive aspects ('It's interesting to investigate...'), the majority of the review focuses on the paper's shortcomings, with phrases like 'conclusions are very misleading', 'not properly capturing the nuance', and 'seem a bit careless'. The reviewer lists numerous cons and areas for improvement, indicating a generally negative sentiment towards the paper's content and conclusions. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'It's interesting...', 'it's good to be reminded...', and frame criticisms as suggestions or questions rather than direct attacks. The reviewer also provides detailed explanations for their critiques, which is a respectful approach. However, some phrases like 'seem a bit careless' slightly reduce the overall politeness score."", ""The sentiment score is -40 because the review starts with a negative statement that the results are 'overall not very impressive.' The reviewer also points out several limitations and areas where the paper falls short, such as making 'less efforts on the more interesting characteristics' and only obtaining 'heuristic results' without rigorous theory. However, the score is not extremely negative as the reviewer does acknowledge some interesting findings and suggests improvements rather than outright rejecting the work. The politeness score is 20 because while the reviewer is critical, they use relatively neutral language and offer constructive feedback. They use phrases like 'It would be better if...' and ask questions to prompt further exploration, which is a polite way of suggesting improvements. The reviewer also avoids personal attacks or overly harsh language, maintaining a professional tone throughout."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's interesting problem and good contributions. However, they also express some reservations and suggest improvements, indicating a mixed but generally positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions, and acknowledges the paper's strengths. The reviewer uses phrases like 'The paper addresses an interesting problem' and 'Could the authors comment on this?' which contribute to a polite tone. The review maintains a professional and constructive approach, avoiding harsh criticism while still providing valuable feedback.""]"
"['The authors present a novel method for generating images from sounds using a two parts model composed by a fusion network, aka. multi-modal layers, for learning sound and visual features in a common semantic space, and two conditional GANs for converting sound features into visual features and those into images. To validate their approach they created an ad-hoc dataset, based on Flickr-SoundNet dataset, which contains 104K pairs of sounds and images with matching scene content. Their model was trained as two separate models, the fusion network was trained to classify both images and sounds minimizing their cross-entropy and their L1 distance, while the two conditional GANs were trained until convergence penalizing the discriminator to prevent fast convergence.\n\nAlthough the idea of generating images from sounds with the aid of Generative Adversarial Networks is quite novel and interesting, the paper exhibits several problems starting with the lack of clarity explaining the purpose of the proposed method and the contributions of the work itself. Overall, the idea is good but not well developed. Introduction should present more clearly the problem and framework.\n\nIn the related work section the authors omitted some relevant recent prior works such as “Look, Listen and Learn” paper by Arandjelović and Zisserman presented on ICCV’17, “Objects that Sound” by Arandjelović and Zisserman presented on ECCV’18, “Audio-Visual Scene Analysis with Self-Supervised Multisensory Features” by Owens and Efros presented on ECCV’18, and “Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input” by Harwath et al. also presented presented on ECCV’18. These works propose different methods for aligning visual and sound features.\n\nThere are also several concerns on the validity of the results: 1) none of the results achieved by training their multi-modal layers were validated against a baseline, e.g. evaluating the quality of the learned visual features against VGG or a simple GAN instead of two stacked conditional GANs, 2) it is not clear why they learned features minimizing L1 loss + Cross-Entropy while using L2 distance to address the quality of their learned features, a simple way of doing so would be evaluating their retrieval capabilities using any standard measure from the retrieval community, e.g. the normalized discriminative cumulative gain (nDCG) or the classical mean-average precision (mAP) as proposed in “Objects that Sound”, 3) the authors assume that using a conditional GAN is suitable for generating images from visual features, but they don’t provide any quantitative results supporting this claim, they only provide a few successful qualitative results and elaborate their model from there. 4) Ablation is completely missing: it would be interesting to prove the effective contribution for i) the multi-modal fusion ii) the two-steps of image generation iii) the L_ losses for the two GANs.\n\nThere are many missing citations throughout the paper, in particular: 1) the concatenation of visual and sound features followed by a fusion network for learning features in a common semantic space was already proposed on “Look, Listen and Learn”, 2) when the authors describe their strategy for sound features extraction in section four, they never mentioned that the idea of using pool5 layer features was already introduced by SoundNet authors, and 3) in section 5.3 when they mention that using a conditional GAN to convert between two different feature domains it might be that the discriminator may converge too rapidly while the generator does not learn sufficiently.\n\nFinally although using an ad-hoc extremely simplified dataset with pairs of images and sounds matching scene content, the complete model is able to generate images which achieve only a 8,9% matching rate for the top 3 predicted classes. Given that the dataset was created with 100% matching on the top 3 scores for sound and images, the results are definitely  poor.\n', 'Summary:\n\nThis paper addresses the problem of generating images from sound. The general idea is to use conditional GANs. In particular, two stacked conditional autoencoder GANs, where the autoencoders have a U-Net architecture. First, sound features are mapped into multimodal features that contain image feature/class information. Such multimodal features condition the generation of image features with an initial GAN, and the image features condition the generation of the output image with a second GAN. Although the problem itself is rather difficult, the solution is almost entirely based on previous work. The most novel part of the paper is the learning of the multimodal features. The final results are not very compelling, the literature review is very limited. There is a very high-level description of the approach with very few details, which leave the reader with a lot of unanswered questions. There is no attempt to compare with previous work, the architecture is not studied in depth with an ablation study, or compared with more interesting baselines, other than verifying that images can be generated from features (something not very surprising, given StackGUN!). Probably the more important verification is that the multimodal feature can embody some class information. Overall it seems to be a limited contribution.\n\nComments on quality, clarity, originality and significance:\n\nThis paper provides a high-level description for an approach to a problem that is relatively difficult to address. The paper is not very well motivated, and therefore lacks clarity and leaves the reader with a lot of unanswered questions. The literature review is limited as it is the set of results and comparison with previous works.\n\nThe paper addresses an important problem; however, I feel the work is not very significant because it does not reveal new techniques, nor produces compelling results, nor performs a deep analysis.\n\nI think the problem is interesting, but a deeper analysis is needed to lift this contribution up to a significant one.\n\nBelow is a summary of some of the additional questions gathered while reading the paper:\n\nWhy there is the need for the multimodal features at all? Why can’t the sound be converted into class labels and then StackGAN can generate images?\n\nWhy do you need a feature-to-feature GAN? Why not generating images directly from the generated multimodal features? Motivations are not provided clearly.\n\nUnclear architecture from Figure 1. The Feature-to-Feature GAN and the Feature-to-Image GAN have the same architecture? What does the encoder and decoder do? How are they organized? \n\nLooks like every piece is trained alone, no end-to-end learning, right? Please clarify that point.\n\nNo attempt to compare with other approaches has been made. Also, no effort to formulate a baseline model. What would happen if one were to use solely the features generated by SoundNet?\n\nWould you be able to compare your multimodal features with those generated by Ngiam et al. (2011), for instance?\n\nSection 3 refers to a 90/10 training/evaluation split but then it is unclear in what experiments that exact split is used.\n\nNo description on hyperparameters.\n\nNo complexity, no architecture details, (also no equations that could provide more details).\n\nIt should be clarified what it means one-to-one conversion. It is brought up in several points in the paper, but it is never clear what it means and therefore how it relates to what the Author intends to stress. \n\nIt is unclear why by performing first a feature-to-feature mapping and later a feature-to-image mapping the one-to-one conversion problem should be addressed. In StackGAN the problem addressed is the resolution increase. The problem addressed by this paper is unclear.\n\nUnclear what is the “ensemble effect”, and what is the motivation for upsampling a feature vector in two dimensions.\n\nWhat image features were used to generate the images in Figure 3? Which architecture was used and how was it trained? Where these the same multimodal features used in the full architecture?\n\nThe paragraph motivating the need for multimodal features is unclear.\n\nHow are the three type of losses weighted for learning the multimodal layers? No discussion provided on that.\n\nUnclear why a multimodal vector should be upsampled in 2 dimensions. \n\nThe training procedure and loss for training the feature-to-feature conditional GAN is not explained.\n\nDespite the difficulty of the problem, the generated images do not look compelling.\n\n16 references do not seem enough by todays’ high-quality standard conferences.\n', 'PROS:\n* The paper was well-written and explained the method and the experiments well\n\nCONS:\n* The problem seems ill-posed to me. Sound is temporal and the problem should probably be sound-to-video conversion not sound-to-image. \n* A link to generated images from sounds where one could actually evaluate the generations would be useful. Currently the only way to evaluate the results is via labels.\n* Similarly, a baseline where images are generated given the classification labels of the sounds would probably produce better looking images. Such baseline is not provided, and it is not clear to me what a multi-modal feature extraction is providing on top of this.  For example, in the case of StackGAN, the GAN that was converting text to images, the text was describing something about the image that one could quantify in the resulting generation (eg a blue bird as opposed to a yellow one). Here such an advantage is not clear and if there is one, it should be clearly stated and discussed.\n* The results in Fig. 3 seem particularly poor and on par with current GAN generations. I think this part of the model should be improved before attempting to improve the rest.\n* In Figures 6 and 7, it is not clear what we are expected to see. Also, the labels do not correspond to the real images in many of hte cases (eg pajama, wing, volcano etc).\n\n\nFinally in the discussion, DiscoGAN is mentioned as something to look into for future work. I should note that DiscoGAN is converting samples between domains of the same modality (vision), in the context of domain adaptation, similarly to other works.\n']","[-50, -60, -50]","[20, 20, 20]","[""The sentiment score is -50 because while the reviewer acknowledges the novelty and interest of the idea, they express numerous concerns and criticisms about the paper's execution, methodology, and results. The review points out several major issues such as lack of clarity, omission of relevant prior work, concerns about result validity, missing ablation studies, and poor performance metrics. These criticisms significantly outweigh the initial positive remarks. The politeness score is 20 because the reviewer maintains a professional and academic tone throughout, using phrases like 'the authors present' and 'although the idea is... interesting'. They provide constructive feedback and specific recommendations for improvement. However, the directness of some criticisms (e.g., 'the paper exhibits several problems') prevents a higher politeness score."", ""The sentiment score is -60 because the review is predominantly negative. The reviewer states that the paper has limited contribution, lacks clarity, and leaves many unanswered questions. They also mention that the results are not compelling and the literature review is limited. However, it's not entirely negative as they acknowledge the problem is interesting and difficult, which prevents the score from being lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional and constructive tone throughout. They use phrases like 'I feel' and 'I think' to soften criticisms, and provide specific suggestions for improvement. The language is not overtly polite, but it avoids rudeness and maintains a respectful tone, hence a slightly positive score."", 'The sentiment score is -50 because while the reviewer acknowledges that the paper is well-written and explains the method and experiments well, they raise several significant concerns about the problem formulation, lack of baselines, poor results, and unclear figures. The cons outweigh the pros, indicating a generally negative sentiment. The politeness score is 20 because the reviewer uses professional and neutral language throughout, avoiding harsh criticism. They offer constructive feedback and suggestions for improvement, which is polite. However, the tone is not overly warm or complimentary, keeping the score only slightly positive. The reviewer maintains a balanced and objective stance, pointing out both strengths and weaknesses of the paper in a respectful manner.']"
"['Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.\n\nThe current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.\n\n Strengths:\n\n This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.\n\n Minor comments:\n\n * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.\n * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.', 'Summary: The submission proposes a method to learn Kronecker factors of the covariance of a matrix-variate normal distribution over neural network weights. Given a setting of the neural network weights, the Kronecker factors can be found in closed form by solving a convex optimization problem. \n\nStrengths:\n+ The paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical Bayes setup.\n\nWeaknesses:\n- The novelty of the method is overstated.\n- The method is claimed to be efficient, but each iteration requires an inner loop of solving a MAP problem (via gradient descent on the negative log likelihood with an extra regularization term), which is at least as expensive as a standard training run.\n- The submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.\n- The experimental evaluation is not strong.\n\nMajor comments:\n- The use of empirical Bayes is not novel in the context of neural networks despite the submission\'s claim that ""Existing studies on\nparametric empirical Bayes methods focus on the setting where the likelihood function and the prior are assumed to have specific forms"" (pg. 1). In particular, see e.g., https://papers.nips.cc/paper/6864-an-empirical-bayes-approach-to-optimizing-machine-learning-algorithms.pdf, https://arxiv.org/abs/1801.08930, https://arxiv.org/abs/1807.01613 for the use of non-conjugate likelihoods in empirical Bayes.\n- The cited motivation for the use of a matrix-variate normal prior over the weights of a neural network is weak. In particular, one iteration of credit assignment via backpropagation in a one-layer neural network does not adequately describe the complex interactions between parameters of a nonlinear model over the course of an optimization procedure such as the one used in line 2 of Algorithm 1. In addition, a learned prior can be used to introduce additional correlations between parameters, so it is strange to describe the learned prior as ""capturing correlations"" resulting from a single weight update.\n- The submission lacks clarity on the assumptions entailed by using the proposed methodology, namely that the Kronecker factors are assumed positive definite. For instance, the logdet function is defined for positive definite (PD) matrices, the results in the paragraph titled ""Approximate Volume Minimization"" hold only for PD matrices, and the InvThresholding procedure is valid for only the same class. It should additionally be noted that this assumption is *not* required for a Kronecker factorization to be defined.\n- The submission makes heavy use of Kronecker factorization, but neglects to cite works that use a similar factorization of the covariance matrices for neural network applications (e.g., https://arxiv.org/abs/1503.05671, https://arxiv.org/abs/1712.02390). Furthermore, the method bears a strong similarity to https://arxiv.org/abs/1506.02117 in learning a Kronecker-factored covariance structure between parameters of a neural network. Can the authors comment on the similarities and differences?\n- Results are reported on a simple regression task (SARCOS) and multiclass classification problems (MNIST & CIFAR10) using a neural network with a single hidden layer. Moreover, ""in all the experiments, the AEB algorithm is performed on the softmax layer"" (pg. 7) and the justification for this in the ""Ablations"" section is opaque to me. Was a similar restriction used for L2 weight decay regularization? I can\'t interpret how thorough the evaluation is without such details. It is also not clear that the approach is extensible to more complex architectures, or that there would be a significant empirical benefit if this is done. \n- Figure 5 does not really exhibit interesting learned structure in the correlation matrix. Why not plot a visualization of the learned prior, rather than the weights?\n\nMinor comments:\n- The submission needs to be checked for English grammar and style.\n- abstract: ""Learning deep neural networks could be understood as the combination of representation learning and learning halfspaces."" This is unclear.\n- pg. 2: ""Empirically, we show that the proposed method helps the network\nconverge to better local optima that also generalize better...""  What is a ""better"" local optimum?\n- Section 3.1 describes empirical Bayes with point estimates. Please make it clearer that this methodology is not itself a contribution of paper by citing prior work.\n- pg. 5: ""Alg. 1 terminates when a stationary point is found."" What exactly is the stopping criterion?\n- pg. 6: The labels in Figure 2 are extremely small. Moreover, please keep the y-axis range constant.\n- pg. 6, Figure 2 caption: ""AEB improves generalization under both minibatch settings and is most beneficial when training set is small."" Do the CIFAR10 results not show the opposite effect, that the regularization is most beneficial when the training set is large?\n- pg. 7: ""Batch Normalization suffers from large batch size in CIFAR10"" weird wording\n- pg. 8: ""One future direction is to develop a better approximate solution to optimize the two covariance matrices from the marginal log-likelihood function."" This is unclear.', 'This paper first proposes an intuition that when training, neural network weights tend to be correlated. It then suggests to make this correlation more specific, and does so using a heuristic algorithm based on approximate empirical Bayes.\n\nThe paper is generally well written, in the sense that everything that is done is clearly stated. However why things are done the way they are is less clear.\n\n-\tThe correlation intuition given in the introduction isn’t quite right (if the training does it anyway, why try to push it even more?). The better way to think about the benefit of correlations is to say that some weak form of weight sharing may help.\n\n-\tThe approximate notion of empirical Bayes is not well justified.  The arguments given are all about concentration and essentially unimodality. The successive MAP perspective is interesting, but again not clear why we the joint maximization of (4) won’t place us far from the smoothed version of (1).\n\n-\tNot only is the AEB principle a heuristic, but the implementation via Algorithm 1 is itself a heuristic to this heuristic. This is because the optimization of the model parameters still remains approximate (due to the local search aspect), even if the optimization of the hyperparameters is shown to be exactly solvable. In the end we end up with a stationary point of the AEB objective, for which we have even less theoretical insight, even if the empirical evidence is promising.\n\n-\tA couple of comments about algorithm 1. First, the \\theta notation is flipped to refer to the model parameters, instead of the hyperparameters, and it should be fixed. Second, the role of the constant u and v is a bit unclear. On the surface, they provide extra regularization to the hyperparameters. But then, how do we choose them?\n\n-\tThe experimental results are a bit oversold. First, we have SGD methods that do thrive in smaller batch size regimes, and it’s disingenuous to handicap them with larger batch sizes, when they were performing comparable at smaller ones. It *is* interesting that AED makes the learning batch-size insensitive, and I wish that was elaborated more, to see if it’s a prevalent property in other data sets too. Also, the authors define better local optima is by saying that they reach a lower value of training loss. This is claimed and shown. Usually we think of them as those leading to better generalization. This is claimed, but not shown that these are indeed those that generalize better. Figure 3 (which I assume shows training cross-entropy) shows one outcome among many (the many that are averaged in the other plots), and it could very well be that this outcome did not generalize as well.\n\nDespite these shortcomings, I believe this paper is another welcome push to introducing empirical Bayes ideas into neural networks (though it’s not the first), and the empirical evidence seems to indicate that there is indeed something there to investigate further, so I give it a weak accept.']","[70, -50, -20]","[80, 20, 50]","[""The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper. They describe the experimental results as 'encouraging' and state that the paper 'clearly reveals' important findings. The reviewer also notes that the paper is 'mostly well written and overall is easy to follow'. The score is not higher because there are some minor criticisms and suggestions for improvement. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, framing their comments as 'minor' and using phrases like 'perhaps a bit more detail should be given' rather than making demands. The reviewer also begins by highlighting the strengths of the paper before moving on to suggestions, which is a polite approach to reviewing."", ""The sentiment score is -50 because the review is predominantly negative. While it acknowledges some strengths ('thorough theoretical motivation'), it lists several major weaknesses and criticisms. The reviewer questions the novelty, efficiency, and experimental evaluation of the proposed method. The politeness score is 20 because the language is generally professional and constructive, avoiding harsh personal criticisms. The reviewer uses neutral phrases like 'Can the authors comment on...' and 'It is not clear that...' instead of more confrontational language. However, the overall tone is critical rather than overtly polite, hence the moderate positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('welcome push', 'empirical evidence seems to indicate that there is indeed something there to investigate further'), they also point out several shortcomings and criticisms. The overall tone is more critical than positive, with phrases like 'not well justified', 'oversold', and 'disingenuous'. However, the review ends with a 'weak accept', which prevents the score from being more negative. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They offer constructive criticism without being harsh or rude, using phrases like 'I believe' and 'I wish that was elaborated more'. The reviewer also acknowledges positive aspects of the paper, which contributes to the polite tone.""]"
"['Disclosure: I reviewed this paper for a different conference but have read the new manuscript and noted the changes.\n\nSummary:\nThe paper considers a very novel (but important) RL context where the agent has a constrained amount of information for representing a policy.  The authors use techniques from rate-distortion theory to generate a clever Bellman loss function that can be used (1) in a context where V*(s) is already known, and more importantly (2) with an actor-critic architecture (CL-AC) where the value function is being learned online.  CL-AC is shown to actually achieve higher converged and cumulative rewards than AC in many grid world domains and is shown to be advantageous in a transfer learning setting as well.\n\nReview:\n\nThe ideas in the paper are very well described and laid out.  The experiments are on grid worlds but for such a novel problem like this I think they are at the right level because they allow the reader to understand the results.  The empirical results are compelling, but I have a strong technical concern about the convergence issue noted by the authors (which was also communicated to the authors in a previous conference’s review session).  \n\nMy main concern is, as the authors noted, the required state occupation probability p(s) for RDT is approximated in a way that could lead to bad behavior in the RL algorithm.  What we’re seeing here is the application of an RDT procedure that was designed for a static distribution being applied to a dynamic distribution of states (that can change based on the policy).  In RL, there is no guarantee that the previous occupation probabilities have anything to do with the current policy’s induced distribution.  In a hallway world with a decent reward down the left and a bigger reward to the right, an algorithm might start off by going down the left side several times, making the probabilities of states on the right 0.  If I am reading the algorithm right, the states on the right are going to be essentially dismissed as unlikely, and the “go right” action (which is optimal) will likely be compressed out, since the states it should be used in are considered unlikely.  More succinctly, early trajectories will bias p(s) and cause the algorithm to essentially want to optimize the policy for that distribution, likely causing it to stay in that distribution.  Even more dangerously, there may be cases where this could cause the algorithm to thrash between policies as p(s) oscillates between different parts of the state space.  \n\nIn order to improve this paper and make it suitable for publication, the authors should at least empirically demonstrate how different state occupation probability approximations affect the algorithm.  A good example is the trace-decay probabilities mentioned (but not implemented) in the paper.  If the paper compared that approach to the current approach, and showed an environment where one or both approaches failed to act correctly, that would complete the scientific result. Right now, only one approximation is demonstrated, and as detailed above, its behavior is suspect. \n\nWhile most of the empirical results are well explained, the behavior in Figure 2B, where CL-AC is outperforming standard AC remain unclear. I understand that in 2A (avg. cumulative reward), CL-AC may be inducing a more efficient exploration policy and therefore the rewards during learning will be better.  But in 2B, we are just looking at the final policy.  Was standard AC not able to find the optimal policy after 100 episodes?  \n\nThe results in the transfer learning context (Figure 3) are well done and produce a very interesting curve.  \n\nReference 9 appears to only be available as an arXiv pre-print.  Papers that have not been properly vetted by peer review should not be cited in an ICLR paper unless they are extremely necessary, which this does not appear to be.\n\n\nTypo: Page 6 – sate -> state\n\n', '(score raised from 6 to 7 after the rebuttal)\nThe paper explores the application of the rate-distortion framework to policy learning in the reinforcement learning setting. In particular, a policy that maps from states to actions is considered an information theoretic channel of limited capacity. This viewpoint provides an interesting angle which allows modeling/learning of (computationally) bounded-rational policies. While capacity-limitation might intuitively seem to be a disadvantage, intriguing arguments (based on solid theoretical foundations, rooted in first principles) can be made in favor of capacity-limited systems. Two of the main-arguments are that capacity-limited policies should be faster to learn and be more robust, i.e. generalize better. After thoroughly introducing these arguments on a less formal level and putting them into perspective with regard to reinforcement learning and related work in the literature, the paper demonstrates these properties in a toy grid-world example. When compared against a vanilla actor-critic (AC) algorithm, the capacity-limited version is shown to converge faster and reach better final policies. The paper then extends the basic version of the algorithm, which requires knowledge of the optimal value function, towards simultaneously learning the value function. While any theoretical guarantees are lost, the empirical results are still in line with the theoretical benefits, outperforming vanilla AC and producing better results in previously unencountered variations of the grid-world environment.\n\nThe paper is very well written and the toy-examples illustrate the theoretical advantages in a very nice and intuitively understandable way. The topic of modeling capacity-limited RL agents and exploring how capacity-limitation is an advantage, rather than a “bug” is very timely and important. In particular, rate-distortion theory might provide key-insights into building agents that generalize well, which is among the major open problems in reinforcement learning. The paper is thus very timely and highly relevant to a broad audience. \n\nThe main weakness of the paper is that it is of quite limited novelty and that the brute-force approach towards using Blahut-Arimoto in RL is unlikely to scale to large, complex state-/action-spaces without major additional work. Continuous state-/action-spaces are in principle covered by the theory, but they come with additional caveats and subtleties (I appreciate the authors using discrete notation with sums instead of integrals). Additionally, when simultaneously learning the value function (in the online setting), any guarantees about Blahut-Arimoto convergence are lost. However, solving either of these issues is hard and many attempts have been made in the communications community. Despite these weaknesses I argue for accepting and presenting the paper at the conference for the following reasons: \n- modelling capacity-limited agents via ideas from rate-distortion theory (which is very closely related to free-energy optimization, such as ELBO maximization, Bayesian inference and the MDL principle) is an underrated topic in reinforcement learning. On a conceptual level, the strong idea is that moving away from strict optimization and infinite-capacity systems is not a shortcoming but can actually help building agents that perform better and generalize better. This is not a well established idea in the community. The paper does a good job at introducing the general idea, illustrating it intuitively with toy examples and pointing out relevant literature.\n- Simultaneously learning the value function is necessary in the RL setting, but breaks quite a bit of the theory. However, very similar ideas seem to work quite well empirically in other settings, such as for instance ELBO maximization in VAEs, where the “value function” is the log-likelihood (under the decoder), which is learned simultaneously while learning a “policy” (the encoder) under capacity limitation (the KL term). Similar arguments can be made for modern InfoBottleneck-style objectives in deep learning. Based on this empirical observation, it is not unlikely that simultaneous learning of the value function works reasonably well without catastrophically collapsing in other settings and tasks. \n- While achieving a solution that strictly lies on the rate-distortion curve might be crucial in communications, it might be of lesser significance for building RL agents that generalize well - slight sub-optimalities (solutions that lie off the RD curve) should still yield interesting agents. Therefore, losing theoretical guarantees might be less severe for simply exploring how much the idea can be scaled up empirically.\n\nMinor issues:\n1) While the paper, strictly speaking, introduces a novel algorithm and the Bellman loss function (which requires knowledge of the optimal value function), I think that the main contribution is a clear and well-focused introduction of rate-distortion theory in the context of RL, including very illustrative toy examples. I do consider this an important contribution.\n\n2) Transfer to novel environments. The final example (Fig. 4) does show that the capacity limited agent performs better in novel environments. However, I’m not entirely convinced that this demonstrates “superior transfer to novel environments” (from the abstract). While the latter might very well arise from capacity-limitations, I think that in the example in the paper there is not too much transfer going on, but the capacity-limited agent simply has a more stochastic policy which helps if unknown walls are in the way. After all, the average accumulated reward of the capacity limited agent does also decrease significantly in the novel environment - it simply does a slightly better random walk than the AC (correct me if I’m wrong, of course). On page 7, last paragraph this is phrased as: “agents retain knowledge of exploratory actions”. In my opinion this wording is a bit too strong to simply describe increased stochasticity.\n\n3) Since the paper does provide a good overview over the literature, I think it would help to mention that the current main approach towards generalizing (deep) RL is via hierarchical RL (options framework, etc) and provide a good reference.\n\n4) At the very end of the intro you might also want to mention that rate-distortion has been used before in the context of decision-making (not RL), for instance under the term rational inattention.\n\n5) Page 5, last paragraph: the paper mentions that one Blahut-Arimoto iteration is enough. This is an empirical observation, justified by the toy experiments. However, the wording sounds like this is a generally known fact. Please rephrase to emphasize that this must not necessarily hold true in general and that convergence behavior might crucially depend on this.\n\n6) It would be good to give readers some guidance towards choosing beta if doing an exhaustive grid-search is infeasible. I am aware that there is no good general rule or recipe, but perhaps something can be added to the discussion (even if it is just mentioning that there is no good heuristic, etc. - however, there should be plenty of research in communications that deals with estimating the RD curve from as few points as possible). \n\n7) Please consider adding this reference - it has a very similar objective function (but for navigating towards multiple goals) and is very much in line with some of the theoretical arguments.\nInformational Constraints-Driven Organization in Goal-Directed Behavior - Van Dijk, Polani, 2013.\n', '## Summary\n\nThe authors identify a synergy between the rate distortion (RD) and reinforcement learning (RL) literature. RD work shows how to optimise resources when capacity is limited and the authors transfer this idea to RL and posit a novel algorithm based on the Actor critic algorithm. In experiments this is shown to learn more quickly and transfer between similar tasks more easiliy than the conventional AC algorithm.\n\nThis is a genuinely inciteful piece of work and may be of very significant interest to the community. Particuarly to those in transfer learning, heirarchical learning and other areas of RL where an adaptable rate limited policy is an advantage. \n\nThe experiments are limited to a single domain, and ideally this would be demonstrated across more than just those examples explored. However, I think that the value of the theoretical advance, and the clarity/readability of the paper warrants acceptance.']","[-20, 80, 90]","[60, 90, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty and importance of the work, they express a 'strong technical concern' about convergence issues and request significant additional empirical demonstrations to make the paper suitable for publication. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions for improvement. They use phrases like 'The ideas in the paper are very well described' and 'In order to improve this paper' which maintain a collegial tone. The reviewer also discloses their previous involvement with the paper, demonstrating transparency and professionalism."", ""The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, praising it as 'very well written' with 'very nice and intuitively understandable' examples. They argue for accepting the paper despite some weaknesses, highlighting its timeliness and relevance. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone. The reviewer balances praise with gentle criticism, using phrases like 'I appreciate' and 'Please consider'. They offer suggestions diplomatically and acknowledge the difficulty of addressing certain issues. The review maintains a professional and courteous tone throughout, even when pointing out limitations."", ""The sentiment score is 90 (highly positive) because the reviewer describes the work as 'genuinely inciteful' and 'of very significant interest to the community'. They also recommend acceptance despite limited experiments, indicating strong approval. The politeness score is 80 (quite polite) as the reviewer uses respectful language throughout, acknowledging the value of the work and providing constructive feedback. They balance praise with gentle suggestions for improvement, maintaining a professional and courteous tone.""]"
"['This article compares different methods to train a two-layer spiking neural network (SNN) in a bio-plausible way on the MNIST dataset, showing that fixed localized random connections that form the hidden layer, in combination with a supervised local learning rule on the output layer can achieve close to state-of-the-art accuracy compared to other SNN architectures. The authors investigate three methods to train the first layer in an unsupervised way: principal component analysis (PCA) on the rates, sparse coding of activations, and fixed random local receptive fields.  Each of the methods is evaluated on the one hand in a time-stepped simulator, using LIF neurons and on the other hand using a rate-approximated model which allows for faster simulations. Results are compared between each other and as reference with  standard backpropagation and feedback alignment.  The main finding is that localized random projections outperform other unsupervised ways of computing first layer features, and with many hidden neurons approaches backpropagation results. These results are summarized in Table 8, which compares results of the paper and other state-of-the-art and bio-plausible SNNs. PCA and sparse coding work worse on MNIST than local random projections, regardless if the network is rate-based, spike-based or a regular ANN trained with the delta rule. Feedback Alignment, although only meant for comparison, performs best of the algorithms investigated in this paper.\n\nIn general the question how to train multi-layer spiking neural networks in a bio-plausible way is very relevant for computational neuroscience, and has attracted some attention from the machine learning community in recent years (e.g. Bengio et al. 2015, Scellier & Bengio 2016, Sacramento et al. 2018). It is therefore a suitable topic for ICLR. Of course the good performance of single-layer random projections is not surprising, because it is essentially the idea of the Extreme Learning Machine, and this concept has been well studied also for neuromorphic approaches (e.g. Yao & Basu, 2017), and versions with local receptive fields exist as well (Huang et al. 2015 ""Local Receptive Fields Based Extreme Learning Machine""). While the comparison of different unsupervised methods on MNIST is somehow interesting, it fails to show any deeper insights because MNIST is a particularly simple task, and already the CIFAR 10 results are far away from the state-of-the-art (which is >96% using CNNs). Another interesting comparison that is missing is with clustering weights, which has shown good performance for CNNs e.g. in (Coates & Ng, 2012) or (Dundar et al. 2015), and is also unsupervised.\n\nThe motivation is not 100% clear because the first experiment uses spikes, and shows a non-negligible difference to rate models (the authors claim it\'s almost the same, but for MNIST differences of 0.5% are significant). All later results are purely about rate models. The authors apparently did not explore e.g. conversion techniques as in (Diehl et al. 2015) to make the spiking results match the rate versions better e.g. by weight normalization.\n\nI would rate the significance to the SNN community as average, and to the entire ICLR community as low. The significance would be higher if it was shown that this method scales to deeper networks or at least can be utilized in deeper architectures. Scrutinizing the possibilitites with random projections on the other hand could lead to more interesting results. But the best results here are obtained with 5000 neurons with 10x10 receptive fields on images of size 28x28, thus the representation is more than overcomplete, and of higher complexity than a convolution layer with 3x3 kernels and many input maps.\n\nBecause the results provide only limited insights beyond MNIST I can therefore not support acceptance at ICLR.\n\nPros:\n+ interesting comparison of unsupervised feature learning techniques\n+ interesting topic of bio-plausible deep learning\n\nCons:\n- only MNIST, no indications if method will scale\n- results are not better than state-of-the-art\n\n\nMinor comments:\n\nThe paper is generally well-written and structured, although some of the design choices could have been explained in more detail. Generally, it is not discussed if random connections have any advantage over other spiking models in terms of accuracy, efficiency or speed, besides the obvious fact that one does not have to train this layer. \n\nThe title is a bit confusing. While it\'s not wrong, I had to read it multiple times to understand what was meant.\n\nThe first sentence in the caption for Fig. 2 is also confusing, mixing the descriptions of panel A and B. Also, in A membrane potentials are shown, but the post-membrane potential seems to integrate a constant current instead of individual spikes. Is this already the rate approximation of Eq. 2? Or is it because of the statement in the caption that they both receive very high external inputs. In general, the figures in panel A and B do not make the dynamics of the network or the supervised STDP much clearer. \n\nPrincipal Component Analysis and Sparse Coding are done algorithmically instead of using a sort of nonlinear Hebbian Learning as in Lillicrap 2016. It would have been interesting to see if this changes the comparatively bad results for PCA and SC.\n\nIn Fig. 3, the curve in the random projections case is not saturated, maybe it would have been interesting to go above n_h = 5000. As there are 784 input neurons, a convolutional neural network with 7 filter banks already would have around 5000 neurons, but in this case each filter would be convolved over the whole image, while with random projections the filter only exists locally. \n\nIn Eq. 1, the notation is a bit ambigous: The first delta-function seems to be the Dirac-delta for continuous t, while the second delta is a Kronecker-delta with discrete t.\n\nIn A.1 and A.4.2 it is stated that the output of a layer is u_{t+1} = W u_t + b but I think in both cases it should be W a_t + b where a_t = phi(u_t). Otherwise, you just have a linear model and no activations. \n\nIn Table 3, a typo: ""eq. equation""   ', 'In this work authors benchmark a biologically plausible network architecture for image classification. The employed architecture consists of one hidden layer, where input to hidden layer weights W1 are either trained with PCA or sparse coding, or are kept fixed after random initialization. The output layer units are modeled as leaky integrate-and-fire (LIF) neurons and hidden to output connections W2 are tuned using a rate model that mimics STDP learning dynamics in the LIF neurons. The authors compare classification results on MNIST and CIFAR10 datasets, where they also include results of an equivalent feed-forward network that is trained with standard error backpropagation.\n\nThe authors find that in the bio-plausible network with a large hidden layer, unsupervised training of input to hidden layer weights does not lead to as good of a classification performance as achieved through fixed random projections. They furthermore find that localized patch-style connectivity from input to hidden layer further improves the classification performance.\n\nOverall the paper is well-written and easy to follow, but I fail to see any significant contribution in this work. As compared to the findings of Hubel & Wiesel, how bio-plausible are random projections for low-level feature representation? One may also argue that unsupervised tuning of W1 may require a lot more training data than available in MNIST or CIFAR10. The authors also need to take the capacity of their network into account; they draw conclusions based on a biologically-plausible network, but one that only has two feed-forward layers. It is hard to imagine that a more accurate biologically-plausible vision model would prefer random projections over low-level feature extractors that are well-tuned to the input statistics.\n\nRegarding the observation that localized fields perform better than densely connected layers, I find it simply in line with physiological findings (starting from the work of Hubel & Wiesel) and artificial neural network architectures they inspired like CNNs.', ""Summary:\nThe authors propose a benchmark of biologically plausible ANNs on the MNIST dataset with an emphasis on local learning rules (ruling out backpropagation, and enforcing small receptive fields). They find that random projection (RP) networks provide good performance close to backpropagation and outperform other local learning rules based on unsupervised learning.\n\n\n\nEvaluation:\nA well-executed work but with major limitations: it is based mostly on MNIST, analysis of spiking network is limited, and deep biologically plausible learning rules are not investigated.\n\nDetailed comments:\n\nWhile the paper reads well, choosing how to evaluate the contribution for such benchmark paper is a bit difficult, as the novelty is by definition more limited than in papers proposing a new approach.\nIn the following I chose to focus on what information such benchmark may bring to the field for addressing to challenges ahead.\n\n1.\tStrengths\nThe authors made the effort of implementing several biologically plausible learning rules, including Feedback alignment, and sparse coding. In particular, the idea of using local unsupervised learning rules as baselines for learning the hidden layer is a good idea to extend the range of tested algorithms.\n\n2.\t“Easy” dataset\nIt is unclear to me in which way MNIST result can help evaluate the next challenges in the field. While it is good to know that simple algorithms can achieve close to state of the art, I am not sure this is enough for a paper submitted in 2018. Ideally, most of the analysis could be reproduced at least for CIFAR10 (as the authors started to do in table 2).\n\n3.\tLimited architectures\nMost of the analysis is restricted to one single layer. However, biologically plausible algorithms have also been proposed that can in principle apply to multiple layers. In addition to feedback alignment (implemented in the manuscript in the single hidden layer case), you can find relatively simple approaches in the literature, for example\n“Balduzzi, David, Hastagiri Vanchinathan, and Joachim M. Buhmann. Kickback Cuts Backprop's Red-Tape: Biologically Plausible Credit Assignment in Neural Networks. AAAI. 2015.” Given the dominant view that depth is key for learning challenging datasets, not exploring this option at all in a benchmark seems a significant weakness.\n\n4.\tSpiking networks\nWhile the authors seem to emphasize spiking as an important aspect of biological plausibility (by using LIF neurons and STDP). The challenges of such approaches seem to be largely unaddressed and the main take home message is a performance similar to the corresponding rate models. It would be very interesting, for example, to see how many spikes (or spikes per neurons) are need per example to achieve a robust classification.\n\n5.\tOverall objective behind biological plausibility\nExtending the previous point, the results are to some extent limited to accuracy. If one wishes to achieve biological plausibility, more aspect can be taken into consideration. For example:\n-\tDuring test: the average number of activated neurons, the average number of activated synapses. \n-\tDuring training: the overall number of activations needed to train the algorithm.\nIn relation to these consideration, a more concrete discussion about the potential benefits of biological plausibility would be helpful.\n""]","[-40, -20, -20]","[60, 50, 60]","[""The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('interesting comparison', 'interesting topic'), they ultimately do not support acceptance and point out several limitations ('only MNIST', 'results are not better than state-of-the-art'). The overall tone is more negative than positive. The politeness score is 60 because the reviewer uses respectful language throughout, acknowledges positive aspects, and provides constructive feedback. They avoid harsh criticism and use phrases like 'I would rate' and 'I can therefore not support' rather than more direct negative statements. The reviewer also provides detailed explanations for their assessment, which is a polite way to deliver criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is well-written and easy to follow, they express skepticism about the significance of the work's contribution. The reviewer questions the biological plausibility of the findings and suggests that the conclusions may be limited due to the network's simplicity and the datasets used. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, offering constructive criticism and suggestions without using harsh language. They acknowledge positive aspects of the paper before presenting their concerns, which contributes to the overall polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper ('well-executed work', 'good idea to extend the range of tested algorithms'), they also point out 'major limitations' and several areas for improvement. The overall tone suggests the paper falls short in several important aspects.\n\nThe politeness score is moderately positive (60) as the reviewer maintains a professional and constructive tone throughout. They use phrases like 'it is unclear to me' and 'it would be very interesting' rather than harsh criticisms. The reviewer also balances critiques with acknowledgments of the paper's strengths.\n\nThe language is consistently respectful and aimed at improving the work rather than dismissing it, even when pointing out significant weaknesses. The reviewer offers specific suggestions for improvement, which is a polite way to address shortcomings in academic writing.""]"
"['The paper addresses the problem of increasing and decreasing the number of hidden nodes (aka, dimensionality) in the network such that the optimization will not enter the plateaus of saddle points. The opening or closing of tunnels (filters) guarantee the existence of “new escape directions” and faster convergence. \n\nStrengths:\n+ provide a new perspective of designing the shape/dimensionality of a network in a dynamic manner. \n+ provide theoretic proof of CNNs and FCs on the contribution to the gradient after cloning. \n\nWeakness:\n- Experiments are very weak to verify the theory.\n\nDetailed comments:\n\n- Eqn. (6) seems to provide a unified evaluation on the contribution of two units to the gradients. How does it relate to the experiments? It gives me a sense that the manuscript is isolated between theory (Section 2 and 3) and verification (experiments).\n- Why does the blue curve get stuck in a flat area? A better staring learning rate could alleviate the plateau bottleneck. \n- The experiment settings are a little bit simple, even for the most complicated one in Section 4.4, where there are five conv layers and the tunnel opening only involves one single filter. Do authors conduct more filters opening in more layers? How about the closing case? There is no result/analysis in the experiments. \n- Why authors claim the blue curve in the left figure 2, a “flat area”? It seems working as the orange one (loss decreases normally). \n- Another big concern is that the proposed method is supposed to prevent network from saddle points and faster convergence, which is verified. And yet, the ultimate goal is to improve the performance. I am surprised that there is no such result at all in the manuscript (for example, error rate goes down on cifar/mnist/etc). \n\nIn summary, I do recognize the theoretical effort the paper has provided; however, the experiments seem not to verify the proposed method in a professional manner. \n\n', 'Summary:\n\nThis paper presents a new strategy for escaping saddle points by adding and removing hidden units during training. The method essentially finds conditions where adding a hidden unit does not change the overall input-output map, and uses these as constraints to add a hidden unit that maximally increases the gradient norm (thus potentially getting learning unstuck). Experiments show that the method can improve training speed relative to the same network with randomly added new units.\n\nMajor comments:\n\nThis paper presents interesting theoretical ideas and clearly separates the opportunities for adding/removing hidden units without changing the input-output map from the impact on the gradient due to the change in parametrization. \n\nThe experiments show that the proposed method can speed up learning when a network is genuinely stuck at a saddle point. Importantly however, the experimental evaluation intentionally seeks saddle points using Newton’s method, such that learning is genuinely stuck, before adding the additional units. It is therefore less clear whether this method can offer speedups to network training in practice. Do NNs come close enough to saddle points to benefit from the method when beginning from typical initializations? Experiments on ImageNet begin from a specifically chosen random seed that happens to enter a very flat region. How many random seeds were tried before finding this one? This would speak to the importance of these findings in general. The paper would benefit greatly from applying the proposed method to networks trained under standard protocols, to identify the speed up (if any) it can confer for the average case. It would also be important to account for wall clock time, as the proposed method involves potentially expensive steps (at least in its straightforward form).\n\nThe paper notes several other strategies for expelling from saddle points. The experimental evaluation could be improved greatly by including comparisons to these alternatives. Does the proposed method escape more quickly, or have other merits relative to these alternatives?\n\nThe clarity of the paper is good overall but the title could be improved to be more informative of the content of the method.\n\nOverall the significance of the paper is not clearly established because the evaluations mostly consist of internal comparisons, in somewhat unnatural settings (where networks are initialized right next to saddle points). The theoretical observations, however, seem promising.\n\nMinor comments:\n\nI could not understand the motivation for closing the tunnel—it seems as though optimization proceeds more quickly if it remains open. \n\nThe paper discusses a range of relevant work but would benefit from citing other incremental learning work in neural networks, in particular:\n\nY. Bengio et al., Convex Neural Networks, NIPS 2006\n\nF. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR 2017\n', 'This is an interesting submission and I appreciate especially connecting to a body of literature which is not normally well known in our community (e.g. Fukumizu&Amari). I think the perspective is definitely new and probably quite relevant not only for practical approaches to escape saddles but also to understand learning in deep learning.  I have a few notes and suggestions:\n\n1) Name of the paper: \nI think is not descriptive of the approach and actually the words “magic” makes it sound strange. I think this will reduce the amount of people reading the work. Please consider something more descriptive like: “Escaping saddle points by increasing capacity”. Or something else more inspired, but that also hints what the work is about. \n\n2) Notation: \nThe notation used is not ML friendly (or generically) to the average reader. I strongly suggest to use b_v for bias, W_uv for weights, and not theta_v and theta_uv which is not typical notation. ‘u’ and ‘v’ are somewhat non-typical choices either, though I understand that they come from the graph notation. Transfer functions are usual sigma. In the text you explain the process by starting with a u’ and then add the clone which is u. Normally you should have started with u and add the clone that is u’. x_u for the value of unit u (assuming this is in the middle of a deep net) is also quite a strange notation. I can guess the authors might be from a slightly different community, but I’m worried about people from the target audience (ICLR) being turned away from the work or even worse confused because of notations.\n\n3) Related work\nThere is the Net2Net work that is related to what is going on here that is not cited (https://arxiv.org/pdf/1511.05641.pdf). I think there was some follow-up work after this.\n\n4) Symmetry breaking\nI do not understand how symmetry is broken. If I clone a unit, and have a new variant of it u’ that now has the same outbound connections but multiplied by alpha (while the original unit by 1-alpha) then while the norm of the gradients differ, their direction does not. Wouldn’t this mean that the units will track each other and hence no tunnel is open? In Net2Net dropout was used to break symmetry (i.e. a source of noise that would pick one path over the other). There is no source of noise here to break symmetry. \n\n5) Diagrams and analysis\nConnected to this, I feel like this could have been represented clearly with a diagram showing the net before and after. There could be some analysis, a more extended discussion of where the symmetry breaking comes from, empirical evidence that it does. I’m not necessarily worried that experiments are not scaled up, I’m more concerned that the hypothesis and solution is only tested by means of change in performance. What is this tunnel doing? How does it change the Hessian at the saddle? Any visualization to reinforce the intuition of what the approach is doing? \n\n\n6) Closing tunnels & re-organizing\nI don’t understand the mechanism for reorganizing weights and closing tunnels. It seems first of all to confirm my intuition that there is no symmetry breaking since we can “close” the tunnel by simple algebraic manipulation. So if those two units always stay in sync how do you actually change the error surface? How do you take advantage of this extra capacity to solve anything. Regardless, when it comes to re-organizing, it seems you pick two of these units that are in sync (previous tunnel I guess) and collapse them to open a new tunnel, right? How does this change anything? Which unit needs to be cloned? Any? So then why is the previous tunnel not efficient anymore? \n']","[-20, 20, 50]","[50, 60, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the experimental validation. The review starts positively by mentioning the paper's strengths, but then lists several weaknesses and detailed criticisms, concluding that the experiments do not verify the proposed method adequately. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They acknowledge the theoretical effort and use phrases like 'I do recognize' and 'I am surprised' rather than using harsh or dismissive language. The reviewer also provides constructive feedback and specific suggestions for improvement, which is a polite approach to criticism."", ""Sentiment Score (20): The review begins with a positive tone, acknowledging the 'interesting theoretical ideas' and clear presentation. However, it also expresses significant concerns about the practical applicability and experimental design. The balance of positive and negative comments, with a slight lean towards positive, justifies a mildly positive score of 20.\n\nPoliteness Score (60): The reviewer maintains a professional and respectful tone throughout. They use phrases like 'The paper would benefit greatly from...' and 'The clarity of the paper is good overall' which are constructive and polite. Even when expressing concerns, the language remains courteous. The reviewer also offers specific suggestions for improvement, which is a polite way to provide criticism. The absence of harsh or dismissive language contributes to the moderately high politeness score of 60."", ""The sentiment score is 50 (slightly positive) because the reviewer starts by calling the submission 'interesting' and appreciates certain aspects of the work. However, they also raise several concerns and suggestions for improvement, balancing out the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms constructively (e.g., 'I think', 'Please consider', 'I suggest'), and acknowledges the potential value of the work. The reviewer also explains their concerns in detail, showing engagement with the paper. The use of phrases like 'I appreciate' and 'I'm not necessarily worried' further contribute to the polite tone.""]"
"['This work builds on the ICML paper from Saxe et al (2017) in which the compositionality property of LMDPs was exploited to solve multi-task hierarchies. The paper extends this work by proposing a method that learns incrementally such hierarchies instead of pre-defining them by design. Some experimental results illustrate the method on two toy problems, a 1D corridor and a corridor of rooms.\n\nThe paper deals with an interesting and hard problem. Learning hierarchies while solving an MDP is a much harder problem than solving the flat MDP or solving the hierarchical MDP. The authors leverage the compositionality of optimal controls of the LMDP framework to learn incrementally the hierarchies. Surprisingly, the proposed method not only learns those hierarchies, but also is more effective in terms of exploration.\n\nOn the positive side, the main idea is very interesting and has a lot of potential. The authors combine many techniques under the powerful framework of LMDPs such as hierarchical RL, low-rank factorization, and count-based exploration. The authors do a good job describing their approach (at a higher level).\n\nOn the negative side, the paper looks a bit incremental, given the prior existing work. I also found the paper unclear in many aspects lacking some relevant technical details (see below). The narrative is sometimes superficial or focused mainly on intuitions and analogies. Overall, it is difficult to assess the significance of this work and the results give the impression of limited applicability, beyond the described toy problem.\n\n1- First, in order to combine optimal controls, the LMDPs need to be solved for each different boundary state, i.e., do you require to solve as many LMDPs as possible states? If that is the case, I don\'t think it makes sens to talk about exploration/exploration trade-off, since you really need to visit all states a priori.\n\n2- I cannot understand what is learned and what is required a priori. the authors state that if ""the multi-scale structure of the domain is known a priori, the decomposition (...) explicitly specified"". What does exactly that mean? If what the method does is an incremental version of the low-rank factorization proposed in Earle at al (2018), I think the presentation can be better described in those terms.\n\n2- Regarding exploration/exploitation tradeoff. From section 3, it seems that authors focus on a particular spatial problem and define already some exploration choices. But this means that the choice about when a state is integrated in the current MDP is already done, so no real trade-off exists?\n\n3- The narrative in Section 3.2 is not very rigorous. The authors just mention the computational problem to keep consistency between layers and then just argue that ""in practice"" using count-based exploration everything works. I think a more principle approach is necessary.\n\n4- Experiments I: what do the authors means by ""exploration""? Is it just Boltzmann exploration? I can think of an exploration strategy that would choose an unseen state with probability 1 and would bring the agent to the goal in one shot.\n\n5- Experiments II: I like the benchmark but, how does the result depend on the structure of the problem? What happens if I the rooms have very different sizes?\n\n6- I miss some references that are very relevant to this work:\n\n- the ICAPS paper ""Hierarchical Linearly-Solvable Markov Decision Problems"" by Jonson et al. seems to be the first proposing a hierarchical embedding of LMDPs.\n\n- other factorization techniques exist, e.g., ""Incremental Stochastic Factorization for Online Reinforcement Learning"", Barreto et al (AAAI\' 16), to uncover an MDP structure.\n\nThere are also some minor grammar mistakes:\n\n""passive dynamics then become"" -> ""passive dynamics then becomes""\n""reward function r"" -> ""reward function R""\n""Howver"" -> ""However""\n""the room contains"" -> ""the room that contains""\n""have simply add"" -> ""have simply added""\n""spacial and temporal"" -> ""spatial and temporal""\n...', 'pros:\nThe experiment results demonstrate better performance of the proposed method.\nCons: \n1. It does not compare with any existing HRL (with simple adaptation to multitask settings), except for Q learning under different exploration strategies. \n\n2. Also the method is tested only on one type of domain-gridworld, which seems very limited, especially for supporting the claims, including “the proposed method can uncover a deep hierarchical abstraction of task ensemble in a complete online fashion” and “the process of learning the hierarchical structure online does not significantly slow learning”\n\n3. There is no implementation details of the proposed in a algorithmic form.\n\n4. There is also no ablation test or discussion of parameters that affect the performance of the experiment results, for example, learning rate, size of the domain. \n\n5. There are a lot of notations used without definition. The paper should be more self-contained by providing accurate definition and citations.\n\nFor example, what are are Interior state,  boundary state, finite exit problem\nIs V(s) computed with based on R_i or R, or both?\nWhat is the control cost refer to? Is it related to R_i or R somehow?\n\n6. Some references are cited wrongly. \nMachado et al (2018) cited is not about HRL. Please double check\nThe references should be updated to reflect the latest information, for example,\nIt might be better to cite the AAAI17 version of the option-critic architectures \nBetween MDPs and Semi-MDP: A framework for Temporal Abstraction in Reinforcement learning should use the 1999 version \n\n \n7. Minor issues:\ntask it is -> task is\nZ^l \\in R^{N\\times N}\n Howver -> however\n', 'Major comments:\n\nThis paper builds on previous work in hierarchical LMDPs and extends the core ideas to an online setting.  Essentially, we incrementally construct a hierarchy by adding new states to upper-level MDPs every once in a while; these are loosely initialized and the parameters are then refined with additional experience.\n\nOverall, I felt that this paper lacked a substantial enough contribution. \n\n* The key contributions over previous work seems to be entirely contained in Sec. 3.1 and 3.2: (1) when we have visited k new states, add a new state to the hierarchy, and (2) initialize its parameters with intuitive values.\n\n* To me, this level of contribution is below the bar for ICLR.  The ideas seem simplistic and likely to work only in the simplest of domains.\n\n* The improvement over previous work is marginal.\n\n* I think the paper is lacking in clarity.  I do not think I could re-implement the paper, given the level of detail presented.\n\n* I was very disappointed in the experiments.  Not only were they on gridworld-like domains (see below), but it was not clear if the improvements were significant in any way.\n\n* While I thought the discussion in Sec. 3.3 was interesting, it didn\'t seem to be a ""contribution""; it felt like some adhoc thoughts.\n\n* Keeping per-state counts is only workable in small state space domains.\n\nMinor comments:\n\nPlease fix the formatting of your citations.\n\nThere are numerous typos and spelling errors.  Please correct them.\n\nI am strongly opposed to the use of gridworlds, or anything like them, in modern RL research.  While many ideas work fine in small, toy domains, they simply do not scale.  As a field, we need to move past them and focus more on algorithms and ideas that have more practical relevance.\n\nPros:\n+ Core ideas seem promising\n+ Leveraging mathematical structure is a great strategy for constructing algorithms with desirable properties\n\nCons:\n- Very limited experimental results\n- Not clear if improvements are significant\n- Hierarchy construction seems to be too limited to work in any reasonably sized problem\n- No evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method\n\n']","[20, -60, -70]","[60, 20, -20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting nature of the problem and the potential of the main idea. They mention several positive aspects, such as the combination of techniques and good high-level description. However, they also express concerns about the paper being incremental and unclear in some aspects, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, balancing criticism with praise. They use phrases like 'On the positive side' and 'On the negative side' to structure their feedback, and their questions and suggestions are framed constructively. The reviewer also points out minor grammar mistakes politely at the end. While not overly effusive, the tone remains professional and courteous throughout."", ""The sentiment score is -60 because the review is predominantly negative. While it acknowledges one positive aspect (better performance of the proposed method), it lists seven major criticisms, including limited comparisons, restricted testing domain, lack of implementation details, absence of ablation tests, undefined notations, incorrect citations, and minor issues. This indicates a significantly negative overall sentiment. The politeness score is 20 because the language used is generally neutral and professional. The reviewer presents criticisms in a straightforward manner without using harsh or rude language. The use of phrases like 'It might be better to' and the objective listing of issues contribute to a slightly positive politeness score. However, the lack of any explicitly polite language or praise beyond the initial positive comment prevents a higher score."", ""The sentiment score is -70 because the reviewer expresses strong negative opinions about the paper's contribution, clarity, and experimental results. They state that the paper 'lacked a substantial enough contribution,' the ideas seem 'simplistic,' and they were 'very disappointed in the experiments.' The few positive comments ('Core ideas seem promising') are outweighed by the numerous criticisms. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism ('I am strongly opposed to the use of gridworlds') and dismissive language ('this level of contribution is below the bar for ICLR'). The reviewer doesn't use overtly rude language, but the tone is more critical and less constructive than would be ideal for a peer review, hence the slightly negative politeness score.""]"
"['The paper proposes contextual role representation which is an interesting point. \nThe writing is clear and the idea is original.\nEven with the interesting point, however, the performance improvement seems not enough compared to the baseline. The baseline might be carefully tuned as the authors said, but the proposed representation is supposed to improve the performance on top of the baseline.\nThe interpretation of the role representation is pros of the proposed model. However, it is somehow arguable, since it is subjective. \n\n- minor issues: \nThere are typos in the notations right before Eq. (8). \n', '\n[Summary]\nThis paper proposes “a role interaction layer” (briefly, RIL) that consists of context-dependent (latent) role assignments and role-specific transformations: Given an RIL layer, different dimensions of an embedding vector are “interacted” based on Eqn. (5), Eqn. (7), etc. The authors work on IWSLT De->En and WMT En->De, En->Fi to verify their proposed algorithm with case study included. \n\n[Pros]\n(+) I think the idea/thought of using a “role interaction layer” is interesting.  The case study in Section 5.3 demonstrates different “roles”. Also, different RIL architectures are designed.\n(+) The paper is easy to follow.\n\n[Cons & Details]\n(1) As stated in the abstract, “…, but that the improvement diminishes as the size of data grows, indicating that powerful neural MT systems are capable of implicitly modeling role-word interaction by themselves…” (1) The main concern is that, considering RIL does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline. (2) Why the NMT systems trained on large dataset can “implicitly modeling role-word interaction”, while small dataset cannot? Any analysis?\n\n(2) For the “matched baseline”, page 5, you increase the dimensionality of the models. But an RIL is an additional layer, which makes the network deeper. Therefore, a baseline with an additional layer should be implemented. \n', '\n\n[Summary]\nThis paper proposes a “role interaction layer” (RIL) to capture the context-dependent (latent) role for each token.\n\n\n[clarity]\nThe writing is basically clear.\nHowever, It is hard for me to get the motivation and goal of this paper.\nIs the main purpose of the proposed method “improving the performance” or “interpretability”?\n\n\n[originality]\nIt seems that the proposed method consists of several known methods.\nMoreover, even though the purpose differs, technically the proposed method is closely related to the method proposed in [Shu+,2018].\nTherefore, the technical novelty of the proposed method is limited.\n\n[Shu+,2018] Raphael Shu, Hideki Nakayama, “Compressing Word Embeddings via Deep Compositional Code Learning”, ICLR-2018.\n\n\n[significance]\nThe contribution of this paper is not very clear.\nThe improvements from the baseline method (Matched) is less than 1 point BLEU as shown in Table 1 and 2, which is not a significant improvement.\n\n\n\nOverall, this paper is basically well written. However, this paper seems a technical report rather than a conference paper.\n\n']","[-20, -20, -40]","[50, 50, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting points, clear writing, and original idea, they express concerns about the performance improvement and the subjectivity of the interpretation. The phrase 'Even with the interesting point, however, the performance improvement seems not enough' indicates disappointment. The politeness score is moderately positive (50) as the reviewer uses respectful language, acknowledges positive aspects, and frames criticisms constructively. They use phrases like 'interesting point' and 'clear writing' before presenting concerns. The minor issues are presented politely as 'minor issues'. The overall tone is professional and constructive, balancing positive and negative feedback without being overly harsh or effusive."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting idea', 'easy to follow'), they express significant concerns about the method's effectiveness on large datasets and the lack of proper baselines. These concerns outweigh the positives, resulting in a somewhat negative overall sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positives before presenting criticisms, and framing concerns as questions or suggestions rather than harsh criticisms. The reviewer maintains a professional tone without using overly formal or informal language."", ""The sentiment score is -40 because the review expresses several criticisms and doubts about the paper's contribution, motivation, and significance. The reviewer states that the technical novelty is limited, the improvements are not significant, and the paper seems more like a technical report than a conference paper. However, there are some positive comments about the writing being 'basically clear', so the score is not extremely negative. The politeness score is 20 because the language used is generally professional and constructive, avoiding harsh criticism. The reviewer uses phrases like 'It is hard for me to get' and 'It seems that' rather than making blunt negative statements. The reviewer also acknowledges some positive aspects, like the paper being 'basically well written'. However, the politeness is not extremely high as the criticism is still quite direct in places.""]"
"['The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. \n\nThere are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There\'s a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It\'s very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they ""successfully complete the task"" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?\n\nThe primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn\'t feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it\'s not so much that humans are bad localizers but that the model\'s NLU/NLG system is quite weak and maybe there\'s a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.\n\nAs I understand the paper right now:\n1. Humans talking to one another do very well on the task and achieve success very quickly. \n2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).\n3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.\n\nThere are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn\'t seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what\'s going wrong.  The results in this paper, don\'t convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. \n\nI haven\'t touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.\n\nI found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  \n\nMinor: URLs intermittently anonymized page 12 vs 19', 'The paper introduces a new task called ""Talk the Walk"", where a tourist and a guide has to communicate in natural language to reach a common goal. It also introduces strong baselines for the task. The descriptions are thorough and clear. My only worry is that the task is too hard and has too many complexities to be a stand alone task.  Future work will probably focus on sub-parts of the task.', 'The paper introduces a new dataset ""Talk the Walk"" that are dialogs\nbetween a guide and a tourist, where the guide is to help the tourist\nnavigate to a target location.  The guide has access to a map and the\ntarget location, but he relies on the tourist to communicate her\nstate (location) by natural language.\n\nPros.:\n\nThe task represented in the dataset can be highly challenging, and\nrespectable effort went into creating the dataset based on real city\nneighborhoods.   The description and analysis of the dataset are\ndetailed.  The paper is well written up to the end of page 3.\n\nThe dataset by itself is a good contribution to the scientific\ncommunity when it is shared.  There could be many topics open\nfor studying within the same data.\n\nThe list of references and related work is exceptionally thorough\nand useful for researchers interested in the topic.\n\nCons.:\n\nThe description of the experiments done with the dataset, however,\nsuffers from being overly cryptic.  The methods are not sufficiently\nmotivated, very few alternatives are presented and argued against,\nand the several sections give a dry report of the sequences of things\nthe authors did.  It is not clear how others may find value in the\nresults and conclusions.\n\nWhile the paper opens with the emphasis of a real-world setting,\nafter a series of simplifications (e.g. landmark typing, perfect perception)\nit seems that much of the full complexity of the natural task is taken out,\nand the main goal of the study is no longer clear.\nFor example, since there are only 9 types of landmarks,\nin a small neighborhood there are not many combinations to draw reference to.\nSimple observation sequences of such can easily narrow down the location\nuncertainty.  It is important to highlight what the remaining open\nissues are that make the task still challenging.\n\nMisc.:\n\nExamples are missing in the discussion of the experimental tasks.\ne.g. in the study of emergent language, what could be a message that a\ntourist may generate to describe his location?  what makes it hard for\nthe guide to decode it?  Likewise, what could be an example state of\nthe tourist and the description of that state in natural language?\nWithout the examples, it is difficult for the reader to have a sense\nof the challenges in each task.\n\nTable 8 is the first place where (finally) some utterances are presented.\nHowever the description in the table or in the text is not sufficient\nto convey the point that is supposed to be explained by the example.\n\nThe descriptions of the landmarks are restricted to the type of business\nat the location with 9 possibilities.  Is the list of 9 exhaustive?\nAre there any exceptions  (e.g. schools)?  How are such exceptions\nrepresented and treated in the dialogs?\n\nTo what extent the difficulty of the tasks depends on the variability\nof the combination of landmarks visible at each location?\n\nWhat could be a simplest way to do this without neural-modeling?\ne.g. with the many limitations that are built into the task and its\nrepresentations, will a simple decision tree based instruction method suffice?\nOr a traditional algorithm that relies on repeated exploration and evaluation?\nIt is surprising that such possibilities are not even mentioned.\nA complex neural architecture does not seem to be well justified unless\nit is motivated by the need to overcome limitations of a classical method.\n\nIrrelevant to the research effort, a thought about the dataset is that,\nin these days with popular uses of GPS, the reliance on such dialogs for\nnavigation feels a bit backwards.\n']","[-60, 50, -20]","[20, 75, 50]","[""The sentiment score is -60 because the reviewer expresses significant confusion and skepticism about the paper's methodology, results, and claims. They point out several issues and inconsistencies, stating that the paper is 'very confusing to read' and 'fails to clearly explain its contributions or justify its claims.' The reviewer also questions the validity of some of the paper's main conclusions. However, the score is not at the extreme negative end because the reviewer does acknowledge some positive aspects, such as the dataset contribution.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone throughout. They use phrases like 'I found hard to follow/justify' and 'Additional justification and analysis would be appreciated' rather than using harsh or rude language. The reviewer also frames their criticisms as questions or areas needing clarification, which is a polite way to express concerns. However, the score is only slightly positive because the review is still quite direct in its criticisms and does not use overtly polite language or praise."", ""The sentiment score is 50 (slightly positive) because the reviewer praises the paper for introducing a new task with thorough and clear descriptions, and strong baselines. However, they express concern about the task's complexity, which tempers the overall positivity. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before gently expressing their concern. Phrases like 'My only worry is...' indicate a considerate approach to criticism. The reviewer also uses neutral, professional language without any harsh or dismissive terms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'respectable effort', 'well written', 'good contribution'), there are significant criticisms. The reviewer points out several cons, including 'overly cryptic' experiment descriptions, lack of motivation for methods, and insufficient examples. The overall tone suggests the paper needs substantial improvements.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They begin by highlighting the pros before moving to the cons, which is a polite approach. The language used is constructive rather than harsh, with phrases like 'it is important to highlight' and 'it is difficult for the reader to have a sense of'. The reviewer also offers suggestions for improvement, which is a polite way to provide criticism. However, the score is not higher because the review is quite direct in its criticisms, without much softening language.""]"
"['This paper suggests an exploration driven by uncertainty in the reward space.\nIn this way, the agent receives a bonus based on its squared error in reward estimation.\nThe resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.\n\nThere are several things to like about this paper:\n- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it\'s uncertainty in *value* that is important).\n- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.\n- The algorithm does appear to outperform the basic DQN baseline on their experiments.\n\nUnfortunately, there are several places where the paper falls down:\n- The authors wrongly present prior work on efficient exploration as ""exploration in state space"" ... rather than ""reward space""... in fact prior work on provably-efficient exploration is dominated by ""exploration in value space""... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to ""counts"" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853\n\n- So this algorithm falls into a pretty common trope of algorithms of ""exploration bonus"" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)\n\n- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around ""randomized value functions""\n\n- I don\'t think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.\n\nOverall, I do like the idea behind this paper... I just think that it\'s not fully thought through... and that actually there is better prior work in this area.\nIt could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests ""exploration by uncertainty in value space""... e.g. ""deep exploration via randomized value functions""', 'The authors develop a new algorithm for reinforcement learning based on adding another agent rewarded by both the extrinsic environment reward and the TD-errors of the original agent, and use the state-action pairs visited by the added agent as data for the original.\n\nThis co-evolutionary process could be more broadly described as a student agent that learns from the trajectories of a teacher agent, which visits trajectories with high reward and/or high student TD-error.\n\nThe algorithm is not proved to converge to the optimal Q.\nAlgorithm (1) by not using epsilon-greedy on Q_hat\nhas an initialization-based counter-example in the tabular bandit case\ne.g.\n  MDP being a bandit with two arms with rewards 100 and 1000 respectively\n  Q_hat that initially is X for first arm and Y for second arm, with X > 100 > Y\nThis could be solved by, for example, adopting epsilon greedy.\n\nPrioritized Experience Replay (Schaul et al. https://arxiv.org/pdf/1511.05952.pdf), which suggests using the TD-error to change the data distribution for Q-learning, should be also a baseline to evaluate against.\n[Speculative: It feels like a way to make prioritized experience replay that instead of being based on time-steps (state, action, reward, new state) is based on trajectories, and this is done by doubling the number of model parameters.]\n\nOn quality:\nThe evaluation needs different experience replay baselines.\n\nOn clarity/naming:\nHere \'uncertainty in reward space\' is used to refer to TD-error (temporal difference), I found that confusing.\nHere \'intrinsic motivation\' is used but the \'intrinsic motivation\' proposed depends on already having an externally defined reward.\n\nPros:\n+ ""Prioritized Experience Replay for Trajectories with Learning""\nCons:\n- Not evaluated against experience replay methods.\n- No plots showing number of gradient descent steps (as the proposed method has double gradient descent updates than the baselines)\n- No proof of correctness (nor regret bounds).\n', ""This paper considered the idea of accelerating sampling process by exploring uncertainty of rewards. The authors claimed more efficient sampling by building a reference policy and an exploration policy. Algorithm was tested on grids and Atari games.\n\nThe authors proposed eDQN, which is more like exploring the uncertainty of Q values instead of rewards. The reviewer is also expecting to see the convergence guarantee of eDQN.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding.\n""]","[-20, -20, 50]","[60, 50, 75]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('There are several things to like about this paper'), they also point out significant shortcomings ('Unfortunately, there are several places where the paper falls down'). The reviewer criticizes the authors' understanding of prior work and suggests that the experimental results are not particularly illuminating. However, the overall tone is not entirely dismissive, as the reviewer still sees potential in the idea ('I do like the idea behind this paper').\n\nThe politeness score is moderately positive (60) because the reviewer uses respectful language throughout. They begin by highlighting positive aspects before moving on to criticisms. The criticisms are presented as observations rather than personal attacks ('The authors wrongly present...', 'I don't think the experimental results are particularly illuminating...'). The reviewer also acknowledges the possibility of being wrong ('It could be that I am wrong, but...') and offers constructive suggestions for improvement. The language used is professional and academic, avoiding harsh or rude expressions."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also point out several significant issues with the paper. The reviewer mentions problems with the algorithm's convergence, lack of proper evaluation against relevant baselines, and issues with clarity and naming. These criticisms outweigh the positive aspects, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use neutral language to describe the paper's content and issues, and provide specific suggestions for improvement without using harsh or dismissive language. The reviewer also acknowledges positive aspects ('Pros') of the work, which contributes to the polite tone."", ""The sentiment score is 50 (slightly positive) because the review acknowledges the paper's merits, such as being well-organized and easy to understand, while also providing constructive criticism and suggestions for improvement. The reviewer notes the paper's contribution to accelerating sampling processes but also expresses expectations for additional elements like convergence guarantees. The politeness score is 75 (fairly polite) as the reviewer uses neutral and respectful language throughout, avoiding harsh criticism. They acknowledge the paper's strengths and provide suggestions in a constructive manner, using phrases like 'The reviewer is also expecting to see' rather than demanding changes. The mention of written errors is presented gently, noting that they didn't influence understanding.""]"
"[""-- Paper summary --\n\nThe primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration.\n\n-- General Commentary --\n\nThe overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn’t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time?\n\nI have plenty of concerns with the submission itself, listed below:\n\n- First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn’t properly check the paper before submission.\n\n- While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what’s being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least.\n\n- The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (‘uncalibrated’, ‘temp-scal' and ‘BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot.\n\n- As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include ‘Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks’ (Bradshaw et al, 2017), and ‘Calibrating Deep Convolutional Gaussian Processes’ (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models.\u2028\n\n- With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion.\u2028\n\n- I can’t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the ‘alternative hypothesis’ being mentioned, and the investigation is entirely limited to the offline setting, so I’m not entirely sure what distinction the authors are trying to make here.\u2028\n\n- In the same section, the authors then remark that ‘Our results open new perspectives to improve the variational approximation…’ and ‘we believe our results might foster further research in…’, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can’t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers.\u2028\n\n- The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper’s lack of identity and focus.\n\n- There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient.\u2028\n\n- There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by ‘conf’ in Equation 4.\u2028\n\n- Referring to ‘datasets’ as ‘databases’ in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by ‘uses BNNs’? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission.\n\n- In their discussion of the results, the authors state that ‘We cannot conclude that BNNs are calibrating at the cost of losing accuracy’, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score.\n\n-- Recommendation --\n\nUnfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference’s standards.\n\nPros/Cons summary:\n\n+  The proposal yields good results in the provided experiments\n-   Minor contributions that are not convincing enough\n-   Muddled presentation of ideas\n-   Dubious or weakly motivated design choices\n-   Poorly written with plenty of typos\n-   Difficult to follow"", 'This paper proposes the use of Bayesian inference techniques to mitigate the issues of miscalibration of modern Deep and Conv Nets. \n\nThe presentation form of the paper is unsatisfactory. The paper seems to imply that Bayesian Deep Nets are used to calibrate Deep/Conv Nets, so I was expecting something like post-calibration using Bayesian Deep Nets. After reading through the paper a few times, it seems that the Authors are proposing the use of Bayesian inference techniques to infer parameters of Deep/Conv Nets in order to improve their calibration compared to non-Bayesian counterparts. This is the only contribution of the paper, and I believe it is insufficient. Guo et al., (2017) already points out that regularization of modern Deep/Conv nets improves calibration, so the fact that Bayesian Deep/Conv Nets are calibrated is not surprising, giving that the prior over the parameters act as a regularizer. \n\nIt is surprising to see ECE values above one - unless these have been scaled by a factor of 100 - but this is not mentioned anywhere.\n\nPrevious work shows that Monte Carlo Dropout for Conv Nets offers well calibrated predictions (https://arxiv.org/abs/1805.10522), so I think a comparison against this inference method should be included in the paper. \n\nThe paper makes a number of imprecise claims/statements. A few examples:\n\n- ""Bayesian statistics make use of the predictive distribution to infer a random variable by computing the expected value of all the possible likelihood distributions. This is done under the posterior distribution of the likelihood parameters"" - very unclear and imprecise explanation of Bayesian inference\n\n- ""When using neural networks to model the likelihood"" - the likelihood is a function of the labels given model parameters', 'This paper presents an approach for calibrating the predictions of deep neural networks. The idea is quite simple and straightforward - simply use a more expressive model (Bayesian neural network with amortized inference). Surprisingly, the results show that this simple approach outperforms many of the recent approaches, such as those based on temperature scaling. A trick that they use is to control the KL term in the approximation of the ELBO using a hyperparameter (but this has been used in prior work on Bayesian neural nets).\n\nOverall, the idea as such is not that novel (just applying a Bayesian neural network with amortized inference) but the results look quite impressive. However, although the paper seems to advocate that a simple Bayesian neural network is enough to get well-calibrated probabilistic predictions, recent work has shown that even Bayesian uncertainties may be inaccurate, especially in case of model mis-specifications or due to the use of approximate inference. For example, see  ""Accurate Uncertainties for Deep Learning Using Calibrated Regression"" ( Kuleshov et al, 2018). \n\nIt is quite surprising that the proposed approach works so well but there isn\'t much of an insight as to why it works well. Is it the amortized inference that helps, or something else? I think a more detailed analysis needs to be done. Even some empirical analysis that, for example, shows that using MCMC gives inferior results than amortized inference would help here.\n\nBesides, I would also like to point out that there is some recent work on trainable calibration measures. See ""Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings"" (Kumar et al, 2018). It would be good to discuss this.\n\nOverall, the paper has a rather straightforward idea which seems to give good results. However, it doesn\'t offer any new insights as to why it works, especially since recent work, such as the one I mentioned above, has shown that taking a simple Bayesian approach that provides uncertainty estimates doesn\'t quite address the problem being studied here.']","[-80, -70, -20]","[-20, -20, 50]","[""The sentiment is highly negative (-80) based on several factors: The reviewer states the paper's goal is 'modest', the scope is 'limited', and they have 'plenty of concerns'. They use strong negative language like 'disappointed', 'sloppiness', and 'problematic'. The review concludes that the paper is 'neither significant enough nor sufficiently explored' and 'well below the conference's standards'. The politeness score is slightly negative (-20) because while the reviewer maintains a professional tone overall, there are instances of blunt criticism. They describe the paper as 'rushed and incomplete' and mention 'sheer negligence'. However, they do use some softening language like 'I encourage the authors' and acknowledge some positive aspects, which prevents the score from being lower."", ""The sentiment score is -70 because the reviewer expresses significant dissatisfaction with the paper. They describe the presentation as 'unsatisfactory', state that the contribution is 'insufficient', and point out several issues with the paper's claims and comparisons. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite critical and direct. They use phrases like 'unsatisfactory', 'insufficient', and 'imprecise claims/statements' without softening the criticism. The reviewer also doesn't offer many positive comments or constructive suggestions for improvement, which contributes to the slightly impolite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the results are impressive, they express several criticisms and concerns. They note that the idea is not very novel, question why the approach works so well, and suggest that more analysis is needed. The reviewer also points out that recent work has shown limitations of the proposed approach. However, the score is not deeply negative because the reviewer does recognize some positive aspects of the paper.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'It would be good to discuss this' and 'I think a more detailed analysis needs to be done' rather than making harsh demands. The reviewer also acknowledges positive aspects of the work, such as the 'impressive' results, which contributes to the polite tone. However, the score is not extremely high as the review is still critical and doesn't use overtly polite language.""]"
"['This paper proposes a neural network architecture PCnet for the prediction of intensive and extensive chemical properties of molecules and materials. The authors claim that the use of prior chemical knowledge such as Mulliken electronegativity, bond strength and orbital information improves prediction accuracy. While the idea of incorporating chemical domain knowledge in the interactions of an atomistic neural network is interesting in principle, this paper has severe issues ranging from presentation over the proposed approach to the results.\n\nFirst and foremost, I would like to point out that the results in Table 1 are cherry-picked since the authors fail to cite neural network architectures that outperform their approach, e.g. for U0: 0.45 kcal/mol [MPNN, Gilmer et al 2017, ICML], 0.31 kcal/mol [SchNet, Schütt et al, NIPS 30, 2017], 0.26 kcal/mol [HIP-NN, Lubbers et al., JCP 148, 2018]. This is especially apparent since Figure 4 is obviously inspired by Fig. 1 in [SchNet, Schütt et al, JCP 148, 2018]. \n\nThe authors use a variety of heuristics and approximations such as a ""charge transfer ability"", bond strength, exponential decay of distances and overlaps of atomic orbitals which are multiplied ""to mix all things up"", to arrive at the PixelChem representation which is then fed into an atomistic neural network (PCnet). Combining these chemical features in such a way is neither well-motivated, nor does it lead to an improvement in accuracy compared to state-of-the-art networks.\nEven for the intensive properties (gap, HOMO, LUMO), where PCnet is supposed to have an advantage due to its use of orbital information, MPNN, SchNet and even GC and GG-NN in Table 1 outperform the proposed approach. Parameterization of the chemical features and training the PCnet end-to-end might have improved results and seems like a missed opportunity.\n\nFurther issues:\n- The manuscript is riddled with typos, grammatical errors as well as confusing sentences.\n- The authors claim that PCnet is applicable to periodic structures, however, this is never demonstrated. Beyond that their definition of periodic PixelChem does only include adjacent cells, while for a unique representation more cells might be required.\n- The ""benefits"" listed in Section 2.3 compare selectively to previous work. E.g., invariances, uniqueness, asymmetric interactions are also fullfiled by the neural networks listed above. A comparison of the PixelChem representation to the Coulomb matrix is not sufficient here.\n- The PCnet architecture uses PReLU nonlinearities. While this is fine for equilibrium predictions, for other configurations this prohibits the prediction of a smooth PES.\n\nOverall, I believe that it is important to incorporate chemical knowledge into neural networks. However, neither the approach nor the results convince me that this has been achieved here.', 'This paper proposes an architecture for learning representations on molecules. The paper contains a number of typos, and presents results and an entire paragraph that is a near duplicate copy from prior work (which is not cited). In particular Table 1 is a near copy of Table 2 appearing in [1]. \n\nI find it particularly suspicious that the authors have a near copy of this prior table, which reports the ratio of MAE to chemical accuracy, and not MAE directly. The authors have the exact same numbers as this prior Table 2 but claim they are reporting MAE directly. Despite this table being a direct copy, it conveniently omits the columns from [1] which outperform the results presented here.\n\nAlso the paragraph describing this table is nearly identical to the paragraph in [1], that is the authors write\n""These baselines include 5 different hand engineered molecular representations, which then get fed through a standard, off-the-shelf classifier. These input representations include the Coulomb Matrix (CM, Neese (2003)), BoB, Bonds Angles, Machine Learning (BAML, Huang & von Lilienfeld (2016)), Extended Connectivity Fingerprints (ECPF4, Rogers & Hahn (2010)), and Projected Histograms (HDAD, Faber et al. (2017a)) representations. In addition to these hand engineered features we include the Molecular Graph Convolutions model (GC, Kearnes et al. (2016)), the original GG-NN model(Li et al., 2015) trained with distance bins and DTNN.""\n\nIn [1] it was written\n\n""These baselines include 5 different hand engineered molecular representations, which then get fed through a standard, off-the-shelf classifier. These input representations include the Coulomb Matrix (CM, Rupp et al. (2012)), Bag of Bonds (BoB, Hansen et al. (2015)), Bonds Angles, Machine Learning (BAML, Huang & von Lilienfeld (2016)), Extended Connectivity Fingerprints (ECPF4, Rogers & Hahn (2010)), and “Projected Histograms” (HDAD, Faber et al. (2017)) representations. In addition to these hand engineered features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from Kearnes et al. (2016), and the original GG-NN model Li et al. (2016) trained with distance bins.""\n\nThe footnote in the table is also a duplicate copy of the footnote in [1].\n\nTheirs: ""As reported in DTNN. The model was trained on a different train/test split with 100k training samples vs\nabout 110k used in our experiments.""\n\n[1]: ""As reported in Schutt et al. ¨ (2017). The model was trained on a different train/test split with 100k training samples vs 110k used in our experiments.""\n\n\nThe proposed method itself is a variant of the MPNN framework introduced in [1], yet underperforms the original results in [1], as well as improved MPNNs (or GNNs) shown in [2,3]. \n\n1. https://arxiv.org/pdf/1704.01212.pdf\n2. http://papers.nips.cc/paper/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions\n3. https://arxiv.org/pdf/1806.03146.pdf\n\n', 'This paper suggests a new architecture for representing and predicting properties of molecules (""Pixel Chem""). The authors report that this architecture produces better results than previous methods on one of the QM9 dataset properties (U0) but performs worse than many of the other reported methods on other quantities. \n\nOverall, this paper is extremely unclear, contains typos in most sentences, and provides insufficient justification for the both a) the design choices made and b) the claims made wrt the performance of the model. Additionally, at least one critical baseline [1] is missing which outperforms the proposed model. In order to improve this paper, I would suggest the authors do the following: \n\n1) Heavily edit and rewrite the paper focusing on clarity of communication.\n2) Provide justifications for why design choices were made. For example, the authors state that they include a charge and energy matrix to ""mix all the things up."" Why does it make sense to combine these factors in this way? \n3) Provide a more comprehensive evaluation of the model, showing improved performance across more than one target and including appropriate baselines, including [1]. \n\n[1] Gilmer, Justin, et al. ""Neural message passing for quantum chemistry."" arXiv preprint arXiv:1704.01212 (2017).']","[-80, -90, -70]","[-20, -30, -20]","[""The sentiment score is -80 because the review is highly critical of the paper, pointing out 'severe issues' and stating that neither the approach nor the results are convincing. The reviewer also accuses the authors of cherry-picking results and failing to cite better-performing architectures. The politeness score is -20 because while the reviewer maintains some level of professional language, there are instances of blunt criticism and accusatory language, such as 'cherry-picked' and 'neither the approach nor the results convince me'. The reviewer does not use overtly rude language, but the tone is decidedly negative and critical throughout, with little attempt to soften the criticism or acknowledge positive aspects of the work."", ""The sentiment score is highly negative (-90) because the reviewer expresses strong suspicion of academic dishonesty, pointing out near-duplicate text and results from prior work without proper citation. They also note that the proposed method underperforms existing approaches. The politeness score is somewhat negative (-30) because while the reviewer maintains a professional tone overall, they use phrases like 'particularly suspicious' and directly accuse the authors of copying without attribution, which is quite harsh criticism. The reviewer provides detailed evidence to support their claims but does not soften their language or offer many constructive suggestions for improvement."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper is 'extremely unclear,' contains 'typos in most sentences,' and provides 'insufficient justification' for its claims and design choices. They also mention that the proposed model is outperformed by at least one critical baseline. The reviewer suggests major revisions, indicating a strong negative sentiment towards the current state of the paper. The politeness score is -20 because while the reviewer provides constructive feedback and suggestions for improvement, the language used is quite direct and somewhat harsh. Phrases like 'extremely unclear' and pointing out typos in 'most sentences' come across as slightly impolite. However, the reviewer does maintain a professional tone overall and offers specific recommendations for improvement, which prevents the score from being lower.""]"
"['\n\nThe authors describe an anomaly/novelty detection method based on handcrafted features + VGG based features. \n\nI think the paper is out of the scope of the conference (the only part dealing with learned representations uses VG), plus it addresses a problem whose relevance is not correctly motivated. Finally, the method is quite basic, and is not compared to any state of the art method for novelty detectiobn.  \n\nIn ""... the detection of visual anomalies perceived by human observer is an open challenge… "" can you provide references of people working in this particular problem?\n\nThe review of related work seems obsolete, can you provide more recent references (in addition to ""historical"" ones). More importantly, please provide references of anomaly detection from textures\n', 'The submission investigates the problem of detecting perceptual anomalies in visual textures.\nIt proposes features from three different models, the Portilla & Simoncelli texture model (PS), the Spatial Vision model by Schuett and Wichmann (SW) and CNN features from the VGG network. From these features it trains two anomaly detectors: one out of the box one-class SVM and a 3 layer neural network. The network is optimised with a loss function that encourages output values for the original texture to be larger than for a white-noise image obtained by shuffling its pixels. At the same time the range of output values for the original texture is encouraged to be small.\n \nThe performance of the different approaches is evaluated using synthetic anomalies. However no distinction between perceptually striking and perceptually negligible anomalies is made and quantitative results are only reported for all synthetically generated anomalies.\nTwo attempts are made to control if an approach specifically picks up on perceptually striking anomalies.\na) detection rate on gaussian noise as a proxy for perceptually negligible anomalies\nb) anecdotal evidence from visual inspection.\n\nI do not think that either of the two controls is sufficient to make a clear statement about which method is best in detecting perceptually striking anomalies. Therefore my main concern is that the performance evaluation is not suitable to achieve meaningful results.\n\nFurthermore the technical depth of the submission appears fairly limited. The main original contribution is the CNN loss that is introduced. However, the loss does not strike me as particularly compelling. It resembles a classifier between textures and white noise samples with the same pixel-wise statistics. I am not sure why this should be particularly suited to detect perceptual anomalies.\n\nFinally, showing quantitative results from only two textures does not feel like a very comprehensive analysis.\n\nIn general  the submission tackles an interesting research topic. However, to show meaningful results I believe that one has to collect psychophysical data for the anomalies of interest to distinguish between anomalies that are perceptually detectable and those that are not.\nWith such a test set one could then start testing hypothesis on which feature representation is most appropriate to model the perceptual results or optimise features directly to match human psychophysical results (similar to the study by Berardino et al. 2017 [1]). In its current form I am not sure what I can learn from the submission both in terms of anomaly detection and feature spaces particularly suited to detect perceptual anomalies in visual textures. \n\n[1] Eigen-distortions of hierarchical representations\nA Berardino, V Laparra, J Ballé, E Simoncelli\nAdvances in neural information processing systems, 3530-3539', 'This paper considers detecting anomalies in textures. For this task they use VGG-19 features and two human-inspired features from Portilla & Simoncelli and Schutt & Wichmann.\nWith these features, they train one-class anomaly detectors. One such anomaly detector is a one-class SVM, and they introduce a loss for one-class neural networks.\n\nThe novelty in this paper comes from the problem setup which I have not seen treated before. The loss function they propose also appears original.\n\nHowever, comparisons are limited. They compare against OC-SVMs, but these are known to be weaker than several types of anomaly detectors [1]. This paper would also do well to ground itself in more recent research on deep anomaly detection [2]. Likewise, the problem setting is limited. In all, experimentation could use more breadth and depth.\n\n[1] Andrew F. Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, Weng-Keen Wong. Systematic Construction of Anomaly Detection Benchmarks from Real Data. ODD, 2013.\n[2] Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. ICLR, 2017.', 'This paper focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task.\n\nNovelty:\nIt is the first time I have seen this novelty detection task. This task could be part of the novelty of the paper. Another novelty comes from the new objective function they introduce.\n\nWeakness:\n1. The motivation of the new objective function is not clear to me. It seems that they first design the objective function and then build the interval-based decision function. There is not much intuition given.\n2. The experiment lacks of real data. Synthesized anomalies never exists in application. If it is a paper about application, real data is needed.\n3. The baseline is too simple. CNN could definitely beat SVM in image classification. Also using extracted features could be better than directly performing SVM on pixels.\n4. Do not see the results of OC-SVM in Table 1 even though they say they beat it.\n5. Also, I do not see any other reference work for this novelty detection problem. If it is a new problem, a clear definition of the problem is needed. If it is not, more references are needed.\n\nThe writing of the paper is clear and easy to understand. But based on all the weakness above and lack of novelty, I think the paper should be rejected for now.']","[-70, -50, 20, -60]","[-20, 50, 50, 50]","[""The sentiment score is -70 because the reviewer expresses several significant criticisms: the paper is out of scope, the problem's relevance is not well-motivated, the method is basic, and there's a lack of comparison to state-of-the-art methods. These are major issues that suggest the reviewer views the paper negatively. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the feedback or offer encouragement. The reviewer uses phrases like 'out of scope' and 'quite basic' without hedging, and the requests for additional references are phrased as commands rather than suggestions. This directness, while not impolite per se, lacks the courtesy often seen in more tactful reviews."", ""The sentiment score is -50 because while the reviewer acknowledges the submission tackles an interesting topic, they express significant concerns about the methodology and results. They state that the performance evaluation is not suitable to achieve meaningful results, the technical depth is limited, and the analysis is not comprehensive. These criticisms outweigh the positive aspects, resulting in a negative overall sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout. They offer constructive criticism and suggestions for improvement rather than harsh dismissals. Phrases like 'I do not think' and 'I am not sure' soften the critique. The reviewer also acknowledges positive aspects, such as the 'interesting research topic', which contributes to the polite tone."", 'The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty of the problem setup and the originality of the proposed loss function. However, they also point out limitations in comparisons and experimentation, which tempers the positive sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language without harsh criticism. They offer constructive feedback and suggestions for improvement, citing relevant literature to support their points, which is a polite way to provide critique in academic reviews.', ""The sentiment score is -60 because the review is overall negative, recommending rejection of the paper due to several weaknesses and lack of novelty. However, it does acknowledge some positive aspects like the novelty of the task and the clear writing. The politeness score is 50 because the language used is professional and constructive, offering specific feedback without being harsh. The reviewer uses phrases like 'It is the first time I have seen' and 'The writing of the paper is clear and easy to understand', which are polite ways to give feedback. Even when pointing out weaknesses, the tone remains respectful and objective.""]"
"['The authors present a large synthetic dataset for 3D scenes with templated descriptions.  They then use the model of Eslami 2018 to this new domain.  The previous work appears to already introduce all the necessary mechanisms for 3D generalization from multiple viewpoints, though this work embeds language instead of a scene in the process.  Minor note: A bit more discussion on this distinction would be appreciated.  Also, it appears that the previous work includes many of the rendered scenes also present here, so the primary focus of this paper is on the use of a language encoder (not necessarily a trivial extension).\n\nThe model appears to perform well with synthetic data though very poorly with natural sentences.  This may be in part due to the very small dataset size.  It would be helpful to know how much of the performance gap is due to scaling issues (10M vs 5.6K) versus OOVs, new syntactic constructions, etc.  In particular, the results have ~two deltas of interest (NL vs SYN) and the gap in the upper bound from 0.66 to 0.91.  What do new models need to be able to handle to close these gaps?\n\nRegarding the upper bound, there is some discussion that annotators might have had a strict definition of a perfect match.  Were annotators asked about this? The current examples (B.2), as the authors note, are more indicative of failings with the synthetic language than the human annotators.  This may again be motivation for collecting more natural language which would resolve some of the ambiguity and pragmatics of the synthetic dataset.\n\nIt would also be helpful to have some ablations included in this work. The most obvious being the role of $n$ (number of scene perspectives).  How crucial is it that the model has access to 9 of 10 perspectives?  One would hope that given the limited set of objects and colors, the model would perform well with far fewer examples per scene, learning to generalize across examples.\n\nSince the primary contributions of the paper are a language dataset and a language encoder for the existing model of Eslami 2018, those should be discussed and ablated in the paper rather than relegated to the appendix.\n\nMinor note:  the related work mentions grounding graphs which are core to work from Tellex and Roy, but omits existing fully neural end-to-end models in grounding (e.g. referring expressions work).\n', 'The main contributions of the work are the new datasets and the overall integration of previous modeling tools in such a way that the final architecture is able to encode semantic spatial relations from textual descriptions. This is demonstrated by an implementation that, given textual descriptions, is able to render images from novel viewpoints. In terms of these two contributions, as I explain below, I believe there is space to improve the datasets and the paper needs further analysis/comments about the merits of the proposed approach. So my current overall rating is below acceptance level.\n\nIn terms of data, authors provide 2 new datasets: i) a large datasets (10M) with synthetic examples (images and descriptions) and ii) a small dataset (6k) with human textual descriptions corresponding to synthetic images. As the main evaluation method of the paper, the author include direct human evaluation of the resulting renderings (3 level qualitative evaluation: perfect-match/partial-match/no-match). I agree that, for this application, human evaluation is more adequate than comparing a pixel-level output with respect to a gold image. In this sense, it is surprising that for the synthetic dataset the perfect match score of human evaluation for ground truth data is only 66%. It will be good to increase this number providing a cleaning dataset. \n\nRelated to the previous comment, it will be good to provide a deeper analysis about the loss function used to train the model.   \n\nIn terms of the input data, it is not clear how the authors decide about the 10 views for each scene.\n\nIn terms of the final model, if I understood correctly, the paper does not claim any contribution, they use a model presented in a previous work (actually information about the model is mostly included as a supplemental material). If there are relevant contributions in terms of model integration and/or training scheme, it will be good to stress this in the text.\n\nWriting is correct, however, authors incorporate important details about the dataset generation process as well as the underlying model in the supplemental material. Given that there is a page limit, I believe the relevant parts of the paper should be self-contain. ', 'This paper presents a system to map natural language descriptions of scenes containing spatial relations to 3D visualizations of the corresponsing scene. The authors collect a dataset of different scenes containing objects of varying shapes and colors, along with several descriptions from different viewpoints. They train a model based on the Generative Query Network, to generate scenes conditioned on multiple text descriptions as input, along with associated camera angles. Empirical results using human evaluators demonstrate better performance compared to baselines and the authors perform a good analysis of the model, showing that it learns to ground the meaning of spatial words robustly. \n\nPros:\n1. Well-executed paper, with convincing empirical results on the newly collected dataset. \n2. Nice analysis to demonstrate that the model indeed learns good semantic representations for spatial language. \n\nCons:\n1. The positioning of this paper with respect to recent work is disappointing. In both the Introduction and Related Work sections, the authors talk about dated models for spatial reasoning in language (pre-2012). There have been several pieces of work that have looked at learning multimodal representations for spatial reasoning. These are a few examples:\n   a) Misra, Dipendra, John Langford, and Yoav Artzi. ""Mapping Instructions and Visual Observations to Actions with Reinforcement Learning."" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017.\n   b) Michael Janner, Karthik Narasimhan, and Regina Barzilay. ""Representation Learning for Grounded Spatial Reasoning."" Transactions of the Association of Computational Linguistics 6 (2018): 49-61.\n   c) Paul, Rohan, et al. ""Grounding abstract spatial concepts for language interaction with robots."" Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press, 2017.\n   d) Ankit Goyal, Jian Wang, Jia Deng. Think Visually: Question Answering through Virtual Imagery. Annual Meeting of the Association for Computational Linguistics (ACL), 2018 \n\nEven though Gershman & Tenenbaum (2015) demonstrate weaknesses of a specific model, some of the above papers demonstrate models that can understand things like ""A is in front of B"" = ""B is behind A"". A discussion of how this paper relates to some of this prior work, and an empirical comparison (if possible) would be good to have.\n\n2. The introduction reads a bit vague. It would help to clearly state the task considered in this paper upfront i.e. generating 3D scenes from text descriptions at various viewpoints. In the current form, it is hard to understand the task till one arrives at Section 3.\n\nOther comments:\n1. What is the difference between the bar graphs of Figures 5 and 6? Is the one on Figure 5 generated using the sentences (and their transforms) from Gershman & Tenenbaum? If so, how do you handle unseen words that are not present in your training data?\n2. Would be helpful to clearly explain what the red and black arrows represent in Figure 7.\n3. How does the model handle noisy input text i.e. if the object descriptions (shape/color) are off or if some of the input text is incorrect (say a small fraction of the different viewpoints)? \n\n------\nEdit:\nThank you for the author response. Even if you consider the story to be the same across literature (which in this case is not, since the more recent models handle spatial relations that the previous ones failed on), it\'s still worth doing due diligence to the recent work, especially so that the reader gets a better sense of how to position your work amongst these.']","[-20, -30, 50]","[50, 50, 75]","[""Sentiment score: The review starts with a neutral tone, acknowledging the authors' work. However, it then points out several limitations and areas for improvement, suggesting a slightly negative sentiment. The reviewer notes poor performance with natural sentences, calls for more ablations, and suggests that the main contributions should be discussed more thoroughly in the main paper rather than the appendix. These critiques outweigh the initial neutral tone, resulting in a slightly negative score of -20.\n\nPoliteness score: The language used is generally polite and professional. The reviewer uses phrases like 'it would be helpful' and 'one would hope' rather than making direct demands. They also acknowledge the authors' work and provide constructive feedback. However, the review is not overly effusive in its praise, maintaining a professional distance. This results in a moderately positive politeness score of 50.\n\nThe scores reflect a balanced review that leans slightly critical in content while maintaining a polite and professional tone throughout."", ""The sentiment score is -30 because the reviewer expresses that the paper is 'below acceptance level' and suggests several areas for improvement. However, they do acknowledge some positive aspects, such as the contributions of new datasets and the integration of previous modeling tools. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'it will be good to' and 'if I understood correctly,' which maintain a polite tone while providing feedback. The reviewer also acknowledges the merits of the work, such as agreeing that human evaluation is more adequate for this application, which contributes to the overall politeness of the review."", ""The sentiment score is 50 (slightly positive) because the reviewer begins by highlighting the paper's strengths and contributions, noting it is 'well-executed' with 'convincing empirical results'. However, they also point out significant shortcomings, particularly in the literature review and positioning of the work. The politeness score is 75 (fairly polite) as the reviewer uses respectful language throughout, framing criticisms constructively as suggestions for improvement rather than harsh judgments. They use phrases like 'would be helpful' and 'it would help to' when making recommendations. The reviewer also thanks the authors for their response in the edit, showing courtesy. While critical, the tone remains professional and supportive of improving the paper rather than dismissive.""]"
"['The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set. This scenario is well-motivated by the ""researcher example"" given throughout the paper. The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain. Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).\n\nThe key idea is to learn a mapping from the source domain to the target domain. This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain. In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.  Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.\n\nPros:\n- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.\n- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.\n- The paper reads well and is easy to follow.\n\nCons:\n- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"". Further, there a number of experimental details that need to be further elaborated upon. e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this). It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?\n- Some assumptions are not explicitly stated. In particular, it is unclear what the assumption on the size of the unlabelled test set is. This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.\n- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further? Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?\n- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.\n\nOverall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research. Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches). Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task. It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.\n', 'This paper proposes to combine unsupervised adversarial domain adaptation with prototypical networks and finds that the proposed model performs well on few-shot learning task with domain shift, much better than other few-shot learning baselines that do not consider. Specifically it tests on Omniglot with natural image background and cliparts to real images.\n\nIt is true that current meta-learning approaches do not address the problem of domain shift, and as a result, the testing domain has to be the same with the training domain. However, this paper rather than proposing solution address the meta-learning problem, albeit the title “meta domain adaptation”, only brings few-shot learning to domain adaptation. Here’s why:\n\nIn order for a meta-learning model to be called “meta domain adaptation,” the type of adaptation cannot be seen during training, and the goal is to test on adaptation that the model has not seen before. Indeed, each task in meta domain adaptation should be seen as a pair of source task and target task. \n\nThe problem with the current model is that during training, it is trained to target at one specific type of test domain--the generator network G aims to generated images that align with the unsupervised  test domain X_test. Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.\n\nIn short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation. Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair. For the rest of my review, I will treat the paper as “few-shot learning with domain adaptation” for more appropriate analysis.\n\nFor the experiments, there seems to have a great win of the proposed algorithm against the baselines. However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison. Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation. Then use the same network to extract the features and then using the nearest neighbor to retrieve the classes. Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.\n\nAnother concern is that the evaluation of domain adaptation does not have much varieties. Only two domains shifts are evaluated in the paper, specifically Omniglot + BSD500 and Office-Home. BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered. Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.\n\nIn conclusion, the paper presents a interesting combination of ProtoNet + Adversarial DA + Cycle consistency. However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers. Therefore, I recommend reject.\n\n---\nNote: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".\n\n===\nAfter rebuttal:\n\nI would like to thank the authors for the response and updating the draft. They have addressed 1) the title issue and 2) adding domain adaptation baselines. Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines. \n\nHowever, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty. The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).', 'The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup. The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.\n\nPros:\n1. A new few shot learning with domain shift problem is studied in the paper.\n2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario. The experimental improvements on omniglot seem quite substantial. \n\nCons:\n1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline? It seems that both are using meta-learning with domain adaptation technique. What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?  I feel the baseline in domain adaptation area is a bit limited.\n2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)? \n3. It seems the domain shift in the paper is less dramatic. i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.\n4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.\n\nMinor:\n1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?\n2. In the caption of figure 2, there should be a space after `"":"".']","[50, -60, 20]","[80, 20, 60]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's importance, novelty, and readability while also pointing out significant concerns. The pros mentioned are substantial, but the cons are equally weighty, indicating a balanced yet slightly favorable view. The reviewer concludes that the paper would be a good fit for the venue with some elaborations, further supporting a mildly positive sentiment. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. Phrases like 'I encourage the authors' and 'Can the authors comment on this further?' demonstrate a collegial tone. The reviewer also offers helpful suggestions and expresses hope for the paper's impact, showing consideration for the authors' work."", ""The sentiment score is -60 because the reviewer's overall assessment is negative, recommending rejection and pointing out several significant flaws in the paper's approach and methodology. However, it's not entirely negative as the reviewer acknowledges some interesting aspects and improvements after rebuttal. The politeness score is 20 because while the reviewer maintains a professional tone and offers constructive criticism, there are instances of direct criticism without much softening language. The reviewer uses phrases like 'It is true that...' and 'I would like to thank the authors...' which add a degree of politeness, but also includes more blunt statements like 'the paper cannot be qualified for...' and 'lacks new insights/novelty'. The language is generally respectful but not overly polite."", ""The sentiment score is slightly positive (20) because the review begins with acknowledging the authors' proposal and its improvements in experiments. It lists some pros, indicating positive aspects. However, it also includes several cons and questions, which balance out the positivity, resulting in a mildly positive overall sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, poses questions and suggestions politely (e.g., 'Can you clarify...', 'I wonder whether...'), and maintains a professional tone. The reviewer also acknowledges the positive aspects before presenting criticisms, which is a polite approach. The language is not overly formal or excessively polite, hence not reaching the highest scores, but it's consistently respectful and constructive.""]"
"[""The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. "", ""The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. "", 'It is often argued that one of the roles of pooling is to increase the stability of neural networks to \ndeformations. This paper presents empirical evidence to contest this assertion, or at least qualify it.\n\nI appreciate empirical studies that question some of the widely accepted dogmas of deep learning. \nFrom this point of view, the present paper is certainly interesting.\n\nUnfortunately, the actual evidence presented is quite weak, and insufficient to draw far reaching \nconclusions. An obvious objection is the authors only consider two datasets, and a very small number of \nmore or less standard pooling methodologies. The effect of pooling is evaluated in terms of cosine \nsimilarlity, which is not necessarily a good proxy for the actual performance of a network.\n\nA more serious issue is that they seem to very readily jump to unwarranted conclusions. For example, \nthe fact that stability to deformations (by which I necessarily mean the specific type of deformations \nthat they consider) tends to decrease in the middle layers of neural networks during training does not \nmean that starting with a neural network with less stability would be better. Maybe some kind of \nspontaneous coarse-to-fine optimization is going on in the network. Similarly, it is obvious that smoother \nfilters are going to lead to more stable representations. However, they might be less good at discriminative \ntasks. Just because smoother filters are more stable does not automatically mean that they are more desirable.\n\nStability to deformations is an important but subtle topic in computer vision. For starters, it is difficult \nto define what kind of deformations one wants to be insensitive to in the first place. A useful model would \nlikely incorporate some notion of deformations at multiple different length scales. \n\nJust showing that one network is better than another wrt some arbitrarily defined simple class of deformations \nwith no reference to actual recognition performance, speed of training, or interpretation of the nature of \nthe deformations and the learned filters is not very convincing. I would particularly like to emphasize the \nlast point. I would really like to understand what pooling actually does, not just at the level of ""if you \nturn it off, then cosine similarity will decrease by this much or that much.""', 'This paper asks what is the role of pooling in the success story of CNNs applied to computer vision. \nThrough several experimental setups, the authors conclude that, indeed, pooling is neither necessary nor sufficient to achieve deformation stability, and that its effect is essentially recovered during training. \n\nThe paper is well-written, it is clear, and appears to be readily reproducible. It addresses an interesting and important question at the interface between signal processing and CNNs. \n\nThat said, the paper does not produce any clear novel results. It does not provide any theoretical result, nor any new algorithm. Its contributions consist of three empirical studies, demonstrating that (i) the benefits of pooling in terms of deformation stability can be achieved through supervised learning the filters instead (sec 3), (ii) the mechanism to obtain stability through learning essentially consists on reducing the bandwidth of (some) filters (sec4), and (iii) that this mechanism is data-dependent (sec 5). None of these studies strike the reviewer as particularly revealing. Moreover, the reviewer felt that the authors could have built on those findings to ask (and hopefully answer) a few interesting questions, such as:\n-- Nowhere in the paper there is a discussion about critical Nyquist sampling and the need to reduce the bandwidth of a signal prior to downsampling it in order to avoid aliasing. Average pooling provably does it, and learnt filters do it provided they indeed become bandlimited. What are the links between deformation stability and the ability to avoid aliasing? \n-- How many lowpass antialiasing filters are needed per layer to provide sufficient stability? \n-- Also, the authors should relate this study with similar works that do the same in speech (e.g. https://www.isca-speech.org/archive/Interspeech_2018/abstracts/1371.html). \n\nIn conclusion, my impression is that this paper requires a major iteration before it can be of widespread interest to the community. I encourage the authors to think about the above points. \n\n\n ']","[-20, -20, -30, -20]","[50, 50, 50, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also express significant concerns ('Cons') and criticize the paper's main argument as having 'insufficient' evidence. The overall tone suggests the reviewer is not fully convinced by the paper's conclusions. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, presenting both pros and cons objectively without using harsh or dismissive language. They offer constructive criticism and suggestions for improvement rather than outright rejection, maintaining a polite and academic tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also express significant concerns ('Cons') and criticize the paper's main argument as having 'insufficient' evidence. The overall tone suggests the reviewer is not fully convinced by the paper's conclusions. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, presenting both pros and cons objectively without harsh criticism. They offer constructive feedback and suggestions for improvement, maintaining a courteous tone even when expressing doubts about the paper's conclusions."", ""The sentiment score is -30 because while the reviewer appreciates the empirical study questioning accepted dogmas, they express significant concerns about the weakness of the evidence and unwarranted conclusions. The overall tone is more critical than positive. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's interesting aspects before presenting criticisms. They use phrases like 'I appreciate' and 'I would like to emphasize', which maintain a polite tone even while expressing disagreement. The reviewer also offers constructive suggestions for improvement, which contributes to the polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (well-written, clear, reproducible, addresses an interesting question), they also express significant criticisms. The reviewer states that the paper doesn't produce novel results, lacks theoretical contributions or new algorithms, and requires 'a major iteration' to be of widespread interest. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive feedback. They use phrases like 'I encourage the authors' and offer specific suggestions for improvement, which contributes to a polite tone despite the critical content.""]"
"['\nThis paper focuses on the reduction of training time by various mechanisms. By introducing a time gate during training, it controls when a neuron (weights?) can be updated during training. By introducing and additional budget term in the loss function, training costs (number of computations) are reduced by one order of magnitude. \nA major advantage of the newly introduced Gaussian-gated LSTM (g-LSTM; I suggest using a capital G for Gauss, e.g., GgLSTM).\n\nExperiments are carried out on the adding-problem from 1997; the sequential MNIST and the sequential CIFAR-10 problem. In all experiments, g-LSTM converges faster. A few things would be of interest:\n- clearly state the stopping criterium for training. Especially, I would still be interested to see, how Fig. 3d continues; it seems that the network begins to collapse (also a and be are interesting to see).\nThe ""This work"" in Table 2 is confusing; I would expect it to appear behind g-LSTM; \nIt appears that in the budgeted g-LSTM some units are not used at all (Figure 5b); Please comment on that.\n\nIn general, the paper makes the impression that it is overselling the contribution a bit too much. It would be nice to question the outcomes more and investigate the g-LSTM for the existence of possible problems which might be introduced by the omission of computations.', 'The work takes inspiration from a recent work on phased LSTM, and proposes to add a Gaussian gate based on time to LSTM cells. With this additional gate, the network can skip updating the states by closing the time-gate, as a result enabling longer memory persistence, and better gradient flow. The authors also propose to add a budget term to force the time-gate to be closed most of the time as a way to save compute. Empirical results suggest the Gaussian-gated LSTMs perform better than regular LSTMs on tasks with long temporal dependencies. The authors also propose to use a curriculum training schedule in which the variance of the gaussian gates is continuously increased to speed up training of LSTMS.  \n\npros:\n1. The paper is clearly written and easy to follow;\n2. The way the authors introduce time-dependent gating into LSTM is easy to follow and re-implement;\n3. Experiments on various tasks of long temporal dependencies do show improvement over the standard LSTM cells;\n4. The experiments on the adding task does show gLSTM is less sensitive to initialization than the phased LSTM;\n5. The experiments on setting curriculum training schedule to improve convergence on LSTMs are insightful. \n\ncons:\n1. The work was framed as an easier-to-optimize alternative to the time-based gating mechanism introduced in phased LSTMs, which takes a parametrization form that is much harder to learn, the gating mechanism covered by the new model gLSTM however, is much more limited. The parametrization introduced in Phased LSTMs allows the memory cells and outputs of LSTMs to be updated periodically. gLSTMs on the other hand only allows updates within a single window over the entire sequence; As a result, one would expect phased LSTM to outperform gLSTM on tasks with periodical temporal dependencies;  \n2.  The empirical results are not convincing enough. gLSTM performs noticeably worse than several state-of-the-art work on improving long-term dependencies in RNNs. The authors did not give any explanation in the performance gap.', 'In the vein of recent work on learning “ticking” behaviour for LSTMs such as Phased LSTM, this paper proposes to add additional data independent gates to LSTM units that are defined as Gaussian functions of time indices.\nThe performance of the modified g-LSTM is compared to LSTM on the Addition, sequential MNIST and sequential CIFAR-10 tasks. The authors argue that g-LSTM results in better performance and has faster convergence on these tasks. \n\nAdditionally, it is proposed that one can reduce the amount of computations performed by the network by adding a computation budget term to the optimized loss that encouraged the cells to update less often. Finally, a technique for gradually transitioning from a g-LSTM to an LSTM during training is proposed, with the objective of speeding up training over a regular LSTM.\n\nThe paper is well written and easy to understand in general. However, the main results of this paper are experimental, and I am not entirely convinced by the experiments that g-LSTM is an improvement over the LSTM baseline for certain scenarios.\n\nOne broad reason for my doubts is that the comparisons don’t seem to utilise proper hyperparameter tuning for the baseline LSTM. Network sizes, learning rates, decay schedules, initialisations etc. all appear to be fixed, so one can not be sure of the “real” performance or convergence behavior of the models. Biased gate initializations are not used, though they have been used successfully in past work to aid in long term memory.\n\nI should note that for long term memory problems such as those proposed by Hochreiter and Schmidhuber (1997), the proposed LSTM did not use a forget gate (or even BPTT) and used biased gate initialisations. However, these features are useful for more realistic tasks, and popular LSTM designs are biased towards them instead of toy problems.\n\nI would consider the addition problem and sequential MNIST and CIFAR-10 to be interesting and difficult toy tasks for initial validation of ideas (and more extensive hyperparameter searches). It is unclear if the proposed techniques will perform provide  improvements over a well-tuned baseline for some realistic tasks, or are they suitable only for toy problems. ']","[50, 50, -20]","[70, 80, 60]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and advantages of the new method, but also points out areas for improvement and suggests the paper may be overselling its contribution. The politeness score is 70 (fairly polite) as the reviewer uses respectful language, makes suggestions rather than demands, and phrases criticisms constructively. The reviewer uses phrases like 'I suggest,' 'It would be nice,' and 'Please comment on that,' which contribute to a polite tone. The review provides specific, constructive feedback without harsh criticism, maintaining a professional and courteous approach throughout."", ""The sentiment score is 50 (slightly positive) because the review begins with a neutral description of the work and lists several pros, indicating positive aspects. However, it also includes significant cons, which balance out the positives. The politeness score is 80 (quite polite) because the reviewer uses professional and respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The reviewer's tone remains objective and constructive, avoiding harsh or dismissive language even when pointing out limitations."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is well-written and easy to understand, they express significant doubts about the experimental results and the effectiveness of the proposed g-LSTM compared to the LSTM baseline. The reviewer points out several concerns, such as lack of proper hyperparameter tuning and doubts about the applicability of the technique to realistic tasks. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'I am not entirely convinced' and 'I should note that' which maintain a polite tone while expressing concerns. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the polite tone.""]"
"['This paper presents ODD, a method that rejects incorrectly labeled / noisy examples from training on the fly. The motivation is sound, that with the capacity of modern neural networks, it\'s easy to memorize the mislabeled data and thus hurt generalization. If we could reject such mislabeled data, we may be able to get a more generalizable model. The authors made an observation that when training with large learning rate, examples with correct labeling and incorrect labeling exhibits different loss distributions. The authors further noticed that the loss distribution of incorrectly labeled examples can be simulated using eq.(1). Therefore, by setting a threshold that corresponds to a percentile of the incorrectly labeled loss distribution, the authors are able to reject incorrect examples.\n\nSome comments:\n1. Eq.(1) basically assumes all the noise is uniformly distributed among classes. What if only 2 classes are easily mislabeled while others are fine?\n2. Section 4.1.3 and Section 4.4 Sensitivity to Noise are confusing. Please clarify the importance and rationale for such analysis.\n3. Cosine schedule is used in the experiments. However, since the method does not work well with small learning rate, why not using a fixed large learning rate and decrease it after noise rejection? Also, in section 4.4 Sensitivity to E, the analysis of the sensitivity to the number of epochs is coupled with a changing learning rate. It would be better to see an experiment with the two decoupled.\n4. The loss of an example is averaged over h epochs. It will better to clarify how the simulated distribution generated in such case since the distribution is dependent on fc(.), which is changed between two epochs.\n5. Except for the first experiment, all other experiments are only compared with ERM, the vanilla algorithm. It would be better to show a comparison with other methods.\n6. Please show a precision/recall of the examples that are marked as ""noise"" by the method.\n7. I assume this method will remove a lot of hard examples. How does this affect training? Does this make the network more error-prone to harder instances?', 'The paper aims to remove potential examples with label noise by discarding the ones with large losses in the training procedure. The idea also applies to the setting where instances may contain large noise. The proposed method may have an implicit trade-off between the robust to label noise and feature noise, which explains why the proposed method also has good performances on instance-dependent label noise. The paper is well-written and has sufficient experiments. \n\nThe discussions in Section 2.2 is unclear for me. What is ""p_n(\\ell)""? What is ""p-th percentile of a distribution""? How reasonable thresholds are derived for the uniform label noise? Why the method will generalize to other types of label noise?\n\n===\nAfter reading the rebuttal, it is still unclear of how to determine the thresholds for finding incorrect labels. The authors empirically demonstrated a procedure to statistically find a threshold under the assumption that the label noise is uniform. However, theoretical guarantees are lacking. The extension to other types of label noise is also very intuitive. Although the proposed method is simple and effective, the lack of an effective method for choosing the threshold is a major concern for real-world applications. Are there some other ways to determine the threshold? For example, cross-validation method?', 'Thanks for the rebuttal. But, I am still not very convinced with the proposed results. For CIFAR-100 (0%), you get about 0.2% gain, for ImageNet (0%), you get about 0.2% loss in top-5 accuracy, and for WebVision, you get about 0.3% gain. I am not sure whether you can call these as statistically significant gains. I believe such gain/loss can be obtained with many other tweaks, such as the learning rate scheduling, as the authors have done. \n\nI believe extensive testing the proposed method on many real noisy datasets, not the synthetically generated ones, and showing the consistent gains would much strengthen the paper. But, at the current version, the only such result is Table 5, which is, again, not very convincing to me. \n\nSo, I still keep my rating. \n\n=======\n\nSummary:\n\nThe authors propose a simple empirical method for cleaning the dataset for training. By using the implicit regularization property of SGD-based optimization method, the authors come up with a method of setting a threshold for the training loss statistics such that the examples that show losses above the threshold are regarded as noisy examples and are discarded. Their empirical results show that ODD (their method) can outperform other baselines when artificial random label noise is injected. They also show ablation studies on the hyperparameters and show the final result seems to be robust to those parameters. \n\nPros:\n- The method is very simple\n- The empirical results, particularly on the synthetic noisy training data, seems to be encouraging.\n- The ablation study argues that the method is robust to the hyperparameters, p, E, and h.\n\nCons:\n- I think the results remains to be highly empirical. While it is interesting to see the division of the loss statistics in Figure 2, I am not very convinced about the real usage of the proposed method. The result in Table 5 shows that ODD can outperform ERM for real world datasets, but the improvement seems to be marginal. Moreover, the hyperparameter p was set to 30 for that experiment, but how did the authors choose that parameter? Clearly, if you choose wrong p, I think the performance will degrade, and it is not clear how you can choose p in real applications. The ablation studies are only with synthetic noisy label data, so I think the result is somewhat limited. \n- \n\nI think the paper shows interesting results, but my concern is that it seems to be quite empirical. The positive results are particularly on the synthetic data case. \n\n\n']","[50, 20, -60]","[80, 60, 20]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's sound motivation and novel approach, but also provides several critical comments and suggestions for improvement. The overall tone is constructive rather than dismissive. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions (e.g., 'Please clarify...', 'It would be better to...'), and acknowledges the authors' work positively. The reviewer maintains a professional and courteous tone while providing detailed feedback."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper is well-written and has sufficient experiments. They also mention the proposed method has good performances. However, they express concerns about unclear discussions and lack of theoretical guarantees, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as personal observations ('unclear for me'), and acknowledges positive aspects before presenting concerns. They also mention reading the rebuttal, showing engagement with the authors' responses. The language is professional and constructive, without any rudeness, but also not overly formal or deferential."", ""The sentiment score is -60 because the reviewer expresses significant skepticism about the paper's results and methodology, stating they are 'not very convinced' multiple times and keeping their original (presumably negative) rating. However, they do acknowledge some positive aspects like 'interesting results' and 'encouraging' empirical findings, preventing an extremely negative score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout, using phrases like 'I believe' and 'I think' to soften criticisms. They also thank the authors for their rebuttal and provide a balanced summary with both pros and cons. However, the overall critical nature of the review prevents a higher politeness score.""]"
"['Summary: The paper considers a variational inference strategy for learning neural networks with binary weights. In particular, the paper proposes using a structured recognition model to parameterise the variational distribution, which couples the weights in different layers/filters in a non-trivial way. The gradient of the expected likelihood term in the variational lower bound is estimated using the REINFORCE estimator. This paper adjusts this estimator to use the gradient of the log-likelihood wrt the samples. Experiments on several image classification tasks are provided.\n\nevaluation:\n\npros:\n- the idea of the proposed approach is interesting: using variational inference for binary weight neural networks. While recent work on VI for discrete variables only focused on discrete latent variable models, this work shows how VI can be used for binary neural networks.\n \ncons:\n- the writing, in my opinion, needs to be improved [see my comments below]. The VI presentation is cluttered and the justification of using the pseudo-reward for reinforce is not clear.\n- the experimental results are mixed and it\'s not clear to me how to interpret them/compare to the baselines -- what is the goal here: computational efficiency, compression or accuracy?\n\nSome specific questions/comments:\n\n+ What is the input of the policy/recognition network? It\'s not clear from the paper whether this includes the inputs of the current batch or outputs or both? If so, how are variable batch sizes handled? What is the input to this network at test time? In contrast to generative models/VAEs, the weights here are global parameters and it\'s not clear to me these should be varied for different data batches.\n\n+ related to the question above: how is prediction handled at test time? Say the parameters of the variational distribution over weights are generated using the recognition network, then 100 weights are sampled given these parameters which then give 100 predictions -- should these be then averaged out to get the final prediction? I\'m not quite sure I understand why the paper chose to *pick the best one* out of 100 predictions and the justification/criterion for this procedure.  \n\n+ The writing is not very clear at places, and it does not help that the references being merged with the text. I\'m also not sure about some of the technical jargons/terms used in the papers:\n- reinforcement learning: is this really a reinforcement learning problem? If you tackle this problem from a pure variational perspective, reinforce is used to obtain the gradient of the expected log-likelihood wrt the variational parameters. But instead of using the log likelihood, a learning signal that depends on the gradient of the log-likelihood is used.\n- concrete weights -- what are these? I assume they are just binary weights sampled from the variational approximation.\n- middle of page 3: p(w|X, Y) = p_\\theta(w): this is not precise as p_\\theta(w) is only an approximation to the exact posterior, which then allows us to lower bound the log marginal likelihood. ""common practice in modern variational approximation"": This is the standard way of deriving the lower bound and has been used for many years.\n\n+ the reinforce estimator tends to have high variances since it does not make use of the gradient of the function in the expectation. This paper adjusts the vanilla estimator with a learning signal that involves the gradient. Could you comment on the bias/variance trade-off of the resulting estimator? Much of recent literature on learning discrete variables, as far as I understand, propose ways to not to have to use the vanilla reinforce, for example Concrete, Relax or rebar, albeit the focus on latent variable models.\n\n+ model selection and uncertainty measure: the paper mentions these potential advantages of the proposed approach over deterministic binarisation schemes, but does not fully explore and test these.\n\n', ""This paper proposes to binarize all parameters of a CNN where the binary parameters are generated from another policy neural network (let's call it parameter generator). The parameter generator network has a special nested structure to regularize parameters within layers and filters. All parameters in CNN and parameter generator network are jointly trained. Since the gradient is hard to back propagated through binary variables, the paper adopts reinforcement learning approach to back-propagate rewards to the parameter generator. \n\nThe experiments look solid. The results show that the proposed approach is slightly worse than BinaryConnect (baseline) on MINST, CIFAR10 and CIFAR100, but outperforms BinaryConnect on ImageNet by a large margin. The ablation study also verified the need for the proposed nested parameter structure.\n\nThe paper is well written. The proposed method (1) is able to provide the posterior distribution of parameters so that we can use that in other applications such as confidence estimation and model selection (2) is memory and power efficient due to binarization. \n\nHaving said that, the experimental setting falls short. For the proposed method, the paper samples 100 binary networks from the parameter generator and pick the best one, which seems not correct. The results of the proposed method should be based on the average of these 100 binary networks, rather than picking the best one, because we won't be able to know which binary network is the best. Using the best one seems label leakage to me. For now, I'll give the benefit of doubt. Please clarify this during the feedback phase."", 'The paper treats network binarization as learning a bernoulli probability that each weight is 0 or 1. It uses a hierarchical approach to factor the probability in to parameters shared between weights/kernels/layers. The result is a network that can be used to sample a binary network. The main novelty is that full-precision weights are not needed during training. \n\n- There is a variance issue underlying this idea. The goal is to find a binary network that has high performance, but the probabilistic formulation could have a high variance in terms of the performance of the resulting binary networks. In the experiments, the variance is not shown, rather the authors sample 100 networks and pick the best one based on validation set accuracy? (they just say ""pick the best one""). Should this be accounted for in the objective function? The prior has very high variance (p=0.5). The authors claim that this ""demonstrates the versatility"" -- how?\n- Related to the above, there is a high variance associated with he REINFORCE estimator. In the appendix, the authors use a baseline value to alleviate this, but no discussion is provided in the main text or the experiments.  \n- For the stochastic version of binary connect, the authors report the best out of 100 trials. I would rather like to see the mean and confidence intervals, for this as well as the proposed method.\n\n- Please provide a comparison of the number of hyper parameters used vs the number of binary network parameters. Is it feasible to store the ""master network"" in memory for small devices? It seems you need more parameters than the original network as you have weight specific parameters + kernel/filter/layers specific parameters? Is there any generalization between these hyperparameters that can be shown in experiments e.g. using a compact hierarchy?\n- More generally, how do you see this method being used in practice? Do you sample each binary network on the device? \n- How is this better than other methods of training binary nets, which have better accuracy than your approach (according to Table 2)?\n- In the experiments, the hierarchical structure used for hyperparameters is not clearly described. \n\n- I found the exposition in Section 3.2 to be very confusing using f(*)  whereas it is very simply described in words. The policy network is hierarchical upon layers/filters/kernels/weights. What is ""s"" in equations (1) etc.?\n- In Section 3.3, I found the connection to MDPs tenuous, whereas it is easy to understand that you are using REINFORCE to estimate the gradient of the expectation. \n- The pseudo-reward is completely ad-hoc. Since this is a ""1-step MDP"" (bandit problem?), the reward is maximized when the probability of w=1 is related to the sign of the gradient. In the end, we seem to have arrived at something that is basically similar to BinaryConnect (i.e. using the sign of the gradient). \n- Only experiments with an uniform prior are shown. Can the prior be used in some ways?\n\n- In the experiments, the activation functions used are not described. \n- Some numbers are missing in Table 3.\n- One advantage of TernaryConnect is that it allows sparsity by allowing zero weights, whereas binary nets only allow +/- 1 weights. That is, Ternary Connect should not be dismissed simply and the performance of Ternary Connect should be shown on all the datasets.\n- The error for VGG on CIFAR-10 is very high compared to SOTA (close to 94% accuracy). \n- The legend on Figure 3 is not readable, the font size can be increased. \n- Page 1 ""regardless of the availability ..."" - citation needed. \n- Page 2, experiments are not a separate contribution.\n']","[-20, 50, -30]","[50, 75, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('the idea of the proposed approach is interesting'), they also highlight several significant concerns. These include issues with the writing clarity, mixed experimental results, and unclear comparisons to baselines. The cons outweigh the pros in the review.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'in my opinion' and frame criticisms as questions or suggestions rather than harsh statements. The reviewer also balances negative points with positive ones, showing a fair approach. However, the score is not higher because the review is direct in pointing out flaws and doesn't use overly polite language or excessive praise."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's solid experiments, good writing, and innovative aspects, but also points out a potential issue with the experimental setting. The overall tone is constructive and appreciative of the work, despite the concern raised. The politeness score is 75 (quite polite) due to the reviewer's use of respectful language throughout, starting with positive aspects before mentioning concerns, and giving 'the benefit of doubt' to the authors. The reviewer uses phrases like 'The paper is well written' and 'Having said that' to soften criticism, maintaining a professional and courteous tone."", ""The sentiment score is -30 because while the reviewer acknowledges some novelty in the approach, they raise several significant concerns and criticisms about the method, its evaluation, and its presentation. The overall tone suggests skepticism about the paper's contributions and effectiveness compared to existing methods. The politeness score is 20 because the reviewer maintains a professional and constructive tone throughout, phrasing criticisms as questions or suggestions for improvement rather than direct attacks. They use polite language like 'Please provide' and 'I found', which softens the critique. However, the score is not higher as the review is quite direct in pointing out flaws and doesn't include many positive remarks or encouragement.""]"
"['This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input.\n\nMy biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage).\n\nIt would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet\'s depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here.\n\nThat said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio).\n\nThe fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process.\n\nI think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model.\n\nOverall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used.\n\n\n\nMiscellany:\n\n- In the abstract: ""is substantially better in quality"", compared to what?\n\n- In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don\'t tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective.\n\n- Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3.\n\n- ""Conditioning Deep Generative Raw Audio Models for Structured Automatic Music"" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference.\n\n- In the contributions of the paper, it is stated that ""the generated audio is practically identical to ground truth as can be seen in Figure 4"" but the CQTs in this figure are visibly different.\n\n- I don\'t think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio).\n\n- At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued.\n\n- Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning.\n\n- Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities.\n\n- The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without).\n\n- In Section 4.3, specify the unit, i.e. ""Delta < 1 second"".\n\n- For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks.\n\n- In Section 4.3 under ""global conditioning"", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used?', 'This paper describes the use of a wavenet synthesizer conditioned on a piano-roll representation to synthesize one of seven different instruments playing approximately monophonic melodic lines.  The system is trained on MIDI syntheses rendered by a traditional synthesizer.\n\nWhile the idea of end-to-end training of musical synthesizers is interesting and timely, this formulation of the problem limits the benefits that such a system could provide.  Specifically, it would be useful for learning expressive performance from real recordings of very expressive instruments.  For example, in the provided training data, the trumpet syntheses used to train this wavenet sound quite unconvincing and unexpressive.  Using real trumpet performances could potentially learn a mapping from notes to expressive performance, including the details of transitions between notes, articulation, dynamics, breath control, etc.  MIDI syntheses have none of these, and so cannot train an expressive model.\n\nWhile the experiments show that the proposed system can achieve high fidelity synthesis, it seems to be on a very limited sub-set of musical material.  The model doesn\'t have to learn monophonic lines, but that seems to be what it is applied on.  It is not clear why that is better than training on individual notes, as Engel et al (2017) do.  In addition, it is trained on only 9 minutes of audio, but takes 6 days to do so.  This slow processing is somewhat concerning.  In addition, the 9 minutes of audio seems to be the same pieces played by each instrument, so really it is much less than 9 minutes of musical material.  This may have implications for generalization to new musical situations and contexts.\n\nOverall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could.\n\n\nMinor comments:\n\n* The related work section repeats a good amount of information from the introduction. It could be removed from one of them\n\n* Table 1: I don\'t understand what this table is describing.  SynthNet is described as having 1 scalar input, but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input.\n\n* The use of the term ""style"" to mean ""timbre"" is confusing throughout and should be corrected.\n\n* Figure 1: I do not see a clear reason why there should be a discontinuity between L13 and L14, so I think it is just a poor choice of colormap.  Please fix this.\n\n* Page 5: MIDI files were upsampled through linear interpolation.  This is a puzzling choice as the piano-roll representation is supposed to be binary. \n\n* Page 7: ""(Table 3 slanted)"" I would either say ""(Table 3, slanted text)"" or ""(Table 3, italics)"".\n\n* Page 8: ""are rated to be almost twice as better"" this should be re-worded as ""twice as good"" or something similar.\n', 'This paper proposes several architecture changes to a WaveNet-like dilated convolutional audio model to improve performance for MIDI-conditioned single-instrument polyphonic music generation.\n\nThe experimental results and provided samples do clearly show that the proposed architecture does well at reproducing the sounds of the training instruments for new MIDI scores, as measured by CQT error and human preference.  However, the fact that the model is able to nearly-exactly reproduce CQT is contrary to intuition; given only note on/off times, for most instruments there would be many perceptually-distinct performances of those notes.  This suggests that the task is too heavily restricted.\n\nIt isn\'t clearly stated until Section 4 that the goal of the work is to model SoundFont-rendered music.  (The title ""SynthNet"" is suggestive but any music generated by such an audio model could be considered ""synthesized"".)  Using a SoundFont instead of ""real"" musical recordings greatly diminishes the usefulness of this work; adding and concatenating outputs from the single-note model of Engel et al. removes any real need to model polyphony, and there\'s no compelling argument that the proposed architecture changes should help in other domains.\n\nOne change that could potentially increase the paper\'s impact is to train and evaluate the model on MusicNet (https://homes.cs.washington.edu/~thickstn/musicnet.html), which contains 10+ minutes of recorded audio and aligned note labels for each of ~5 single instruments (as well as many ensembles).  This would provide evidence that the proposed architecture changes improve performance on a more realistic class of polyphonic music.\n\nAnother improvement would be to perform an ablation study over the many architecture changes.  This idea is mentioned in 4.2 but seemingly dismissed due to the impracticality of performing listening studies, which motivates the use of RMSE-CQT.  However, no ablation study is actually performed, so it\'s not obvious what readers of the paper should learn from the new architecture even restricted to the domain of SoundFont-rendered music generation.\n\n\nMinor points / nitpicks:\n\nOne of the claimed contributions is dithering before quantization to 8-bits.  How does this compare to using mixtures of logistics as in Salimans et al. 2017?\n\nS2P3 claims SynthNet does not use note velocity information; this is stated as an advantage but seems to make the task easier while reducing applicability to ""real"" music.\n\nS4P1 and S4.1P4 state MIDI is upsampled using linear interpolation.  What exactly does this mean?  Also, the representation is pianoroll if I understand correctly, so what does it mean to say that each frame is a 128-valued vector with ""note on-off times""?  My guess is it\'s a standard pianoroll with 0s for inactive notes and 1s for active notes, where onsets and offsets contain linear fades, but this could be explained more clearly.\n\nWhat is the explanation of the delay in the DeepVoice samples?  If correcting this is just a matter of shifting the conditioning signal, it seems like an unfair comparison.\n\nS1P2 points (2) and (3) arguing why music is more challenging than speech are questionable.  The timbre of a real musical instrument may be more complex than speech, but is this true for SoundFonts where the same samples are used for multiple notes?  It\'s not clear what the word ""semantically"" even means with regard to music.\n\nThe definition of timbre in S3.2P2 is incomplete.  Timbre is not just a spectral envelope, but also includes e.g. temporal dynamics like ADSR.\n\nSpelling/grammar:\nS1P3L4 laborius -> laborious\nS1P3L5 bypassses -> bypasses\nS3.2P2L-1 due -> due to\nS4.4P2L1 twice as better than -> twice as good as\nS4.4P2L3 basline -> baseline\n']","[-40, -30, -20]","[60, 50, 50]","[""The sentiment score is -40 because the reviewer expresses significant concerns about the paper's fundamental approach, particularly the use of synthetic data, which they feel 'defeats the purpose' of the work. They state that the paper has 'fundamental flaws' and 'overstates its contributions'. However, the review is not entirely negative, as the reviewer acknowledges some positive aspects like the architectural modifications and the RMSE-CQT error measure, preventing a more negative score. The politeness score is 60 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I think' and 'I feel' to soften criticisms. They also acknowledge positive aspects of the work and provide constructive feedback and suggestions for improvement. The language is not overly formal or deferential, but it avoids harsh or rude expressions, striking a balance between critique and courtesy."", ""The sentiment score is -30 because while the reviewer acknowledges the idea as 'interesting and timely', they express several significant concerns about the paper's approach and limitations. They point out that the system doesn't fully utilize the potential benefits of end-to-end training, has limited musical material, and has concerning processing times. The overall tone suggests that the paper has potential but falls short in its current form. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'interesting idea' and 'potentially an interesting system' to soften criticism, and provide specific suggestions for improvement. The language is not overly formal or polite, but it avoids harsh or rude phrasing, striking a balance between critique and respect."", 'The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (\'The experimental results and provided samples do clearly show that the proposed architecture does well\'), they also express significant concerns and limitations (\'Using a SoundFont instead of ""real"" musical recordings greatly diminishes the usefulness of this work\'). The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout, offering specific suggestions for improvement (\'One change that could potentially increase the paper\'s impact is to train and evaluate the model on MusicNet\') and providing detailed feedback without using harsh or dismissive language. The reviewer also acknowledges positive aspects before critiquing, which is a polite approach.']"
"['Summary:\nThis paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \\theta) \\log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.\n\nCritique:\nThe gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih & Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  \n\nConclusion: Due to lack of novelty, I recommend rejection.\n\n\nMiscellaneous points:\n“...there exist no simple solutions to circumvent this problem.”  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. \n\n“...after training q(z|x) is a very good approximation to the true posterior p(z|x).”  That’s not necessarily true.  \n\nEquation #2 should be just equal to Equation #1.\n\n“Kingma & Welling (2013) proposed to minimize L(\\theta) using stochastic gradient descent on a training set...”. First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma & Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.\n\nBefore Equation #11, the reference to Equation #4 should be to Equation #5.\n\n“...the weighting...depends only on \\theta_D and not on \\theta_E” (p 4). D and E should be switched.', 'This paper propose to use important sampling to optimize VAE with discrete latent variables. Basically, the methods proposed by this paper is rather simple and trivial. There are some discussions on why important sampling is not a good choice for VAE. Please refer: https://stats.stackexchange.com/q/255756\n\nMoreover, if you focus on VAE with discrete latent variable, you should compare at least with Gumbel-Softmax: https://arxiv.org/abs/1611.01144\n', ""In computing the gradient of the ELBO, the main challenge lies in computing the gradient of the reconstruction loss with respect to the encoder parameters. VAEs traditionally rely on reparameterization in order to obtain a low-variance estimate, but there are a number of other gradient estimators that one can apply. The authors here proprose to use a trick that is known, but perhaps not widely known: If we introduce an importance sampling distribution, then we can use samples from this distribution to compute an importance-weighted estimate of the gradient. The idea is now that we can compute the gradient w.r.t. the encoder parameters as a simple importance-sampling estimate, which obviates then need for reparameterization, or likelihood-ratio estimators. The authors then apply this trick to train VAEs with discrete latent variables.\n\nWhile I think that the idea that the authors present in this paper is worth further exploration, the paper in its current form is not sufficiently mature to appear at ICLR. The two areas where this paper would benefit from improvement are\n\n1. Discussion of related work. \n\nWhile the authors seem to suggest that there has been no work on VAEs with discrete latent variables, there has in fact been quite a lot of work, including work on VAEs that contain both discrete and continuous variables (e.g. [8-10], but I'm almost certainly missing further references). There has also been a large body of work on continuous relaxations of discrete variables that are amenable to reparameterization (e.g. [6-7], and references therein). There has also been a line of work relating importance sampling to variational objectives (see [1-3] as key references). Finally, there is also related work on reweighted-wake-sleep style objectives (see [4]) which similarly don't require reparameterization. From what I can tell, none of these references are cited or discussed as related work. In order to place this work in context, I would rewrite 2 to discuss approaches to gradient estimation in this space, which then makes it much easier to explain how this approach differs. \n\n2. Empirical evaluation.\n\nThe authors only evaluate on MNIST and F-MNIST, and don't compare to any existing approaches. More than a couple of reconstructions, what I would like to see is an analysis of gradient variances, asymptotic ELBO estimates. I would also like to see a larger set of problems. Finally I would like to see a clear comparison to other methods based on, e.g., continuous relaxations. \n\n\nReferences\n\n[1] Y. Burda, R. Grosse, and R. Salakhutdinov, “Importance Weighted Autoencoders,” arXiv:1509.00519 [cs, stat], Sep. 2015.\n\n[2] T. Rainforth et al., “Tighter Variational Bounds are Not Necessarily Better,” arXiv:1802.04537 [cs, stat], Feb. 2018.\n\n[3] G. Tucker, D. Lawson, S. Gu, and C. J. Maddison, “Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives,” arXiv:1810.04152 [cs, stat], Oct. 2018.\n\n[4] T. A. Le, A. R. Kosiorek, N. Siddharth, Y. W. Teh, and F. Wood, “Revisiting Reweighted Wake-Sleep,” arXiv:1805.10469 [cs, stat], May 2018.\n\n[5] A. Mnih and D. J. Rezende, “Variational inference for Monte Carlo objectives,” arXiv:1602.06725 [cs, stat], Feb. 2016.\n\n[6] G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein, “REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models,” in Advances in Neural Information Processing Systems, 2017, pp. 2624–2633.\n\n[7] W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud, “Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,” arXiv preprint arXiv:1711.00123, 2017.\n\n[8] J. T. Rolfe, “Discrete Variational Autoencoders,” arXiv:1609.02200 [cs, stat], Sep. 2016.\n\n[9] E. Dupont, “Learning Disentangled Joint Continuous and Discrete Representations,” arXiv:1804.00104 [cs, stat], Mar. 2018.\n\n[10] B. Esmaeili et al., “Structured Disentangled Representations,” arXiv:1804.02086 [cs, stat], Apr. 2018.""]","[-80, -70, -30]","[20, -30, 50]","[""The sentiment score is -80 because the review is predominantly negative, recommending rejection due to lack of novelty. The reviewer points out that the proposed method is equivalent to an existing technique (REINFORCE) and has been previously used in similar contexts. The politeness score is 20 because while the reviewer maintains a professional tone and provides detailed feedback, the overall message is critical. The reviewer uses phrases like 'Due to lack of novelty, I recommend rejection' which is direct but not impolite. The reviewer also provides constructive feedback and points out specific areas for improvement, which adds a degree of politeness to an otherwise negative review."", ""The sentiment score is -70 because the reviewer expresses a strongly negative opinion about the paper, calling the proposed methods 'simple and trivial' and suggesting that important sampling is not a good choice for VAE. They also point out a lack of comparison with relevant existing methods. The politeness score is -30 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't use any softening language or acknowledge any positive aspects of the paper. The phrases 'rather simple and trivial' and the direct criticism without any constructive feedback contribute to the negative politeness score. However, it's not extremely impolite, as it doesn't use personal attacks or explicitly offensive language."", ""The sentiment score is -30 because while the reviewer acknowledges that the idea is 'worth further exploration', they clearly state that 'the paper in its current form is not sufficiently mature to appear at ICLR'. The reviewer then provides two major areas for improvement, indicating significant concerns with the current state of the paper. However, the score is not extremely negative as the reviewer sees potential in the idea.\n\nThe politeness score is 50 because the reviewer uses respectful and constructive language throughout. They frame their criticisms as areas for improvement rather than outright flaws, using phrases like 'would benefit from improvement' and 'I would like to see'. The reviewer also provides detailed suggestions and references to help the authors improve their work, which is a polite and helpful approach. However, the score is not extremely high as the review maintains a professional tone rather than being overtly friendly or complimentary.""]"
"['This paper aims for open-domain question answering with distant supervision. First, the authors proposed an aggregation-based openQA model with sentence discriminator and sentence reader. Second, they use a semantic labeler to handle distant supervision problem by utilizing other span supervision tasks, and propose two different denoising methods. They run experiments on 3 open-domain QA datasets and achieve SOTA.\n\n\nStrengths\n\n1) Their semantic labeler and exploration of two different denoising methods are interesting and meaningful.\n2) They conducted experiments on 3 widely-used open-domain datasets, and the performance gain is impressive.\n\n\nWeakness\n\nAlthough there is an impressive performance gain, the contribution of the paper seems to be marginal.\n1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot. Also, the architecture of the models in this paper are almost identical to Chen et al (ACL 2017) and Lin et al (ACL 2018).\n2) Thus, the contribution is more on semantic labeler and denoising method. However, this contribution is marginal as well since its role is almost the same as sentence discriminator plus pretraining methods which have widely used already.\n\n\nQuestions\n\n1) What exactly is the difference between semantic labeler and sentence discriminator? For me, it seems like both of them label each sentence `yes` or `no`. My thought is sentence discriminator is only trained on the target dataset (distant supervision dataset) while semantic labeler is also trained (either jointly or separately) trained on the source dataset (span supervision dataset). (If my thought is wrong, please let me know, I would like to update my score.)\n2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. Similarly, Min et al (ACL 2018) have pretrained both QA model and sentence selector on SQuAD. While I think pretraining sentence selector on SQuAD is almost identical to sentence labeler with SSL method, could you give exact comparison of these different methods? For example, remove sentence labeler, and pretrain both sentence discriminator and reader on SQuAD, or jointly train them on SQuAD & target dataset.\n\n\nMarginal comments\n\n1) At the beginning of Section 2.4.1, it says the semantic labeler is able to transfer knowledge from the span supervised data — however, the authors should be careful since people usually refers to `knowledge` as an external knowledge. This method is more like better learning of accurate sentence selection, not transferring knowledge.\n2) Please mention the TriviaQA data you used is Wikipedia domain, since there are two different domains (Wikipedia and Web).\n3) In References section, the conference venues in many papers are omitted.\n\n\nOverall comments\n\nThe paper explored several different methods to deal with distant supervision via sentence labeling, and I really appreciate their efforts. While the result is impressive, the idea in the paper is similar to the methods that have widely used already.', 'This paper proposes a new open-domain QA system which gives state-of-the-art performance on multiple datasets, with a large improvement on QuasarT and TriviaQA. Given the significant results, I would vote for its acceptance.\n\nThe contributions of the paper can be summarized into two parts. The first is an efficient base open-domain QA model; and the second includes several denoising methods of passage selection. I hope the authors could address the following issues during rebuttal, which I believe will make the paper stronger.\n\n(1) The proposed base open-domain QA method (+DISTANT) itself improves a lot, which I think is the major contribution of the paper. It will be very helpful if the authors could provide ablation test to the sentence discriminator/reader modules to give better clues about why it works so well. Is it mainly because of the usage of DrQA style encoder?\n\n(2) Although the paper has ""denoising"" emphasized in the title, I actually do not see this holds as a contribution. First, the proposed semantic labeler training strategies only improve significantly on TriviaQA. The improvement on Quasar-T and SearchQA is relatively marginal. I am wondering whether this is because the domain shift between SQuAD and QuasarT and SearchQA.\n\n(3) (Cont\'d from point 2) Second, the proposed SSL and CSL are not compared with any decent baselines. It will make more sense if the authors apply the Re-Ranker or S-Norm to the proposed base open-domain QA model, and compare the improvement from different methods (Re-Ranker, S-Norm, SSL, CSL). It is likely that the S-Norm could also improve on TriviaQA and the Re-Ranker could improve over Quasar-T and SearchQA. Therefore without any experimental evidence, again, the ""denoising"" part cannot be regarded as a contribution with positive results.\n\nAdditional question: would the authors release the code for public usage if the paper gets accepted?', 'This paper shows that a sentence selection / evidence scoring model for QA trained on SQuAD helps for QA datasets where such explicit per-evidence annotation is not available.\n\nQuality:\nPros: The paper is mostly well-written, and suggested models are sensible. Comparisons to the state of the art are appropriate, as is the related work description. the authors perform a sensible error analysis and ablation study. They further show that their suggested model outperforms existing models on three datasets.\nCons: The introduction and abstract over-sell the contribution of the paper. They make it sound like the authors introduce a new task and dataset for evidence scoring, but instead, they merely train on SQuAD with existing annotations. References in the method section could be added to compare how the proposed model relates to existing QA models. The multi-task ""requirement"" is implemented as merely a sharing of QA datasets\' vocabularies, where much more involved MTL methods exist. What the authors refer to as ""semi-supervised learning"" is in fact transfer learning, unless I misunderstood something.\n\nClarity:\nApart from the slightly confusing introduction (see above), the paper is written clearly.\n\nOriginality:\nPros: The suggested model outperforms others on three QA datasets.\nCons: The way achievements are achieved is largely by using more data, making the comparison somewhat unfair. None of the suggested models are novel in themselves. The evidence scoring model is a rather straight-forward improvement that others could have come up with as well, but merely haven\'t tested for this particular task.\n\nSignificance:\nOther researchers within the QA community might cite this paper and build on the results. The significance of this paper to a larger representation learning audience is rather small.']","[-20, 60, 20]","[50, 70, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths and impressive results, they express significant concerns about the novelty and contribution of the work. The reviewer states the contribution seems 'marginal' multiple times and points out similarities to existing methods. However, they do appreciate the authors' efforts, which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the authors' efforts, and frames criticisms as questions or suggestions rather than harsh statements. They also balance negative points with positive ones. The reviewer maintains a professional tone without being overly formal or deferential."", ""The sentiment score is 60 (moderately positive) because the reviewer starts by stating they would vote for acceptance due to the significant results, which is a strong positive signal. However, they also raise several issues and requests for clarification, tempering the overall positivity. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, framing their critiques as suggestions (e.g., 'I hope the authors could address...', 'It will be very helpful if...') rather than demands. They also acknowledge the paper's strengths before diving into areas for improvement. The tone remains professional and constructive throughout, without any rudeness, though it stops short of being overly deferential or excessively polite."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges several pros of the paper, such as it being well-written, having sensible models, and outperforming existing models on three datasets. However, there are also significant cons mentioned, like overselling the contribution and lack of novelty, which prevent a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral and professional language throughout, balancing criticism with praise. They use phrases like 'mostly well-written' and 'sensible' when describing positive aspects, and 'slightly confusing' or 'over-sell' when critiquing, which maintains a respectful tone without being overly deferential.""]"
"['This paper points out a important issue in current continual learning literature: Due to the different settings and different evaluation protocols of each method, comparison between methods are usually not fair, and lead to distinct conclusions.\nThe paper is in general easy to understand except a few drawbacks listed in the cons.\n\nPros:\n1. This paper investigates an important problem, aka, how does the methods compare to each other with the same evaluation protocol.\n2. Experiments are performed on the previous methods, which could be used as a baseline for future works in this field.\n3. Proposes to combine discriminative model with generative model to save computation when using generative model to store rehearsal examples.\n\nCons:\n1. Details of each experiments are missing. \nDifferent methods are evaluated under the ""incremental task learning"", ""incremental domain learning"",  ""incremental class learning"" settings. However, to my knowledge, some of the methods will not work under all of the three settings, as the author also suggest that XdG only works with task id. However, I think there are a few more. For example, the LwF methods has multiple sets of output neurons, which implicitly assumes the task id is known. It is not described in the paper how to evaluate it under ""incremental domain learning"", aka, how to decide which set of output to use if task id is not available during testing. Another example, the results in table 3 and 4 indicates that EWC with task id is better than without. However, original EWC does not take task id during testing, it is not described how to introduce dependency on the task id for EWC.\n2. Using the term feedback connection is misleading to the reader since the described method is just using an encoder/decoder structure. In my opinion this is different from feedback connection in which higher layer is an input for lower layers. Autoencoder or encoder/decoder structure is more appropriate.\n3. There is some contribution in the RtF part, namely the saved computation compared to DGR. However, subjectively, I think this contribution is not very significant. The same thing can be achieved with DGR by sharing the network between the discriminative model and the discriminator in GAN. In my opinion this is more a design bonus in using generative replay than a major methodology innovation.\n\nConclusion:\nThe first part that compares different methods is worth publishing given more details are provided. I\'m more than happy to give a higher score if the authors are able to provide more details and the details are reasonable.', ""summary: The paper claims to make three contributions\n1. It surveys the current literature on preventing catastrophic forgetting during lifelong learning. It explains the apparent inconsistencies in reported results by distinguishing three types of deployment scenarios, categorizing the evaluation procedures in the literature accordingly.\n2. The paper conducts two sets of simulated experiments on MNIST data to understand which existing methods (do not) work well. It finds that deep generative replay (DGR) that learns to generate imaginary new samples from previously seen training data, potentially augmented with soft labels seems to work best in these specific experiments but potentially doubles the computational cost. \n3. To reduce computational cost without sacrificing much accuracy it proposes to integrate the ability to learn to generate imaginary samples into the learning of the classifier itself. It does this by augmenting a symmetrical VAE with a softmax classification layer connected to the final hidden layer of the encoder. \n\nComments about significance:\n1. I'm not entirely sure if the paper does a good job separating contributions 2 & 3 above cleanly so that each can stand on its own and be fully trust-worthy.  \n2. In particular, the experimental evaluation depends on the NN architectures chosen. Here the choice of architectures that were used for the best performing approach in the experiment (DGR & the classifier) were simply combined together to motivate the new approach. However, this feels a bit too simplistic. for example, what would happen if you replaced the simple 2-hidden-layer NN with a much more sophisticated network for each classifier, but still had a simple VAE to generate samples? the combination is no longer likely to be this easy but it would likely work more accurately than anything shown in table 3. \n\nNovelty: This reviewer feels that augmenting a 2-hidden-layer VAE with a softmax classification layer does not seem to be a very significant new contribution by itself. The fact that it is being motivated for the specific problem of reducing catastrophic forgetting during lifelong learning is the main novelty here, but the relative amount of novelty might to be somewhat limited when viewed from this perspective. "", 'This paper summarizes previous lifelong learning methods and identifies three different continual learning scenarios. Based on that, it draws a conclusion that DGR+distill outperforms other methods on all these scenarios. Further, the paper proposes unified model that combines a replay generator and a classification model. The proposed RTF model achieves comparable performance with DGR+distill and is approximately two times faster than DGR+distill.\n\nMy biggest concern is the novelty of the model, since RTF is still a replay-based method that is very similar as DGR+distill. Empirically it can be expected that RTF should behave similar as DGR+distill as well. And the result in this paper justifies that. So the main contribution comes from the efficiency boost by the integrated model strategy. That is, by replacing a separate generative model by a symmetric VAE. Besides that, there seem to be no significant contribution of the proposed model.\n\nIn my opinion, this paper look somewhat incremental. The first five pages are mostly reviews of previous methods, and the model it propose behave very similar to a previous method.']","[20, -20, -50]","[60, 50, 20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the importance of the paper's topic and its contributions, but also lists several significant cons. The overall tone suggests the paper has merit but needs improvements. The politeness score is moderately high (60) as the reviewer uses respectful language, acknowledges the paper's strengths, and offers constructive criticism without harsh language. The reviewer also expresses willingness to give a higher score if improvements are made, which is a polite way to encourage the authors to address the concerns."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some contributions of the paper, they express significant doubts about the novelty and significance of the work. The reviewer questions the separation of contributions, the experimental design, and the overall novelty of the approach. However, the score is not deeply negative as the reviewer does recognize some value in the paper's survey and experimental work. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They frame their criticisms as 'comments' and 'feelings' rather than harsh judgments, and use phrases like 'I'm not entirely sure' to soften their critique. The reviewer also acknowledges the paper's contributions before presenting their concerns, which is a polite approach to peer review."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the novelty and contribution of the paper. The reviewer states that the proposed model is very similar to existing methods and behaves similarly, indicating a lack of significant innovation. The phrase 'My biggest concern' and the description of the paper as 'somewhat incremental' clearly convey a negative sentiment. However, the review does acknowledge some positive aspects, such as the efficiency boost, which prevents the score from being more negative. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and respectful manner. They use phrases like 'In my opinion' and avoid harsh or insulting language. The tone is direct but not rude, maintaining a level of politeness appropriate for academic discourse.""]"
"['This paper sets out to build good bilingual word alignments from the information in an NMT system (both Transformer and RNN), where the goal is to match human-generated word-alignments as measured by AER. At least that’s how it starts. They contribute two aligners: one supervised aligner that uses NMT source and target representations as features and is trained on silver data generated by FastAlign, and one interpretability-based aligner that scores the affinity of a source-target word-pair by deleting the source word (replacing its embedding with a 0-vector) and measuring the impact on the probability of the target word. These are both shown to outperform directly extracting alignments from attention matrices by large margins. Despite the supervised aligner getting better AER, the authors proceed to quickly discard it as they dive deep on the interpretability approach, applying it also to target-target word pairs, and drawing somewhat interesting conclusions about two classes of target words: those that depend most of source context and those that depend most on target context.\n\nUltimately, this paper’s main contribution is its subtraction-based method for doing model interpretation. Its secondary contributions are the idea of evaluating this interpretation method empirically using human-aligned sentence pairs, and the idea of using the subtraction method on target-target pairs. The conclusion does a good job of emphasizing these contributions, but the abstract and front-matter do not. Much of the rest of the paper feels like a distraction. Overall, I believe the contributions listed above are valuable, novel and worth publishing. I can imagine using this paper’s techniques and ideas in my own research.\n\nSpecific concerns:\n\nThe front-matter mentions ‘multiple attention layers’. It would probably be a good idea to define this term carefully, as there are lots of things that could fit: multiple decoder layers with distinct attentions, multi-headed attention, etc.\n\nIn contrast to what is said in the introduction, GNMT as described in the Wu et al. 2016 paper only calculates attention once, based on the top encoder layer and the bottom decoder layer, so it doesn’t fit any definition of multiple attention layers.\n\nEquation (1) and the following text use the variable L without defining it.\n\n‘dominative’ -> ‘dominant’\n\nIs there any way to generate a null alignment with Equation 3? That is, a target word that has no aligned source words? If not, that is a major advantage for FastAlign.\n\nSimilarly, what exactly are you evaluating when you evaluate FastAlign? Are you doing the standard tricks from the phrase-based days, and generating source->target and target->source models, and combining their alignments with grow-diag-final? If so, you could apply the same tricks to the NMT system to help even the playing field. Maybe this isn’t that important since the paper didn’t win up being about how to build the best possible word aligner from NMT (which I think is for the best).\n\nI found Equations (7) and (8) to be confusing and distracting. I understand that you were inspired by Zintgraf’s method, but the subtraction-based method you landed on doesn’t seem to have much to do with the original Zintgraf et al. approach (and your method is much easier to the understand in the context of NMT than theirs). Likewise, I do not understand why you state, “we take the uniform distribution as P(x) regarding equation 8 for simplicity” - equation 9 completely redefines the LHS of equation 8, with no sum over x and no uniform distribution in sight.\n\nThe Data section of 4.1 never describes the NIST 2005 hand-aligned dataset.\n\nThe conclusions drawn at the end of 4.4 based on ‘translation recall’ are too strong. What we see is that the Transformer outperforms Moses by 2.8 onCFS, and by 3.7 on CFT. This hardly seems to support a claim that CFT words are the reason why Transformer yields better translation.\n\n4.5 paragraph 1: there is no way to sample 12000 datasets without replacement from NIST 2005 and have the samples be the same size as NIST 2005. You must mean “with replacement”?', 'The authors study various NMT models and show that most of them do not learn a good alignment model but still achieve good translation. They suggest that the translated output contains two types of words (i) contributed from source (CFS) and (ii) contributed from target (CFT) and hypothesize that even if the alignment is not good the CFT words contribute to the the better quality of translation (as these are not dependent on alignment).\n\nI have a few questions for the authors:\n\n1) Section 3 is titled ""Proposed Methods for Inducing Word Alignment"". This gives an impression that you are proposing some methods. However, the methods listed in  section 3.2 is from existing work (as you have already acknowledged by citing them correctly). I request you to rename the section or make this more explicit.\n\n2) I am not very convinced about the idea of using a 0 vector to dropout a particular word. Did you try any other options for this ?\n\n3) R_0 and R depend on the model. I mean these scores are as good as the model itself. Therefore I am not sure if it is correct to use these scores to determine CFT and CFS words. Ideally, CFT and CFS words should be a global phenomenon irrespective of the model being used. For example, while translating from a language which does not use articles (a, an, the) to a language which uses such articles, all articles in the target language would be CFT words, irrespective of the model being used. However, this is not the case in your definitions (the definitions are tied to the model scores).\n\n4) Shouldn\'t you also consider a certain margin  in Equation 11. I mean shouldn\'t the LHS be greater than the RHS by a certain margin for the word to be classified as CFS. \n\n5) Conceptually, I don\'t agree with the idea that a word is either CFS or CFT. What about words which require both source and target information? Equation 11 does not completely account for it because the quantity on the LHS does depend on the target words and the quantity on the RHS does dependent on the source words. It is difficult to isolate of effect on source/target.\n\n6) Can you comment on the quality of the silver alignments as the analysis presented in the paper will depend on the quality of these alignments.  \n', ""This paper empirically evaluates whether NMT can predict word alignment. This is done by measuring the alignment error rate between silver-data generated from FastAlign and various methods to extract alignment from NMT. The conclusions are that NMT attention does not predict alignment well, and the proposed method of training an additional alignment extraction model performs better. \n\nUnfortunately, I do not appreciate the motivation of the work. Attention is not alignment. Yes, we can try to extract alignment from attention or other parts of the model. But we should really not expect attention to do alignment, because it was simply not designed to do so. \n\nSo I am not surprised by the first result that AER for attention-based alignment is bad. I am also not suprised by the second result where training an explicit word alignment model (Eq 6) gets better AER, because that is what the model is designed for. \n\nI do understand that probing alignments might increase our ability to understand and interpret these NMT models. I also thought the CFS and CFT analysis was interesting. But unfortunately I don't think the overall problem attacked by this paper is sufficiently well-motivated for a full paper at ICLR. I do think this will be suitable at, for example, a workshop on visualizing/interpreting neural networks in applications. \n\nAdditional note: Please be more specific whether the AER results in Sec 4.2-4.4 are based on scoring on gold alignments in the NIST2015 data, or silver alignments from FastAlign. The former is fine, but the latter might make the results biased for the explicit alignment model trained on the same alignments. ""]","[50, 20, -50]","[80, 80, 20]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's valuable and novel contributions, stating they could imagine using the techniques in their own research. However, they also point out several areas for improvement and clarification. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames their suggestions as 'specific concerns' rather than harsh criticisms. They also use phrases like 'I believe' and 'I can imagine' to soften their statements. The reviewer maintains a professional and courteous tone while providing detailed feedback."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the authors' work and findings, showing interest in the topic. However, they also express several concerns and questions, which tempers the overall positivity. The politeness score is high (80) as the reviewer uses respectful language throughout, framing their concerns as questions and requests rather than criticisms. They use phrases like 'I request you to' and 'Can you comment on', which are polite ways of suggesting improvements. The reviewer also acknowledges the authors' correct citations, which adds to the politeness. The slightly lower than perfect score is due to the reviewer expressing disagreement with some concepts, though this is done in a professional manner."", ""The sentiment score is -50 because the reviewer expresses significant doubts about the motivation and relevance of the paper, stating they 'do not appreciate the motivation of the work' and don't think it's 'sufficiently well-motivated for a full paper at ICLR'. However, they do acknowledge some interesting aspects, preventing a more negative score. The politeness score is 20 because the reviewer uses generally polite language ('Unfortunately', 'I do understand', 'I do think') and offers constructive suggestions, but also directly criticizes the paper's foundation. The reviewer maintains a professional tone throughout, avoiding harsh language while clearly expressing their concerns.""]"
"['This paper unifies both classification and regression task based on the polar prototype network. For classification, the prototypes for all classes are chosen in advance based on a max-margin principle, while the embedding of all instances is then optimized to have small cosine distance to assigned prototypes. For the regression, the output value is interpolated between the two prototypes. Experiments on classification, regression, and combined tasks show the method can achieve good results.\n\nThe idea of using the prototype and the polar system is interesting, and the whole paper is well-written. However, there are still some problems and questions about this paper.\n1. There are two problems with using the max-margin prototypes. First, to maximize the smallest distance between two prototypes, the authors use MC or evolutionary algorithms to do the optimization, which may be time-consuming, and it may be extremely difficult when the prototype space is high dimension. Second, the previous approach indeed obtains discriminative prototypes, but we lose the **class correlation**. In the extreme case, it is equal distance between all prototypes, but some similar classes will have a smaller prototype distance than others. For example, the prototype distance between ""cat"" and ""dog"" should not be the same as that between ""car1"" and ""car2"". The semantic consideration in the paper can solve this problem to some extent, but there needs more evidence.\n\nUsing the pre-defined prototype is also considered in the paper ""M. Perrot et al. Regressive Virtual Metric Learning. NIPS15"". \n\n2. For the unified output space\nOne main contribution is that based on the polar system, the method unifies both classification and regression tasks in the same space. We can also do this in basic embedding algorithms. In the embedding space, a method can do both classification and regression with the nearest neighbor rule (based on majority voting and average respectively). The authors should compare with such kinds of methods in the experiments.\n\n3. Experiments\nFrom the experiments, using semantic cannot improve a lot for the classification task. The authors can try more datasets to validate is this the common scenario. The reviewer strongly suggests the authors should compare with more methods. For example, in some papers the prototypes are learned simultaneously (Snell et al. Prototypical networks for few-shot learning. NIPS17; Wen et al. A discriminative feature learning approach\nfor deep face recognition. ECCV16); while in other cases, there are no prototypes as we optimize the triplet/contrastive loss directly. First, the authors can compare classification performance with these approaches; besides, some visualization results can also show the used prototypes or embeddings. \nThe main advantage of the method is not stressed clearly in the experiments part. The authors can clarify it in later versions.\n\nThe final rating depends on the authors\' response.', 'Proposal to use polar regression for prediction problems. To do so, one maps the target variable into ""maximally separating prototypes"" laid in the D-hypersphere. For classification, the learning problem reduces to minimizing the angle between D-dimensional feature vectors and the associated D-dimensional polar prototype. A similar strategy applies to regression, where the continuous target variable is squeezed to the range of the hypersphere.\n\nThe authors claim that their method unifies, as opposed to much prior art, classification and regression approaches. I disagree with this claim, since we usually approach classification as a (normalized!) regression problem. In some cases the normalization is on the entire output space (single-label classification as in ImageNET), and in some other cases this normalization happens separately in each component of the output space (multi-label classification as in COCO). It is even possible to train an ImageNET classifier using mean squared error given unit-norm feature vectors (Tygert et al, 2017). As such, the ""unification"" proposed by the paper seems a bit blurry to me.\n\nI am unconvinced about the impact shown by the experiments. Table 1 shows accuracies far from the state-of-the-art (91% for all methods in CIFAR-10 versus 97% SOTA, 65% for the proposed method versus 75% SOTA) and throw some separation statistics without a clear correlation to accuracy. The experiment on semantic priors is inconclusive, as all non-baseline results are within error bars. The impact of Section 3.3. is also unclear, since obtaining semantic (digit rotation) interpolations in MNIST is a common feat achieved by unsupervised learning algorithms with decent feature learning.\n\nThe results from section 3.2 are interesting, although I would be interested in seeing a reduction-to-classification baseline, where the years are clustered to set up a classification problem, and the prediction is fine-tuned by a local regression.\n\nNote: Regressing to (random) polar prototypes was proposed in https://arxiv.org/abs/1704.05310', 'This paper proposes a unified framework for both classification and regression and a combination of both using pre-designed prototypes distributed on a hypersphere with max separation. It is nice to see an alternative to the dominant cross-entropy loss and l2 loss for deep classification and regression respectively, also the ability to tackle both in a shared output space is a plus. However, the experiments are limited and not convincing that the proposed framework offers a genuine alternatives to existing formulations. \n\nPros:\n\n•\tThe over idea appears to novel, despite its connections to various previous attempts to angular separation (Hasnat et al., 2017; Liu et al., 2017a; Wang et al., 2018; Zheng et al., 2018).\n•\tIt is nice to see a framework that can perform classification/regression multi-task learning. Many computer vision problems have this nature, e.g. object recognition and pose estimation; face recognition and age estimation. So addressing both problems jointly can potentially bring in mutual benefits.  \n\nCons:\n\n•\tThe experiments on CIFAR 10/100 seems to be at par with a conventional cross-entropy loss. It would be more convincing if more experiments on other more challenging datasets (e.g. ImageNet) using more powerfully backbone networks (e.g. densenet) can be provided.\n•\tIn the CIFAR experiments the hypersphere space dimension is set to the same as the number of classes. In this case, why not just use a one-hot vector to represent each class, and do L2 normalization to the output of the feature extraction network and then do softmax cross-entropy? As pointed out in Section 4, several works project network outputs to the hypersphere for classification through L2 normalization, which forces softmax cross-entropy to optimize for angular separation (Hasnat et al., 2017; Liu et al., 2017a; Wang et al., 2018; Zheng et al., 2018). These works should be compared here to convince the readers why it is necessary to use evolutionary algorithm or Monto-Carlo sampling to set the prototypes rather than just using one-hot vectors. \n']","[-20, -50, -20]","[60, 20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper is 'interesting' and 'well-written', they raise several significant concerns and problems with the approach. The reviewer states there are 'still some problems and questions' and provides detailed critiques on multiple aspects of the work. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms constructively as suggestions (e.g. 'The authors can try...', 'The reviewer strongly suggests...'). The tone is professional and the feedback is presented in a helpful manner, even when pointing out issues."", ""The sentiment score is -50 because the reviewer expresses significant disagreement and skepticism about the paper's claims and results. They state they 'disagree' with a main claim, are 'unconvinced about the impact', and find some results 'inconclusive'. However, they do acknowledge some 'interesting' aspects, preventing a more negative score. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I disagree' and 'I am unconvinced' rather than more harsh language. They also offer constructive suggestions, such as proposing an additional baseline experiment, which contributes to the politeness. The language is not overtly polite, but it avoids rudeness while delivering criticism, hence the slightly positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('nice to see an alternative', 'ability to tackle both in a shared output space is a plus'), they express significant concerns about the limited and unconvincing experiments. The overall tone suggests that the paper has potential but falls short in its current form. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, balancing criticism with praise, and offering specific suggestions for improvement. They use phrases like 'It is nice to see' and 'It would be more convincing if', which maintain a constructive and polite tone even when expressing concerns.""]"
"[""# overview\nIn this work, Nesterov Accelerated gradient based updates are applied in a distributed fashion to scale SGD based training to multiple nodes without the introduction of further hyperparameters or having to adapt the learning rate schedule from that of single node training on the same data.\n\nEvaluation is carried out on image classification workloads using ResNet model variants across CIFAR10,100, and ImageNet datasets, utilizing from 8-32 nodes. In contrasting test error relative to single node performance, the authors find their method degrades less than other synchronous and asynchronous SGD based approaches as node count increases.\n\nOverall, this work is presented in a fairly clear and logical manner, and the writing is easy to follow.  However the approach described appears to be contingent on very specific worker communication patterns and timing which seem unrealistic for real-world settings (namely that each worker sends exactly one update per N sized block received).  Extrapolating from the curvature of the results shown it doesn't appear that DANA would continue to outperform other methods like ASGD once the worker count scales beyond the 32 node limit evaluated.\n\n# pros\n* no additional hyperparameter tuning required\n* should be easy to drop into existing asynchronous SGD implementations, just need to modify the worker side.\n* does appear to scale slightly better from an accuracy perspective in 16-32 node counts\n\n# cons\n* Biggest criticism is the assumption of block random or round-robin worker update scheduling. Presuming each worker will update master exactly once to determine future parameter position is far from realistic on real hardware (varying capacity, performance, system loads, dealing with stragglers) and should probably be considered a synchronous not asynchronous update.\n* only evaluated on image classification tasks on cifar10, cifar100, imagenet on resnet-20 and resnet-50. Would have been better to evaluate on a more varied set of tasks/models/datasets\n\n# other comments\n* Figure 2 baseline performance reported is a bit misleading/confusing since it was only evaluated on a single worker. Would suggest restricting to a single point rather than some extrapolated line that seems to indicate being run on multiple-workers.\n* Figure 3 should should also show multi-node speedups for the other methods compared for completeness. \n* Section 5.2 should report on percentage scaling efficiency rather than using speedup as it doesn't normalize for worker count.  For instance 16x could be interpreted as good or poor if it was achieved using 16 vs 160 nodes.\n* Section 5.2 there's a small typo: GPUs -> GPU\n* Consider https://arxiv.org/abs/1705.07176 in related work?"", 'The paper addresses an important problem in distributed training of deep learning models, i.e., the gradient staleness vs the parallel performance. Keeping the gradient up-to-date in distributed training is important in order to achieve a low test error and high accuracy, but that comes at a cost: the overhead of more communication and synchronization. Asynchronous methods to update the gradient have been proposed, but they usually suffer from staleness, i.e., the communication latency between the master and the slaves impacts the accuracy and training time since the accumulated gradient already is ""old"" in relation to the model parameters when distributed to the slaves. \n\nThe paper proposes an approach to estimate the future model parameters at the slaves using Bengio-Nesterov momentum, thus reducing the effects of the communication latency (the gap) between the master and the slaves when collecting and distributing the gradient. The novelty is mainly on the application and implementation side of the spectrum, and not so much theoretical novelty. The contribution is relatively incremental, but important and clear. Reducing training times with maintained accuracy is an important practical problem, and we need all kinds of measures to address that.\n\nThe evaluation seems solid and the results are very promising. The comparison is done with relevant ""competitors"" (e.g., both synchronous and asynchronous approaches for distributed training). However, since the goal of distributed learning is improved execution performance, I would have liked to see more performance numbers. \n\nMinor:\n* Page 7, top paragraph. It\'s written that Table 2 shows that DANA easily scales to 32 workers. That information is not shown in Table 2... You don\'r show any execution time / speedup numbers at all for ImageNet input. \n', 'Paper offers an improvement to existing approaches using momentum with SGD for asynchronous training across a distributed worker pool. The key value in the proposal seems to be that it works ""out-of-the-box"" and requires no new parameters to be tuned, while delivering similar final accuracy as other distributed methods.\n\nThe authors begin with an explanation of ASGD training, why it doesn\'t scale - worker lags that lead to gap in parameter that gradients are computed on (worker parameters) vs parameters applied (master parameters). It also discusses the kind of momentum approaches that are in use today and how it helps and hurts. \n\nThe new proposal in this paper is DANA that builds on Nesterov Momentum to reduce the lag between these two sets of parameters by predicting the parameters that should be used for computing gradients at each worker.\n\nPros:\n- A key issue with most optimization methods is the number of hyperparameters to tune. DANA is ""out-of-the-box"" in that it doesn\'t introduce any new hyperparameters thus making it easy to scale the training of any model.\n\nCons:\n- The sweetspot for DANA seems to be between 8-24 workers. In practice these days it is pretty easy to run synchronous SGD for these sizes with a setup of 8 GPUs per machine with a few machines. The tuning of learning rate as a hyperparameter is required anyway, and keeping training synchronous doesn\'t really change that. The only issue is if one often changes number of workers for training, which isn\'t typical.\n- ASGD is useful for a larger number of workers as it is harder to train with SSGD for those because of the additional synchronization overhead. That is one area though where DANA starts to have worse behavior than other ASGD approaches.\n\nComments:\n- Paper assumes block-random scheduling for simulation, however in practice it is quite common to have a few workers that are consistently slower. How does this kind of bias effect their methods?\n']","[20, 60, 20]","[60, 70, 50]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges some strengths of the work, such as its clear presentation and potential benefits. However, they also raise significant concerns about the approach's assumptions and limitations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'fairly clear and logical manner' and 'easy to follow' while also providing specific, actionable feedback without harsh language. The review is structured professionally with separate sections for pros, cons, and other comments, which contributes to its polite tone."", ""The sentiment score is 60 (positive) because the reviewer acknowledges the importance of the problem addressed, praises the solid evaluation and promising results, and recognizes the contribution as important and clear, despite being incremental. The reviewer also offers constructive criticism, which balances the overall positive tone. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. The reviewer uses phrases like 'I would have liked to see' instead of demanding changes, and the overall tone is professional and courteous. The minor comment is presented factually without negative connotations."", ""The sentiment score is slightly positive (20) because the review acknowledges the paper's contribution and highlights some pros, such as the 'out-of-the-box' nature of the proposed method. However, it also points out significant cons and limitations, which temper the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer maintains a professional and objective tone throughout, presenting both pros and cons without using harsh language. The reviewer uses neutral phrases like 'The authors begin with...' and 'Comments:' to structure the feedback, which contributes to the polite tone. The critique is presented constructively, focusing on the content rather than making personal remarks.""]"
"['This paper claims to propose a new iRDA method. Essentially, it is just dual averaging with \\ell_1 penalty and an \\ell_2 proximal term. The O(1/\\sqrt{t}) rate is standard in literature. This is a clear rejection.', 'The submission made a few modifications to the RDA (regularized dual averaging) optimization solver to form the proposed ""iterative RDA (iRDA)"" algorithm, and shows that empirically the proposed algorithm could  reduce the number of non-zero parameters by an order of magnitude on CIFAR10 for a number of benchmark network architectures (Resnet18, VGG16, VGG19).\n\nThe experimental result of the paper is strong but the algorithm and also a couple of statements seem flawed. In particular:\n\n* For Algorithm 1, consider the case when lamda=0 and t -> infinity,  the minimization eq (28) goes to negative infinity for any non-zero gradient, which corresponds to an update of infinitely large step size. It seems something is wrong.\n\n* Why in Step 2 the algorithm sets both g_t and w_t to 0 during each iterate? It looks so wrong.\n\n*The whole paper did not mention batch size even once. Does the algorithm apply only with batch size=1? \n\n*What is the ""MRDA"" method in the figure? Is it mentioned anywhere in the paper?\n\n*What are ""k"", ""c""  in eq (25)? Are they defined anywhere in the paper?\n\n*Theorem states 1/sqrt(t) convergence but eq (28), (31) have updates of unbounded step size. How is this possible?', 'iRDA Method for sparse convolutional neural networks \n\nThis paper considers the problem of training a sparse neural network. The main motivation is that usually all state of the art neural network’s size or the number of weights is enormous and saving them in memory is costly. So it would be of great interest to train a sparse neural network. To do so, this paper proposed adding l1 regularizer to RDA method in order to encourage sparsity throughout training. Furthermore, they add an extra phase to  RAD algorithm where they set the stochastic gradient of zero weights to be zero. They show experimentally that the method could give up to 95% sparsity while keeping the accuracy at an acceptable level. \nMore detail comments: \n\n1- In your analysis for the convergence, you totally ignored the second step. How do you show that with the second step still the method converge? \n\n2- \\bar{w} which is used in the thm 1, is not introduced. \n\n3- In eq 5, you say g_t is subfunction. What is it? \n\n4- When does the algorithm switch from step 1 to step 2? \n\n5- In eq 35 what is \\sigma? \n\n6- What is the relation between eq 23 and 24? The paper says 23 is an approximation for 24 but the result of 23 is a point and 24 is a function. \n\n7- What is MRDA in the Fig 1? \n\n']","[-90, -50, 20]","[-50, 20, 50]","[""The sentiment score is -90 because the review is highly negative. The reviewer states that the paper's claim of proposing a new method is essentially just a combination of existing techniques, and the results are described as 'standard in literature'. The review concludes with 'This is a clear rejection', indicating a strong negative sentiment towards the paper. The politeness score is -50 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't offer any constructive feedback or suggestions for improvement, and the tone is curt and somewhat condescending, especially in the final sentence. The review lacks the typical courtesies or softening language often found in more polite academic discourse."", ""The sentiment score is -50 because while the reviewer acknowledges the strong experimental results, they point out several significant flaws in the algorithm and paper. The review starts positively but quickly shifts to a critical tone, listing multiple issues with the methodology and presentation. The politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language like 'It seems something is wrong' rather than harsh criticisms. They pose questions rather than making accusatory statements, which is a polite way of pointing out potential issues. However, the directness of some statements (e.g., 'It looks so wrong') prevents a higher politeness score."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contribution to addressing an important problem (training sparse neural networks) and notes that the proposed method shows promising experimental results. However, the review also includes several critical questions and points for improvement, which tempers the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer maintains a professional and neutral tone throughout, asking questions and pointing out areas for clarification without using harsh or dismissive language. The reviewer begins by summarizing the paper's main points and contributions before moving on to specific comments and questions, which is a courteous approach. The use of phrases like 'More detail comments:' and the numbered list of questions indicates a structured and respectful way of providing feedback.""]"
"['In the paper, the authors proposed a new saliency map method, based on some empirical observations about the cause of noisy gradients.\nSpecifically, through experiments, the authors clarified that the noisy gradients are due to irrelevant information propagated in the forward pass in DNN. Because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.\nTo avoid noisy gradients, the authors proposed a new backpropagation named Rectified Gradient (RectGrad). In RectGrad, the backward pass is filtered out if the product of the forward signal and the backward signal are smaller than a threshold. The authors claim that, with this modification in backpropagation, the gradients get less noisy.\nIn some experiments, the authors presented that RectGrad can produce clear saliency maps.\n\nI liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. The experiments are designed well to support the claim.\nHere, I would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. Each filter in convolution layer is trained to respond to certain patterns. Because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. I think this does not happen if the network is densely connected without convolutional structures. The trained dense connection will be optimized to remove the effects of occluded parts. Hence, for such networks, the gradient will be zeros for occluded parts.\n\nThe second half of the paper (Sec.4 and 5) are not very much convincing to me.\nBelow, I raise several concerns.\n\n1. There is no justification on the definition of RectGrad: Why Rl = I(al * Rl > t) R(l+1)?\nThe authors presented Rl = I(al * Rl > t) R(l+1) as RectGrad, that can filter out irrelevant passes. However, there is no clear derivation of this formula: the definition suddenly appears. If the irrelevant forward passes are causes of noisy gradients, the modification Rl = I(al > t) R(l+1) seems to be more natural to me. It is also a natural extension to the ReLU backward pass Rl = I(al > 0) R(l+1). Why we need to filter out negative signals in backward pass?\n\n2. The experimental results are less convincing: Is RectGrad truly good?\nIn Sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely. However, it is not clear that those ""nicely looking"" saliency map are truly good ones. I expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those ""nicely looking"" saliency maps are truly good ones.\nSec.5.3 presents some quantitative comparisons, however, the reported Sensitivity and ROAR/KAR on RectGrad are not significant. The authors mentioned that this may be because of the sparsity of RectGrad. However, if the sparsity is the harm, the underlying observations of RectGrad may have some errors. I think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of RectGrad.\n\n[Minor Concern]\nIn Sec.5, the authors frequently refer to the figures in appendix. I think the main body of the paper should be self-contatined. I therefore think that some of the figures related to main results should appear in the main part.\n\n', 'This paper studies how to better visually interpret a deep neural network. It proposes a new method to produce less noisy saliency maps, named RectGrad. RectGrad thresholds gradient during backprop in a layer-wise fashion in a similar manner to a previous work called Guided Backprop. The difference is that Guided Backprop employs a constant threshold, i.e. 0, while RectGrad uses an adaptive threshold based on a percentile hyper-parameter. The paper is well-written, including a comprehensive review of previous related works, an meaningful meta-level discussion for motivation, and a clear explanation of the proposed method. \n\nOne of my biggest concern is regarding the experiment and evaluation section. Conclusions are drawn based on the visualization of a few saliency maps. I am not sure how much I can trust these conclusions as the conclusions are drawn in a handy-wavy manner the examples are prone to cherry-picking and . For example, this is the conclusion in the Adversarial Attack paragraph: “we can conclude that Rectified Gradient is  equally or more class sensitive than baseline attribution methods”. As pointed out by the paper, the conclusion can be drawn from Figure 8 in the main paper and Figure 10 in Appendix A.1. However, the proposed method tends to produce a saliency map with higher sparsity, therefore the difference may appear more apparent. It is stretching to conclude that it is more class sensitive without further quantitative validation. \n\nEvaluation appears to be a common concern to the work on saliency maps.  The existing quantitative evaluation in the paper seems disconnected to the visual nature of saliency maps. Concretely, when can we say one saliency map looks better than another? Since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? Is it true that it produces less pixels on the background? If so, can we evaluate it with foreground-background segmentation annotation to prove that point? Though how to evaluate saliency maps remains an open question, I feel some discussion on this paper would make the paper more insightful. ', 'Summary of the paper:\nThis paper proposed RectGrad, a gradient-based attribution method that tries to avoid the problem of noise in the attribution map. Further, authors hypothesize that noise is caused by the network carrying irrelevant features, as opposed to saturation, discontinuities, etc as hypothesized by related papers. \n\nThe paper is well written and easy to read through. \n\nStrengths:\n- Formally addresses a hitherto unanswered question of why saliency maps are noisy. This is an important contribution.\n- RectGrad is easy to implement.\n\nQuestions for authors:\n- Since the authors are saying that the validity of their hypothesis is “trivial”, it would be nice to have this statement supported by more quantitative, dataset-wide analyses on the feature map and training dataset occlusion tests. For e.g., what percentage of the test dataset shows attributions on the 10x10 occluded patch? \n- How does RectGrad compare with simply applying a final threshold on other attribution maps? How do the results on training data and feature occlusion change after such a threshold is applied? How do results on adversarial attacks change?\n- Could this method generalize to non-ReLU networks?  \n- Premise that auxiliary objects in the image are part of the background is not necessarily true. For instance, the hand in “lighter” is clearly important to know that the flame is from a lighter and not from a candle or some other form of fire. Similarly, the leaves in the “frog” example. \n- (Optional) As shown in (https://openreview.net/forum?id=B1xeyhCctQ) gradients on ReLU networks overlook the bias term. In the light of this, what is the authors’ take on whether a high bias-attribution is the cause for the noisy gradient-attribution? \n- (Optional) In some sense, RectGrad works because layers closer to the input may capture more focussed features than layers close to input which may activate features spread out all over the image. It would be interesting to see if RectGrad works for really small networks such as MobileNet (https://arxiv.org/abs/1801.04381) where such an explicit hierarchy of features may not be there. \n']","[-20, 20, 60]","[60, 60, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper in the first half, they express significant concerns and skepticism about the second half. The reviewer states that the second half is 'not very much convincing' and raises several major concerns about the methodology and results. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the strengths of the paper before presenting criticisms. They use phrases like 'I liked the first half of the paper' and 'I would like to point out' which maintain a polite tone. The reviewer also offers constructive feedback and suggestions for improvement, which contributes to the politeness of the review."", ""The sentiment score is slightly positive (20) because the reviewer begins with positive comments about the paper being well-written and having comprehensive content. However, they express significant concerns about the experimental evaluation, which tempers the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They phrase concerns as suggestions and use phrases like 'I feel' to soften critiques. The reviewer maintains a professional tone without using harsh or dismissive language, even when expressing doubts about the paper's conclusions."", ""The sentiment score is 60 (positive) because the reviewer starts by acknowledging the paper is well-written and easy to read. They also highlight the strengths of the paper, noting its important contribution and ease of implementation. The overall tone is constructive and appreciative of the work. However, it's not extremely positive as the reviewer raises several questions and potential issues.\n\nThe politeness score is 80 (quite polite) because the reviewer uses respectful language throughout. They frame their feedback as 'questions for authors' rather than criticisms, and use phrases like 'it would be nice to have' and 'Could this method...?' which are polite ways of suggesting improvements. The reviewer also marks some questions as 'Optional', showing consideration for the authors' time and effort. The language is professional and constructive without being overly deferential.""]"
"['This paper presents a new method for fully- and semi-supervised few-shot classification that is based on learning a general embedding as usual, and then learning a sub-space of it for each class. A query point is then classified as the class whose sub-space is closest to it.\n\nPros: This is a neat idea and achieves competitive results. Learning a sub-space per class makes intuitive sense to me since it’s plausible that there is a lower-dimensional subspace of the overall embedding space that captures the properties that are common to only examples of a certain class. If this is indeed the case, it seems that indeed classifying query examples into classes based on their distances from the corresponding sub-spaces would lead to good discrimination. \n\nCons: First, an inherent limitation is that this approach is not applicable to one-shot learning, and I have doubts in its merit for very low shot learning (explained below). Second, I’m missing the justification behind a key point used to motivate the approach, which requires clarification (explained below). Third, I feel that certain aspects of the approach were unclear (details to follow). Finally, I feel more analysis is needed to better understand the differences of this method from previous work (concrete suggestions follow). For semi-supervised learning, the novelty regarding how the unlabeled examples are incorporated is limited, as the approach used is previously-introduced in Ren et al, 2018.\n\nOverall, even though I like the idea and the results are good, there are a few points, mentioned in the above section that I feel require additional work before I can strongly recommend acceptance. Most importantly, relating to getting more intuition about why and when this works best, and tying it in better with previous approaches. \n\nA key point requiring clarification.\nThere is a key fact that the authors used to motivate this approach which remains unclear to me: why is it the case that this approach is less sensitive to outliers than previous approaches? In Figure 1, an outlier is pictured in each of subfigures (a) and (b) corresponding to Matching and Prototypical Networks, but not in subfigure (c) which corresponds to PSN. No explanation is provided to justify this conjecture, other than empirical evaluation that is based on the overall accuracy only. In particular, since SVD is used to obtain the sub-spaces, instead of an end-to-end learned projector that directly optimizes the query set accuracy, it’s not clear why if a support point is an outlier it would not affect the sub-space creation. If I’m missing something, please clarify!\n\n(A) Comments on the approach.\n(1) Why define X_k as the support set examples minus the class prototype instead of just the support examples themselves? The latter seems simpler, and should have all the required information for shaping the class’ subspace.\n(2) Note that if X_k is defined as [x_{k,1}, \\dots, x_{k,K}] as proposed in the above point (ie. without subtracting the class mean from each support point) then this method would have been applicable to 1-shot too. How would it then compare to a 1-shot Prototypical Network? Notice that in this case the mean of the class is equal to this one example.\n(3) In general, the truncated SVD decomposition for a class can be written using the matrices U, \\Sigma and V^T with dimensions [D, n], [n, n] and [n, K] respectively, where D is the embedding dimensionality and K is the number of support points belonging to the given class. The middle matrix \\Sigma in the non-truncated version would have dimensions [D, K]. Does this mean that when truncating, n is enforced to be smaller than each of D and K? This would mean that the dimensionality n of the sub-space is limited by the number of the support examples, which in some cases may be very small in few-shot learning. Can you comment on this?\n(4) How to set n (the dimensionality of each subspace) is not obvious. What values were explored? Is there a sweet spot in the trade-off between the observed complexity and the final accuracy?\n\n(B) Comparison with Prototypical Networks.\n(1) In what situations do we expect learning a sub-space per class to do better than learning a  prototype per class? For example, Figure 4 shows the test-time performance as a function of the test ‘way’. A perhaps more interesting analysis would be to compare the models’ performance as a function of the test *shot*: if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space? \n(2) Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.\n\n(C) Clarifications regarding the semi-supervised setup.\n(1) Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.\n(2) Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? In Ren et al, 2018, this was prevented by creating an additional labeled/unlabeled split even for the training examples. Therefore they use strictly less overall information at meta-training time than if that split weren’t used. To be comparable with them, it’s important to apply this same setup.\n\n(D) Additional minor comments.\n(1) “To work at the presence of distractors, we propose to use a fake class with zero mean”. Note that this was already proposed in Ren et al, 2018. They used a zero-mean, high-variance additional cluster whose aim was to ‘soak up’ the distractor examples to prevent them for polluting legitimate clusters (this was the second model they proposed).\n(2) In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or ‘ways’ at test time than what was used at training time.\n(3) Gidaris and Komodakis (2018) is described in the related work as using a more complicated pipeline. Note however that their pipeline is in place for solving a more challenging problem than standard few-shot classification: they study how a model can maintain the ability to remember training classes while rapidly learning about new ‘test’ classes.\n(4) In the last line of section 5.3, use N-way instead of K-way since in the rest of the paper K was used to refer to the shot, not the way.\n', 'This paper proposes a Projective Subspace Network (PSN) for few-shot learning. The PSN represents each support set of classes as a subspace obtained by SVD. Then the method calculates distances between a query and classes by the projection error to the subspace. Instead of using the prototype of the class center, the subspace representation is more robust to outliers. Though the contribution seems to be incremental, it is a reasonable improvement upon Matching Networks and Prototypical Networks.\n\nPros. \n+ The proposed subspace method is simple and reasonable\n+ The performance is better than some related works on few-shot learning. \n\nCos. \n- The authors claimed that subspace representation is more robust to noise within each class samples. However, this is not supported by experiments. The authors evaluated the distractor classes. However, this is not the case when the outlier existed within each class samples. \n\n- For semi-supervised few-shot learning, the authors proposed a fake class with zero means. The effect of this fake class is not evaluated. \n\n- The dimensionality of subspace (n) seems to be not written.\n\n- The sensitivity analysis of the dimensionality of subspace is missing. For subspace methods, it is essential to evaluate the performance w.r.t the dimension. \n\n- Descriptions in the related work section should be improved. It is unclear how the proposed method is related to K-means, K-modes, and K-prototype. Also, the authors wrote that works (Chan et. 2015, Sun et al. 2017) use PCA or SVD to reduce the dimensionality of feature representation in neural networks. However, both methods do not perform dimensional reduction. PCANet (Chan et al . 2015) obtains convolutional filters by applying PCA to input images or feature maps. SVDNet (Sun et al. 2017) applies SVD for obtaining decorrelated weights in a neural network. \n', 'This paper considers the problem of few-shot learning and proposes a new embedding-based approach. In contrast to previous work (such as Matching Networks and Prototypical Networks) where distance is computed in pure embedding space, this work proposes computing a low-dimensional subspace to represent a class and using the distance from an embedded query point to this subspace. The low-dimensional subspace for a class is computed by running truncated Singular Value Decomposition on the normalized embeddings of all points in the support set for that class and using the top n left singular vectors as the basis for the class\'s subspace. The authors also propose an extension to their model to the semi-supervised few-shot learning setting by incorporating masked-mean computation and zero-mean cluster for distractor items (both ideas borrowed from Renn 2017 for prototypical networks). Experiments are conducted on Mini-Imagenet in the few-shot learning setting and on Mini-Imagenet and Tiered-ImageNet in the semi-supervised few-shot learning setting.\n\nPros:\n- Proposed idea is novel and proposes an interesting change to existing embedding-based few-shot learning techniques.\n\nCons:\n- Performance benefit is a bit disappointing; Mini-Imagenet few-shot performance improvement relative to Prototypical-Nets is minimal (barely 1% for 5way-5shot and 20way-5shot case). For semi-supervised experiments, there is bigger improvement for Mini-Imagenet (4% for both without distractors and with distractors) but less so for Tiered-ImageNet (close to 0% for without distractors and with distractors).\n\nRemarks:\n- The paper seems to be missing what the dimensionality of the subspace is for the experiments? Was this picked using validation set performance?\n- In first paragraph of page 2, it seems too strong to say ""...this makes our paper unparalleled to previous studies""; maybe change to ""...this make our proposed model novel relative to previous work""\n- Is there previous work that has involved back-propagating through SVD? It would be useful to mention these as references.\n- In Figure 1, it is visually shown how outliers can negatively impact Matching-Networks and Prototypical-Networks but not visually shown how PSN is resistant to them?\n- The claim is made that the proposed method is more robust to outliers. Is there more of a justification that can be given for this? Either in terms of some intuition or an experiment that can be run? For example, can it be shown that outliers cause the prototype of a class to move a lot (in terms of distance from original prototype without outliers) whereas the original subspace compared to subspace with outliers is less different by measuring this on Mini-Imagenet?\n- Typo on page 5: ""in what follwos"" => ""in what follows""\n- In Discussion, paper states, ""Moreover, the Prototypical Network makes use of the class mean and can be easily incorporated in our testbed"": what does this mean exactly?']","[-20, 20, 20]","[60, 50, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('neat idea', 'achieves competitive results'), they express several concerns and state that 'additional work' is needed before they can 'strongly recommend acceptance'. The overall tone suggests more work is required, which leans towards a negative sentiment.\n\nThe politeness score is moderately positive (60) because the reviewer uses respectful language throughout, balancing criticism with praise. They use phrases like 'please clarify' and 'Can you comment on this?', which are polite ways of requesting information. The reviewer also acknowledges the strengths of the paper before diving into criticisms, which is a courteous approach. However, it's not extremely high as the review is still direct in its criticisms."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contribution as a 'reasonable improvement' and lists some pros, but also provides several cons and areas for improvement. The overall tone is balanced, leaning slightly positive. The politeness score is moderately positive (50) as the reviewer uses neutral language and provides constructive feedback without harsh criticism. They use phrases like 'should be improved' and 'it is unclear' rather than more confrontational language. The reviewer also acknowledges the positive aspects of the paper before listing the areas for improvement, which is a polite approach to reviewing."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty of the proposed idea and its interesting approach to existing techniques. However, they also express disappointment with the performance benefits, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses professional language throughout, offers both pros and cons, and phrases criticisms as 'remarks' or suggestions rather than direct attacks. They also use polite language like 'it would be useful' and 'can it be shown' when requesting additional information or clarification.""]"
"['In the paper, author investigate the use of copy mechanisms for the question generation task. It evaluates on the SQuAD dataset. The model is a popular seq2seq/encoder-decoder model with copy mechanisms using pointer networks. \n\nPros:\nIt is well motivated. For the question generation task, a word to be predicted can be from either a global vocabulary list or copied from the given documents (location vocabulary).  There are some overlap between these two vocabulary lists.  This paper mainly investigates this issue.\n\nIt is well written and easy to follow.\n\nInteresting analysis of human/automatic metrics.\n\nCons:\nThe tricks here are a bit of ad hoc. It is better to have a systemic study.\n\nBaseline results are too low. E.g., officially QANet results (from the paper) on SQuAD v1 is around 82.7 (my implementation obtains 83.1). However in the paper, its best result is 72.6 in terms of F1 score. \n\nThe authors only evaluated on one dataset. It is hard to convincing.\n\nIt is lack of comparison results of question generation in literature. \n', 'The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:\n\n1) In model section, the task definition is not clear. It is expected to see what\'s the question generation task studied in this paper. An example or a model overview will definitly help.\n\n2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf ""Seo, Minjoon, et al. ""Bidirectional attention flow for machine comprehension."" arXiv preprint arXiv:1611.01603 (2016).""  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.\n\n3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?\n\n4) In the training, how to deal the ground truth that are not in the vocab? The authors stated ""using a modified heuristic described below"", but no follow-ups in the paper.\n\n5) The paper is not well written and organized. Small typos: in introduction, \'and and answer span\', \'and output and output sequences\'. In model, \'Glorot initialization\', \'Bahdanau attention\', it is not the common way to cite others\' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder.\n\nI have read the authors\' detailed rebuttal. Thanks.', 'This paper presents question generation models by designing variations of copying mechanism and reward functions. Experimental results show that different copying mechanism can improve upon basic seq2seq models, some of the reward functions also produce better results. I think the results are interesting, especially the ones compared with human evaluation (fig. 1), but it\'s might be better to explain on which aspect each of the feature contributes to the improvement. For instance, the authors can give some insights based on empirical results on what kind of questions will benefit from each type of copying. \n\n\nThe authors should better organize table 1 and 2, and inform the readers on what is the consistent conclusion (if any). For table 2, there is no result for adding ""adversarial discriminator"" only. Also the item ""+226"" on the second row, is that an error?\n']","[20, -70, 50]","[50, -20, 60]","[""The sentiment score is slightly positive (20) because the review lists several pros including that the paper is well motivated, well written, and contains interesting analysis. However, it also lists several significant cons that temper the positive sentiment, such as ad hoc tricks, low baseline results, lack of dataset variety, and missing comparisons. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout and frames criticisms constructively as 'Cons' rather than using harsh or dismissive language. The reviewer also acknowledges positive aspects before moving to criticisms, which is a polite approach. However, the language doesn't go out of its way to be overtly polite or complimentary either, maintaining a fairly neutral professional tone overall."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states the paper is 'poorly written and hard to follow', 'lacks novelty', and lists several major criticisms. There are very few positive comments, mainly just acknowledging the importance of the topic. The politeness score is -20 because while the reviewer uses some polite phrases like 'It is expected to see' and 'Thanks' at the end, the overall tone is quite blunt and critical. Phrases like 'poorly written' and 'not clear' are used without much softening language. The reviewer does provide specific recommendations for improvement, which prevents the score from being lower, but the direct criticism and lack of positive reinforcement make the review come across as somewhat impolite."", ""Sentiment score: The review starts with a positive tone, acknowledging the interesting results and improvements shown by the paper. However, it also suggests areas for improvement, indicating a mixed but generally positive sentiment. The score of 50 reflects this balanced but slightly positive view. Politeness score: The language used is professional and constructive. The reviewer offers suggestions politely, using phrases like 'it might be better' and 'the authors should,' which are courteous ways to provide feedback. The tone is respectful throughout, earning a score of 60 for being notably polite without being overly deferential.""]"
"['This paper presents a new adversarial defense method.  I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.  The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value.  The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all).  They use both partially converged perceptron models as well as fully converged perceptron models. \n\nPros:\n\t1. The defense technique does not require knowledge of the attack method\n\nCons:\n\t1. The paper is incredibly difficult to understand due to the writing.\n\t2. The performed experiments are insufficient to determine  whether or not their technique works because:\n\t\ta. They don\'t compare against any other defense techniques.  In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect.\n\t\tb. They do not show the results of an attack by an adversary that is aware of their technique.  (i.e. F(PR(A(F,PR,x))))  Many alternative defense techniques will work much better if we assume the adversary does not know about the technique.\n\t\tc. Their comparison against random noise is not an apples-to-apples comparison.  Instead of perturbing uniformly within a range, they perturb according to a normal distribution.  The random noise perturbations also have a much larger L2 distance than the perturbations from their technique.  To believe that their method is actually better than training with random noise I\'d like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method.\n\t\td. They only show results for one value of epsilon and one value of ""# of colors"" for their technique.  Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large ""# of colors""/small range per color and the attacker chooses a large epsilon).\n\t3. Their use of machine learning models is quite ad-hoc.  In particular they use a perceptron trained on algorithmically generated data.  And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model.\n\t4. The authors partly deanonymize the paper through a github link\n\t5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length.\n', 'The authors propose a defense technique to make the NN model more robust to adversarial events by redrawing the images and use them for training the model so that the model can prevent future attacks. The idea itself is simple but seems to be effective as shown in Tables 2 and 3.\n\nWhat I’m missing here is a simple experiment to see the difference in accuracy performance between 1) when using PR for training the model (which is Case B in Table 2) against attacks versus 2) when not using PR for training the model against other attacks (similar to Table 2, Testing phase but using other attack models not PR as an attack model).\n\nIt is not quite clear to me why using PR as a defense mechanism helps the NN model.  I see its utility when training the NN model but using it as a defense mechanism is not quite clear why it works. \n\nMinor:\nIt is not quite clear how the author chose the hyperparameters, maybe by changing those hyperparameter the attacker could have much clever ways to attack the NN model.', 'This paper proposed the pixel redrawing approach to generate distorted training images to improve the performance of the deep networks, which hopefully can be used to prevent future attacks. The key idea is to randomly perturb the pixel values according pre-defined range and probabilities.\n\n\nThe proposed method is quite simple and is similar to denoising autoencoders in flavor.  My concern is that a pre-defined noisy perturbation may not be general enough to tackle various types of attacks. Ideally the perturbation should take into account properties of the input images, both pixel-wise and structure-wise. Unfortunately the proposed method ignores such information. The performance improvement seems quite limited judging from the results.   \n\n\n']","[-70, 20, -30]","[-20, 60, 20]","[""The sentiment score is -70 because the review is predominantly negative. While it acknowledges one pro (defense technique not requiring knowledge of attack method), it lists numerous cons including difficulty in understanding, insufficient experiments, ad-hoc use of machine learning models, and excessive paper length. The reviewer expresses significant doubts about the paper's methodology and results. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and direct. Phrases like 'incredibly difficult to understand' and 'quality of the paper does not justify the additional length' come across as somewhat harsh. The reviewer doesn't soften criticisms with polite language or positive reinforcement, leading to a slightly impolite overall tone."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the effectiveness of the proposed technique and its simplicity. However, they also express some reservations and request additional experiments, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions, and avoids harsh or dismissive statements. They use phrases like 'What I'm missing' and 'It is not quite clear to me' which are polite ways to express concerns. The review maintains a professional and constructive tone throughout."", ""The sentiment score is -30 because while the reviewer acknowledges the proposed method, they express significant concerns about its effectiveness and generalizability. The reviewer states that the performance improvement seems 'quite limited,' which indicates a negative sentiment. However, it's not entirely negative as they do recognize the proposed idea.\n\nThe politeness score is 20 because the reviewer uses relatively neutral language and avoids harsh criticism. They use phrases like 'My concern is...' and 'Ideally...', which are polite ways to express criticism. The reviewer also acknowledges the proposed method before presenting their concerns, which is a polite approach in academic reviews. However, the score is not higher because the review doesn't include explicitly positive or encouraging language.""]"
"['This paper tries to build a neural net to learn Nash equilibrium of games, even though it has been proved that no uncoupled algorithm can do that, except on specific games, as the ones considered in the example (0-sum, potentiel, solvable by iterated elimination of dominated strategies, etc.).\n\nThe algorithm proposed is a classical neural net without any insight (I believe its behavio must more or less be similar to regret matching).\n\nIn table 10, I do not think that the underline case is the NE. \n\nIn table 12, the algorithm si conveniently initiated at the NE.', 'The paper presents an approach to infer optimal strategies by learning the payoff function of 2 player games with a neural network Q(s, a), where a are the agent actions and s the context (e.g., action of the other player). The strategy is inferred by considering the inputs as free variables at test time and maximizing the learnt Q over its input variables by backpropagation.\n\nThe structure of the neural network in itself is not particularly original. The originality of the paper is to show that experimentally, in some 2-player games (and small sequential games, using an RNN), the policy that is inferred is close to a Nash equilibrium. While this is an interesting result in itself, the games used in the experiment are pretty easy to solve with existing algorithms, so the experimental evidence that this approach can work in practice in difficult cases is weak.\n\nThe motivation and intuition fail to be convincing. There is an excessive usage of analogies with intelligence and life in general that are not particularly enlighting (e.g., ""Even though its hardware is damaged, the software in\nthe cloud (or the eternal soul, arguably) of the mosquito [...]"", the value of the analogy between the cloud and the soul is unclear), and in the end there is no clear explanation of why it should work in practice. \n\nI think the paper in its current form is not ready for publication. More formal arguments and/or stronger experimental evidence are necessary.\n ', 'The paper proposes a method of searching for a Nash equilibrium strategy in games where the strategy-to-payoff mapping is defined by a neural network. The idea is to perform gradient optimization of the payoff w.r.t. the strategy. Preliminary results on tic-tac-toe and variations of the prisoner’s dilemma task are presented. The paper has an interesting idea at the core. However, it is poorly written, does not properly discuss the related works and does not present a convincing method or experimental results. \n\nPros:\n* The paper considers an interesting question of exploring the applications of neural networks to the game theory problems.\n* The idea of the paper is reasonable. It makes sense to me to perform gradient-based search over the strategies (assuming that the payoff is differentiable).\n\nCons:\n* Writing\n- The paper is over the mandatory length limit of 10 pages.\n- The paper makes a grandiose claim: “this paper provides a revolutionary way for reinforcement learning and a possible road toward general A.I.” However, there are arguably no revolutionary ideas, and certainly no reinforcement learning experiments!\n- Despite the claim, the novelty of the paper is limited. There is no discussion of the related work: optimization of the neural networks w.r.t. the inputs [1]; related RL ideas such as model-based learning [2,3] and Monte-Carlo Tree Search [4].\n- The problem being solved is never formally stated. As far as I understand, Nash equilibrium is usually defined (1) in mixed strategies, while the paper seems to consider pure strategies; (2) in the scenario where every player attempts to maximize their payoff, while in the paper the players attempt to achieve some pre-fixed value of the payoff.\n- The flow of the paper is generally poor. Instead of presenting a general solution and then showcasing its applications, the paper iterates on similar ideas multiple times. For example, all four algorithms are just gradient-based optimization of either the weights or the inputs to a model.\n- The paper provides extremely misleading analogies and explanations. I am quite sure that a mosquito brain is not a one hidden layer fully-connected neural network! Also, the example of avoiding a moving hand is poor: since the outcome is life or death, the learning should happen via evolution, not during the lifetime of a single insect. The claim that the neural networks with sigmoid activation functions are less prone to local optima is questionable as well.\n\n* Method and experiments\n- The proposed method is essentially a greedy gradient-based planning procedure. For this to work, we need to have a very good environment model. This is a strong assumption that is not discussed.\n- The experiments are performed on very simple synthetic problems: matrix games and tic-tac-toe. They do not suggest that the method is general and can work on harder problems, say, Sokoban [2].\n- The experiments do not present any baselines, so it is unclear how well the method performs compared to the alternatives. One obvious candidate is gradient-free optimization, such as Nelder-Mead, and gradient descent with momentum, which can be less prone to local optima.\n\n[1] Brandon Amos, Lei Xu, J. Zico Kolter “Input Convex Neural Networks”, ICML 2017\n[2] Racanière et al. “Imagination-Augmented Agents for Deep Reinforcement Learning”, NIPS 2017\n[3] David Ha, Jürgen Schmidhuber “Recurrent World Models Facilitate Policy Evolution”, NIPS 2018\n[4] Thomas Anthony, Zheng Tian, David Barber “Thinking Fast and Slow with Deep Learning and Tree Search”, NIPS 2017']","[-70, -50, -60]","[-30, 20, 20]","[""The sentiment score is -70 because the reviewer is highly critical of the paper's approach and findings. They point out fundamental flaws in the methodology, stating that it's been proven that the proposed approach can't work except in specific cases. The reviewer also criticizes the lack of insight in the algorithm and questions the accuracy of some results. The politeness score is -30 because while the reviewer doesn't use explicitly rude language, the tone is quite dismissive and blunt. Phrases like 'without any insight' and 'conveniently initiated' suggest a lack of respect for the authors' work. The reviewer doesn't offer any positive feedback or constructive suggestions for improvement, which contributes to the overall negative and impolite tone."", ""The sentiment score is -50 because the reviewer expresses several criticisms and concludes that the paper is not ready for publication. They mention that the experimental evidence is weak, the motivation and intuition are not convincing, and more formal arguments or stronger evidence are necessary. However, they do acknowledge some positive aspects, such as the originality of the approach and interesting results, which prevents the score from being more negative. The politeness score is 20 because the reviewer uses generally professional language and offers constructive criticism. They avoid harsh or personal attacks, instead focusing on the paper's content. The reviewer also acknowledges the interesting aspects of the work. However, the tone is not overly polite or enthusiastic, maintaining a more neutral professional stance."", ""The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges an 'interesting idea at the core' and lists some pros, the majority of the review focuses on significant cons, including poor writing, limited novelty, misleading explanations, and weak experimental results. The reviewer states that the paper 'does not present a convincing method or experimental results,' which strongly indicates a negative sentiment. The politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language to describe the issues. They begin by acknowledging positive aspects before detailing the negatives, which is a polite approach. However, some statements (e.g., 'grandiose claim,' 'extremely misleading analogies') are more direct criticisms, preventing a higher politeness score.""]"
"[' This paper proposed a so-called ISS-GAN framework for data hiding in images, which  integrates steganography and steganalysis processes in GAN. The discriminative model simulate the steganalysis process, and the steganography generative model is to generate stego image, and confuse steganalysis discriminative model. \n\nOverall the application seems interesting. My concern is its use in real secure information transmission systems: it can fool human eyes but what is its capacity against decoding algorithms; if the intent is to transmit some hidden information, how the receiver is supposed to decode it; is there something similar to the public key in encryption systems? These basic questions/concepts should be made clear to the reader to avoid confusion. \n\nThe evaluation protocol should be clarified and especially on how the PSNR is calculated (i.e., using the reconstructed secret image and real one?) ', 'The paper is on applying GAN to steganography, and it is especially suited for people who does not know steganography. In fact, 1/4 of the paper is spent on introducing (in a very clear and catchy way) the basics. State of the art section nonetheless shows that the authors are grounded with the very last related work. The novel GAN framework is called ISS-GAN. Simply speaking, ISS-GAN is built by the combo: <steganography generative, steganalysis discriminative>. In the details, the implementation is not straightforward, and embeds interesting ideas. Two generative and two discriminative models are intertwined: one generative for the stego image generation process, the other generative for the secret image extraction process. The two discriminative models are for ensuring that the distribution of cover images is indistinguishable from the stego distribution and to ensure that the distribution of extracted secret images is indistinguishable from the distribution of the original secret images. In rough terms, the adversarial double loop serves to ensure that the embedding and the extraction function preserve both the aspect of the original and the secret images. PSNR and SSIM metrics are employed to give a quantitative check of the approach. \nObvious questions are about the scalability of the approach, since it applies to 256x256 miniatures, which act well on the figures of an ICLR paper but poor in reality. At the same time, is it reasonable to show images such that Figure 3? It is obvious stega processes are now very effective, and stressing that by showing seemingly same images it is not very illustrative. Figure 4 typos are reated to row column ordering, but even in this case is hard to spot something without spending time in magnifying the pdf playing at look at the differences… Figure 5 and 6 are even worse, with tens of practically equal images. Less images with differences pictures should have been better. In addition, with these qualitative results, it is hard to give a weight to the quantitative comparisons, in terms of both the PSNR and SSIM metrics.\nThe most obvious and perhaps elementary question I have is how risky is that the ISS GAN model can be replicated by an attacker which sniffs the secret images by simply having a training set which approximatively is the same of the primary sender?\nThe other questions are about how much realistic is that the images are affected by the noises of Table 5, nowadays?\n', ""Paper uses Generative Adversarial Networks' (GAN) paradigm to achieve Steganography/Steganalysis but  fails to dig deeper into obtained results to explain suitability of GAN for Steganography/Steganalysis or the types of embedded images (textures, patterns, complexity etc.) most suitable.\n\n- Figure 3/Page 7: Providing the residual differences between covers and stego-images would be appreciated. \n- Define Acronyms before using them: ISS-GAN for example (Integrated Steganography-Steganalysis (ISS)).\n- General concepts like State of the Art should not in our humble opinion be used through acronyms (SOTA).\n - Page 3/Paragraph (Deep Learning based Steganography):  Clean your references:\n     - Paper (Volkhonskiy et al.,2017) , Paper (Dong et al., 2018), Paper (Shi et al.,2017).\n\nIn need of a serious rewriting:\n - No need for 1. Introduction's first paragraph. Good intention for a simplistic definition scenario rendered redundant by following paragraph.\n - Multiple needless repetitions especially for definitions such as those of Steganography/Steganalysis (Paragraphs 4 & 5, page 4).\n - Ill-articulated phrases:\n    - Introduction: - 'Steganalysis as the counterpart, is an attack to the steganography'.\n                               - 'Since their birth, steganography and steganalysis promote the progress of each other.' Just say complementary processes.\n    - Paragraph 3/Page3: 'Their model is suitable for embedding secret with the random key.'.\n\nIn need of more proofing (Phrasing errors): \n  - Abstract: 'Its application on data generation' instead of 'It’s application on data generation'.\n  - Introduction: 'Imagine that you are' instead of 'Imaging that you are'.\n  - Multiple unjustified uses of  'the':\n       - we propose 'the' novel framework.\n       - You are on 'the' trip to New Orleans\n  ""]","[-20, 60, -50]","[50, 20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the application as 'interesting', they express several concerns and questions about the paper's content and methodology. The reviewer points out missing information and unclear aspects, which suggests the paper needs improvement. However, the criticism is not harsh, hence the score is only mildly negative. The politeness score is moderately positive (50) as the reviewer uses neutral language and phrases their concerns as questions rather than direct criticisms. They use phrases like 'My concern is' and 'should be clarified' which are polite ways of pointing out issues. The reviewer also acknowledges positive aspects ('the application seems interesting') before presenting concerns, which is a polite approach to criticism."", ""The sentiment score is 60 (positive) because the reviewer begins with praise for the paper's clarity and comprehensive introduction to steganography. They also commend the authors for being up-to-date with recent related work and describe the novel GAN framework in detail, highlighting its interesting ideas. However, the score is not higher due to some criticisms raised about the scalability of the approach and the presentation of results. The politeness score is 20 (slightly polite) because the reviewer maintains a professional tone throughout, avoiding harsh language. They present their criticisms as 'obvious questions' rather than direct criticisms, which softens the impact. However, the politeness is not extremely high as the reviewer does directly point out issues with the figures and presentation of results without much cushioning language."", ""The sentiment score is -50 because the review starts with criticism about the paper's failure to dig deeper into results, which sets a negative tone. However, it's not entirely negative as it acknowledges the paper's use of GANs for steganography. The politeness score is 20 because while the reviewer uses some polite phrases like 'in our humble opinion' and 'would be appreciated', the overall tone is quite direct and critical. The reviewer points out numerous areas for improvement, including rewriting needs and proofing errors, which could be perceived as somewhat impolite. However, the criticism is presented in a professional manner with specific suggestions, which prevents the score from being negative.""]"
"['Contributions:\n\nThe main contribution of this paper is the proposed DelibGAN for text generation. The framework introduces a coarse-to-fine generator, which contains a first-pass decoder and a second-pass decoder. Instead of using a binary classifier, the discriminator is a multiple instance discriminator. Two different variants of DelibGAN are proposed, with experiments showing that DelibGAN-II performs the best.\n\nStrengths:\n\n(1) Novelty: I think this paper contains some novelty inside. Using a coarse-to-fine generator is an interesting idea. However, as I noted below, the paper is not well-executed.\n\nWeaknesses:\n\n(1) Presentation: This paper is easy to follow, but poorly written.\n\nFirst, the paper is too repetitive. For example, the two-pass decoding process has been repeatedly mentioned too many times in the paper, such as the paragraph above Eqn. (8). Please be concise. \n\nSecond, when citing a paper, there should be a space between the word and the cited paper. For example, in the first paragraph of the introduction section, instead of ""(RNN)(Mikolov et al., 2011)"", it should be ""(RNN) (Mikolov et al., 2011)"". This should be corrected for the whole paper.\n\nThird, in the first paragraph of the introduction section, I am not sure why (Wang & Wan, 2018a) is cited here. This is not the first paper that points out the problem. One should refer to [a], which is also not cited in the paper. \n\nMissing reference: I also encourage the authors citing [b] since it is directly related to this work, which is about using GAN for text generation.  \n\n[a] Sequence Level Training with Recurrent Neural Networks, ICLR 2016\n[b] Adversarial Feature Matching for Text Generation, ICML 2017\n\n(2) Evaluation: My main concern lies in the experimental evaluation, with detailed comments listed below.  \n\nQuestions:\n\n(1) In Algorithm 1, there exists the pretraining process of G_1 & G_2. However, it is not clear to me how this this pretraining is implemented, since the output of G_1 is not observed, but is hidden and imagined by the model. So, what is the training signal for pretraining G_1? Can the authors provide more details? Please clarify it. \n\n(2) In experiments, why the authors only compare with SeqGAN, SentiGAN & MaskGAN? One would naturally ask how the proposed model compare with RankGAN, TextGAN and LeakGAN. For example, what are the corresponding results of RankGAN, TextGAN & LeakGAN in Table 1 to 5? This should not be difficult to compare with, based on the availability of Texygen.\n\n(3) Besides using the perplexity, the authors also use the novelty and diversity terms defined in (11) & (12) for evaluation. This is good. However, why not using some commonly used metrics in the literature, such as BLEU and self-BLEU? I agree these metrics are also not perfect, but it will be helpful to also report these results for benchmark purposes. \n\n(4) Instead of using datasets like HappyDB & SSTB, it would be helpful to also report results on some benchmark datasets such as COCO captions and EMNLP News as used in the LeakGAN paper.  What are the results looking like on these two datasets?\n\n(5) The results in Table 1 & Figure 2 is misleading. They do not necessarily mean the proposed model is better, as the NLL value only measures how the generated sentence look like a real sentence, but it does not measure the diversity of generated sentences. For example, a model that only repeatedly produces one very realistic sentence would also achieve a very high NLL score.  \n\n(6) Table 3 & 5 shows the human evaluation results. However, how this is performed is not clear at all. For example, is the human evaluation performed using AMT? How many samples are used for human evaluation? Also, how many workers has one sentence been evaluated by? Without all these details, how reliable this result is is questionable.\n\nMinor issues:\n\n(1) In Eqn. (3), since only two classes exist, there is no need to say ""softmax"", use ""sigmoid"" function is enough for illustration. \n\n(2) In the line below Eqn. (5), ""bias"" => ""baseline""\n\n(3) In Eqn. (3), there is "")"" missing. \n\n(4) In Figure 2, there are two methods ""SentiGAN"" and ""DelibMLE"" with the same green color. Which one is which is hard to see. \n\n(5) In the first paragraph of related work, MaskMLE => MaskGAN.  \n', ""Summary:\nThe authors propose a novel adversarial learning framework consisting of a coarse-to-fine generator and a multiple instance discriminator to generate high-quality sentences without supervision, along with two training mechanisms to make the generator produce higher-quality sentences.\n\nThe coarse-to-fine generator is implemented as a two-stage decoder where the first stage decoder is initialized with a noise vector z and produces an initial sequence by sampling from its distribution at each step. The second stage decoder is able to attend to first stage decoder hidden states when generating its sequence and is initialized with the last hidden state of the first state decoder\n\nTwo training frameworks are defined to help the coarse-to-fine generator learn to produce coherent text. In both frameworks, a multi-instance learner is used as a discriminator and the overall score for the sequence is an average of word-level scores. \n\nIn DelibGAN-I, the first stage decoder is just trained to minimize the negative log likelihood of producing the sampled sequence. I’m not sure I understand how G1 learns to generate coherent text by just maximizing the probability of its samples.\n\nIn DelibGAN-2, the word-level scores from passing the first stage decoder’s output through the discriminator is used to score the sequence. This makes more sense to me w.r.t to why the first stage generator could learn to produce coherent text.\n\nReview:\nThe writing could be clearer. Spacing is consistently an issue in w.r.t. to citations, referencing equations an figures, and using punctuations. Equations are difficult to understand, as well. See Equation 5. I’m confused by how the discriminator score of x_t is computed in equation 5. It seems to be the score of the multi-instance learner when applied to the concatenation of the previous generated tokens and the most recently generated token. This isn’t really a roll-out policy, however, since only a single step is taken in the future. It’s just scoring the next token according to the discriminator. In this case, I’m not sure what the summation over j is supposed to represent. It seems to index different histories when scoring word x_t. The authors should clarify exactly what the scoring procedure is for sequences generated by G2 and make sure that clarification matches what is written in the equation.\n\nBecause deliberation has previous been explored in Xia et al., 2017, the novelty of this work rests on the application of deliberation to GAN training of generative text models, as well as the development of the multi-instance discriminator for assigning word-level scores to the first-stage decoder's actions. However, it’s difficult to know how much of the improvement in performance is due to the modeling improvement because the evaluations are missing key details.\n\nFirst, no information about the model’s hyperparameter setting were provided. Naturally, having two generators would double the number of parameters of the total model. Was an ablation run that looked at doubling the parameters of a single-generator? How powerful are the models being used in this new setup. With powerful models such as the OpenAI GPT being available for generative modeling, how powerful are the base units in this framework?\n\nSecond, I don’t have much intuition for how difficult the tasks being performed are. Because the authors aren’t evaluating on particularly difficult language datasets, they should provide statistics about the datasets w.r.t to vocabulary size, average length, etc.\n\nConsequently, a lot of information that is necessary for evaluating the strengths and weaknesses of this paper are missing from the write-up.\n\nQuestions:\nEquation 2 is slightly confusing. Is the representation of the word at that time step not conditioned on previous and future hidden states?\n\nIs the multi-instance discriminator used in DelibGAN-II or is G2 only scored using the sentence level discriminator score?\n\nSmall:\n-Wang & Wan, 2018a are definitely not the first work to discover that using word-level losses to train models for sentence-level evaluations is suboptimal\n-Please fix issues with spacing. There are issues when citing papers, or referencing equations, or using punctuation.\n-Equation 3 is missing a closing parenthesis on the softmax"", ""This paper proposes using a deliberation network approach to text generation with GANs. It does this by using two decoders where the discriminator is trained on the second decoder and signals from that training are also used to improve the first decoder.\n\nSince both approaches have been published before (deliberation networks and policy-gradient based GAN training), the novelty seems to be limited. Additionally, the experiments appear to mostly be focused on single sentence text generation, which is usually not an issue for maximum likelihood trained models. The poor samples from DelibMLE appear much worse than typical maximum likelihood trained language models. Since the model sizes and hyperparameters are not given, it's possible this is due to an overly small model. The lack of a detailed description of the model hyperparameters would also make this work very hard to reproduce.\n\nThere's also very little description of the datasets, for example, the length of a typical example. Finally, while the human evaluation results are interesting, the lack of standard deviations make it hard to measure the significance of the results, especially when only 100 sentences from each model is rated.""]","[-30, -30, -50]","[20, 20, 0]","[""The sentiment score is -30 because while the reviewer acknowledges some novelty in the paper, they express significant concerns about the execution, presentation, and evaluation. The review lists more weaknesses than strengths and raises several critical questions about the methodology and results. However, it's not entirely negative as it does recognize some positive aspects.\n\nThe politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I think' and 'I encourage' which soften criticism. They also provide constructive feedback and specific suggestions for improvement. However, some statements are quite direct (e.g., 'poorly written', 'misleading'), which prevents a higher politeness score. The reviewer balances critique with acknowledgment of the paper's strengths, maintaining a generally respectful tone while delivering honest feedback."", ""The sentiment score is -30 because while the reviewer acknowledges some novel aspects of the work, they express several concerns and criticisms. They point out issues with clarity, missing details, and question the novelty and effectiveness of the proposed approach. However, it's not entirely negative as they recognize some positive aspects.\n\nThe politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'The authors propose' and 'Please fix issues'. They ask questions and provide constructive feedback rather than harsh criticism. However, the tone is not overly polite or deferential, maintaining a neutral academic style with some direct critiques."", ""The sentiment score is -50 because the review is generally critical of the paper, pointing out limitations in novelty, experimental design, and lack of detailed information. However, it's not entirely negative as it acknowledges some interesting aspects (e.g., human evaluation results). The politeness score is 0 (neutral) because the reviewer maintains a professional tone without being overly polite or rude. They present criticisms directly but without harsh language, focusing on the content rather than personal attacks. The review uses phrases like 'appears to be', 'it's possible', which soften the criticism slightly, but overall maintains a neutral, matter-of-fact tone typical of academic peer reviews.""]"
"['The authors of this paper compare the robustness of CNN and CapsNet to global translation on the MNIST dataset. Both models were trained on the standard training set of MNIST, and then tested on a set with digits shifting from the upper left corner to the lower right corner. The results of both models were poor. To improve it, the authors add some shifted digits to the training set, and the performances of both models were significantly enhanced. Moreover, the performance of CNN was better than that of CapsNet in the experiments. Generally speaking, the work presented in this paper is clear and straightforward. However, the work is not significant enough to publish as a ICLR paper. Below is my major comments.\n\n1. There are lots of typos and grammatical errors everywhere in the paper. Thus, the manuscript was not well prepared.\n\n2. It is unclear which CapsNet and what settings were used in the experiment.  \n\n3. It is well-known that convolutional networks are good at capturing local patterns from the images, while capsule networks enhance it to consider global configurations of the local patterns, and robust to affine transformation. Obviously, the experiments presented in this manuscript is too simple. Lots of work should be done in the investigation. For example,o on the training set and shifted test set, the authors can enlarge the background and keep the digits in the original size to make it as a local pattern in the image. Will it be detected by CNNs with larger receptive fields for the images? How is it compared with CapsNets? \n\n4. How are both models compared on other (perhaps more complicated and larger) datasets?\n\nIn summary, the work presented here is interesting, but lots of work should be done in order to make it publishable.\n\n', 'Thanks for the submission of you work. As far as I understood it correctly you deal with the idea to test the shift invariance of a given model on Helvetica digits. You propose that idea as general quantifiable approach.\n\nIn general, your paper is well-written, compact, in a good style and with a length of 5 pages really short.\n\nIn the introduction, you cite the work of Hinton and Sabour and describe the Capsule framework in general. I think that this description is not really precise. The aforementioned contributions are really different and your general explanation is technically incorrect.\n\nThe major concerns about your contributions are:\n1. Helvetica digits are not in American digit style, e.g. check the digit one. The MNIST database consists of American handwritten digits. Why you’re not using an American digit font? What is the impact on your model due to that change?\n2. Fig. 1 in your contribution: It seems that your digits are too small compared to MNIST digits. Is that true? Note that MNIST digits are size normalized.\n3. What is the outstanding advantage of your proposal compared to a simple shift of a MNIST digit or the usage of the affNIST dataset?\n4. Page 4: You are mentioning that it is interesting that the reconstructed image looks like a handwritten digit. Why you think it is interesting? I would assume that this is a natural behavior since your network was trained to do so.\n\nPlease clarify the questions above and highlight what is the advantage of your method compared to simple shifts of MNIST digits or even the usage of affNIST. Right now, I’m not seeing a real advantage neither a scientific contribution.  \n', ""Authors present a study to compare global translation invariance capabilities of CNNs and CapsuleNets.\nThe paper doesn't introduce any novel concept or technique but it simply compares two established techniques on MNIST dataset. The interest on this paper is rather limited. Besides many technical concepts are not really accurate on how they are presented. It needs further attention and improvements.\nThe paper reads more like a review papers than a new research article. I remain to my initial decision.\n""]","[-20, -50, -70]","[50, 50, -20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the work is 'clear and straightforward' and 'interesting', they also state that it's 'not significant enough to publish' and that 'lots of work should be done' to make it publishable. The overall tone suggests that the paper needs substantial improvement. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'Generally speaking' and 'It is unclear' which maintain a professional and courteous tone, even when pointing out shortcomings."", ""The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('well-written, compact, in a good style'), they express major concerns and state they don't see 'a real advantage neither a scientific contribution.' This indicates a generally negative sentiment, though not extremely harsh. The politeness score is 50 because the reviewer uses polite language ('Thanks for the submission', 'Please clarify'), frames criticisms as questions or suggestions, and maintains a professional tone throughout. However, it's not extremely polite, as the critique is direct and doesn't include many softening phrases or compliments."", ""The sentiment score is -70 because the reviewer expresses a clearly negative opinion about the paper. They state that the paper's interest is 'rather limited', doesn't introduce any novel concepts, and needs 'further attention and improvements'. The reviewer also mentions that many technical concepts are not accurately presented. The final statement 'I remain to my initial decision' implies a rejection, further contributing to the negative sentiment. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the criticism or offer encouragement. Phrases like 'doesn't introduce any novel concept' and 'needs further attention and improvements' are blunt and could be perceived as somewhat impolite in academic discourse. However, the reviewer doesn't use explicitly rude language, which is why the score isn't lower.""]"
"[""This paper presents a GAN approach adapted for multi-modal distributions of single class data. The generator is trained to generate samples in the low density areas of the data distribution. The discriminator is training to distinguish between generated and real samples and hence is able to discriminate between normal data (=real) and anomalous data (=generated, in low density areas of the normal data). \n\nTo force the model to map the different modes of the data, a categorical latent variable is used that represents the potential distribution modes. Both a one-hot code and a Gaussian mixture model are explored. This is not a novel approach, however, no citations are provided.\n\nTo force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used. The authors cite OCAN [zheng18], which in turns cites [Dai, NIPS17]. This approach has the advantage that no threshold needs to be fine-tuned since the discriminator can directly be used for anomaly detection.\n\nConstraints derived from both these goals are included in the loss function, which, in addition, includes terms to encourage diversity and similarity of the generated samples.\n\nThe model is tested on a proprietary dataset of real manufacturing product. The dimension of the data is 280 (after proprietary feature extraction). The authors compare their approach to 9 other anomaly detection methods. The reported performance is the highest. The OCAN method has similar performance. The authors specify that fine-tuning is need for all other methods (except OCAN). Fine-tuning is performed on the same data used for testing, hence providing a marked advantage. However, I do not understand why OCAN is listed in table 1 with both fine-tuning and no fine-tuning (raw). This is not explained and should be clarified. In any case, the combination of Complementary-GAN and the multi-modal latent variable seem to be very effective on this dataset. To understand whether this approach is really superior, other benchmark datasets should be tested.\n\nThe article is technically sound. The citations are generally ok, except for the missing citation related to the use of latent categorical codes for push the model into mapping multiple modes of the data. The math is reasonable, although some notations are a bit hard to follow. The English needs to be improved. There are many grammatical errors and the paper needs to be proof-read. Some errors make it hard to understand the text. In particular the adjective modal is used throughout the paper as a noun instead of 'mode'. There are also several LaTex formatting errors which lead to some gibberish and some of the figures are too small making them unusable when the paper is printed.\n\nOverall, I think the paper is incremental, as it combines two previously published methods. It also lacks generality as only one (proprietary) dataset is used. English needs to be proof-read and formatting errors fixed.\n"", 'The authors consider anomaly detection problem.\nThey claimed that typical methods do not perform well on multi-modal distributions.\nThe authors trained GAN to generate samples in low-density regions of the original probability density. Since a discriminator is trained to classify such samples from real data, mainly belonging to high-density regions of the original probability density, then the discriminator can be used to detect anomalies.\n\nComments\n1) the authors wrote that they develop a multi-modal one-class generative adversarial network based detector to distinguish anomalies from normal data (products). Which products?\nDid the authors develop their approach specifically only for some very particular engineering problem (from Samsung)? If so, then the better place for such papers is some industrial journal/conference. If not, then the authors should provide comparisons using other datasets than  Samsung data\n2) the paper contains enormous number of\n- misprints, e.g. “meets all standards and requirements, fraud detection citepzheng2018one where it discriminate a “ (page 1, section 1)\n- mistakes in the text, bad wording, e.g. ""Experiments demonstrate that our model outperforms the state-of-the-art one-class classification models and other anomaly detection methods on both normal data and anomalies accuracy, as well as the F1 score"" (page 1, abstract)\n- mistakes in formulas, e.g. problems with subscripts in (7), problems with expectation sign in (9), etc.\nas a result, readability of the paper is very low. From the general explanation of the idea of the proposed algorithm in section 1 it is not possible to understand how it works\n3) the general idea (train GAN to generate samples in low-density regions of the original probability density; since a disrciminator is trained to classify such samples from real data, mainly belongning to high-density regions of the original probability density, then the discriminator can be used to detect anomalies) is nice. However, partially due to very bad text, partially due to errors in formulas it is not possible to understand technical description of the algorithm, e.g.\n- why do we need H()?\n- how do we model Q(c|x)?\n- how do we model p(c|x)?\n- how constants tau and C are selected?\n- etc\n4) Experimental section is not sufficient. The authors considered only one dataset which is proprietary\n5) The authors claimed that they do not need to tune the threshold to detect anomalies from other observations. But is this actually true? In fact, for the discriminator, which is used to detect anomalies, we still need to select some threshold expressing our confidence in whether the considered observation is anomalous or not\n\nConclusion\n- the idea is interesting\n- the paper is not OK for very high standards of ICLR\n', 'The paper presents an anomaly detection method called MMOCGAN which is claimed to work well on high-dimensional datasets with limited, multimodal data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance:\n- Results are presented on a single private dataset, and I don’t see any indication that the dataset will be shared with the community. This is problematic because there is no way for the community to reproduce and validate these results. I don’t think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.\n- Given the small number of non-pass products in the dataset (22), it’s unclear to me whether a held-out test set was used, or if hyperparameter selection was performed on the full set of non-pass products.\n- The use of a pre-trained, domain-specific feature extractor is briefly mentioned but no details (what is the architecture, on which data it’s been trained, etc.) are provided.\n- The central idea in the paper is to have the generator capture the ""complementary distribution"" of the data-generating distribution. The way in which this distribution is defined is not specific enough (it depends on hyperparameters C and epsilon for which there is no clear prescribed value). On a conceptual level it seems to me that for a data-generating distribution corresponding to a low-dimensional manifold embedded in a high-dimensional space the complementary distribution will essentially be uniform random noise, and in that case it’s unclear to me how it’s supposed to ""simulate anomalies"".\n- The way the proposed method is presented makes it look ad-hoc: several moving parts (InfoGAN, generator loss term encouraging it to learn the ""complementary distribution"", feature matching regularization term, pull-away loss term, discriminator entropy term) are connected together and their individual inclusion in the final loss is loosely justified. In practice, looking at the results it’s impossible for me to tell which term is necessary and which is not.\n- The word ""modal"" is used throughout as a noun. I’m not sure if the authors mean ""model"", ""mode"", or ""modality"", but based on the context I assume they mean ""mode"" as in ""mode of the distribution"".\n- The use of an InfoGAN architecture and loss is not credited clearly enough to Chen et al. and may give the impression to a casual reader that the idea is novel to this paper. The paper also does not make it clear how the number of categories or modes for the latent variable should be chosen, and what was the value used for the experiments.\n- The paper is legible, but there are several grammatical errors and typos throughout that make it harder to read than necessary.']","[-20, -50, -70]","[50, 0, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'technically sound', 'very effective on this dataset'), they also point out several limitations and areas for improvement. The review concludes that the paper is 'incremental' and 'lacks generality', which contributes to the overall negative sentiment. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, offering constructive criticism and specific suggestions for improvement without using harsh language. They acknowledge the paper's strengths while also pointing out areas that need work, which is a balanced and respectful approach to peer review."", ""The sentiment score is -50 because while the reviewer acknowledges that 'the idea is interesting', they also point out numerous issues with the paper, including poor readability, mistakes in formulas, insufficient experimental section, and concerns about the claimed contributions. The overall tone suggests the paper is not suitable for the high standards of ICLR in its current form. The politeness score is 0 (neutral) because the reviewer maintains a professional tone throughout, neither being particularly polite nor rude. They directly state the issues without using overly harsh language, but also don't use many polite phrases or softeners. The review focuses on the content and quality of the paper rather than personal criticisms."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states they cannot recommend acceptance and lists several major issues with the paper, including lack of reproducibility, unclear methodology, and insufficient details. However, it's not entirely negative as they do acknowledge the paper's central idea.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone. They use phrases like 'I find that' and 'it's unclear to me' rather than making blunt accusations. They also offer constructive feedback and explanations for their concerns. However, the overall tone is still quite critical, preventing a higher politeness score.""]"
"[""In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.\n\n1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?\n\n2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.\n\n3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?\n\n4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?\n\n5) is the Original GAN exactly the Ian's original GAN or WGAN?\n\n6) Have you tried this approach using small sized generator (having  small number of parameters)?\n\n\n\n"", 'This paper proposes a method for ensembling GAN’s for capturing diversity in the target space. This done by a convex combination of GAN’s that are sequentially trained by trying to approximate the real distribution by fixing the previous generators. The paper theoretically shows that this approach converges to the optimal theoretical distribution.\n\nComments \n\n1)  What will be the performance of Original GAN and Incremental GAN by finding optimal weighting ($w_i$) parameters for each of them?\n2)  Can you increase the number of parameters of the generator by no of generators used in the incremental GAN’s and compare the performance?\n3) The abstract first line you have written ‘possibly distribution’ instead of ‘probability distribution’\n4) Table 1 ‘Incremental GAN’ doesn’t show consistently improved performance in comparison ‘Original GAN’. Can you train a few more generators and verify it?', 'The paper introduces an incremental training method for GAN\'s for capturing the diversity of the input space. The paper demonstrate that the proposed method allows smaller distances between the true and generated distribution. I find the idea interesting, but fear that the 60-100 small ensemble models could be replaced by a larger model.\n\nI am curious about why we need incremental training when it seems like we could directly train all the networks jointly. The corresponding generative model is simply stronger so all the convergence arguments would still hold. Is the statistical distance a reasonable estimate for you to determine whether you need an additional generator for incremental training?\n\nAlso what are the generator architectures for the experiments? How can you put 60-100 generators within the GPU memory? The latent variable dimension seem to be only 1 for each of your generator? That seems to be seriously handicapping the capacity of each individual generator (to just some data points), so the ensemble distribution might be obtained simply by using a larger dimension z?\n\nThere are also other measurements that are used by the GAN community, such as inception score, FID score and samples. It seems also reasonable to verify the effectiveness of this method on CIFAR or LSUN datasets, where the method would have a greater improvement because the data distributions are more complex.\n\nMinor points:\n- How do you measure the ""Wasserstein distance"" for high-dimensional distributions? \n- What not set $\\omega_i$ to be always 1? The subsampling process introduced in Algorithm 2 seem to enforce this, and you do this for all the experiments.\n- Fix citation typos.\n- Fix \\mathbf for vector quantities, such as x and z.\n- Since the generative models have the same architecture, does the non-convex argument becomes moot when you have a mixture of 2 generators?']","[20, 20, -20]","[80, 60, 50]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the reasonableness of the authors' approach and provides constructive feedback. The review doesn't express strong enthusiasm but offers suggestions for improvement, indicating a generally positive but reserved sentiment. The politeness score is high (80) as the reviewer uses respectful language throughout, phrasing criticisms as questions or suggestions rather than direct criticisms. The reviewer consistently uses phrases like 'could you' and 'it would be better,' which maintain a polite and collaborative tone. The review focuses on the content and methodology without any personal remarks or harsh language, further contributing to its politeness."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's theoretical contribution and proposes a method for ensembling GANs. However, the reviewer also raises several questions and points out inconsistencies, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses neutral language and frames their comments as questions or suggestions rather than direct criticisms. The reviewer also points out a typo in a constructive manner. The overall tone is professional and respectful, without being overly formal or effusive."", ""The sentiment score is slightly negative (-20) because while the reviewer finds the idea interesting, they express several concerns and doubts about the method. They question the need for incremental training, the effectiveness of the approach compared to using a larger model, and suggest additional experiments and measurements. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions, and acknowledges positive aspects of the work. They use phrases like 'I find the idea interesting' and 'I am curious about' which maintain a polite tone while expressing concerns. The review provides constructive feedback without being harsh or dismissive.""]"
"['This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model\'s state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.\n\nPros:\n-Paper was easy to understand\n-Detailed analysis of model behavior. The breakdown analysis between ""what"" and ""where"" was particularly interesting.\n-Attention results appear interpretable as claimed\n\nCons:\n-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.\n-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?\n-I would also recommend including a citation for :\nSukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. ""End-to-end memory networks."" NIPS 2015.\n\n\nGeneral questions:\n-Was there an effect of attention grid coarseness on performance?\n-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?', 'Summary.\nThe paper proposes a variant model of existing recurrent attention models. The paper explores the use of query-based attention, spatial basis, and multiple attention modules running in parallel. The effectiveness of the proposed method is demonstrated with various tasks including RL (on Atari games), ImageNet image classification, and action recognition, and shows reasonable performance. \n\nStrengths.\n- An interesting problem in the current CV/RL community.\n- Well-surveyed related work.\n- Supplemental materials and figures were helpful in understanding the idea.\n\nvs. Existing recurrent attention models.\nIn Section 2, the proposed model is explained with emphasizing the differences from existing models, but there needs a careful clarification.\n\nIn this paper, attention weights are computed conditioned on a query vector (which solely depends on the RNN’s state) and the Keys (which are generated by a visual encoder, called vision core). In the landmark work by Xu et al. (ICML ‘15, as already referenced), attention weights are computed very similarly - they used the hidden state of RNN followed by an additional layer (similar to the “query”) and visual features from CNN followed by an additional layer (similar to the “keys”). The only difference seems the use of element-wise multiplication vs. addition, but both are common units in building an attention module. Can authors clarify the main difference from the existing recurrent attention models?\n\nTraining details.\nIn the supervised learning tasks, are these CNN bases (ResNet-50 and ResNet-34) trained from scratch or pre-trained with another dataset?\n\nMissing comparison with existing attention-based models.\nThe main contribution claimed is the attention module, but the paper does not provide any quantitative/qualitative comparison from another attention-based model. This makes hard to determine its effectiveness over others. Notable works may include:\n\n[1] Sharma et al., “Action recognition using visual attention,” ICLR workshop 2016.\n[2] Sorokin et al., “Deep Attention Recurrent Q-network,” NIPS workshop 2015.\n[3] Choi et al., “Multi-Focus Attention Network for Efficient Deep Reinforcement Learning,” AAAI workshop 2017.\n\nMinor concerns.\nThe related work section would be helpful if it proceeds the current Section 2.\nTypos', ""\n[Summary]\n\nThis paper proposed a soft, spatial, sequential, top-down attention model which enable the agent and classifier actively select important, task relative information to generate the appropriate output. Given the observations, the proposed method uses a convLSTM to produce the key and value tensor. Different from multi-head attention, the query vector is generated in a top-down fashion. The authors proposed to augment the spatial feature with Fourier bases which is similar to previous work. The authors verify the proposed method on both reinforcement learning and supervised learning. On reinforcement learning, the proposed method outperformed the feedforward baseline and LSTM baseline. On reinforcement learning task, the proposed method achieves compelling result with more interpretable attention map that shows the model's decision. \n\n[Strength]\n1: The proposed model is a straightforward extension of the multi-head attention to visual input. Compare to multi-head attention, it generates the query vector in a top-down manner instead of pure bottom up, and the authors verify the proposed choice is better than LSTM baseline empirically.\n\n2: The authors verify the proposed method by extensive experiments on reinforcement learning tasks and also try supervised learning tasks. The attention visualization and human normalized scores for experts on ATARI show the effectiveness of the proposed method. \n\n[Weakness]\n1: The soft spatial top-down attention is very common in vision and language domain, such as VQA. As the authors mentioned, the proposed method is very similar with MAC for CLEVER. The sequential attention is also explored in previous VQA work. Thus the novelty of the proposed method is limited. \n\n2: Multi-head attention for NLP tasks are usually composed with multiple layers. Will more layer of attention help the performance? The paper is less of this ablation study. \n\n3: The proposed method is worse compared with other baselines on supervised learning tasks, on both imagenet classification and kinetics. I wonder whether the recurrent process is required for those tasks? On table 2, we can observe that with sequence length 8, the performance is much worse,  this may be caused by overfitting. \n\n4: If the recurrent attention is more interpretable, given other visualization methods, such as gradcam, I wonder what is advantage?\n\n5: I would expect that the performance on Kinetics dataset is better since sequential attention is required on video dataset. However, the performance is much worse compare of the baseline in the dataset. I wonder what is the reason? is there ablation study or any other results on this dataset? ""]","[60, 20, -20]","[70, 60, 60]","[""The sentiment score is 60 (moderately positive) because the reviewer lists several pros and finds the paper easy to understand and interesting, while also providing constructive criticism and suggestions for improvement. The tone is generally positive, but not overwhelmingly so. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, frames criticisms as suggestions or questions rather than direct attacks, and acknowledges the paper's strengths. The reviewer maintains a professional tone without being overly formal or deferential."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges strengths of the paper such as addressing an interesting problem, having well-surveyed related work, and helpful supplemental materials. However, they also raise several concerns and request clarifications, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions (e.g. 'Can authors clarify...'), and acknowledges positive aspects before discussing limitations. The reviewer maintains a professional and constructive tone, avoiding harsh or dismissive language even when pointing out potential weaknesses in the paper."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out several weaknesses and limitations. The review begins with a neutral summary and lists two strengths, but then follows with five weakness points, which outweigh the positive aspects. The politeness score is moderately positive (60) because the reviewer uses professional and respectful language throughout. They present their critiques as questions or observations rather than harsh criticisms, and use phrases like 'I wonder' and 'I would expect' to soften their comments. The reviewer also acknowledges the paper's strengths before discussing its weaknesses, which is a polite approach to peer review.""]"
"[""The authors argue that they propose a method to find adversarial examples, when the data lie on a Riemannian manifold. In particular, they derive a perturbation, which is argued to be the worst perturbation for generating an adversarial example, compared to the classical Euclidean derivation.\n\nI strongly disagree with the proposed (Riemannian) geometric analysis, because there are several technical mistakes, a lot of arbitrary considerations, and flawed assumptions. In particular, my understanding is that the proposed method is not related at all with Riemannian geometry. For justification I will comment some parts:\n\n#1) In Section 1 paragraph 4, and in Section 2.3 after Eq. 14, the sentences about the gradient of a function that is defined on a manifold are strange and unclear. In general, the gradient of a function defined on a manifold, points to the ascent direction. Thus, if I understood correctly the sentences in the paper support that the gradient of such a function is meaningless, so I think that they are wrong.\n\n#2) How the $\\ell_2$-ball on a manifold is defined? Usually, we consider a ball on the tangent space, since this is the only Euclidean space related to the manifold. Here, my understanding is that the authors consider the ball directly on the manifold. This is clearly wrong and undefined.\n\n#3) To find the geodesic you have to solve a system of 2nd order non-linear ODEs, and there are additional details which I will not include  here, but can be easily found in the Riemannian geometry literature. Also, I think that the Lemma 2.2. is wrong, since the correct quantity of Eq. 3 is $ds^2 = g_ij(t)d\\theta^i d\\theta^j dt^2$, where $dt\\rightarrow 0$ based on the included proof. This is clearly not a sensible geodesic, it is just the infinitesimal length of a line segment when $t\\rightarrow 0$, which means that the two points are infinitesimally close.\n\n#4) If $x, y$ is on a Riemannian manifold then the $x+y$ operator does not make sense, so Eq. 7 is wrong. In particular, for operations on Riemannian manifolds you need to use the exponential and the logarithmic map.\n\n#5) Continuing from #4). Even if we consider the perturbation to be sufficiently small, still the $x+\\epsilon$ is not defined. In addition, the constraint in Eq. 8 is wrong, because the inner product related to the Riemannian metric has to be between tangent vectors. Here the $\\epsilon$ is an arbitrary quantity, since it is not defined where it actually lies. In general, the derivation here is particularly confusing and not clear at all. In my understanding the constraint of Eq. 8 is a purely linear term, and $d$ is not the geodesic distance on a manifold. It just represents the Mahanalobis distance between the points $x$ and $x+\\epsilon$, for a matrix $G$ defined for each $x$, so it is a linear quantity. So Eq. 9 just utilizes a precondiner matrix for the classical linear gradient.\n\n#6) The Eq. 12 is very flawed, since it equalizes a distance with the Taylor approximation error. I think that this is an unrealistic assumption, since these terms measure totally different quantities. Especially, if $d$ is the geodesic distance.\n\n#7) The upper bound in inequality Eq. 13 comes from Eq. 12, and it is basically an assumption for the largest absolute value of the Hessian's eigenvalues. However, this is not discussed in the text, which are the implications?\n\n#8) I find the paper poorly written, and in general, it lacks of clarity. In addition, the technical inconsistencies makes the paper really hard to follow and to be understood. I mentioned above only some of them. Also, there are several places where the sentences do not make sense (see #1), and the assumptions made are really arbitrary (see #6). The algorithms are not easy to follow. Minor comments, you can reduce the white spaces by putting inline Eq. 3, 4, 5, 6, 9, 10, 14, and Figure 2. The notation is very inconsistent, since it is very unclear in which domain/space each quantity/variable lies. Also, in Section 2.5. the authors even change their notation.\n\nIn my opinion, the geometrical analysis and the interpretation as a Riemannian manifold is obviously misleading. Apart from the previously mentioned mistakes/comments, I think that the proposed approach is purely linear. Since actually the Eq. 14 implies that the linear gradient of the loss function, is just preconditioned with the Hessian matrix of the loss function with respect to the input $x$. Of course, if this function is convex around $x$, then this quantity is the steepest ascent direction of the loss function, simply on the Euclidean space where $x$ lies. However, when this function is not convex, I am not sure what is the behavior when all the eigenvalues of the Hessian are set to their absolute value. Also, the (arbitrary) constraint in Eq. 13 implicitly sets a bound to the eigenvalues of the Hessian, which in some sense regularizes the curvature of the loss function. To put it simple, I think that the proposed method, is just a way to find a preconditioner matrix for the linear gradient of the loss function, which points to the steepest direction. This preconditioner is based on the Hessian of the loss function, where the absolute values of the eigenvalues are used, and also, are constrained to be bounded from above based on a given value.\n\nGenerally, in my opinion the authors should definitely avoid the Riemannian manifold consideration. I believe that they should change their perspective, and consider the method simply as a local preconditioner, which is based on the Hessian and a bound to its (absolute) eigenvalues. They should also discuss what is the implication by setting the eigenvalues to their absolute values. However, since I am not an expert in the field of adversarial examples, I am not sure how novel is this approach.\n\n\n"", '1.\tSome motivation of extending the adversarial examples generation on manifold should be there.\n2.\tEven if \\epsilon is small, if x is on a manifold, x+\\epsilon may not, so I am not sure about the validity of the definition in Eq. (7) and what follows from here. One solution is putting the constraint d(x, Exp_x(\\epsilon)) \\leq \\sigma, which implies that g(\\epsilon, \\epsilon) \\leq \\sigma. \nAlso, x and \\epsilon lies in completely different space, \\epsilon should lie on the tangent space at x. So, I don’t understand why x+\\epsilon makes sense? It makes the rest of the formulation invalid as well.\n3.\tI don’t understand why in Eq. (12), d(x, x+\\epsilon)^2 = |m(x)|? Do authors want it to be equal, otherwise, I can not see why this equality is true.\n4.\tIn Lemma 2.3, please make H in \\mathbb{R}^{n\\times n} instead of \\mathbb{R}^n \\times \\mathbb{R}^n (same issue for Lemma 2.4), later does not make sense in this context. Also, why not write |H|=U|\\Sigma|U^T, instead of what you have now. \n5.\tNo need to prove Lemma 2.3 and 2.4. These are well-known results in matrix linear algebra.\n6.\tIt’s nice that the authors generalize to l_p ball and can show FGSM as a special case.\n7.\tSome explanation of Algo. 2 should be there in the main paper given that it is a major contribution in the paper and also authors put a paper more than 8 pages long, so as a reader/ reviewer I want more detailed explanation in the main body.\n8.\tIn Algorithm 1, step 7: “Update the parameters of neural network with stochastic gradient” should be updated in the negative direction of gradient.\n9.\tAlgorithm 2 is clearly data driven. So, can authors comment on special cases of Algorithm 2 when we explicitly know the Riemannian metric tensor, e.g., when data is on hypersphere.\n10.\tCan authors comment on the contemporary work https://arxiv.org/pdf/1807.05832.pdf, as the purpose is very similar.\n11.\tThe experimental validation is nice and showed usefulness of the proposed method.\n\n\nPros:\n1.\tA framework to show the usefulness of non-Euclidean geometry, specifically curvature for adversarial learning.\n2.\tNice set of experimental validation.\n\nCons:\n1.\tSome theorems statement can be ignored to save space, e.g., Lemma 2.3 and 2.4. And instead, need some explanation of Algorithm 2 in the main text. Right now, not enough justification of additional page.\n2.\tNot sure about the validity of the main formulation, Eq. (7) and other respective frameworks when data x is on a manifold.\n\nMinor comments:\n1.\tIn page 2, “In this case, the Euclidean metric would be not rational.”-> “In this case, the Euclidean metric would not be rational”.\n2.\t “However, in a geometric manifold, particularly in Riemannian space, the gradient of a loss function unnecessarily presents the steepest direction.” Not sure what authors meant by “unnecessarily presents”\n3.\tNo need to reprove Lemma 2.2, just give reference to a differential geometry textbook like Chavel or Boothby.\n\nI want the authors to specifically address the cons. ', ""In the paper, the authors proposed to solve the learning problem of adversarial examples from Riemannian geometry viewpoint. More specifically, the Euclidean metric in Eq.(7) is generated to the Riemannian metric (Eq.(8)). Later, the authors built the correspondence between the metric tensor and the higher order of Taylor expansions.  Experiments show the improvement over the state-of-the art methods.\n\nSome questions:\nFirst of all, the idea of introducing Riemannian geometry is appealing.\nIn the end, a neural network can be roughly viewed as a chart of certain Riemannian manifold.\nThe challenging part is how can you say something about the properties of the high dimensional manifold, such as curvature, genus, completeness etc.\nUnfortunately, I didn't find very insightful analysis about the underlying structure.\nWhich means, hypothetically, without introducing Riemannian geometry we can still derive Eq.(14) from Eq.(12), Taylor expansion will do the work.\nSo more insights about metric tensor G determined manifold structure can be very helpful.\n \nSecond, Lagrange multipliers method is a necessary condition, which means the search directions guided by the constraint may not lead to the optimal solutions.\nIt would be better if the authors can provide either theoretical or experimental study showing certain level of direction search guarantee.\n \nLast, the experiment results are good, though it lacks of detailed discussion, for example could you decompose the effect achieved by proposed new Riemannian constraint and neural network architecture? Merely demonstrating the performances does not tell the readers too much.\n""]","[-80, -20, -20]","[-20, 50, 50]","[""The sentiment score is -80 because the reviewer strongly disagrees with the proposed method, pointing out numerous technical mistakes, arbitrary considerations, and flawed assumptions. The review is overwhelmingly negative, with phrases like 'strongly disagree,' 'clearly wrong,' and 'obviously misleading' used throughout. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. Phrases like 'clearly wrong,' 'very flawed,' and 'poorly written' are used, which come across as somewhat harsh. However, the reviewer does provide detailed explanations for their criticisms and uses some hedging language ('I think,' 'in my opinion'), which prevents the score from being even lower."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('nice set of experimental validation', 'framework to show usefulness'), they also raise significant concerns about the validity of the main formulation and suggest removing some parts of the paper. The overall tone suggests the paper needs substantial revisions. The politeness score is moderately positive (50) as the reviewer uses professional language throughout, offers both pros and cons, and phrases criticisms as suggestions or questions rather than direct attacks. They use polite phrases like 'Can authors comment on...' and 'please make...' which contribute to a respectful tone. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a professional, matter-of-fact tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the appealing idea of introducing Riemannian geometry, they express several concerns and criticisms. They mention a lack of insightful analysis, question the necessity of the Riemannian approach, and suggest the need for more detailed discussion in the experiments. However, the tone is not entirely negative, as they recognize the good experimental results and the potential of the approach.\n\nThe politeness score is moderately positive (50) because the reviewer uses respectful and constructive language throughout. They frame their criticisms as questions or suggestions rather than direct attacks. Phrases like 'It would be better if...' and 'could you...' indicate a polite approach to offering feedback. The reviewer also acknowledges positive aspects of the work, such as the 'appealing' idea and 'good' experimental results, which contributes to the overall politeness of the review.""]"
"['1. This papers leverages the concept of wavelet transform within a deep architecture to solve the classic problem (especially for wavelet analysis) of change point detection. The authors do a reasonably comprehensive job of demonstrating the efficacy of the proposed framework using various synthetic and real data sets with both gradual and abrupt changes\n\n2. The concept of pyramid network idea is not really new, in the context of CNN it has been established quite well. The paper should highlight this fact by citing papers such as ""Lin, Tsung-Yi, et al. ""Feature Pyramid Networks for Object Detection."" CVPR. Vol. 1. No. 2. 2017."" \n\n3. Involving wavelet transforms in deep nets have been done before. This paper attempts to learn wavelet transform parameters by involving them as trainable layers. But even this kind of idea is also emerging in the community. Papers such as ""Fujieda, Shin, Kohei Takayama, and Toshiya Hachisuka. ""Wavelet Convolutional Neural Networks."" arXiv preprint arXiv:1805.08620 (2018)"" need to be discussed in this context. \n\n4. The biggest issue in my mind is that I feel ""Chung et al 2016"" is still a very similar framework as the proposed one. While authors argue that it uses more like CNN architecture and the proposed method may pick up the multi-scale features better, comparison with this seems to be most appropriate. This will also clearly identify the benefits of the wavelet structure to the filters and multi-resolution analysis approaches.\n\n5. RCNN term has been used for CNN+RNN architecture. This may not be a good terminology to use since RCNN is a very popular term referring to Region based CNN for detection and localization purposes.\n\n6. AUC metric, I believe is the - area under ROC curve, this needs to be spelled out, how it is computed? at least in the Appendix\n\nxxxxxxxxxxxxxxxxxxx\n\nAppreciate the authors\' rebuttal, updated my score.', ""This paper proposes a pyramid based neural net which both decomposes a signal into several scales (learning the basis functions to do that) and processes the resulting bands in a scale invariant manner. The method is applied to 1D signals with underlying processes occurring at different time scales where the task is change point detection.\n\nPros:\n* Nice model and model formulation - learning the basis functions both for the low and high frequency is a nice idea. I also liked the way weights are shared across scales. In particular that the information flow between consecutive scales is shared, as well as through time.\n\n* Writing is very clear and method is well motivated\n\nCons:\n* I found the experimental validation a bit limited - the presented results are nice and for the problem quite comprehensive but I would have wanted something a bit more complicated than change point detection. Specifically, since the natural world is full of scale free phenomena it would have been much more interesting with other tasks (generative models? natural images? many options). I feel this would have made the case for the paper much stronger.\n* There's also very little analysis of what is learned from the data - how do the kernels look like and how do they correspond to known wavelets? to the data? would be nice to understand what's going on here.\n\n\nBottom line:\nI like the proposed model and for what it is it's quite good but it would have been a much more convincing paper with more experiments demonstrating the power of the method and analyzing it."", 'The paper presents an interesting approach to change point detection. I agree we need more general model to capture the change. However, unfortunately, they did not place the contribution correctly with respect to existing literature. The comments for prior work seem to be highly biased. For instance,  in Section 2, ""these methods either have unrealistic assumptions, such as defining changes as a large difference in covariance matrix"". I would like to comment that, covariance change can capture a large number of changes in real applications and these are not unrealistic assumptions. \n\nThe ""pyramid"" recurrent neural network seems to be a extension of RNN using the idea of multi-scale structure. Could be interesting.\n\nThe paper gives too much emphasis on the ""merit"" of the neural networks on capturing the change patterns. However, there is a very important aspect been ignored or hiding: in order to train neural networks to capture anomaly patterns, since neural networks are highly over-parameterized model, usually there won\'t be a large number of samples for anomalies. Therefore, in many situations, it is simply unpractical to train neural networks to capture post-change samples. \n\nThere is a large body of literature on change point detection in statistics etc. (the author mentioned one, Chen and Zhang 2015, more over, the comment that ""they can only detect abrupt change"" is wrong, the method is quite general).\n\nThe paper fails to have any comparison with existing methods. For instance, how does the proposed method compare with hoteling T-square statistic, or CUSUM statistic, or generalize likelihood ratio statistic, or MMD statistic (non-parametric approach)? Without any comparison, it does not make sense to claim proposed method is superior. \n']","[20, 50, -60]","[60, 75, 20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's comprehensive demonstration of the framework's efficacy and the reasonably good job done by the authors. However, they also point out several issues and areas for improvement, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and providing constructive feedback. They use phrases like 'reasonably comprehensive job' and 'appreciate the authors' rebuttal', which contribute to the polite tone. The reviewer also offers specific suggestions for improvement without using harsh or critical language."", ""The sentiment score is 50 (slightly positive) because the reviewer expresses appreciation for the paper's model and writing clarity, but also points out limitations in experimental validation and analysis. The reviewer uses phrases like 'nice model,' 'writing is very clear,' and 'I like the proposed model,' indicating positive sentiment. However, they also mention 'cons' and suggest areas for improvement, balancing the overall sentiment.\n\nThe politeness score is 75 (quite polite) because the reviewer maintains a respectful and constructive tone throughout. They begin by acknowledging the paper's strengths and use phrases like 'I liked' and 'nice idea' to express approval. When presenting criticisms, they use softer language like 'I found' and 'I would have wanted,' rather than harsh or direct criticism. The reviewer also provides specific suggestions for improvement, which is a polite way to offer feedback."", ""The sentiment score is -60 because the review is predominantly critical. The reviewer points out several significant issues with the paper, including misplaced contributions, biased comments on prior work, and lack of comparison with existing methods. While the reviewer acknowledges some interesting aspects ('interesting approach', 'Could be interesting'), these are outweighed by the criticisms. The politeness score is 20 because the reviewer uses relatively polite language ('unfortunately', 'I would like to comment') and acknowledges some positive aspects, but doesn't go out of their way to be overly polite. The criticism is direct but not rude, maintaining a professional tone throughout.""]"
"['Authors extends stacked hourglass network with inception-resnet-A mudules and a multi-scale approach for human pose estimation in still RGB images. Given a RGB image, a pre-processing module generates feature maps in different scales which are fed into a set of serial stack hourglass modules each responsible for a different scale. Authors propose an incremental adaptive weighting formulation for each stack-scale-joint. They evaluate proposed architecture on LSP and MPII datasets.\n\npositive:\n- Having an adaptive weight strategy is a necessary procedure in multi-loss functions where cross-validation or manual tuning of fixed weights are expensive. While the weights are functions of the loss, it is not analyzed and evaluated thoroughly, e.g. evolution of weights for each joint-stack-scale. Even it is not given in the section 5.2.1. So it is hard to judge effectiveness of the proposed loss. \n\nnegative:\n- In general experiments section is the most weakness of the paper. I comment some points in the following:\na) Multi-scale image processing is not a novel idea in computer vision and specifically in human pose estimation. The authors have not compared their methods with most recent papers in the literature and I believe the results are not state-of-the-art (see [1] for instance which is a multi-scale approach).\nb) What is the effect of each scale in the results and for each joint? This must be analyzed and shown visually or numerically.\n\n- The number citations is not enough.\n\n- The writing must be improved.\n\noverall:\nRegarding mentioned comments, I believe the paper needs a huge extra effort (both analytically and numerically) to be adequate for publication. Therefore, I recommend rejection.\n\n\n[1] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning feature pyramids for human pose estimation. In: ICCV. (2017)', ""Summary: \n\nThis paper proposes a modification to the original hourglass network for single pose estimation using (1) a preprocessing network to generate multiscale feature maps (2) a upgrade of hourglass network backbone (3) using a adaptive loss weighting strategy. It yields some improvements over the original baseline. \n\nCons: \nThe method is somewhat incremental with respect to original work and only compares algorithm 2 years ago without comparing to current SOTA methods. \n\nBy checking the online benchmarks of MPII, the Tang et al., ECCV'18 results has already reach 92.8 while the proposed one is 91.2, which is significant lower than other SOTA strategies. \n\n\n\n"", 'In this paper a method for pose estimation is proposed, which is based on the well known neural model “stacked hourglass networks”. The novelty in the proposed paper is a multi-scale formulation, which creates multiple scales from the input image and feeds them into different hourglass modules. The different scales are weighted differently, where the weights for a given scale depend on the error obtained on previous scales.\n\nImportant issues:\n\nThe novelty seems to be not sufficient to me, as multi-scale solutions are not new in computer vision and have been applied a lot in pose estimation as well, be it deep neural models or older techniques. In particular in deep models, multi-scale techniques have been proposed extensively for resolution preserving image to image mappings (which is done here), beginning with quite “old” techniques (in deep learning time scales) going back to 2012 (Farabet, C., Couprie, C., Najman, L., LeCun, Y., 2012. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In: ICML.), or formulations which integrate scales and layers, starting with hyper-columns in 2015 (Hypercolumns for object segmentation and fine-grained localization, CVPR 2015) and many more recent variants.\n\nIn 2018, multi-scale formulations are now standard techniques in computer vision with deep networks. I am not sure how the proposed method makes a difference. Also, I am wondering whether there shouldn’t be some parameter sharing between the models of the different scales, as is often done in the literature now to reduce model capacity.\n\nThe way the multi-scale Since different resolutions are created is particular. Why not just simply subsample the input images to different resolutions? Why are these trained layers needed as a preprocessing?\n\nWeighting different scales is not fundamentally new. We also don’t know whether it improves performance, it is not part of the ablation study.\n\nThe method has been compared to several methods, but which are not state of the art anymore. Most papers are from 2016, the field advanced quickly. The performance gains of the multi-scale formulation are pretty low, and overally, the method is not state of the art on the targeted benchmark. Obviously I do not want to say that a paper needs to be state of the art on a benchmark to get published, even at a top-level conference, far from it. However, if the methodological and theoretical contribution of the paper is rather minor, than the performance and evaluation should be flawless.\n\nMinor remarks:\n\nThe writing of this paper is somewhat fuzzy, using non-standard technical language, which I could not decipher and which seems to me somewhat misleading. Examples are:\n\n- “the stacked hourglass network theoretically increases the stacked depth”: how does theory tell us anything about depth in this context?\n\n- “difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass”: I don’t understand what this means, simply. This phrase is also repeated several times in the paper in different places\n\n- “information loss caused by the functional consistency of hourglass networks”: I don’t understand what functional consistency means here, and why it leads to information loss.\n\nThe description of Algorithm 1 does not seem to be necessary to me. It basically follows from the equations 4-6, which are executed in order and per layer. \n']","[-70, -50, -60]","[20, 0, 20]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer recommends rejection and points out several weaknesses, including insufficient experiments, lack of comparison with recent literature, and inadequate writing. The few positive comments are outweighed by the critical ones. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I believe' and 'I recommend' rather than harsh language. The reviewer also acknowledges some positive aspects before detailing the negatives, which adds to the politeness. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score."", ""The sentiment score is -50 because the review is generally negative, pointing out that the method is 'somewhat incremental' and performs significantly lower than state-of-the-art strategies. However, it's not entirely negative as it acknowledges some improvements over the original baseline. The politeness score is 0 (neutral) because the language used is direct and factual without being particularly polite or rude. The reviewer states criticisms plainly without using harsh language, but also doesn't employ particularly courteous phrasing."", ""The sentiment score is -60 because the reviewer expresses significant concerns about the novelty and contribution of the paper. They state that the proposed method lacks sufficient novelty, does not improve much on existing techniques, and is not state-of-the-art. The reviewer also points out several issues with the methodology and writing. However, the score is not extremely negative as the reviewer acknowledges some positive aspects and provides constructive feedback. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and provide detailed explanations for their concerns. They also use phrases like 'I am not sure' and 'I don't want to say' which soften the critique. The reviewer offers specific suggestions for improvement, which is a polite way to provide feedback.""]"
"['This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1)  the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where  the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. \n\nThis paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.\n\nIn the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to  analyze the reason. \n\nIn addition, as claimed from the introduction, the contribution of this paper was  to reduce the overhead of expensive quantization. However, no experimental results on computation time  and parameter size have been shown. ', 'The paper is a little hard to follow and some parts are poorly written. While the authors claim that they use the greedy approach (in sec 2) for quantization where both B and \\alpha are learned in a greedy way, it is not clear why there is convergence difference between the two as claimed by the authors in section 3.1. Moreover, the authors claimed faster convergence of B than \\alpha because fewer bit clips are observed from the left subplot of Figure 2. However, this conclusion is not quite convincing because 1) on the right subplot of Figure 2, it seems that \\alpha also becomes more stable after 80k iterations; 2) the fewer bit clips may comes from using a stepwise learning rate decay scheme. Thus, the motivation for using another phase to train the \\alpha is not strong.\n\nThe iterative quantization approach has limited novelty. It is similar to many quantization methods like BinaryConnect and Xnor-Net, except that the quantization step is not done immediately after the BP and model updates, but after some iterations of full-precision training. Moreover, these methods also use full-precision weights for update during training.\n\nClarity in the experiments section is a little better than the previous sections. However,\n- The proposed method only performs comparably with TTQ, and shows significant accuracy drop on the Cifar-10 and Cifar-100 datasets (especially on Cifar-100)\n- On the ImageNet dataset, there is a large accuracy drop of the proposed method compared to Zhou et al. (2018). Though the authors said that they believe their proposed model can reach a higher accuracy by using layer-by-layer quantization as in Zhou et al. (2018), it is hard to verify this claim due to lack of the corresponding results. Thus, efficacy of the proposed method on large datasets or models are hard to evaluate.', 'This work addresses the issue of quantization for neural network, and in particular focus on Ternary weight networks. The proposed approach has two phases, the first phase performs quantization and de-quantization at certain iterations during training, where the schedule of these operations are hyperparameters specified a priori. The second phase focuses on training the scaling factor. The first phase is similar to the iterative quantization method proposed in “Retraining-Based Iterative Weight Quantization for Deep Neural Networks”, and differs in that this work performs the quantization and de-quantization operations more frequently.\nThis work also proposed a modified version of L2 regularization, but it’s not clear how much benefit it provides compared to a regular L2 regularization. There is also a shuffling approach, but seems to provide limited improvement.\nThe experiment results in general does not provide convincing evidence that the proposed method outperforms existing approaches. For example, the ResNet-32 on CIFAR-10 result does not perform better than the one reported in “Trained ternary quantization”, and the ImageNet result is also worse than some existing works.\nThe work is lack of novelty and the results do not show significant improvement over existing approaches.\n']","[-20, -60, -70]","[50, 20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is clearly presented, they point out several areas for improvement and limitations. The reviewer notes that the paper's method performs worse than some competitors and lacks analysis on computation time and parameter size, which were claimed as contributions. However, the tone is not entirely negative, as the reviewer also provides constructive feedback. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, offering suggestions for improvement rather than harsh criticism. Phrases like 'it can be improved by' and 'it is better to' indicate a polite and constructive approach to feedback. The reviewer also begins with a positive comment about the paper's clarity before moving on to areas of improvement, which is a polite way to structure feedback."", ""The sentiment score is -60 because the review is predominantly negative. The reviewer points out several issues with the paper, including difficulty in following the content, poor writing in some parts, unconvincing arguments, limited novelty, and significant accuracy drops in experiments. The few positive aspects (e.g., 'Clarity in the experiments section is a little better') are outweighed by the criticisms. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'it is not clear' and 'this conclusion is not quite convincing' rather than more harsh language. The reviewer also acknowledges some positive aspects and the authors' claims, even if they disagree with them, which adds to the politeness."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the work lacks novelty, doesn't provide convincing evidence of outperforming existing approaches, and doesn't show significant improvement. They also point out that some results are worse than existing works. However, it's not entirely negative as they do acknowledge some aspects of the work, hence not a -100 score. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone without using harsh language. They describe the work objectively and provide specific examples to support their points. The language is not particularly warm or polite, but it's not rude either, leaning slightly towards politeness in academic context.""]"
"['The paper presents an extension of the MXFusion language that allows the use of probabilistic modules. These modules are defined as a set of random variables and a specific probabilistic distribution. The modules also contain dedicated inference methods. Using these modules, one can use probabilistic distributions with inference methods tailored to the distribution, which are usually more efficient than generic inference systems.\nThe paper presents several examples using Gaussian process models, evaluated by comparison with GPy and the standard spare gaussian process method implemented in MXFusion.\n\nOverall, the paper is well written and clear, and all claims are justified. The idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules. The related work section provides a good positioning of the approach.\nI have not found any specific problems in the paper, the quality is rather high. However, the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.\nMoreover, the results of the experimental test do not seem to me to be able to support the main objective of the extension, which is to give the possibility to exploit more specific probabilistic model and inference methods to achieve better results than an approach using general methods.\nAs far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?\n\nAs for minor issues that I can point out, one concerns the definition of shape in the Variable of m.sigma2 (figures 1, 2, 3). I do not know the used in MXFusion, thus this might not be an error, but it seems that in the shape definition something is missing. It is written that shape=(1,), is it correct or is there an error? In case of absence of error, what does the empty argument mean?\n\nThe power benchmark is not described.\n\nIn references, Thomas V. Wiecki is mentioned with and without the first letter of middle name. I suggest to uniform the references.\n\nTypos:\n- Abstract: ""... but also sophisticated probabilistic model*s* such as ...""\n- Sec. 1, first row of page 2: The sentence ""this would bring the a lot of benefits ..."". The ""the"" word should be deleted.\n- Sec. 1 refers to a section after 4 which does not exist in the paper.\n- Page 5: remove the full stop before the colon in the 4th row.\n- Page 5: ""The log_pdf method of the SGPR module compute*s* the above variational lower bound""\n- Sec. 6: the sentence ""MXFusion aims at closing the gap between having specialized, highly performant algorithms and generic, easily maintained generic algorithms by introducing probabilistic modules."" should be corrected.\n\n\nPros\n- The extension allows the use of modules that define specific probabilistic distribution/inference methods\n- It seems easy to extend the system with other modules\n- Its a really useful extension...\n\nCons\n- The performance presented in the paper is not entirely convincing\n- ... but it is just an extension of an existing system', 'The paper works with the modularization of PPLs with natural inspiration for the successful modularization recently introduced in all deep learning softwares. \n\n* Within your so-called probabilistic modules you package dedicated inference methods that are tailored for this particular class of problems and argues that this will perform better than using a general purpose solver. For each specific case this does of course make a lot of sense. However, when it comes to the relevant case (especially within probabilistic programming) when we have a (often complex) combination of several probabilistic modules, how do you then leverage the tailored solvers? What is it that guarantees that these are relevant in the new combined construction? \n\n* Related to the above you write in your conclusion that ""Once an inference algorithm is chosen, it remains the same across a probabilistic model. However, given a specific probabilistic model, e.g., a conjugate model, a specialized inference algorithm that exploits the mathematical properties of that particular model will always produce inference results that are as good or better than the generic inference in terms of both accuracy and efficiency."" This is of course true and it is also part of some existing PPLs, for example Birch via their so-called ""delayed sampling"": \nhttp://proceedings.mlr.press/v84/murray18a/murray18a.pdf\nThe implementation there is very different from what you propose. As far as I can understand you require hard-coding of each specific model, whereas in the paper mentioned above they seem to automate att conjugate gradient calculations to a much greater extent. Why is it better to insist on hard-coding this for each probabilistic module? and how can you guarantee smooth functioning when several probabilistic modules are combined in complex ways?\n\n* In the inference method that you briefly sketch in Section 3 you make use of VI and the intractable integrals that results are then handled using Monte Carlo. What is the gain of using VI + Monte Carlo compared to direct use of Monte Carlo? Via direct use of some kind of Monte Carlo method you would be able to guarantee performance and do proper analysis, whereas with VI you loose that capability. However, VI does of course have other pros, but my question arises due to the fact that you end up using Monte Carlo anyway.\n\n* You write that ""In PPLs, a probabilistic model is often presented as a graph of random variables..."". This is certainly true and the word ""often"" is very important in this sentence. At the same time, is not one of the key reasons for using PPLs compared to probabilistic graphical models that it offers a richer model class compared to probabilistic graphical models? While I perfectly respect you choice to specifying models in MXFusion using using probabilistic graphical models I do find this quite restrictive and it seems to miss some of the key possibilities with PPLs.\n\n* In your BLR example (which is very instructive by the way) you compute the solution via MAP. This is also find rather puzzling since that removes another great feature of PPLs, namely to work with probability distributions throughout the entire inference stage. The user can then of course choose to extract whatever point estimate might be needed in the end. Why do you remove this possibility by insisting on a specific point estimate? or is this just a particular choice of this example and not a general design choice?\n\n\nThe paper contains a lot of issues related to the use of the English language and would benefit from proper proofreading.\n', 'In this paper authors present a new Probabilistic Programming Language (PPL) MXFusion. Similarly to the languages for the deep learning (TensorFlow, PyTorch, etc.), this language introduce probabilistic modules that are used as building blocks for complex probabilistic models. Introducing modularity to the probabilistic programming, raises the problem of inference for probabilistic models. Since, we cannot obtain the exact solution on practice we have to resort to approximate inference methods. The approximate inference methods can be either generic, thus, being suitable for many probabilistic models but resulting in poor approximation, or specific, thus, having good approximation quality, but only for specific probabilistic models. Authors propose to address this problem by encapsulating specific inference methods in corresponding probabilistic modules. Doing so, one can perform approximate inference for every module with the best suitable inference technique. Authors demonstrate interface of MXFusion for three well known probabilistic models: Bayesian linear regression, deep kernel learning, Bayesian Gaussian process latent variable model.\n\nApproaching the problem of building complex probabilistic models by introducing modular PPL is an important direction of study. But, regarding this paper I have the following concerns.\n- In my opinion, the structure of the paper can be greatly improved. From general words about modularity and approximate inference authors dive to the very specific cases of probabilistic models. Following such structure, authors don’t give a clear answer to the following questions. Why the paradigm of encapsulating inference methods in probabilistic modules is legitimate for constructing complex probabilistic models? What inference methods and probabilistic models can we use as building blocks? Do we need to be aware of specific inference methods that are encapsulated or we can use any blocks in any order as we do in deep learning frameworks?\n- Novelty of that paper is the new design of PPL. That is an interesting and important question for the community, but maybe ICLR paper is not the best format to present such kind of novelty. \n- From the specific examples in the paper, legitimacy of such modular structure is clear only for variational inference (that seems to be a common knowledge) and variational approximation of gaussian processes. But the application area of MXFusion remains unclear. Verbatim examples of code for the specific examples doesn’t make the difference between MXFusion and other PPLs clear, because it can be treated as encapsulation of the code into some classes, that can be implemented in other languages as well.\n- Comparison with other frameworks can be improved. In experimental section authors provide comparison with GPy framework in terms of RMSE and log-likelihood for gaussian process with 50 inducing points. As I understood both frameworks use the same inference methods and achieve the same performance, so the experiment can be considered as sanity check for MXFusion. The paper could benefit from comparison between different inference methods and providing benchmarks for inference time.\n\nOverall, the paper proposes a new PPL that is an important direction of study, but have several drawbacks and conference paper format is not the best way to present such kind of novelty.\n\nTypos:\n- Page 1, “despite the different of DNNs…” -> “despite the difference of DNNs…”?\n- Page 2, missing reference of the section\n- Page 2, section 3, “... sightly different form.” -> “... slightly different form”?']","[20, -20, -30]","[60, 60, 50]","[""The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper is well-written and clear, they also express some reservations about the novelty and impact of the work. The reviewer notes that the idea isn't entirely new and that the experimental results don't fully support the main objective. However, they do highlight several pros and the overall tone is more positive than negative. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and offering constructive criticism. They phrase their concerns as questions rather than direct criticisms, and provide helpful suggestions for minor improvements. The tone is professional and courteous, even when pointing out limitations."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('very instructive'), they raise several critical questions and concerns about the paper's approach and methodology. The overall tone suggests the reviewer finds significant issues with the work that need addressing.\n\nThe politeness score is moderately positive (60) as the reviewer maintains a professional and respectful tone throughout. They phrase their criticisms as questions or suggestions rather than direct criticisms, using phrases like 'how do you then leverage', 'Why is it better', and 'I do find this quite restrictive'. The reviewer also acknowledges positive aspects and shows respect for the authors' choices ('I perfectly respect your choice').\n\nHowever, the final comment about language issues is more direct and slightly lowers the politeness score. Overall, the review is constructive and polite, but with clear criticisms."", ""The sentiment score is -30 because while the reviewer acknowledges the importance of the topic, they express several concerns about the paper's structure, novelty, and clarity. The overall tone is more critical than positive, but not entirely negative. The politeness score is 50 because the reviewer uses respectful language throughout, framing criticisms as 'concerns' and suggestions for improvement rather than harsh judgments. They also acknowledge the importance of the research direction. The reviewer maintains a professional tone, even when pointing out typos, which contributes to the politeness. However, the score is not higher as the review doesn't include many explicitly polite phrases or compliments.""]"
"['Overall, this is a interesting paper on an important topic: knowledge extraction from Neural Networks.\nEven though the authors seem propose a novel approach to knowledge extraction, the paper would \ndramatically benefit from two additions:\n- an empirical evaluation on at least two more datasets (as is, the paper uses a single dataset)\n- an illustrative-but-realistic example of how at least one rule is extracted from each layer of the neural network \n\nOther comments:\n- on page 4 (1st paragraph in 3.3), the authors talk about a ""test set"" that, it turns out, it is extracted from the actual training set (1st paragraph of 4.1); the authors should use a more careful terminology\n- from the paper, it seems that the authors tried a single randomly chosen set 1000 random inputs in 4.1; they should most definitely try several such sets\n- Figure 1 should have a legend in the image, rather than as a 2-line caption\n ', 'Summary: This paper proposes a novel knowledge extraction method using M-of-N rules\nto help interpret hidden features in a Convolutional Neural Network (CNN). While the idea itself is interesting, I think that the paper is still in a very early stage and needs more work before it can be accepted. Detailed comments below.\n\nPros: \n1. The paper proposes a new algorithm to interpret CNNs.\n2. The paper is reasonably well written.\n\nCons: \n1. The experimental evaluation is quite weak. The authors present their (partial) results on a single dataset and also seem to generalize some of the findings in a rather misleading way. \n2. The proposed method is not compared against any baseline though there are ample rule-based methods to understand NNs in literature.\n2. It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? \n\nDetailed Comments:\n1. I think that the paper is missing a very clear discussion on what is novel about the proposed method in contrast with recent work on explaining NNs (or black box models) using rule based approaches. Examples of relevant papers include ""Anchors: High-Precision Model-Agnostic Explanations"" by Ribeiro et. al. and ""Interpretable & Explorable Approximations of Black Box Models"" by Lakkaraju et. al. among others. \n2. Another important piece of discussion that is missing is how the proposed search technique for extracting M-of-N rules is novel compared to a lot of prior literature which deals with the same problem\n3. I would strongly encourage the authors to experiment with at least three different datasets and multiple CNN architectures. \n4. I would really like to see the output of the proposed approach. What kinds of rules are being generated at each stage? It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? \n5. It would be good to do a simple user study to demonstrate that human users are able to understand something useful from the generated explanations. ', ""The paper proposes to rewrite each neuron of a neural network as a M-of-N decision rule. An measure of rule complexity (which takes into the account the number of terms in the rule) is proposed, and an approximate rule induction algorithm which binarizes the neurons using an information gain criterion is provided.\n\nThe paper gives no evaluation of the accuracy of extracted rules on a test set. Instead, the fidelity to network is evaluated on the train set for individual neurons (see below).\n\nSince even for moderately-sized networks the method would result in lots of rules, the authors propose instead to use the proposed algorithm and complexity criterion as a tool for understanding the complexity of concepts detected by a layer. The results, gathered in Figure 1, suggest that for some layers complex rules do approximate the behavior of neurons, while for other layers a neuron can't be replaced with a single but complex M-of-N rule.\n\nThe ideas presented in this paper seem rudimentary and require further exploration before being publishable. First of all, the main result of complexity-vs-layer doesn't differentiate between failures of the approximate rule induction algorithm (the terms in rules are considered for inclusion in a single order, reducing the search space) and the genuine complexity of the rules - this can be verified by evaluating a more exhaustive algorithm on at leas a few neurons (not necessarily on all of them).\n\nSecond, if the rules are extracted in a layerwise fashion,  their errors accumulate for deeper layers. However, Algorithm 1 suggests that each neuron is replaced by a rule independently from others, and moreover that it requires the true value of the neurons in the layer below, not of their rule counterparts. This means that the rules can't be combined and explains why the paper doesn't provide any measure for aggregate rule accuracy.\n\nSimilarly, the robustness of rules to adversarial examples is meaningless - it seems that applying the rules results in a system which is less accurate (only 8/10 of rules mimick their neurons with no rule complexity penalty) overall, but also makes fewer adversarial examples.\n\nMinor remarks.\nPlease don't ever produce Figures such as Figure 1: no legend (description in text), color selection is not black and white friendly, font is so small that the axis labels are hard to read... In fact, the poor quality of the Figure by itself should be sufficient  to reject the paper for not abiding to the author guidelines (sec. 4.3: All artwork must be neat, clean, and legible.)"", 'This paper is looking at how to extract knowledge from CNNs to help improve explainability and robustness against an adversarial attack.  It is using a known technical call M-of-N rules.\n\nThis problem of explainability of NN\'s is an important one and rules are a good step in that direction.  \n\n+ The paper is generally well written\n\n- The contribution seems to be relatively small\n- The evaluation is limited, only 1 dataset and only 1 technique evaluated\n\nGeneral advice for work in AI explainability:\nWhen one looks at the problem of AI explainability it is important to describe who the target audience is for the explanation.  Is it a machine learning expert, who wants to debug the model?  Is it an end user who wants to better understand why the prediction was made?  Is it a regulator who is trying to ensure the model\'s predictions are fair?\n\nEach of these personas will come with different needs and different technical backgrounds, so an assumption that some artifact (a rule set?) is ""explainable"" may apply to one group, but not the other group.  For example, rules are likely to be more explainable to a ML expert, but may not be to an end user, unless they are very small.\n\n']","[20, -50, -70, 20]","[60, 50, -20, 60]","[""The sentiment score is slightly positive (20) because the reviewer describes the paper as 'interesting' and on an 'important topic', indicating some positive aspects. However, they also suggest significant improvements, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, such as 'the paper would dramatically benefit' and 'the authors should', rather than using harsh or demanding language. They also start with a positive note before moving to suggestions. The review is constructive and professional in tone, offering specific recommendations without being overly critical or rude."", ""The sentiment score is -50 because the review starts with a generally negative tone, stating that the paper 'needs more work before it can be accepted.' The reviewer lists more cons than pros and provides several critical comments. However, it's not entirely negative as the reviewer acknowledges some positive aspects ('interesting idea', 'reasonably well written'). The politeness score is 50 because the reviewer uses professional and respectful language throughout. They provide constructive criticism and suggestions for improvement rather than harsh criticism. Phrases like 'I would strongly encourage' and 'It would be good to' indicate a polite and helpful tone, even when pointing out weaknesses in the paper."", ""The sentiment score is -70 because the reviewer expresses significant criticism and skepticism about the paper's methodology and results. They describe the ideas as 'rudimentary' and requiring 'further exploration before being publishable'. They point out several major flaws in the approach and lack of proper evaluation. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite critical and dismissive. They use phrases like 'Please don't ever produce Figures such as Figure 1' and state that the poor quality of the figure 'should be sufficient to reject the paper'. This direct criticism, while not impolite in the context of academic review, is certainly not couched in polite language."", 'The sentiment score is slightly positive (20) because the reviewer acknowledges the importance of the problem and mentions that the paper is generally well written. However, they also point out limitations such as a small contribution and limited evaluation, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and provides general advice for improvement without being harsh or dismissive. The reviewer balances positive comments with areas for improvement and offers helpful suggestions, maintaining a professional and courteous tone throughout the review.']"
"['Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.', 'Summary of the paper:\nThe paper proposes an algorithm to find solution to the maximum likelihood problem that could generalize well. The paper argues from the point of view that purely optimizing over the likelihood could result in solution that corresponds to poor local minimum which does not generalize well. By introducing a certain prior on weight, there exists a solution that could generalize. The solution arrived by introducing the prior makes it stable under perturbations of the training data. Recurrent update rules are derived for computing the integrals and hence the solution could be calculated. The authors discuss about the convexity of the effective loss when the variance is large. \n\nThe paper itself is very bad in its presentation. In terms of technical presentation, it is missing a lot of details, which makes reading and understanding the paper very hard.\n1.\tIt does not come with any proper literature review and introduction to the formulation of the problem. \n2.\tThe presentation of the methodology is also missing a lot of the explanation for many of the details used in the method. For example, in section 2, I do not quite understand the reasoning behind setting the probability P(y|x,w) = (1+1/T lnP(y|x,w))^T. Also, why R_t(w)/Q_(t+1)(w) could be approximated by 1. \n3.\tThe theoretical results come in plain words without proper mathematical presentation and the proofs for the statements are not well organized. The correspondence between the proofs and statements are not clear.\n4.\tThere seems to be no experiments conducted to support the practical use of the method proposed in the paper.\n\nOverall, I feel the paper is not ready for publication as a conference paper. The lack of details especially for the technical presentation part make it very hard to read. And the presentation of the results seem to be short of clarity and organization. Further, no experiments showing the practicality of the method are included in the paper.\n', ""Summary: Non-convex learning problems can have multiple solutions with different generalization properties, thus it is important to find solutions that generalize well. The goal of this paper is to derive an algorithm for finding a solution to the learning problem with the best possible generalization properties. This is achieved by using a Bayesian approach in which the parameters w (e.g., weights of a network) are random variables and the effective loss (integral wrt to w) is minimized in lieu of the usual loss. The paper assumes that each component of the weight vector w is Gaussian and derives a formula for updating the mean and covariance of said Gaussians (this is an SGD method). The paper claims that the resulting effective loss is convex for large variances (sigma > threshold), nonconvex for small variances (sigma < threshold), and converges to original loss as sigma goes to zero. The paper also claims that when sigma=0 there are trivial solutions that are unstable as data changes, but that when sigma=threshold (assuming this is what is meant by end of convexity) there are non-trivial solutions that are less sensitive to data changes and hence the most generalizable.\n\nComments: the goal of the paper (finding minima that generalize well) is an excellent one. But the paper is not clearly written and appears to oversell the contribution. In particular, the title speaks about SGD, dropout, generalization and critical points “at the end of convexity”. Naturally, a reader is inclined to think that the paper will study SGD and dropout for deep learning and analyze generalization properties of the solutions found by those methods. In reality, there is very little in the paper about SGD, dropout, and generalization. The connection with SGD is merely because the method for updating mu and sigma is an SGD method. The connection with dropout is mentioned in passing in one paragraph and it is not very clear. The connection with generalization is claimed but never quite explained (there are no generalization bounds in the paper). As far as understand, the paper considers the minimization of the effective loss, uses a Gaussian approximation for computing the effective loss, and focuses primarily on the characterization of convexity as a function of sigma as well as a characterization of the critical points depending on whether the effective loss is convex (sigma above a threshold) or not (sigma below the threshold). The main claim appears to be that critical points at the critical threshold lead to solutions that generalize well, but a detailed explanation of why this is the case isn't given. If my digest of the paper is the correct one, then modifying the title, abstract and intro to make this clear would have helped a lot. \n\nBeyond the high-level lack of clarity about the contribution of the paper, the writing lacks precision and rigor, and many things are undefined (though one can figure them out after reading many times back and forth). Specifically: \n\n1) It is not explained why the probability of each training sample can be expressed as a product of factors close to 1, with the product taken over the epochs.\n\n2) It is not explained why each factor can be modeled as a product of Gaussians\n\n3) At nearly the top of page 3, a product over n is substituted by a product over t, with x_n replaced by x_t and so forth, but the total number of products goes up from N to TxN. What is the value of y_{NT} and x_{NxT}? Do the authors mean that y_n should have been replaced by y_{n,t} and we now have two indices? Or do the authors mean that the same mini batch of N samples is reused, and so indices should be corrected accordingly? \n\n4) It is not clear why replacing R_t/Q_{t+1} by 1 is an adequate approximation.\n\n5) At the top of equation 4, there is a product, but no index wrt which the product is taken. Right after it says the index is t, but there is no t in the expression. Should mu_0 be mu_t and similarly for sigma? \n\nIn short, a promising direction, but the contribution of the paper appears to be over claimed and the writing of the paper needs significant improvement before the paper can be accepted for publication.""]","[-90, -80, -50]","[-30, -20, 20]","[""The sentiment score is highly negative (-90) because the reviewer states that the presentation is 'critically weak' and they 'failed to understand' the paper's objective and contributions. The reviewer also mentions 'many editing problems' and 'problematic' English, indicating a very poor overall impression. The politeness score is somewhat negative (-30) because while the language isn't overtly rude, it's quite blunt and doesn't attempt to soften the criticism. Phrases like 'critically weak' and 'failed to understand' are direct and potentially harsh, showing little concern for the authors' feelings. However, the reviewer doesn't use personal attacks or explicitly impolite language, which prevents the score from being even lower."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'very bad in its presentation,' 'missing a lot of details,' and 'not ready for publication.' They list multiple significant issues with the paper, including lack of proper literature review, unclear methodology, poorly organized theoretical results, and absence of experimental support. The only slightly positive comment is about the paper's argument, but it's overshadowed by the criticisms.\n\nThe politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite blunt and critical. Phrases like 'very bad in its presentation' and 'not ready for publication' are direct and potentially harsh. However, the reviewer does use some softening language like 'I feel' and 'seem to be,' which prevents the score from being even lower. The review also maintains a professional focus on the paper's content rather than making personal attacks."", ""The sentiment score is -50 because the review is generally critical of the paper, pointing out several issues with clarity, precision, and overselling of contributions. However, it does acknowledge the goal of the paper as 'excellent,' which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'the goal of the paper is an excellent one' and offer constructive criticism. The language is not overtly rude, but it's also not excessively polite, maintaining a neutral to slightly positive tone in terms of politeness. The reviewer provides specific points for improvement, which is helpful, but the overall critical nature of the review keeps the politeness score from being higher.""]"
"['*Update after discussion period*\nI remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don\'t think is asking for too much given a discussion period of three weeks.\n\n\nSummary:\nThe authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers\' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network\'s ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network\'s generalization performance. \n\nStrengths:\n+ Very straightforward and easy to follow \n+ Technically sound\n\nWeaknesses:\n- Feels trivial\n- The claims seem to be too general\n\nConclusion:\nI\'m torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I\'m not really sure what we learn from the paper.\n\n\nSpecific comments:\n\n- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?\n\n- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.\n\n- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (""orientation selectivity [plays] a causally important role in object recognition"" – abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.\n\n- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?\n', 'COMMENTS RELATED TO REVISION:\nThe new analysis that has been added takes a step towards getting at the relationship to invariance. This is a positive. In general comments, the authors state as important contributions: \n\n""1) to the debate on the harmfulness/importance of selectively activated units in DNNs [1-3] by presenting concrete examples where selectivity is important for generalization, and 2) to the neuroscience community, where, although orientation selectivity has been extensively analyzed for these 60 years since [4], its functional importance in a natural environment has remained unanswered.""\n\nOn point 2, while this provides an example of an artificial network where orientation plays an important role, it\'s a stretch to generalize this to conclusions concerning functional importance in neuroscience.\n\nOne point 1, I agree that this paper takes steps in the right direction, but ultimately the overall conclusions still feel as though they are a natural and implied consequence of limiting orientation selectivity. It is noted that  orientation selectivity is ""not just a superficial byproduct of object recognition, but is causally indispensable for object recognition"". The idea that selectivity for oriented edges is indispensible for object recognition again is a conclusion that feels as though it would be shocking if this were not true. The notion of it being a superficial byproduct of object recognition presupposes that the purpose of the system is to recognize objects. Again, it would be surprising if there were vestigial features in the network that are learned, but play no important role - especially among early layers. \n\nI think the paper might be strengthened by re-working this second point to more strongly establish the causality the authors claim. \n\nORIGINAL COMMENTS:\nThis paper presents interesting analysis and an ablation study on orientation selectivity in neural networks. This is analyzed with respect to generalization performance in decisions made. Overall, the paper is well written and interesting. However, I have a number of comments / concerns as described below:\n\nPositives:\n- The paper presents an in depth analysis of the role of orientation selectivity in neural hierarchies. This style of analysis is sorely lacking and fits the theme of learning representations\n- The paper itself is well written and quite polished\n- The authors have taken great care to rule out any possible confounds through experiments that are not identical to the main claims or objective (specifically the study of section 3.3)\n\nTo address:\n- The only concern I have (and it is somewhat significant), is the notion of ""generalization"". This is a rather loaded term, and it is not clear on cursory inspection where this generalization comes from. Is it a function of invariance among higher layers to scene geometry? What are the fundamental underpinnings of these observations outside of correlation to orientation selectivity.\n- It seems almost a tautology that removing orientation selectivity would impair performance. I would be more satisfied, and render a higher rating if I felt there was a ""smoking gun"" with respect to evidence. The conclusion is convincing, but the reasoning comes across as somewhat vague. With that said, it is also understood that this is a non-trivial matter to address and perhaps this paper is an important first step.', 'The paper presents a study on orientation selectivity in DNNs for image classification, arguing that this type of selectivity in the lower layers is crucial for generalization. This hypothesis is tested through an ablation study, which the authors interpret as a suggestion of the existence of a causal relation.\n\nThe authors tackle a very interesting problem that seems to have not received yet enough attention. The paper is quite well-written and clear, making it understandable also to non-experts. The only concern I would have is about the causal claims in Section 3.4. I’m not completely sure the ablation experiments are the correct way to “prove” causality, as opposed to somehow trying to intervene on the orientation selectivity directly. On the other hand, this seems complicated to prove and the approach proposed in the paper seems a pragmatic solution. I would possibly hedge slightly the causal claims.\n\nFrom an outsider point of view, I think the paper provides an interesting contribution to the discussion on orientation selectivity. I particularly appreciated its clarity and reproducibility.\n']","[-60, -20, 80]","[20, 60, 90]","[""The sentiment score is -60 because the reviewer remains unconvinced and expresses significant doubts about the paper's contributions and generalizability. They mention being 'torn' and point out several weaknesses, including that the results seem 'trivial' and the claims 'too general'. The update after the discussion period also indicates dissatisfaction with the authors' response. However, the reviewer does acknowledge some strengths, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'I'm torn' and 'potentially interesting observations' which soften the criticism. The language is not overtly polite, but it maintains a respectful tone throughout, even when expressing disagreement."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('interesting analysis', 'well written', 'polished'), they express significant concerns about the paper's conclusions and the strength of its evidence. The reviewer suggests that the paper's main conclusions feel like 'natural and implied consequences' rather than novel insights, and calls for stronger evidence to establish causality. The politeness score is moderately positive (60) as the reviewer maintains a professional and respectful tone throughout, balancing criticism with praise. They use phrases like 'This is a positive' and 'I agree that this paper takes steps in the right direction' to soften their critiques, and offer constructive suggestions for improvement rather than outright dismissal."", ""The sentiment score is 80 (positive) because the reviewer expresses a very positive view of the paper, praising its interesting topic, clarity, and contribution to the field. They only have a minor concern about causal claims, which slightly reduces the score from being maximally positive. The politeness score is 90 (very polite) due to the reviewer's consistently respectful and constructive tone. They use phrases like 'very interesting problem', 'quite well-written and clear', and 'I particularly appreciated', which demonstrate a high level of courtesy. The reviewer also frames their concern gently, using phrases like 'I'm not completely sure' and suggesting to 'hedge slightly the causal claims' rather than demanding changes. This polite and considerate approach throughout the review justifies the high politeness score.""]"
"['After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThis paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. \n\nPros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. \n\nCons: In spite of its theoretical contributions, this paper has a few major issues. \n\nQ1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. \n\nQ2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \\le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices?\n\nQ3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? \n\n', 'The rebutal and the revision of the paper solve my comments.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe paper presents a new characterization of generalization error bound for general deep neural networks in terms of the depth and width of the networks and the spectral norm of weight matrices. The proof follows the setting of Bartlett et al. 2017 with new development on the Lipschitz properties of neural networks.\n\nPros:\n1. The paper provides a solid improvement over previous bounds on generalization error.\n2. The presentation of the result and proofs is clear and easy to follow.\n3. It does case studies specially on widely used network structures CNN, ResNet, etc.\n\nCon:\n1. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018.  If it is vacuous, it is hard to justify the claims that given the generalization error, which is vacuous for all bounds, the paper\'s bound allows the choices of larger dimensions of parameters and larger spectral norms of weight matrices.\n2.  The L_w has the factor ""products of B_{d,2}s"" which, however, does not show up in the final generalization bound (The equation right above Appendix B). This products may introduce an additional $D$ under sqrt changing D to D^2 under the sqrt, which changes the order. The authors should give some explanation on this.\n\n3. Is the assumption on the orthogonal and normalized filters in CNN a must thing for the argument or just for convenience of the presentation? The paper should be clearer about this point.\n\n4. The RHS of the equation  in Lemma 2 misses terms related with B_{d,2}.\n5. Typos: Find one typo in Page 3  ""deﬁed as""\n', 'The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width ^2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. \n\nAlthough intended, the bound in Theorem-1 depends on the number of parameters and hold only if m > d * (p^2) = number of parameters (from the last line of proof of Lemma 3, we need \\beta < \\alpha and thus m > h). Such bounds are already know in the literature (see Anthony and Bartlett, 1999). Adaptive (completely norm dependent, like Bartlett et. al. 2017) bounds will be better than explicit dimension dependent bounds. The comparison in Figure-1 which suggest their bound to be better is unfair because they are comparing their specialized bounds for CNN to generic bounds for standard feedforward networks. Same for comparison in Table-2. \n\nIt was already established in Theorem 3.4 (Bartlett et al. 2017) that spectral norms are necessary for any generalization bounds for Deep Neural Networks, thus voiding the claims made in the paper (and discussion) about the importance of spectral norms. \n\nTypos / Errors : \n1. Statement of Lemma 2 does not contain the spectral norms terms. \n2. The third equation in Page 13 should be K <= \\sqrt{pD} max B_{d, 2}; and this changes the bound further. \n\nThe paper introduces some new techniques on mathematical analysis of specialized neural networks. ']","[20, 60, -20]","[50, 70, 20]","[""The sentiment score is 20 (slightly positive) because the review starts with a positive note that the authors have addressed the reviewer's comments, and the reviewer has increased their rating. The review also mentions some pros of the paper, acknowledging its theoretical contributions. However, it also lists several cons and major issues, which balances out the positive aspects. The politeness score is 50 (moderately polite) because the reviewer uses respectful language throughout, such as 'nicely addressed' and 'this paper makes theoretical contributions.' The critique is presented as questions rather than direct criticisms, which is a polite approach. However, the reviewer doesn't use overly formal or deferential language, maintaining a professional tone."", ""The sentiment score is 60 (positive) because the review starts with a positive statement that the authors have addressed the reviewer's previous comments. The review also lists several pros, indicating a generally positive view of the paper. However, it's not extremely positive due to the presence of some cons and suggestions for improvement. The politeness score is 70 (polite) because the reviewer uses professional and respectful language throughout. They present both pros and cons in a balanced manner, and their suggestions are framed constructively rather than critically. The reviewer also points out a typo politely. The language is more formal and objective than overly friendly, which is why it's not scored higher on politeness."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'introduces some new techniques'), they also point out several significant limitations and criticisms. The reviewer suggests that the paper's main claims are not novel or are potentially misleading, and identifies errors in the paper. However, the tone is not entirely negative, as the reviewer does recognize some value in the work.\n\nThe politeness score is slightly positive (20) because the reviewer maintains a professional and objective tone throughout. They present their criticisms in a matter-of-fact manner without using harsh or insulting language. The reviewer also acknowledges the paper's contributions and provides specific, constructive feedback (e.g., pointing out typos and errors). While not overtly polite, the language is respectful and appropriate for academic discourse.""]"
"[""This paper introduces a new way to have more compressed (lower rank) representation of the data in a supervised fashion. The authors motivates their work by saying that such representation are more useful for transfer learning and are more robust to adversarial examples! In order to achieve this, the authors introduce a virtual LR layer and utilize Nystrom technique to make the process more efficient.  The idea introduced in this paper is interesting but the paper is poorly written and organized which makes its through evaluation difficult. Below, I provide more detailed comments.\n\nI don't understand at what frequency the low-rank optimization as the  subproblem to Equation (OPT) is being done. Given that the DNN training  is being done using batched of examples, where do you put the low-rank  optimization, in the end of each epoch? It seems it is in the end of each epoch but it should be clearly stated.\n\nI don't understand motivation for L_N(A). The authors justify it by  saying that a trivial solution for the optimization problem would be setting A+b=0 and they introduce this term to avoid it. However, this is not correct. Note that there are n examples in A and we can only make one of them  zero at a time. (Note that talking about A+b is not also accurate  because they are not of the same size).\n\nIn order to use Nystrom method for low-rank approximation, W needs to be  a symmetric postive semi-definite matrix. I am not sure if the heuristic  procedure introduced in Page 4 is well-justifed. and whether we are  still optimizing the objective function introduced in Page 3. Do we  still have a stable training?\n\nWhat is ResNet N-LR in experiment? The authors introduced ResNet 1-LR and ResNet 2-LR but not ResNet N-LR! I found the description N-LR later but the naming is rather confusing. I would use LR instead of N-LR because it seems it has N LR layer. I would also explain this next to other methods. \n\nNot sure if I understand Bottle-LR. The description in the text is not clear and I don’t understand the motivation for this baseline! Again, this should be described next to other methods.\n\nThe authors do not mention what is their setting for r in the experiments (Table 1).\n\nPage 5, Paragraph after Table 1: First CIFAR-100 should be CIFAR-10.\n\nOne way the authors defend their framework is to have a representation that can be used in transfer learning. Nonetheless, the results in Table 1.b shows that their framework is not doing good for transfer learning.\n\nNot sure if I understand Figure 1. How do you change the number of singular values? I understand this is a hyper parameter for your framework but I am confused how it is being set for N-LR method. Similarly, I don’t understand Table 2 and how you change the embedding dimension. \n\n\nUnlike the claim made by the authors, it seems that VGG19 N-LR does better compared to VGG19 2-LR in Figure 2. "", 'Synopsis: \nOverall, this paper was fairly well written and seems to have an original approach towards inducing low-rank structure on the space of activations in some intermediate layer in a computationally efficient way without changing the underlying model. This training modification does not seem to affect test performance and the low-rank embeddings that are learned seem useful at discriminative tasks. Adversarial robustness also appears improved.\n\nPros:\n--While I am not familiar enough with the background literature on model compression in neural networks, I thought the augmented optimization problem used to induce low-rank structure on the space of activations was interesting and worthy of investigation. The authors appear to get great results in Table 1 & Table 2.\n\nCons:\n--I cannot really gauge the significance of the result against other existing approaches towards low-dimensional representations because of my limited familiarity with the relevant literature. However, I didn’t feel quite convinced by the discussion in the paper that low-rank activations were superior to other kinds of low-rank approximations, for instance to the network weights (c.f. discussion in Appendix A). I think the discussion on this topic could be a bit improved.\n--With respect to the writing, I’m a bit uncertain as to the primary message of the paper. While it seems to introduce a new augmented training approach for generating compressed representations which potentially has practical utility, based on the paper title and scattered discussion it seems to suggest that the representations themselves are interesting, e.g. the idea of having low-rank activations while largely maintaining test performance. I didn’t fully understand the extent to which the results are intriguing or helpful in understanding neural networks. Could this be developed a bit more?\n\nMiscellaneous comments:\n--In Figure 2, the accuracy with respect to adversarial perturbations seems to drop more for VGG19 2-LR (pink curve) than the model VGG19 N-LR (brown), which seems counter to your point on robustness?\n--In Figure 4, why is the behavior of ResNet 2-LR (blue curve) similar to ResNet N-LR (red curve)? I would’ve expected any number of LR layers to increase the sensitivity in intermediate layers to adversarial input perturbations.\n\n\n', 'This paper presents a way to induce low-rank representations in a deep neural network and study its effect on adversarial attacks.\n\nQuality\n\nThe analyses are conducted on several types of problems, first classification tasks for confirmation of the low-rank structure, and then on adversarial attacks. Unfortunately, there is only a very few number of experiments per analysis, making it virtually impossible to infer reliably any trends in the data. Table 1, 2, 3 and 4 contains at best enough information for a proof of concept, but it is not possible to make any conclusion out of them. Also, VGG results only appear in figure 2 where they appear to contradict the conclusions held by the authors. Is it difficult to understand why results from VGG do not appear in any table. \n\nClarity\n\nThe paper is difficult to follow. Introduction gives too much details and even contains methodological information, all of which obfuscates the main message which does get more clear in the latter sections. The sections 2 and 3 are confusing because they do not follow the logic presented in the abstract. The latter states that observations on the low-rank structure of the representations will be done prior to experimentally impose low-rank. However, section 2 presents the low-rank structure imposed on models while section 3 presents the observations of low-rank-representations jointly with the results of imposed low-rank structure.\n\nThere is no clear definitions of what the ""intriguing properties"" are beside the fact that forced low-rank representations yield similar results on classification and are more robust to adversarial attacks on a very limited number of experiments.\n\nOriginality\n\nUsing low-rank representation is not something new and has already been explored in [1] for instance.\n\nSignificance\n\nThere would be an important contribution to make if the author would analyze the effect of low-rank by varying the constraint. However, the current analyses are not pushed far enough to get any useful insight using only a fixed rank and making a minor modification by adding one or two LR-layers. Experiments in table 2 is a good step in this direction nonetheless.\n\n[1] Luo, Ping. ""Learning deep architectures via generalized whitened neural networks."" In International Conference on Machine Learning, pp. 2238-2246. 2017.']","[-30, 60, -50]","[20, 70, 20]","[""The sentiment score is -30 because while the reviewer acknowledges the paper's interesting idea, they express significant concerns about the paper's writing, organization, and clarity. The reviewer points out several issues and misunderstandings, indicating a generally negative sentiment. However, it's not extremely negative as they do recognize some positive aspects. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I don't understand' and 'Not sure if I understand' rather than making accusatory statements. They also offer specific suggestions for improvement, which is constructive. The language is not overly formal or polite, but it avoids rudeness and maintains a respectful tone."", ""The sentiment score is 60 (positive) because the review starts with an overall positive assessment, noting that the paper is 'fairly well written' and has an 'original approach'. The reviewer highlights several pros and finds the results 'interesting and worthy of investigation'. However, it's not a perfect score due to some cons and uncertainties expressed. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging their own limitations ('I am not familiar enough...') and framing criticisms as suggestions for improvement rather than harsh judgments. The use of phrases like 'I think', 'Could this be developed...?', and 'I didn't fully understand' show a considerate approach to feedback. The reviewer also balances positive and negative comments, which contributes to the polite tone."", ""The sentiment score is -50 because the review is generally critical of the paper, pointing out several shortcomings in the analysis, clarity, and significance of the work. The reviewer mentions issues with the limited number of experiments, difficulty in following the paper's structure, and lack of novel contributions. However, it's not entirely negative as the reviewer acknowledges some positive aspects, such as the potential for an important contribution if further analysis was done.\n\nThe politeness score is 20 because while the reviewer is critical, the language used is professional and constructive. The reviewer offers specific suggestions for improvement and frames criticisms in terms of the paper's content rather than attacking the authors personally. The tone is direct but not rude, maintaining a level of academic courtesy throughout the review.""]"
"['In convolutional neural networks, a convolutional filter conducts a series of multiply-accumulate (MAC) operations, which is computationally heavy. To save computational cost, this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy.\n\nSignificance wise, the results look promising, though it is not always the best method to preserve accuracy while doing the pruning. There are a few experiments where CP is better at preserving accuracy. In addition, it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation. In this manuscript, the best results are roughly 50% MAC savings (which is not equal to the saving of the whole networks) with some accuracy drop. It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned.\n\nQuality wise, there seems to be some room to improve from a technical point of view. The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers. This sounds about right, but lacks data support. In addition, even this intuition is correct, it might not be the optimal setting for pruning. We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal. As a counterexample, if the shallow layer is the most computationally heavy, saving 10% MAC ops might save more computational cost than saving 5% in the center layer while preserving the same accuracy level. \n\nThe above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1.\n\nThe method is quite original, and the manuscript is very well written and easy to follow.', 'This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction. In the introduction the authors envision a system similar to a cascaded classifier [Viola and Jones 2001] (I draw this conclusion not the paper). However the wording of the introduction is not clear or it does not align with what is presented in the paper.\n\nThe approach in the paper does not perform early stopping dynamically during the feedforward phase. The approach removes weights which do not impact the accuracy after training has completed and the fine tunes the resulting network.\n\nThe clarity of the introduction must be addressed however the work is still interesting. I recommend the authors try to make the introduction as accessible as possible.\n\n\nAlso there are very general statements like ""The activation layer introduces non-linearity into the system for obtaining better accuracy."" which do not contribute to the message of the paper. The paper will be fine without these statements. Shorter is better.\n\nSection 3 is good as a motivating example. However the conclusion “Thus, our focus is to develop an effective method of choosing a good intermediate result for saving more MAC operations with less accuracy drop” is not very clear. More insights written hear would be better.\n\nOne major flaw is that no analysis with respect to time of computation was performed. GPUs offer the advantage of being optimized for convolutions so it is possible that there is no speedup. Because of this it is unclear if the method would save time. The results clearly show individual computations (MACs) are reduced but it is not clear how this correlates with wall clock time.\n\nWhy do you start with the centre layers? I understand the heuristic you’re using, that the middle layers won’t have high or low-level features, and that they won’t break the other layers as badly if you modify them, but I feel like this is core to your method and it’s not adequately justified. I’d like to see some experiments on that and whether it actually matters to the outcome. Also, you don’t say if you start at the middle then go up a layer, or down a layer. I think this matters since your proposed C10-Net has only 3 convolutional layers.\n\nAll the filters in the same layer share a common checkpoint. Is that a good idea? What is the cost of doing this on a per-filter level? What is the cost on a per-layer level? Discussing runtime estimates for the added computation at training would make it more clear what the cost of the method is.\nIn section 4.4.3. you mention that the majority of weight distributions in CNNs follow the Gaussian manner. Could you cite something of this? You might also want to move that in Step 1 (section 4.1.), since it seems to be your motivation for selection of checkpoint locations (5% and 32%) and I had no idea why you selected those values at that point.\n\nTypos:\nTypo on page 3: “exploits redundancies inter feature maps to prune filters and feature maps”\n\nStructural:\nMaybe section 4.3 should be part of the description of Section 4. Proposed Method, not its own subsection.\n\nMaybe table 2 should go at the end of section 4.4.1, because you describe it the error and columns in section 4.4.1.\n', 'This paper proposes a new method for speeding up convolutional neural networks. Different from previous work, it uses the idea of early terminating the computation of convolutional layers. The method itself is intuitive and easy to understand. By sorting the parameters in a descending order and early stopping the computation in a filter, it can reduce the computation cost (MAC) while preserving accuracy.\n\n1. The networks used in the experiments are very simple. I understand that in the formulation part the assumption is that ReLU layer is put directly after convolutional layer. However, in state-of-the-art network, batch normalization layer is put between convolutional layer and ReLU non-linearity. It would add much value if the authors could justify the use cases of the proposed method on the widely adopted networks such as ResNet. \n\n2. I notice that there is a process that sort the parameters in the convolutional layers. However, the authors do not give any time complexity analysis about this process. I would like see how weight sorting influences the inference time.\n\n3. The title contains the word “dynamic”. However, I notice that the parameter e used in the paper is predefined (or chosen from a set predefined of values). So i am not sure it is appropriate to use the word “dynamic” here. Correct me if i am wrong here.\n\n4. In the experiment part, the authors choose two baselines: FPEC [1]. However, to my knowledge, their methods are performed on different networks. Also, the pruned models from their methods are carefully designed using sensitivity analysis. So I am curious how are the baselines designed in your experiments.\n\nOverall this paper is well-written and points a new direction to speeding up neural networks. I like the analysis in section 3.\n\nI will consider revising the score if the authors can address my concerns.\n\n\n[1] Pruning Filters for Efficient ConvNets. Li et al., ICLR 2017.\n\n']","[50, -20, 20]","[80, 50, 70]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the promising results and the originality of the method, while also pointing out areas for improvement. The review starts with positive aspects ('results look promising', 'method is quite original', 'manuscript is very well written') but also includes constructive criticism and suggestions for enhancement. The politeness score is 80 (quite polite) due to the reviewer's use of respectful language throughout. They use phrases like 'it would be great if', 'there seems to be some room to improve', and offer suggestions rather than harsh criticisms. The reviewer maintains a professional and constructive tone, balancing praise with areas for improvement, which is indicative of a polite and considerate review."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the work is interesting, they point out several flaws and areas for improvement. The review starts with criticisms about the clarity of the introduction and misalignment with the paper's content. It also highlights a 'major flaw' in the lack of analysis regarding computation time. However, the reviewer does mention some positive aspects, like Section 3 being good as a motivating example.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I recommend', 'More insights would be better', and ask questions to prompt further explanation rather than making harsh criticisms. The reviewer also provides specific suggestions for improvement and points out typos in a neutral manner. While not overly effusive, the language is respectful and aims to help improve the paper."", ""The sentiment score is slightly positive (20) because while the reviewer points out several concerns and areas for improvement, they also mention positive aspects like the paper being well-written, pointing in a new direction, and liking the analysis in section 3. The overall tone suggests a willingness to consider revising the score if concerns are addressed, indicating a generally positive but cautious stance. The politeness score is relatively high (70) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions (e.g., 'I would like to see...', 'I am curious how...'), and concludes with a positive note. The reviewer also acknowledges their potential misunderstanding ('Correct me if I am wrong here'), which is a polite way of expressing doubt.""]"
"['To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network.\n\nThe paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness.\n\nHowever, my main concern about the paper is still about its significance. \n1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, “lower value of \\sigma^\\ell(s) are indicative of better separation between classes”, what is the reason of not directly penalizing this value, rather than requesting a “stability” property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper.\n2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of “Proposed + Parseval”. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings.\n\nOther comments:\n1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. \n2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.', 'The paper proposes to use a regularization which preserves nearest-neighbor smoothness from layer to layer. The approach is based on controlling the extent to which examples from different classes are separated from one layer to the next, in deep neural networks. The criterion computes the smoothness of the label vectors (one-hot encodings of class labels) along the nearest-neighbor graph constructed from the euclidian distances on a given layer\'s activations. From an algorithmic perspective, the regularization is applied by considering distances graphs on minibatches. Experiments on CIFAR-10 show that the method improves the robustness of the neural networks to different types of perturbations (perturbations of the input, aka adversarial examples, and quantization of the network weights/dropout0.\n\nThe main contribution of the article is to apply concepts of graph regularization to the robustness of neural networks. The experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.\n\nWhile the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear. There does not seem to be any proof that adersarial examples are supposed to be classified better by keeping the smoothness of class indicators similar from layer to layer. Section 3.4 seem to motivate the use of the smoothness from the perspective of preventing overfitting. However, I\'m not sure how adversarial examples and the other forms of perturbations considered in the experiments (e.g., weight quantization) are related to overfitting.\n\nstrengths:\n- practical proposal to use graph regularization for neural network regularization\n- the proposal to construct graphs based on the current batch makes sense from an algorithmic point of view\n\n\ncons: experimental results are a bit weak -- the most significant results seem to be obtained for ""implementation robustness"", but it is unclear why the proposed approach should be particularly good for this setting since the theoretical motivation is to prevent overfitting. The results vs Parseval regularization and the indications that the metohd works well with Parseval regularization is a plus, but the differences on adversarial examples are tiny.\n\nother questions/comments:\n- how much is lost by constructing subgraphs on minibatches only?\n- are there experiments (e.g., on smaller datasets) that would show that the proposed method indeed regularizes and prevents overfitting as motivated in Section 3.4?\n\n\n', ""This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. The regularisation enforces smoothness on a graph built on the different features at different layers of the NN system. The proposed ideas are quite interesting, and integrates nicely into NN architectures. \n\nA few paths for improvements:\n\n- the 'optimal' choice of the power of the Laplacian, in 3.5, is eluded\n- the figures are not presented ideally, nor in a very readable form - for example, their are 90-degree rotated compared to classical presentations, and the plots are hardly readable\n- the might exist a tradeoff between robustness, and performance (accuracy), that seem to be explaining the proposed results (see Fawzi - Machine Learning 2018, for example)\n- in 4.2, what is a mean case of adversarial noise? Also, it would be good to see the effect of the regularizer of both the 'original' network, and on the network trained with data augmentation. It is not clear which one is considered here, but it would be interesting to study both, actually. \n- the second paragraph of the conclusion (transfer of perturbations) opens interesting perspective, but the problem might not be as trivial as the authors seem to hint in the text. \n\nOverall, very interesting and nice work, which might be better positioned (especially in terms of experiments) wrt to other recent methods that propose to improve robustness in NNs.""]","[-20, -20, 60]","[60, 50, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some improvements and interesting ideas, they express significant concerns about the paper's significance and experimental results. The reviewer states that the main concern 'is still about its significance' and that the results are 'not convincing', indicating overall dissatisfaction despite some positive aspects. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging improvements and using phrases like 'It would be very helpful' when making suggestions. They also frame criticisms as personal opinions ('I am not very convincing') rather than absolute statements. However, the review doesn't go out of its way to be overly polite, maintaining a professional tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express several concerns and limitations. The reviewer points out that the experimental results are 'a bit weak' and the significance of the results is 'unclear'. They also question the relationship between the proposed method and its effectiveness for adversarial examples. However, the score is not deeply negative as the reviewer does recognize some positive aspects, such as the 'practical proposal' and 'solid' experimental evaluation. The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns (e.g., 'unclear', 'a bit weak') rather than harsh criticisms. The reviewer also balances their critique by pointing out both strengths and weaknesses, which contributes to the polite tone. The use of phrases like 'The main contribution...' and 'While the overall concept... is appealing' further adds to the courteous nature of the review."", ""The sentiment score is 60 (positive) because the reviewer describes the paper as 'interesting' multiple times and calls it 'very interesting and nice work' in the conclusion. However, it's not extremely positive as the reviewer also points out several areas for improvement. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive criticism in a gentle manner. They use phrases like 'paths for improvement' and 'it would be good to see' rather than harsh criticisms. The overall tone is professional and courteous, while still providing honest feedback.""]"
"['# Summary\n\nThis work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.\n\n# Quality\n\nThe proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)\n\nA second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).\n\nSome smaller comments:\n\n* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.\n* Input sequences in NLP are not always sentences given as sequences of words.\n* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).\n* The authors say that operations of the same ""type"" can be batched together, but don\'t specify what ""type"" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?\n* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]\') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1\', y2\']\'), reducing the number of memory accesses in the output buffer.\n* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.\n\n# Clarity\n\nThe writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.\n\nThe text contains grammatical errors (""days even weeks"", ""The parallel computing helps""), tautological definitions (""batching [...] means organizing the same operations of computation graphs into batches"", ""padding, which is to pad the input sequences""), unclear use of language (""cooperating with the existing strategies""), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.\n\n# Originality and significance\n\nThe central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors\' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.\n\n# Summary\n\nPros:\n\n  * Useful dynamic batching trick that can lead to speedups\n  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime\n\nCons:\n\n  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage\n  * Some questionable statements and assumptions\n  * Lack of formalization and clear definitions\n  * Paper reads long-drawn-out, subpar writing hurts readability', 'This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits, and experiments on 3 LSTM tasks showed that in several settings the proposed method improved the speed of backward computation.\n\nThe proposed method is straightforward and reasonable in terms of improving the speed of the backward computation. Authors discussed the proposed method on only the neural network toolkits with a dynamic computation strategy, but this kind of optimization can be applied to any existing toolkits even which has a non-dynamic strategy. This point looks a kind of misleading of the discussion on the paper.\n\nThe paper provided a detailed analysis of time consumption on only a success-case (Table 4). Unfortunately, Table 2 and 3 showed that the proposed method does not have a global effectiveness and suggest a necessity for a further discussion about when to use the proposed method. Since this discussion can surely be conducted by comparing analyses of success and failure-cases, authors should provide analyses of all experiments.\n\nA conceivable weakness of the method may be the increase of memory consumption. If the toolkit plan to perform batch operations for summations of gradients, it needs to store all available gradients about each use of the corresponding variables. If the variable has a large shape and is used very frequently (e.g., variables in the softmax layer), the amount of total memory consumed by its gradient tends to be a serious problem. The non-batching strategy can mitigate this problem by discarding gradient information as soon as it is propagated back to all preceding nodes. The paper does not provide any information about memory consumption but it is important to discuss this kind of perspective.\n\nOthers:\nIn Table 2 and 3, please provide the ratio of speeds which are more reasonable to judge the real improvement rather than the one-zero decision (showed as up/down arrows).\nIn Table 4, why the time of the forward propagation slightly increased?\nYou should write a full list of authors of the DyNet paper that the official README provided:\nhttps://github.com/clab/dynet/blob/master/README.md', 'Batching of similar and independent operations in a neural network computation graph is a common way to improve efficiency through computational parallelism. Optimization is often applied to the computation graph by grouping independent operations into batches that can be computed in parallel. \nExisting techniques typically optimizes the feed forward computation and the backward computation follows the same grouping as the feed forward computation, which may not be optimal. In particular, the authors argue that a separate batching strategy should be applied to the back propagation computation to further improve the efficiency and showed that a recurrent network can benefit from such an optimization. The proposed solution is an automatic batching strategy that work for dynamic computation graphs.\n\nPros:\n\n- The results (for both CPU and GPU) show that the proposed method improves on top of two existing batching strategies (by depth and by agenda) across three different tasks. \n\nCons:\n\n- The feed forward and backward (gradient) computation can be viewed as a single computation graph. As such, would applying optimization to this graph achieve the same thing? Some clarifications/discussions will be helpful.\n\n- It is unclear why the proposed method is better for dynamic computation graph.  The benchmark results on CPU show that the proposed method is worse than the baseline for Tree-LSTM.  Will be useful to also do a similar profiling for the cases where the proposed method did not help.\n\n   \n  ']","[-30, 20, 50]","[20, 60, 75]","[""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('useful dynamic batching trick', 'can lead to speedups'), the overall tone is critical. The review points out several significant shortcomings in the paper, including lack of discussion on disadvantages, questionable statements, and poor writing quality. The 'Cons' section outweighs the 'Pros'. However, it's not extremely negative as the reviewer still sees value in the core contribution. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'should have been made' and 'needs significant improvement' rather than harsh language. They also acknowledge positive aspects before critiquing. However, the review is direct in its criticisms without much softening language, preventing a higher politeness score."", ""Sentiment Score (20): The review begins with a neutral summary of the paper's content. It then acknowledges the method as 'straightforward and reasonable' but points out several limitations and areas for improvement. While not overtly negative, the reviewer suggests multiple areas where the paper falls short, indicating a slightly positive but largely critical stance. Politeness Score (60): The language used is professional and constructive throughout. The reviewer offers specific suggestions for improvement without using harsh or dismissive language. Phrases like 'Authors should provide' and 'please provide' are polite ways of requesting changes. The critique is direct but respectful, maintaining a courteous tone while still clearly communicating areas of concern."", ""The sentiment score is 50 (slightly positive) because the review begins with a neutral description of the paper's topic, followed by a balanced list of pros and cons. The positive aspects (improved results across tasks) are noted, but some concerns are also raised. The politeness score is 75 (quite polite) because the reviewer uses professional and respectful language throughout. They present their critiques as questions or suggestions for clarification rather than direct criticisms. The use of phrases like 'will be useful' and 'some clarifications/discussions will be helpful' indicates a constructive tone aimed at improving the paper rather than dismissing it.""]"
"['The authors proposed an unsupervised learning framework to learn multisensory binding, using visual and auditory domain from animal videos as example. First, the visual and auditory inputs are autoencoded, and these latent codes are binding using a recurrent self-organizing network (Gamma-GWR). Furthermore, the authors proposed the expectation learning idea, which is inspire by psychology literature. In short, after the first pass of training using the real data. The authors fine tuned the model to bind the real data from one domain and the reconstructed data from another domain. This could be a good idea, as the authors pointed out, human usually bind all kinds of yellow bird to a same mental \'chirping\' sounds. So, this expectation learning could potentially group the representation to a canonical one. Also, the authors showed in Table 1 that with the expectation learning, the model\'s recognition accuracy is improved a bit. I think it would be interesting to show the reconstruction output example (as in Fig. 3) for both model with and without expectation learning. To see if it is as the authors claim, that the model with expectation learning is reconstructing the missing modality with more canonical images/sounds. (This may not be the goal in other practice, though I\'m convinced it is a potentially good psychological model as it explain well the multisensory imagery effect (Spence & Deroy, 2013). \n\nI found this manuscript quite hard to follow though. The description seems sometime not flowing very smoothly. And there are some clear typos and mess up of math notations make the reading unpleasant. I have noted down several points below, and hope the authors could improve in the next iteration.\n\n1. The description of variational autoencoder is not well written. The citation (Chen, 2016) is not the standard VAE paper people usually cite (unless the author is adopting something specific from the Chen\'s paper.). For example, the authors wrote ""the KL divergence between the encoded representation and a sample from the Gaussian distribution"" which sounds incorrect to me.\n\n2. Why a Variational autoencoder is necessary for visual domain, but a regular autoencoder is used in auditory domain?\n\nTypos:\n1. page 2, 2nd line: a online --> an online\n2. Use subscript I-1 to mean the winner neuron at t-1, I think this is not quite clear. I suggest to follow the notation in (Parisi & Wermter 2017), use I(t-1), which is easier to follow.\n3. page 7, 2nd line: more than 17% for audio.  -> for vision.\n4. page 8, 3rd line: not on the original high-abstraction data. Do the authors mean highly specific data? That seems make more sense.\n5. Several notation mismatch here and there. for example, in formula 6 it is w_j^s, but in the text below it become w_{j,s}.\n', ""Overview and contributions: The authors introduce a model that mimics expectation learning (i.e. learning multisensory representations by training to predict the other modalities from a given modality, for example, image to audio, audio to image). The proposed model is based on an autoencoder structure with a recurrent self-organizing network for\nmultisensory binding of latent representations. The authors perform experiments to show the reconstruction of image and audio signals given the other, as well as discriminative results on audio and image classification.\n\nStrengths:\n1. The paper is well motivated by the point of view of human learning. I really liked the abstract and introduction!\n2. I liked the recurrent self-organizing network presentation and usage.\n\nWeaknesses:\n1. While the paper is well motivated, I believe that the presence of multisensory expectation learning depends heavily on the type of multisensory data. For some modalities such as audio, it is clear what the mapping to language is (audio-to-language transcribing). For language-to-audio, there are multiple audio translations depending on the different tone of voice used by each person. Image-to-audio translation is also a one-to-many mapping. So I have concerns about how the model would work in these cases depending on the data used.\n2. I don't believe that the proposed model achieves state-of-the-art results: from Table 1, image classification performance is outperformed by Inception V3, and for both modalities, I'm not sure why the authors did not compare with more recent baselines. The best audio baseline is from 2016...\n3. It seems that this approach needs paired multisensory data for training, which limits the amount of training data as compared to unisensory models. Also, what if some sensors are noisy or missing? Is this model robust to such cases?\n\nQuestions to authors: \n1. Refer to weakness points.\n2. Can you comment on when you think multisensory expectation learning would work, and when it wouldn't? What types of data do we need, and from which modalities/sensors?\n\nPresentation improvements, typos, edits, style, missing references:\n1. Page 2: Our hybrid approach allowed -> Our hybrid approach allows\n1. Page 7: audiotry recognition -> auditory recognition\n2. Page 7: while improved the visual stimuli in 3% -> and improving ... by 3%\n3. Multiple other typos and awkward phrasing, I would suggest the authors spend more time proof-reading their paper before submission."", ""The paper develops a multi sensory model that binds audio/visual features using a self-organizing binding learning layer that takes as input the latent layers of two separate auto-encoders.  The binding is learned in an unsupervised way through Hebbian learning, which is an interesting way to implement temporal binding. \n\nMy concerns are as below:\n\n- How is the accuracy computed for the experimental data? Is it done moment-by-moment (per frame of the video?)? And if so, how is the accuracy of the audio assessed, since it's sampled at a much higher frequency?\n\n- Fig 2 should be a bar graph, since each class of animal is categorical, and the accuracies should be shown with standard deviations.\n\n- Abstract is written in a very confusing way that does not make clear succinctly what are the contributions of the paper. Generally, the paper may have been assembled in some haste.""]","[20, -20, -20]","[60, 60, 50]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the potential value of the proposed framework and its interesting aspects, such as the expectation learning idea. However, they also express difficulties in following the manuscript and point out several issues, which tempers the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'I think it would be interesting' and 'I hope the authors could improve' which maintain a polite tone. The reviewer also balances critique with positive observations, demonstrating a considerate approach to feedback."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out several significant weaknesses. The reviewer expresses concerns about the model's applicability, its performance compared to state-of-the-art results, and limitations in data requirements. These criticisms outweigh the positive comments about motivation and methodology. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, balances criticism with praise, and phrases concerns as questions or suggestions rather than harsh criticisms. The reviewer also compliments aspects of the paper ('I really liked...', 'I liked...') and offers constructive feedback for improvement."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting aspects of the paper, they express several concerns and point out areas for improvement. The initial paragraph is somewhat positive, but the list of concerns outweighs this, indicating an overall slightly negative sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, framing their concerns as questions or suggestions rather than harsh criticisms. They use phrases like 'My concerns are' instead of more confrontational language. The reviewer also balances critique with recognition of the paper's interesting aspects, which contributes to the polite tone.""]"
"['The paper addresses the problem of computational inefficiency in video surveillance understanding approaches. It suggests an approach called Dynamic Convolution consists of Frame differencing, Prediction, and Dyn-Convolution steps. The idea is to reuse some of the convolutional feature maps, and frame features particularly when there is a significant similarity among the frames. The paper evaluates the results on 4 public datasets. However, it just compares the approach to a baseline, which is indeed applying convnet on all frames. \n\n- State of the art is not well-studied in the paper. Video understanding approaches usually are not just applying convnet on all frames. Many of the approaches on video analysis, select a random set of frames (or just a single frame) [5], and extract the features for them. There is another set of work on attention, that try to extracts the most important spatio-temporal [1-4] information to solve a certain task. These approaches are usually computationally less expensive than applying convnet on all video frames. I suggest the authors compare their model with these approaches.\u2028\n\n[1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al.\u2028\n[2] Recurrent Models of Visual Attention, Mnih et al.\n\u2028[3] Action recognition using visual attention, Sharma et al.\n\u2028[4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al.\n\u2028[5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al.\u2028\n\n- In addition, car and pedestrian detection performance is part of the evaluation process. In this case, the approach should be also compared to the state-of-the-art tracking approaches (that are cheaper to acquire) in terms of computational efficiency and performance. \n- The writing of the paper should also improve to make the paper more understandable and easier to follow.\u2028Some examples:\u20281. Unnecessary information can be summarized. For example, many details on the computational costs in abstract and the introduction can just simply be replaced by stating that “these approaches are computationally costly”. \u20282. Using present tense for the SoTA approaches is more common.“ShuffleNet (Zhang et al. (2017)) proposed two new strategies”. \u20283. Long sentences are difficult to follow: “In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”\n\u2028\u2028+ The problem of large-scale video understanding is an important and interesting problem to tackle. \u2028', 'Summary - This paper proposes a technique to reduce the compute cost when applying recognition models in surveillance models. The core idea is to analytically compute the pixels that changed across frames and only apply the convolution operation to those pixels. The authors term this as dynamic convolution and evaluate this method on the SSD architecture across datasets like PETS, AVSS, VIRAT.\n\nPaper strengths\n- The problem of reducing computational requirements when using CNNs for video analysis is well motivated. \n- The authors analyze a standard model on benchmark datasets which makes it easier to understand and place their results in context.\n\nPaper weaknesses\n- A simple baseline that only processes a frame if \\sum_{ij} D_{ij} exceeds a threshold is never mentioned or compared against. In general, the paper does not compare against any other existing work which reduces compute for video analysis, e.g., tracking. This makes it harder to appreciate the contribution or practical benefit of using this method.\n- The paper has many spelling and grammar mistakes - ""siliarlity"", ""critiria"" etc.\n- Continuous convolutions - It is not clear to me what is meant by this term. It is used many times and there is an entire section of results on it (Table 6), but without clearly understanding this concept, I cannot fully appreciate the results.\n- Section 5.2 - what criteria or metric is used to compute scene similarity?\n- Overall, I think this paper can be substantially improved in terms of providing details on the proposed approach and comparing against baselines to demonstrate that Dynamic-Convolutions are helpful.\n- Design decisions such as cell-based convolution (Figure 3) are never evaluated empirically.', 'In this paper, the authors propose a dynamic convolution model by exploiting the inter-scene similarity. The computation cost is reduced significantly by reusing the feature map. In general, the paper is present clearly, but the technical contribution is rather incremental. I have several concerns:\n1. The authors should further clarify their advantages over the popular framework of CNN+LSTM. Actually, I did not see it. \n2.  What is the difference between the proposed method and applying incremental learning on CNN?\n3. The proposed method reduced the computation in which phase, training or tesing?\n4. The experimental section is rather weak. The authors should make more comprehensive evaluation on the larger dataset. Currently, the authors only use some small dataset with short videos, which makes the acceleration unnecessary. \n']","[-20, -30, -30]","[60, 50, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the novelty of the approach, they express several significant criticisms and suggestions for improvement. The reviewer points out limitations in the state-of-the-art review, comparison with existing methods, and writing quality. However, the tone is not entirely negative, as they recognize the importance of the problem being addressed.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I suggest' and provide specific recommendations for improvement rather than harsh criticisms. The reviewer also acknowledges the importance and interesting nature of the problem being tackled, which adds a positive note. The language is formal and respectful, avoiding any personal attacks or overly negative phrasing."", ""The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper, they point out several significant weaknesses and areas for improvement. The overall tone suggests that the paper needs substantial work, which leans towards a negative sentiment. However, it's not entirely negative as the reviewer does recognize some positive aspects.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use neutral language to describe the paper's weaknesses without being harsh or dismissive. Phrases like 'I think this paper can be substantially improved' and 'It is not clear to me' are polite ways of expressing criticism. The reviewer also begins by acknowledging the paper's strengths, which is a courteous approach. However, the score is not higher because the review doesn't go out of its way to be exceptionally polite or encouraging."", ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges that the paper is clearly presented, they express several concerns and state that the technical contribution is 'rather incremental'. The reviewer lists multiple issues and suggests that the experimental section is 'rather weak', indicating an overall negative sentiment towards the paper's content and methodology. However, it's not extremely negative as the reviewer does note some positive aspects.\n\nThe politeness score is slightly positive (20) because the reviewer uses generally neutral language and frames their concerns as questions or suggestions rather than harsh criticisms. Phrases like 'The authors should further clarify' and 'I have several concerns' are polite ways of expressing criticism. The reviewer also begins with a neutral summary of the paper before presenting their concerns, which is a polite approach. However, the score is not very high as the language, while not rude, is also not overtly courteous or complimentary.""]"
"['This paper proposes new heuristics to prune and compactify neural networks. The heuristics try to consider 1) filter weight and gradient normalization by their size, 2) saliency normalization across layers, 3) saliency accumulation across batch. The author claims that these can address problems previous studies had and experimental results show that the proposed method achieve higher compression ration with less loss of accuracy.\n\nThis paper discusses how to determine the importance of filters. As cited in the paper, there have been various attempts to tackle the same problem and the paper contributes to the series of efforts. The paper introduces a new way to compute such importance values based on their observations. The method is tested on a few dataset and a various models and compared with some previous studies. I like the simple but yet effective method, however, I think it is not good enough for ICLR. \n\n1. What is effective is not very clear.\n\nThe paper pointed out issues of previous studies and proposed the new method based on the observations. However, only the final method is compared with other work and it did not examine which part of the method was essential. The paper needs more detailed analyses on the proposed method. For example, the readers would want to know if the normalization in Eq. (2) is really important or not. The readers would be also interested in a visualization like Fig. 2 without saliency normalization. \n\n2. The numbers of previous studies come only from their papers.\n\nIt is very difficult to know if the proposed method is actually better than the previous methods if the numbers just come from their papers. We want to compare the ideas, but not numbers. The essential ideas of other papers need to be abstracted and tested in the paper by itself. It relates to the first item above. ""Baseline"" should be a baseline method but not models without pruning.\n\nNumbers from other papers are still useful to show that the numbers in the paper are good in an absolute manner.\n\n3. Weak theoretical reasoning\n\nEq. (1) in the paper is not actually used for optimization while some previous methods do. If the proposed method is better than other methods which directly optimizes the loss, should we think that the formulation itself is bad? \n\nThe paper discusses imbalanced pruned pruning results. It needs to show that it is actually bad.\n\n* minor things\n\n** Table 1: Should the first row of ""Wen et al. (2016)"" have ""5-19"" and ""1-5"" or ""4-19"" and ""1-4"" for ""Filters"" and ""Channels"", respectively?\n\n** I\'d recommend another proofreading.', 'This paper proposes a method for pruning CNNs, which considers all filters globally. It normalizes the weights of a filter within a layer and globally across all layers. To incorporate the effect of the data set, the authors additionally compute the normalized gradients and multiply the weight term with it. The third concept that the authors introduce is the idea of accumulating this measure of the importance of a filter for one entire epoch, before pruning.\n\nThe paper is missing an important citation: ""Pruning Convolutional Neural Networks for Resource Efficient Inference"", Molchanov, et al., 2017, which previously introduced many of the concepts proposed in this submission. For example, Molchanov, et al., propose the magnitude of the product of the activation and its gradient as the criteria for pruning. They derive mathematically as to how their criterion relates to the expected change in the overall loss via the Taylor series expansion. They additionally emphasize the importance of normalizing the criterion across network layers and the importance of greater number of updates to the network before pruning for better results. It is important that authors of this submission clearly compare and differentiate their work from the previous very closely related work of Molchanov et al.\n\nAdditionally, in order to understand the contribution of each of the concepts that the authors introduce, i.e., accumulation of saliency, multiplication with gradients, normalization within a layer, normalization across layers, the authors should present ablation studies to show the affect of each of these concepts independently to the overall accuracy of their approach.\n\n\n\n\n', 'In this paper, the authors propose to use cumulative saliency as guidance for model pruning. In particular, when designing saliency, they introduce a balanced formula by taking the filter size and gradient value into account.  The paper is well organized, and extensive experiments are investigated.  However, the novelty is relatively limited. The advantage of the proposed method is marginal on ImageNet, when comparing with the relevant approaches. ']","[-30, -20, -20]","[20, 50, 50]","[""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('I like the simple but yet effective method'), they ultimately conclude that 'it is not good enough for ICLR' and list several significant criticisms. The reviewer points out issues with clarity, comparison methodology, and theoretical reasoning, indicating an overall negative sentiment towards the paper's current state. The politeness score is slightly positive (20) as the reviewer uses generally respectful language, offers constructive criticism, and includes positive comments. They use phrases like 'I like' and 'I'd recommend' which maintain a polite tone. However, the criticism is direct and doesn't use overly softening language, keeping the score from being higher."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's proposed method, they point out significant issues: a missing important citation and the need for ablation studies. This suggests the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language without harsh criticism. They offer constructive feedback and suggestions for improvement rather than outright dismissal. The reviewer maintains a respectful tone throughout, using phrases like 'It is important that authors...' and 'the authors should present...' which are polite ways of suggesting necessary changes."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well organized', 'extensive experiments'), they also point out significant limitations ('novelty is relatively limited', 'advantage... is marginal'). The overall tone suggests the paper has merits but falls short in key areas. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, balancing praise with criticism, and avoids harsh or dismissive phrasing. They present their concerns in a professional manner without being overly critical or confrontational.""]"
"['This paper introduces an approach to pruning while training a network. This is interesting and experiments show interesting results in several datasets including ResNet18\n\nHere are a few comments:\n\n - Pruning or regularization for compression is not new. Alvarez and Wen have used group lasso types as suggested in the paper and some others such as Alvarez and Salzmann (Compression aware training NIPS 2017) and Wen (Coordinating filters ICCV2017) have used low-rank type of while training. How is this different to those? They also do not need any sort of fine tuning and more importantly, they show this can scale to large networks and datasets. \n\n- These last two works I mentioned promote redundancy, similarly to what is suggested in the paper. Would be good to get them cited and compared. Important from those is the training methodology to avoid relevant overheads. How is that happening in the current approach\n\n\n- While I like the approach, would be nice to see how this scale.  All for methods above (and others related) do work on full imagenet to show performance.  For ResNet, cleaning the network is not really trivial (near the block), is that a limitation?\n- Why limiting experiments to small networks and datasets? Time wise, how does this impact the training time?\n- Why limiting the experiments to at most 4 layers? \n- I am certainly not impressed by results on fully connected layers in MNIST. While the experiment is interesting does not seem to be of value as most networks do not have those layers anymore.\n\n- Main properties of this approach are selecting right filters while training without compromising accuracy or needing fine tuning. While that is of interest, i do not see the difference with other related works (such as those I cited above)\n\n- As there is enough space, I would like to see top-1 results for comprehensive comparison. \n\n- I think tables need better captions for being self-contained. I do not really understand what i see in table 5 for instance. \n- Droping 13% of top5 accuracy does not seem negligible, what is the purpose there? Would also be interesting to compare then with any other network with that performance. \n- What about flops and forward time? Does this pruning strategy help there?\n\n', '\n\n==Major comments==\n\nYou need to better explain how the regularization path is obtained for your method. It is not clear to me at all why the iterates from lines 5-8 in Alg 2 provide a valid regularization path. \n\nI am very confused by section 3.4. Is the pruning strategy introduced in this section specific to LBI? In other words, is there some property of LBI where the regularization path can be obtained by sorting the parameters by weight? It seems like it\'s not specific to LBI, since you use this pruning strategy for other models in your experiments. Is this the right baseline? Surely there are other sparsification strategies.\n\n\nHow/why did you select 5e-4 for lambda? You should have tuned for performance on a validation set. Also, you should have tuned separately for each of the baselines. There is no reason that they should all use the same lambda value.\n\nCan you say anything about the suboptimality of the support sets obtained by your regularization paths vs. if you had trained things independently with different regularization penalties?\n\nI am very concerned by this statement: \n""However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers.""\nThis is important. Do you have idea of why it is true? This instability seems like an obstacle for large-scale deployment.\n\nAre your baselines state of the art? Is there anything discussed in the related work section that you should also be comparing against?\n\n\n==Minor comments==\nThe difference between Alg 2 and 3 is mechanical and should be obvious to readers. I\'d remove it, as the notation is complex and it doesn\'t add to the exposition quality. Instead, you should provide an algorithm box that explains how you postprocess W to obtain a sparse network.\n\nYour citation format is incorrect. You should either have something along the lines of ""foo, which does X, was introduced in author_name et al. (2010)"" or ""foo does X (author_name, 2010)."" Perhaps you\'re using \\citet instead of \\citep in natbib.\n\nAlgorithm box 1 is not necessary. It is a very standard concept in machine learning. \n\nOn the left hand of (5), shouldn\'t Prox be subscripted by L instead of P?\n\n\n\n\n\n', 'This manuscript presents a method to prune deep neural networks while training. The main idea is to use some regularization to force some parameters to have small values, which will then be subject to pruning. \nOverall, the proposed method is not very interesting. More importantly, the manuscript only lists the percentage of pruned parameters, but did not compare the actual running time before and after pruning. ']","[-20, -50, -70]","[50, 20, -20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'interesting' and notes 'interesting results', they raise several critical points and express that they are 'not impressed' by some aspects. The overall tone suggests more concerns than praise. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, framing criticisms as suggestions ('would be good to', 'would be nice to see') and asking questions rather than making blunt statements. They also begin with a positive note about the paper being interesting. However, the review doesn't go out of its way to be excessively polite or complimentary, maintaining a professional tone."", ""The sentiment score is -50 because the review raises several major concerns and criticisms about the paper, including confusion about key aspects of the methodology, concerns about the experimental setup, and doubts about the stability and scalability of the proposed approach. However, it's not entirely negative as the reviewer does ask for clarifications rather than outright rejecting the work. The politeness score is 20 because while the reviewer is direct in their criticisms, they phrase most comments as questions or requests for clarification rather than harsh statements. The language is professional and constructive, avoiding personal attacks or overly negative phrasing. The use of phrases like 'Can you say anything about...' and 'I am very concerned by this statement...' maintain a respectful tone while clearly communicating issues."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the proposed method is 'not very interesting' and criticizes the manuscript for lacking important comparisons. There are no positive comments to balance this. The politeness score is -20 because while the language isn't overtly rude, it's quite blunt and dismissive. Phrases like 'not very interesting' and 'only lists' without any softening language or acknowledgment of effort come across as somewhat impolite in the context of a peer review. The reviewer doesn't offer any constructive feedback or suggestions for improvement, which also contributes to the negative politeness score.""]"
"['The paper proposes a new loss function which can be used in the reconstruction term of various auto-encoder architectures. The pixel-wise cost function \\ell(X, X\') = f(X - X\'; a) is defined for pairs of two input images X and X\' and has one positive real-valued hyperparameter a. For small values of t the function f(t; a) behaves like a quadratic function, while for large t it behaves like |t|. As a consequence, it is smooth, everywhere differentiable (like L2) while not penalizing outliers too hard (like L1). The authors present several experiments conducted on MNIST and Celeba datasets, demonstrating that a simple change of a conventional pixel-wise squared L2 distance with the proposed log-cosh cost function improves the FID scores of generated samples as well as the visual quality of reconstructions (including ""the sharpness""). \n\nI would say this is clearly an empirical study (even though the authors claim they provide ""theoretical justifications"", they are rather hand wavy), which is not a bad thing in this case. The message of the paper is very clear and I think the authors did a good job in selling their point. The main (and, perhaps, the only) contribution is the proposal to use the log-cosh function as the reconstruction cost. And this proposal is well justified by the set of experiments. \n\nHowever, there are several major issues:\n(1.1) The objective functions reported in appendix A.1 corresponding to WAE have in fact nothing to do with WAE. In WAE the regularizer penalizes the divergence between the prior distribution p(a) and *the aggregated posterior* distribution \\int_x q(z|x) p(x) dx. In other words, D_MMD(q(z|x) || p(z)) in Eq. 8 should be replaced with D_MMD(\\int_x q(z|x) p(x) dx || p(z)) in order to result in the WAE model. In summary, if the authors indeed used objectives reported in Eq. 8 of Appendix A, they were actually not using WAE but rather some other sort of regularized auto-encoders, which in a way are quite similar to VAEs. \n(1.2)  I am surprised to see the reported FID scores for the Celeba data set. Having worked with this data set myself in combination with VAEs and WAEs, I am impressed with the extremely low FID scores: 46 for the vanilla L2 VAE and 30 for the L2 WAE. Note that while in the appendix the authors say they follow the architectural choices provided in [1] while performing the ""L2 WAE Celeba"" experiment, the authors arrive at FID=30 compared to FID=55 reported in the ""Wasserstein Autoencoders"" paper. Also, based on my experience, achieving FID=46 on CelebA with a vanilla VAE is very impressive. Note that the authors use 10^4 of samples to evaluate the FID scores, which is exactly the same as in [1]. This size is known to be large enough to reduce the variance of FID, so the difference (55 - 30) can not be explained by the fluctuations of FID. Therefore, I ask the authors to (anonymously) share the code and/or checkpoints of the 2 particular trained models: L2 VAE and L2 WAE trained on Celeba. \n\nOther comments:\n(2.1) Note that the reconstruction cost function in VAE should be normalized for every value of the code Z, as it corresponds to the logarithm of the likelihood (density) function -log p(X|Z). L2 and L1 costs both correspond to the well known likelihood (decoder) models (Gaussian and Laplace). However, it is hard to say what decoder model (what type of conditional distribution p(X|Z) ) would give rise to the proposed log-cosh function. In particular, the normalizing constant is not known and may depend on Z. In other words, by exchanging the L2 cost with the log-cosh loss in the VAE one looses the theoretical guarantees supporting VAE, including the fact that the objective is the lower bound on the marginal log likelihood. While this is not necessarily a problem (unless one uses the value of the objective as the bound on the marginal log likelihood, which is not the case in this paper), I would suggest mentioning it. Notice that, for instance, in WAE this problem does not appear, as the reconstruction term there does not involve any likelihood functions and thus does not need to be normalized.\n(2.2) In Figure 2 I don\'t see why the authors did not highlight bad samples in the second row corresponding to their proposed method? I see many badly looking images there. Say, (4, 9) in VAE (MLP) and (8, 9) in VAE (Conv) and (6, 1) in WAE (MLP) and (2, 10) in WAE (Conv), where (i, j) means i-th row, j-th column, indexing starting from 1. \n(2.3) How would the Huber loss perform and how does it compare to the proposed loss?\n\n[1] Wasserstein Autoencoders. Tolstikhin et al., ICLR, 2018.', ""+ well written and explained\n+ well motivated\n- Unclear if it helps prevent blurry images\n- No comparison to similar loss functions or different tasks\n\nThe paper is well written and very easy to follow. I really liked the introduction as it reads well and clearly motivates the problem. The authors correctly highlight the two major issues in VAE's and propose to solve one of them (the reconstruction loss).\n\nOne of the major issues is that the proposed solution does not solve the problem of blurry images. There are two reasons why a generative model might produce a blurry output with an L2 (or L1) loss:\n 1. The training data is noisy and the best fitting generation will average this noise. This is the issue the authors propose to solve.\n 2. A much larger issue is that the generative model might be uncertain about the spatial location of objects. Here, again a blurry generation is the most optimal output. However unlike (1.) a different loss, like L1 or log-cosh, does not address this issue. The blurriness primarily comes from the element-wise nature of the loss function. Hence simply making the loss robust to outliers (in terms of color values) is not enough.\n\nThe second major issue in the paper is a lack of comparison to other alternative loss functions. As the authors mention in their intro, there has been a host of proposed solutions to the blurry generation: optimizing L1, SSIM, a perceptual loss (e.g. VGG features) and many more. However, the authors do not compare to any of them, and simply setup their main comparison with a squared L2 loss. I would expect the authors to at least compare to other simple loss functions. At a minimum a comparison should contain:\n * L2 (not squared)\n * L1\n * SSIM\n\nIn my view the weaknesses currently outweigh the strength of the submission."", 'This paper proposes to change the L2 norm of loss function of VAE into hyperbolic cosh function. The idea  and presentation are clear and straightforward. However, the used cosh function does not convince me since when t=a, f(t,a) will still be very large! Also, they will grow fast with exp|at|. The authors are encouraged to provide more detailed proofs for the advantages of cosh function.\n\nApart from the cosh loss, the Huber loss is well-known robust loss function used in statistics and many computer vision applications, and it has the similar properties of cosh function. I feel surprised that the authors do not aware this and do not compare it in experiments. \n\nThe introduction is a bit confusing. GAN is an implicit generative model as it does not have any explicit density form, but the likelihood and prior of vanilla VAE are Gaussian. I am not clear what is the motivation to introduce the cosh loss function.\n\nIf the authors aim to improve the generative quality, there are several works, such as using PixelCNN or other advanced likelihoods, improve the VAE. Besides these, recently MAE uses mutual information as the regularization to improve the quality. \n\nOverall, this work does not convey any theoretical analysis and significant results over state-of-the-art.\n\n\n\n \n']","[-20, -20, -50]","[60, 60, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('The message of the paper is very clear and I think the authors did a good job in selling their point'), they also point out 'several major issues' and express skepticism about some results. The overall tone suggests more concerns than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, even when critiquing. They use phrases like 'I would suggest' and 'I ask the authors to', which are polite ways of making requests. The reviewer also acknowledges positive aspects before diving into criticisms, which is a polite approach. However, it's not extremely high as the review is still quite direct in its criticisms."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'well motivated'), they also highlight significant weaknesses that 'outweigh the strength of the submission'. The reviewer points out major issues with the paper's approach and lack of comparisons. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive criticism. They use phrases like 'I really liked' and 'The authors correctly highlight', which contribute to a polite tone. However, the review is not overly effusive in its praise, maintaining a professional and balanced approach."", ""The sentiment score is -50 because the review expresses several criticisms and doubts about the paper's approach and contributions. The reviewer states that the cosh function 'does not convince' them, questions the authors' awareness of related work, finds the introduction 'confusing', and concludes that the work lacks 'theoretical analysis and significant results'. However, it's not entirely negative as the reviewer acknowledges that the idea and presentation are clear. The politeness score is 20 because while the reviewer is direct in their criticisms, they use relatively polite language such as 'the authors are encouraged to' and 'I feel surprised'. The tone is professional and constructive rather than harsh or rude, but it's not overtly polite either.""]"
"['The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.\n\nContributions\n-----------------\nA simple method for improving structural diversity.\n\nThe use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.\n\nExperiments suggest that the method is rather effective (albeit only improving translation quality marginally)\n\nI like the proposed discrepancy score based on pairwise BLEU scores.\n\nIssues\n---------\nIt is not clear if teacher forcing was used in the ""tag planning"" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the ""tag planning"" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).\n\nIt is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.\n\nNo details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.\n\nHaving a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.\n\nMinor issues\n-----------------\nCitation for the Xavier method is missing.\n\nNotation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.\n\nThere are many grammatical errors. Please make sure to proofread!\n\n""Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap."" What does this mean?', 'The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the ""Discrete Autoencoders for Sequence Models"" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:\n* comparing with a stronger NMT architecture and larger data-set\n* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.\n* comparison to fully unsupervised latents and some other system, e.g., the system from [1] above\n\nIn the absence of these comparisons and with little novelty, the paper is a clear reject.\n\n[Revision]\n\nGreatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one.', '\nThis paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:\n\n\n§1 ¶1  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vague—I’m not convinced this is meaningful.\n\n§1 ¶2-3 These paragraphs also vague.\n\n§1 ¶5 Why is this approach naive? Is this a well-known method? There are no citations.\n\nFig.1 Very confusing: it looks like the target sentence, “structural tags” and “coding model” form a loop! This example is also confusing because the “structural tags” are non-sensical… they have no relation to this example sentence! I can’t tell if this is because they were made up without relation to the input sentence, or worse, that they’re an actual example from the data, in which case there is something very wrong with the tagger used in the “naive” experiments.\n\nSec. 2.1 What is the motivation behind the heuristics for the “two-step process that simplifies the POS tags”?\n\nSec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these “codes"" (in the form of “simplified” POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called “softplus” in Eq. 2) are never explained.\n\nSec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isn’t needed.\n\nTable 1. I’m not sure what the code accuracy tells us. It’s also unclear to me what is means to “reconstruct” the “original tag sequence” from the codes, esp. given the description in Sec 2.1.\n\nTable 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading… this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.\n\nTable 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; I’m not convinced that the sequences mean anything per se, but it’s a bit like adding some random noise to the decoder state before generating the word sequence.\n\n5.1 “Instead of letting the beam search decide the best … we use beam search to obtain three code sequences with highest scores.” I’m confused: what is the difference?\n\n']","[20, -50, -80]","[60, 20, -20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges several contributions and positive aspects of the paper, such as the 'simple method for improving structural diversity' and the 'rather effective' experiments. However, they also point out several issues, which tempers the overall positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I like' and 'I would urge the authors to consider' which maintain a polite tone. The reviewer also balances positive feedback with areas for improvement, demonstrating a considerate approach to the review process."", ""The sentiment score is -50 because the review is generally negative, pointing out several issues with the paper and concluding with 'the paper is a clear reject.' However, it's not entirely negative as it acknowledges the problem as 'very interesting' and provides constructive feedback. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer specific suggestions for improvement. The reviewer also shows appreciation for the authors' responses in the revision section, which slightly elevates the politeness score. The language is not overly polite but maintains a respectful tone throughout."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'not ready for publication' and criticizes multiple aspects, including poor motivation, confusing modeling choices, and unconvincing experiments. The language used is direct and critical throughout, with phrases like 'poorly motivated,' 'very confusing,' and 'I am skeptical of these numbers.' The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite blunt and dismissive. The reviewer doesn't soften criticisms or offer many constructive suggestions, instead listing numerous issues without much regard for the authors' feelings. However, the language isn't overtly insulting, which prevents an even lower politeness score.""]"
"['This paper reads like the first thoughts and experiments of a physicist or mathematician who has decided to look at word representations and hyponymy. I mean that in both the positive and negative ways that this remark could be construed. On the positive side, it provides an interesting read, with a fresh perspective, willing to poke holes in rather than accepting setups that several previous researchers have used.  On the negative side, though, this paper can have an undue, work-from-scratch mathiness that doesn\'t really contribute insight or understanding, and the current state of the work is too preliminary. I think another researcher interested in this area could benefit from reading this paper and hearing the perspective it presents. Nevertheless, there just isn\'t sufficient in the way of strong, non-trivial results in the current paper to justify conference acceptance. \n\nQuality:\n\n - Pro\n   o Everything is presented in a precise formalized fashion. The paper has interesting remarks and perspectives. I appreciate that the authors not only did find most existing work on modeling hyponymy but provide a detailed and quite insightful discussion of it.  (A related paper from overlapping authors to papers you do cite that maybe should have been included is Chang et al. https://arxiv.org/abs/1710.00880 – which is a bit different in trying to learn hyponyms from text not WordNet, but still clearly related.)\n  -Con\n   o There just isn\'t enough here in the way of theoretical or experimental results. In the end, two ""methods"" of hyponymy modeling are presented: one is a simple logistic regression, which is estimated separately PER WORD for words with 10 or more hyponyms. This performs worse than the methods of several recent papers that the author cites. The other is a construction that shows that any tree can be embedded by representing nodes as ranges of the real line. This is true, but trivial. Why don\'t ML/NLP researchers do this? It\'s because they want a representation that doesn\'t only represent the ISA hierarchy but also other aspects of word meaning such as meaning similarity and dimensions of relatedness. Furthermore, in general they would like to learn these representations from data rather than hand-constructing it from an existing source like WordNet. For instance, simply doing that gives no clear way to add other words not in wordnet into the taxonomy. This representation mapping doesn\'t really give any clear advantage beyond just looking up hyponymy relationships in wordnet when you need them.\n\nClarity:\n - Pro\n   o The paper is in most respects clearly written and enjoyable to read.\n - Con\n   o The mathematical style and precision has it\'s uses, but sometime it just seemed to make things harder to follow. Referring to things throughout as ""Property k"" – even though some of those properties were given names when first introduced – left me repeatedly flicking up and down through the PDF to refresh myself on what claim was being referred to without any apparent need....\n\nOriginality:\n - Pro\n   o There is certainly originality of perspective. The authors make some cogent observations on how other prior work has been naive about adopted assumptions and as to what it has achieved (e.g., in the discussion at the start of section 5.1).\n - Con\n   o There is not really significant originality of method. The logistic regression model is nothing but straightforward. (It is also highly problematic in learning a separate model for each word with a bunch of hyponyms. This both doesn\'t give a model that would generalize to novel words or ones with few hyponyms.) Mapping a tree to an interval is fairly trivial, and besides this is just a mapping of representations, it isn\'t learning a good representation as ML people (or ICLR people) would like. The idea that you can improve recall by using a co-product (disjunction) of intervals is cute, though, I admit. Nice.\n\nSignificance \n - Con\n   o I think this work would clearly need more development, and more cognizance of the goals of generalizable representation learning before it would make a significant contribution to the literature. \n\nOther:\n - p.1: Saying about WordNet etc., ""these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models"" seems misplaced when there is now a lot of work on producing neural graph embeddings (including node2vec, skip-graphs, deepwalk, etc.). Fundamentally, it is just a bad argument: It is no different to saying that words have a fundamentally symbolic representation which cannot be readily used as input to neural NLP models, but the premise of the whole paper is already that we know how to do that and it isn\'t hard through the use of word embeddings.\n - p.2: The idea of words and phrases living in subset (and disjointness etc.) relationships according to denotation is the central idea of Natural Logic approaches, and these might be cited here. There are various works, some more philosophical. A good place to start might be: https://nlp.stanford.edu/pubs/natlog-iwcs09.pdf\n - p.2: The notions of Property 1 and 2 are just ""precision"" and ""recall"", terms the paper also uses. Do we gain from introducing the names ""Property 1"" and ""Property 2"" for them? I also felt that I wouldn\'t have lost anything if Property 3 was just the idea that hyponymy is represented as vector subspace inclusion.\n - p.2: fn.2: True, but it seems fair to more note that cosine similarity is very standard as a word/document similarity measure, not for modeling hyponymy, for this reason.\n - p.4: Below the equation, shouldn\'t it be Q(w\', w) [not both w\'] and then Q(w\', w) and not the reverse? If not, I\'m misunderstanding.', ""This paper explores the notion of hyponymy in word vector representations. It tests the capacity of a logistic regression classifier to distinguish words that are and are not hyponyms using fastText embeddings, and it also describes a method of organizing WordNet relations into a tree structure and defining hyponymy based on this structure.\n\nThe problem of capturing hyponymy relations within vector space models of word representation is an interesting and important one, but it is not clear to me that this paper has made a substantive contribution to it. The paper seems simply to 1) observe that fastText embeddings are imperfect for hyponymy detection with a linear classifier, and 2) reconstruct the fairly natural interpretation of WordNet relations as a hierarchical tree structure, and to re-extract hyponymy relations from that tree structure. As far as I can tell, the paper's “supervised” model does not use embeddings  (or learning) at all.\n\nAssessing the paper's contribution is made more difficult by an overall lack of clarity. The details of the experiments are not laid out with sufficient explicitness, and the reporting of results is also fairly confusing (I am not clear, for example, on what is depicted in Figure 2). The paper is not organized in a particularly intuitive way, nor has it made clear what the contributions might be. \n\nOverall, while I think that this is a worthy topic, I do not think that the contribution or the clarity of this paper are currently sufficient for publication. \n\nAdditional comments:\n\n-The PCA plot is too dense to be a useful visual - it would be more useful to plot a smaller number of relevant points.\n\n-Results should be presented more clearly in table form - there seem to be a large number of results that are not reported in any table (for instance, the results described in Section 3)."", '* Summary of the paper\n\nThis paper studies how hyponymy between words can be mapped to feature representations. To this end, it lists out properties of such mappings and studies two methods from the perspective of how they address these properties.\n\n* Review\nThe goal of this paper: namely, formalizing the hypernymy relation over vector spaces is not only an interesting one, but also an important one -- being able to do so can help us understand and improve vector representations of words and reason about their quality.\n\nIn its execution, however, the paper does not seem ready for publication at this point. Two major issues stand out.\n\nFirst, several things are unclear about the paper. Here\'s a partial list:\n1. Property 3 is presented as a given. But why is this property necessary or sufficient for defining hyponymy?\n2. It is not clear why the measure based definition is introduced. Furthermore, the expression above the statement of property 4 is stated as following from the definition. It may be worth stating why.\n3. Section 3.1 is entirely unclear. The plots in Fig 2 are empty. And in the definition of Q on page 4, the predicate that defines the set states P(w, f(x)). But if the range of P is [0, 1], what does it mean as a predicate? Does this mean we restrict it to cases where P(w, f(x)) = 1?\n\nSecond, there are methodological concerns about the experiments.\n1. In essence, section 3 proposes to create a word-specific linear classifier that decides whether a new vector is a hypernym or not. But this classifier faces huge class imbalance issues, which suggests that simply training a classifier as described can not work (as the authors discovered). So it is not clear what we learn from this section? Especially because the paper says at the just before section 3.1 that ""we are ultimately not interested in property 5"".\n2. Perhaps most importantly, the method in section 4 basically represents a pruned version of WordNet as a collection of intervals. It is not surprising that this gets high recall because the data is explicitly stored in the form of intervals. Unfortunately, however, this means that there is no generalization and the proposed representation for hyponymy essentially remembers WordNet. If we are allowed to do that, then why not just define f(w) to be an indicator for w and P(w, f(w\')) to be an indicator for whether the word w is a hyponym of w\'. This would give us perfect precision and recall, at the cost of no generalization.\n\n* Minor points   \n1. The properties 1 and 2 are essentially saying that the precision and recall respectively are alpha. Is this correct?\n2. Should we interpret P as a probability? The paper doesn\'t explicitly say so, but why not?\n3. The paper is written in a somewhat informal style. Some examples:\n   - Before introducing property 3, the paper says that it is a ""satisfying way"". Why/for whom?\n   - The part about not admitting defeat (just above section 3.1)\n   While these are not bad by themselves, the style tends to be distracting from the point of the paper.\n\n* Missing reference\nSee: Faruqui, Manaal, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. ""Retrofitting Word Vectors to Semantic Lexicons."" In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1606-1615. 2015.\n\nThis paper and its followup work discusses relationships between word embeddings and relations defined by semantic lexicons, including hyponymy.\n']","[-20, -70, -60]","[60, 20, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting read', 'fresh perspective'), they ultimately conclude that the paper is too preliminary and lacks strong results to justify acceptance. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer maintains a professional and respectful tone throughout, balancing critiques with positive observations and using phrases like 'I appreciate' and 'interesting remarks'. They provide constructive feedback and explain their reasoning clearly, which contributes to the polite tone despite the overall negative assessment."", ""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's contribution and clarity. They state that the paper hasn't made a 'substantive contribution' to the field, lacks clarity in experimental details and results reporting, and is not sufficiently organized. The reviewer concludes that the paper is not currently suitable for publication. However, the score is not at the extreme negative end because the reviewer acknowledges that the topic is 'worthy' and 'interesting and important'. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'it is not clear to me' and 'I do not think' rather than making blunt or harsh statements. The reviewer also offers constructive feedback and additional comments for improvement, which contributes to the slightly positive politeness score."", ""The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges the importance of the paper's goal, they state that 'the paper does not seem ready for publication at this point' and outline two major issues: lack of clarity and methodological concerns. They also list several minor points and a missing reference, indicating significant room for improvement. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'it may be worth stating' and 'perhaps most importantly' to soften criticisms. They also begin with a positive note about the paper's goal. However, the overall tone is direct and critical, which prevents a higher politeness score.""]"
"['Summary. The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples. They also study the influence of the smoothness. From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.\n\nPros.  \n* Robustness of neural nets is a challenging problem of interest for ICLR\n* The paper is well written\n* The experimental study is convincing\n* The experimental results for the smoothed gradient attacks are promising\n\nCons.\n* The results of the experimental study are somehow expected\n* the idea of smoothing gradients is not new\n\nEvaluation.\nThe experimental study of the transferability of adversarial examples is well designed. Experimental protocol is convincing. The smoothed gradient attacks improve many previously proposed attacks. Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.\n\nSome details.\nTypos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4; \n* the choice \\sigma = 15 in Section 6.2 should be justified by the following study\n* \\sigma is not given in Figure 3(a)', 'The paper explores how the architecture, smoothness of the decision boundary and test accuracy of a model impacts the transferability of examples produced from it.  The paper provides a couple of novel insights, such as the asymmetry when transferring adversarial examples from one model to another. In addition, a novel method is proposed to enhance the transferability of adversarial examples from any model, through using smoothed gradients.\n\nThe experiments seem to show that the effect is rather large, and also makes the examples more robust to other transformations such as JPEG compression. Overall, these are interesting insights that could lead to further developments in making models more robust to adversarial examples. In particular, deriving adversarial examples that are both transferable and resilient to certain usual image transformations shows that the scope of the issue with adversarial examples may be even greater than what is understood today.\n\nThe paper is rather clear. Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear. Some examples (there are way too many to report them all):\n\n- ""Transfer-based attackS ... since they ...*\n- ""of adversarial exampleS ...""\n- ""from model A can transfer to model B""\n- ""less transferable than *those from* a shallow model""?\n- ""investigations, We "": don\'t capitalize\n- ""the averaging *has* a smoothing effect""\n- ""our motivation are""\n- ""contributed it to""\n- ""available *to the* adversary""\n- ""crafting adversarial perturbationS""\n- ""directly evaluation""\n- ""be fixed 100""\n\nPros:\n- Transferability and robustness of adversarial examples is a very important problem\n- Interesting insights, esp. the construction and evaluation of examples that are more resilient to certain image transformations\n- Experimental results are convincing\n\nCons:\n- Contribution overall may be a bit limited\n- Grammatical errors and odd formulations all over the place', ""This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model. There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...\nThe authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples. Two experimental studies are made for each influence factor from existing architectures. Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.\n\nPros\n-the proposed experimental studies can be interesting to the community\n-many interesting illustrations are provided.\n\nCons\n-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better\n-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.\n-Only two influence factors are studied, again the paper would be more interesting with a more general study\n\nThe paper has an interesting potential but seems a bit limited in its present form.\n""]","[60, 50, -20]","[80, 60, 50]","[""The sentiment score is 60 (moderately positive) because the reviewer expresses a 'rather positive' opinion overall, praising the paper's writing, experimental design, and promising results. However, they also mention some cons and express uncertainty about the novelty, preventing a higher score. The politeness score is 80 (quite polite) due to the reviewer's constructive tone, balanced feedback, and use of phrases like 'well written' and 'convincing'. They provide both pros and cons professionally, and offer specific suggestions for improvement without harsh language. The presence of a few typo corrections slightly reduces the politeness score, as it could be seen as nitpicking, but they are presented matter-of-factly rather than critically."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's novel insights and interesting contributions, while also pointing out some limitations. The reviewer mentions 'interesting insights,' 'convincing experimental results,' and the importance of the topic. However, they also note that the 'contribution overall may be a bit limited' and highlight numerous grammatical errors. The politeness score is 60 (moderately polite) because the reviewer maintains a professional tone throughout, balancing positive feedback with constructive criticism. They use phrases like 'unfortunately' and 'rather clear' to soften negative comments, and provide specific examples of errors to help the authors improve. The reviewer also clearly outlines pros and cons, which is a respectful way to provide feedback."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting potential', 'can be interesting to the community'), they also highlight several limitations and cons. The overall tone suggests that the paper is not quite meeting expectations in its current form. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, balancing criticism with acknowledgment of positive aspects. They avoid harsh or dismissive language, instead offering constructive feedback. The reviewer maintains a professional tone, presenting both pros and cons objectively.""]"
"['Summary:\n\nThe paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.\n\n\nComments:\n\nThe proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. However, the motivation of using RL is missing from the technical contribution. Considering a deterministic policy, using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. \n\nThe motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. \n\nThe literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein).\n\nEmpirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. \n\n\nQuestions:\n\n-How are cold-start situations encountered if items are one-hot encoded?\n-Why is there a strong focus on quick adaptation to user sessions? Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests.\n\n\nMinor:\n\n-Proofreading is necessary\n-Table 1 and 2 would be more readable if they were figures\n-Figure 3 seems to be taken from Tensorflow runtime convergence plots, which could be dropped given the limited space', 'The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. The proposed algorithm with an innovative IRN architecture was intriguing. \n\nThe writing of the paper was not very clear and pretty hard to follow. With this level of clarity, I don’t think it’s easy for other people to reproduce the results in this paper, especially in section 4, where I expect more details about the description of the proposed new architecture. Even though the author has promised to release their implementation upon acceptance, I still think the paper needs a major change to make the proposed algorithm more accessible and easier for reproduce.\n\nSome examples:\nWhat is L_A3C in “L = L_A3C + L_IRN” in the first paragraph of session 4? It looks like a loss from a previous paper, but it’s kind hard to track what it is exactly.\n\n“where Tj,τ is the τ-th imagined item, φ(·) is the input encoder shared by π (for joint feature learning), AE is the autoencoder that reconstructs the input feature, and the discounting factor γ is used to mimic Bellman type operations. … Therefore, we use the one-hot transformation as φ(·) and replace AE with the policy π (excluding the final softmax function), and only back-propagate errors of non-zero entries.”\nThis seems one of the most important components of the proposed algorithm, but I found it’s very hard to understanding what is done here exactly.\n\nRegardless the sketchy description of the algorithm, the empirical results look good, with comprehensive baseline methods for comparison. It’s interesting to see the comparison between different reward function. Maybe the author can also discuss on the impact of the new imagination module on the training time.', 'The paper aimed at improving the performance of recommendation systems via reinforcement learning. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; (2) the trajectory manager (TM) that determines how to roll out the IC under the planning strategy and produces a set of imagined item trajectories; (3) the imagination-augmented executor (IAE) that aggregates the internal data resulting from imagination and external rewarding data to update its action policy.\n\nStrengths of the paper:\n(1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people’s daily lives. \n(2) Experiments were conducted on a publicly available dataset. \n(3) Robustness to cold-start scenario was tested and evaluated in the experiments.\n\nWeaknesses of the paper:\n(1) The motivations of applying reinforcement learning techniques are not convinced to me. There are a lot of supervised learning algorithms to the task of recommendations. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? Is it because reinforcement learning based methods work better than traditional machine learning based ones? The motivations of integrating A3C (Asynchronous Advantage Actor-Critic) but not other techniques into the proposed model are not convinced to me as well. \n(2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines.\n(3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. In equations (2) and (3), what is theta_v? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)?\n(4) The contributions of the paper in terms of theory are somewhat not significant. It seems that the proposed algorithm is built based on and combined by existing algorithms such as A3C. \n\nMinor comments:\n(1) It would be better if the authors can test the proposed model on more datasets. There are many publicly available datasets for testing the performance of recommendation systems.\n(2) Figure 2 is not straightforward. It would be better if the authors can draw the figure in other ways. (I am not sure if the authors have expressed the underlying ideas clearly with Figure 2).\n']","[-20, -20, -20]","[50, 50, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper (e.g., 'interesting inspiration', 'empirical part exceptionally strong'), they also point out several significant weaknesses and areas for improvement. The reviewer questions the motivation for using RL, notes missing literature, and suggests the need for more varied experiments. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'interesting inspiration' and offering constructive feedback. They also pose questions for the authors to consider rather than making outright criticisms. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a fairly neutral, academic tone overall."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the intriguing aspects of the proposed framework and good empirical results, they express significant concerns about the clarity of the paper and its reproducibility. The reviewer states that the paper is 'hard to follow' and needs 'major change', which outweighs the positive aspects mentioned. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, offering constructive criticism and specific examples to support their points. They use phrases like 'I don't think' and 'Maybe the author can' which soften their critiques. The reviewer also acknowledges positive aspects of the work, which contributes to the polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they list more weaknesses and express several concerns about the methodology and contributions. The reviewer's tone is not entirely dismissive, but they are clearly not fully convinced by the paper. The politeness score is moderately positive (50) because the reviewer uses professional language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions for improvement rather than harsh judgments. They use phrases like 'It would be better if' and 'I am not sure if' which maintain a respectful tone even while pointing out flaws.""]"
"['The authors introduce a novel distance function between point sets, based on the ""permutation invariance"" of the zeros of a polynomial, calling it ""holographic"" distance, as it essentially depends on all the points of the sets being compared. They also consider two other permutation invariant distances, and apply these in an end-to-end object detection task. These distance functions have time-complexity O(N^2) unlike the previously proposed ""Hungarian distance"" based on the Hungarian algorithm which is O(N^3) in general. Moreover, they authors show that in two dimensions all local minima of the holographic loss are global minima.\n\nPros: The paper is well written, the ideas are clearly and succinctly presented. Exploiting the connection between 2D point sets and zeros of polynomials is an interesting idea.\n\nCons: The experimental section could be better. For example, the authors could do simple experiments to show how an optimization algorithm would explore the holographic loss surface (in terms of hitting global/local minima) in dimensions greater than two. Also, in the object detection example, no comparison is given with the Hungarian loss based algorithm of Stewart et al. (2016) (at the very least, the authors could train their neural nets using the Hungarian loss, choosing one optimal permutation at the ""transitioning points"") .', 'This paper proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. For a 2D point set, the idea is to define a complex polynomial with the points (interpreted as complex numbers) as roots. To compare two point sets, the loss eventually boils down to evaluating the complex polynomial at all target points. The loss can be extended to high-dimensional point sets without the same theoretical framework. It is a sum over each target point the product of the distance of between the target point and all points of the source set. The claimed advantage of the proposed distance is it is more efficient compare to other metric such as Hausdorff distance or the sum of squared distance between matched point pairs (SMD). Experiments show that the proposed loss can be used in training.\n\nThe idea is interesting and new. However, the paper has a lot of room to improve. Here are a few specific issues that can be addressed in future version:\n\n1) It is unclear what the advantage of the new idea is. I agree it leads to strong gradients compared to Hausdorff distance. However, this way of pulling all points to all points may also be very inefficient, as each point can get pulled toward all target points. These forces can easily cancel each other. \n\n2) As already hinted in the paper, the gradient can be extremely unstable numerically. There is not any solution to address this issue.\n\n3) Experimental results do not show the proposed method has any advantage over others. Also important loss such as the Wasserstein distance should be considered and compared with.', 'This paper proposes permutation invariant loss functions which depends on the distance of sets. They motivate this by problems where one wants to detect objects (say stones in an image) where there is no explicit ordering of objects. \n\nPros: The idea sounds interesting and seems like this approach could work well in practice. The authors provide an interesting algorithm which minimizes this loss function in O(N^2) instead of O(N^3). Moreover, the authors also show interesting properties of the Holographic loss, along with some interesting properties of the minima etc.\n\nCons: My major criticism of this work is that, while this seems like an interesting idea, the authors do not really provide extensive results on real world datasets. They consider a simple proof of concept which is based on MNIST images. I do not think this is sufficient. I would have been much more convinced with this paper if I had seen results on Pascal VOC and COCO, since this is the main motivation of their work.']","[50, -20, 50]","[75, 60, 75]","[""The sentiment score is 50 (moderately positive) because the reviewer begins by acknowledging the novel aspects of the paper and its clear presentation. They mention several pros, including the well-written nature of the paper and the interesting ideas presented. However, the reviewer also points out some cons, particularly in the experimental section, which prevents the sentiment from being highly positive. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively before offering constructive criticism. They use phrases like 'The paper is well written' and 'Exploiting the connection... is an interesting idea,' which demonstrate politeness. Even when pointing out limitations, the reviewer suggests improvements rather than being critical, maintaining a professional and courteous tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'interesting and new', they also state that 'the paper has a lot of room to improve' and list several specific issues. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positive aspects before offering constructive criticism. They use phrases like 'It is unclear' and 'Experimental results do not show' rather than more harsh or direct criticisms. The reviewer also suggests improvements for 'future version', implying an expectation of continued work rather than outright rejection."", ""The sentiment score is 50 (slightly positive) because the reviewer presents a balanced view with both pros and cons. They describe the idea as 'interesting' and potentially useful in practice, highlighting several positive aspects. However, they also express a major criticism about the lack of extensive real-world results, which prevents a more positive score. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'sounds interesting' and 'I would have been much more convinced' rather than harsh or dismissive language. The critique is presented constructively, suggesting ways to improve the paper rather than outright rejecting it.""]"
"['The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \\= p(y).   Figure 1 is used to illustrate this issue. \n\nIf the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \\= p(y|x_T)  ), the optimal invariance can lead the same generalization problem.   Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability.  It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. \n\nIt is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. \n\nReference:\n[1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014.\n[2]Conditional Adversarial Domain Adaptation, NIPS 2018', 'This paper proposed to address domain generalization under inter-dependence of domains and classes. It motivates a new regularization term by analyzing an existing work, DAN. It shows that this term can improve the generalization performance when the classes and domains are not independent. Experiments are extensive and supportive. \n\nI do not have many comments about this paper. It was a joy to read. The proposed idea is well motivated. It is simple and seems like effective. Experiments are extensive. \n\nWhile the regularization term is motivated by analyzing DAN, it would be nice to discuss its application to other domain adaptation/generalization methods. What is even better is to show its effectiveness on another method in the experiments.', 'In this paper, the author(s) propose a method, invariant feature learning under optimal classifier constrains (IFLOC), which maintains accuracy while improving domain-invariance. Here is a list of suggestions that will help the author(s) to improve this paper.\n1.The paper explains the necessity and effectiveness of the method from the theoretical and experimental aspects, but the paper does not support the innovation point enough, and the explanation is too simple.\n2.In this paper, Figure3-(b) shows that the classification accuracy of IFLOC-abl method decreases a lot when γ is taken to 0. Figure3-(c) shows that the domain invariance of IFLOC-abl method becomes significantly worse when γ is 10. The author(s) should explain the reasons in detail.\n3. The lack of analysis on domain-class dependency of each dataset makes the analysis of experimental results weak.\n']","[-20, 90, -20]","[50, 80, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's proposed problem, they point out significant oversights and suggest major additions. The reviewer notes that 'a lot of works has been done' on related issues that weren't addressed, and requests discussion of differences and comparisons with missing references. They also suggest conducting additional analysis. These criticisms indicate the paper needs substantial revision, hence the negative score. However, the tone isn't harshly critical, so it's only mildly negative. The politeness score is moderately positive (50) because the reviewer uses polite, constructive language throughout. They use phrases like 'It is desirable to discuss' and 'It is also suggested', which are courteous ways of making recommendations. The reviewer also provides specific references to help the authors improve their work, which is a considerate gesture."", ""The sentiment score is 90 because the reviewer expresses very positive opinions about the paper. They state it was 'a joy to read', the idea is 'well motivated', and the experiments are 'extensive and supportive'. The only slight criticism is a suggestion for additional discussion and experimentation, which doesn't significantly detract from the overall positive sentiment. The politeness score is 80 because the reviewer uses respectful and encouraging language throughout. They praise the paper's qualities and frame their suggestion for improvement in a constructive manner ('it would be nice to'). The tone is professional and courteous, without being overly formal or effusive."", ""The sentiment score is slightly negative (-20) because while the reviewer provides constructive feedback, they point out several areas for improvement and suggest that the paper lacks sufficient support for its innovation and explanations. The overall tone suggests that significant revisions are needed. The politeness score is moderately positive (60) as the reviewer uses respectful language, framing their comments as 'suggestions' and avoiding harsh criticism. They maintain a professional tone throughout, using phrases like 'will help the author(s) to improve this paper'. The reviewer also acknowledges the paper's efforts in explaining the method's necessity and effectiveness before suggesting improvements.""]"
"['This paper proposes to jointly train a classifier with a domain and label-aware word embedding model and a variational Bayes model for sentiment domain adaptation. The model is evaluated on a standard multi-domain sentiment analysis dataset where it achieves convincing results against strong baselines. Extensive ablations are conducted.\n\nPros:\n- The paper is clearly written. I particularly appreciated Figure 1 as it might otherwise be difficult to see the relation of the different components of the model.\n- The model achieves convincing results and ablations and analyses are extensive.\n\nCons:\n- None of the presented ideas are entirely novel. The model rather combines many existing ideas successfully.\n- The framework consisting of many components (particularly the joint training with CBOW as indicated in the appendix) seems rather brittle and very task-specific. I am concerned if this framework will be able to work on other tasks. I would love to see an evaluation on another dataset. \n- The joint variational Bayes approach seems to be the most interesting aspect of the paper. Despite the ablations, it\'s not entirely clear to me, though, how useful this component is and if it can be applied beyond this particular model. I would like to see one of the baselines models or another model augmented with this component. \n\nQuestions:\n- Do you alternate updates between each of the components or use a more sophisticated multi-task learning strategy when training the word embedding model and the other components jointly? Did you try fine-tuning the trained word embeddings with the classifier?\n- Why do you use this particular affine transformation for learning sentiment-specific word embeddings? Did you try, for instance, an MLP as used in [1]?\n- You say that you use a classifier q_φ(z |D, c) in order to benefit from more freedom of design for Bayesian inference. Could you elaborate why you use this classifier in addition to the label classifier q_φ(y |D, c)? In Table 6, it seems that it doesn\'t help much. The use of the term ""classifier"" is confusing at times, as it seems to be both used to refer to the latent variable and the label classifier.\n\n[1] Tang, D., Wei, F., Yang, N., Zhou, M., Liu, T., & Qin, B. (2014). Learning Sentiment-Specific Word Embedding. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 1, 1555–1565.', 'The authors propose a semi-supervised learning techhnique involving jointly tuning word embeddings and a classifier. The idea is to rapidly adapt models to new domains with minimal (actually, zero) supervision by exploiting such embeddings.\n\nIn effect, this is an approach to ""disentangling"" sentiment from domain, via a variational objective and semi-supervision via the embedding parameters. This is a nice idea as disentanglement enables transfer (or should). The experiments are well-executed (albeit on a limited set of classification tasks). Still the comparisons are relatively exhaustive, and the authors have done a nice job of providing ablations. I found Table 4 and Figure 2 particularly nice. \n\n- The authors should speak to the generalizability of this approach beyond sentiment tasks, as presently it seems largely constrained to this domain. This would of course hinder the potential utility/impact of the work.\n\n- Due to the very concise description of baselines, it was not obvious to me if these were also taking a semi-supervised approach? I *think* the pivot-based model is. But then how was CNN+ELMo trained exactly? With zero target sentiment labels, I presume? If so, an obvious baseline would be to ""pseudo-label"" instances in the target domain first, and then use these predictions as a target to fine-tune the CNN (or whatever), back-propping through to the embeddings. Was this done? In general more details on the baselines and training setups should be included, even if only in the Appendix.\n\n- The strategy, I think, is to learn a prediction model p_\\phi(y|z, D) where \\phi are model parameters. Then this is applied to instances in the target dataset to infer latent z, and then use these inferred labels to fine-tune word embeddings. Specifically, sentiment label information is pushed into embeddings via a modified CBOW objective that effectively shifts the meaning (as codified in the distributed representation) via task specific sentiment. One thing here that confused me is that this would seem to perform this affine transformation to all words, but only certain of these will exhibit domain specific variation with respect to sentiment. Can the authors speak to this? \n\n- I found Eq 7 counterintuitive, since it treats sentiment and domain independently, but the authors explicitly noted above that this is not the case, i.e., words will depend jointly on sentiment + domain. I actually think p(w_i|y) also depends on the domain via M (implicitly), but perhaps this should be made explicit (or perhaps I am misunderstanding something!)\n\n- In 2.1, the authors assume a prior p(z|c) and then say that `\'naively\' marginalizing over this performs poorly. But I not think the particular prior distribution used was not discussed here. Could they elaborate? \n\n- I think i indices are needed on the w\'s in Eq 6?', ""This paper jointly trains a sentiment classifier with a sentiment and domain-aware embedding\nmodel, using both labeled and unlabeled data. When sentiment label is observed, this model is trained\nwith the usual cross entropy and maximum likelihood objectives; for unlabeled data, it uses pseudo\nlabels produced by the sentiment classifier with variational Bayes objective. This idea is not novel but the authors report that there is no previous work that jointly trains sentiment aware embeddings with a sentiment classifier specifically, and makes use of an unlabeled corpus to improve both. However, there are general and broader methods such as 'Toward Controlled Generation of Text' by Hu et al that apply semi-supervised techniques for generation (not classification) with specific constraints (sentiment, domain, etc). There are other recent methods such as 'Improving Language Understanding by Generative Pre-Training' by Redford et al that use the idea of generative pre-training with discriminative fine-tuning that are task-agnostic and achieve very good performance - how does the paper compare to this approach? \nThe experiments and analysis is very well written in the paper. Table 4 also shows very interesting, somewhat surprising results in the paper. The authors say that they will release the code and data for this technique which will be useful for the sentiment analysis research community.  \n""]","[50, 60, 50]","[80, 80, 75]","[""The sentiment score is 50 (slightly positive) because the review begins with a neutral summary of the paper, followed by a balanced list of pros and cons. The pros highlight the paper's clarity and convincing results, while the cons point out lack of novelty and some concerns about the model's applicability. The overall tone is more positive than negative, but not overwhelmingly so. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as constructive feedback or questions rather than harsh judgments. Phrases like 'I particularly appreciated' and 'I would love to see' contribute to the polite tone. The reviewer also asks thoughtful questions at the end, showing engagement with the work rather than dismissal."", ""The sentiment score is 60 (positive) because the reviewer expresses appreciation for the authors' work, describing it as a 'nice idea' and praising the well-executed experiments and exhaustive comparisons. They also compliment specific parts of the paper, such as Table 4 and Figure 2. However, the score is not higher because the reviewer also points out several areas for improvement and clarification. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism in a professional manner. They use phrases like 'The authors should speak to...' and 'Could they elaborate?' which are polite ways of requesting more information or suggesting improvements. The reviewer also balances critique with praise, acknowledging the strengths of the work alongside areas for improvement."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and well-written experiments, but also points out that the idea is not entirely novel and questions how it compares to other recent methods. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as questions or suggestions rather than direct attacks. The reviewer also mentions positive aspects like the well-written analysis and the authors' intention to release code and data, which contributes to the polite tone.""]"
"[""The paper proposes to use the technique in VAT to generate adversarial complementary examples in the (K+1)-class semi-supervised learning framework described by the Bad GAN paper. This leads to a formulation that combines the VAT loss and the (K+1)-class classification loss. The paper also provides analysis regarding why VAT is useful for semi-supervised learning.\n\nPros\n1. It is interesting to bridge two state-of-the-art semi-supervise learning methods in a meaningful.\n2. Some positive results have been presented in Table 1 and Figure 4.\n\nCons and questions\n1. I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here?\n2. Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy.\n3. If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method?\n4. The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak.\n5. There is not ablation study of different terms in the objective function.\n6. In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison."", 'This paper makes the interesting observation that the generative procedure proposed by Bad GAN paper can be replaced by a slightly modified VAT procedure. The reasoning is sound and leverages the intuition that adversarial examples (subject to a sufficiently small perturbation radius) are likely to be closer to a decision boundary than the original sample. \n\nThe paper is generally easy to follow but the presentation could be improved. In particular more could be done to describe the terms in Equation 5. I’m also curious about the behavior of L^true, which is equivalently the fourth term in Eq 1. Even when reading Bad GAN paper, I did not quite understand their claim that this can be correctly interpreted as a conditional entropy term (if they really wanted conditional entropy, they should probably have either done H(p(k|x)) or H(p(k|x, k <= K))). I agree with the authors that the roles of the second and fourth terms overlapped, and I think this is sufficiently interesting to warrant some further elaboration in the paper. I also liked the reminder that power iteration selects a non-unique sign for the first eigenvector (subject to the random vector initialization); I encourage the authors to do an ablation test to convince the reader that “this modification helps to improve convergence speed of the test accuracy.”\n\nThe propositions in this paper were, in my opinion, not particularly insightful. While I think it is nice that the authors went through the effort of providing some formalism to the intuition that VAT has a “push decision boundary away from high-density regions”, I’m less sure if propositions 1 and 2 really provides any additional insight the behavior of VAT. Proposition 1 is pretty weak in that it only covers a 2-class logistic regression; it seems obvious that the adversarial perturbation points in the direction toward the decision hyperplane. If the authors could extend this to more general non-linear classifiers (perhaps subject to some assumptions), that would be more interesting. I don’t think Proposition 2 has any real value and recommend its relegation to the appendix.\n\nI think the biggest weakness of this paper is the experiments. Taking Table 1 at face value, the conclusion that FAT is simply competitive with existing approaches suggests that the additional machinery isn’t particularly useful, providing little more than a vanilla VAT. I also think MNIST/SVHN has run its course as good semi-supervised learning benchmarks and would prefer to see such algorithms being scaled to more complex data. The main argument for why FAT should be prefered over VAT comes from Section 6.2. Figure 4 is more interesting, but is complicated by the fact that FAT checks both possible eigenvectors (+/- u) during training, which requires two forward passes in the classifier; did the authors give a similar treatment to VAT? Please show wall-clock time too. Unfortunately the computational efficiency gain seems to only hold true for MNIST/SVHN, but not for CIFAR. I worry that the observed gains will not sustain once we move to more complicated datasets.\n\n\nPros:\n+ Simple and clean proposal\n+ Easy to read\nCons:\n- Limited insight\n- Weak experiments', 'The authors propose to combine BadGAN framework and VAT to accelerate learning in the semi-supervised setting. The paper shows that the VAT approach is actively pushing the decision boundary away from the high-density regions. While the BadGAN approach pulls the decision boundary to low-density regions. This simultaneous push and pull lead to after convergence in testing accuracy. The authors also report competitive results on standard datasets used for SSL such as SVHN and CIFAR10.\n\nPositives:\nThe approach overcomes some of the difficulties with BadGAN which arise from training a GAN and density estimation network for generating “bad samples” useful for SSL. Instead of using a GAN, the proposed approach uses adversarial samples using VAT that are sufficiently confusing to the current estimate of the classifier. \nThe theoretical justifications for the VAT interpretation are interesting and convincing. The visualizations of the bad samples show qualitatively that the bad samples from the BadGAN and proposed approach differ. Several other visualization aids in understanding the behavior of the algorithm.\n\nNegatives:\nRequires additional hyperparmeter tuning in tau and rho. Tuning these with large validation sets can lead to an overoptimistic estimate of the generalization. How sensitive is the performance to these parameters? \nAs the authors point out, the method has limitations when the number of labeled samples is much smaller. It will be nice to see some results in this aspect.\nPlease include more details to clarify what is meant by ‘the role of the second term of (1) and (2) are overlapped’.']","[-20, -20, 60]","[60, 50, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting', 'positive results'), there are more cons than pros listed, and the overall tone suggests the paper has significant shortcomings. The reviewer points out several issues with the methodology, theoretical justification, and empirical results. The politeness score is moderately positive (60) because the reviewer uses neutral, professional language throughout. They frame criticisms as questions or suggestions rather than direct attacks, and use phrases like 'I don't understand' instead of more accusatory language. The reviewer also acknowledges the positive aspects of the paper before delving into criticisms, which is a polite approach in academic reviews."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting observation', 'sound reasoning', 'easy to follow'), they express more concerns and criticisms. They point out weaknesses in the propositions, experiments, and overall impact of the work. The reviewer suggests the paper's contributions are limited and not particularly insightful. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms constructively. They use phrases like 'I'm curious about', 'I encourage the authors', and 'I think' to soften critiques. The review maintains a professional tone without harsh or rude language, even when expressing concerns."", ""The sentiment score is 60 (positive) because the review begins with a neutral summary of the paper's approach, followed by a 'Positives' section that highlights several strengths of the work, including overcoming difficulties with BadGAN, providing interesting theoretical justifications, and useful visualizations. The 'Negatives' section is relatively mild, focusing on areas for improvement rather than severe criticisms. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging the authors' contributions and framing criticisms as suggestions or areas for clarification. The reviewer uses phrases like 'It will be nice to see' and 'Please include more details' which maintain a courteous tone. The review is constructive and balanced, offering both praise and areas for improvement without harsh language.""]"
"[""This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values.\n\nweakness:\n1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.\n\n2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.\n\n3. There are no titles in the figures and tables in this paper\n\n4. the authors don't confirm the superiority of the proposed method to others.\n\nminor comments\n1. what's the meaning of Target mAP in the table?\n2. It seems that Some figures are cropped from TensorBoard, with some extra shadows."", 'The authors experiment with building an SSD-like object detection network for text detection in documents, by replacing the usual VGG or ResNet base architecture with a light weight model inspired by the original digits classification CNN from [LeCun et al 1999].\n\nThis paper is a pure technical report with no novel contribution: all the authors do is replace the ""body"" network in the well-known SSD architecture with a simpler model (taken from existing literature) and evaluate it on two synthetic benchmarks of their creation.\nThe idea of employing object detection CNNs for OCR is not novel either, as pointed out in the related works section.\n\nBeside the absence of novelty, the paper also suffers from several other serious flaws:\n\n1) One of the main motivations provided by the authors for this work is that existing ""classification [...] detection [...] or segmentation networks, cannot be applied directly, even with finetuning"".\nHowever, no experimental results are reported to justify this claim.\nIn fact, in the experimental section the proposed network is not compared against any existing baseline.\n\n2) The text has serious clarity and formatting issues, in particular:\n- most tables and figures have no caption, and the few that have one are not numbered\n- the text exceed both the 8 pages limit and the extended 10 pages limit allowed in the case of big figures\n- the experimental section is very confusing, in particular the way the authors refer to the various network variants using long code names makes it really hard for the reader to follow the ablation studies\n- given the absence of proper captions and numbering, it is quite hard to understand which table refers to which experiment\n- most of the graphs seem to be in the form of low-resolution bitmaps, which are quite hard to read even on screen\n- many entries in the References section are either missing the venue, or point to an arXiv link even when a proper conference / journal reference would be available\n\n3) Some important details about the network are missing, in particular the authors do not mention how labels are assigned to the network outputs, and only give a vague indication about the losses being used.\nSimilarly, there\'s no mention about the use of NMS, which is also an important component of the two architectures (SSD and YOLO) that inspire this work.\nAssigning labels and performing NMS are actually some of the most crucial components in the training of object proposal / object detection networks, often requiring numerous meta-parameters to be properly configured and tuned, as testified by the meticulous descriptions given in previous works (e.g. YOLO and Fast / Faster / Mask r-CNN).\n\n4) The experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.\nA lot of space is devoted to presenting a detailed ablation study which, in my opinion, doesn\'t contribute much to the overall paper and actually reads more like a report on meta-parameter tuning.\nFinally, starting from Section 5.3.1 the text seems to be copy-pasted without a second read from some differently formatted document, as entire phrases or possibly tables / figures seems to be missing.\n\nIn conclusion, in my opinion this paper does not meet the conference\'s minimum quality standards and should definitely be rejected.', 'Unfortunately, the work does not introduce new contributions, with the point of the paper provided in the introduction:\nIn our experiments, we show that best performing approaches currently available for object detection\non natural images can be used with success at OCR tasks.\n\nThe work is applying established object detection algorithms to OCR. While the work provides a thorough experimental section exploring trade offs in network hyper-parameters, the application of object detection to the OCR domain does not provide enough novelty to warrant publication.\n', 'This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.\n\nMoreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. \n\nThe authors claim “This work is the first to apply modern object detection deep learning approaches to document data” but there are previously published works. For example:\n\nTuggener, Lukas, et al. ""DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects."" ICPR 2018.\nPacha, Alexander, et al. ""Handwritten music object detection: Open issues and baseline results."" DAS 2018.\n\nActually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. \n\nFinally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand.']","[-90, -90, -70, -80]","[-20, -20, 0, -20]","[""The sentiment score is -90 because the review is overwhelmingly negative. The reviewer states that the paper has 'no potential values' and 'doesn't give any new contributions'. They list several weaknesses and no strengths, indicating a very negative sentiment. The politeness score is -20 because while the language isn't overtly rude, it's quite blunt and dismissive. Phrases like 'the paper is written badly' and 'I even can't find any contribution' are quite harsh. The reviewer doesn't use any softening language or acknowledge any positive aspects, which would be more polite. However, they do use some neutral language in parts, preventing the score from being even lower."", ""The sentiment score is -90 because the review is overwhelmingly negative. The reviewer states that the paper has 'no novel contribution,' suffers from 'several other serious flaws,' and concludes that it 'does not meet the conference's minimum quality standards and should definitely be rejected.' The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. For example, phrases like 'pure technical report with no novel contribution' and 'suffers from several other serious flaws' are quite direct. The reviewer also uses strong negative language like 'definitely be rejected' without offering encouragement or positive feedback, which contributes to the slightly impolite tone."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the work 'does not introduce new contributions' and lacks 'enough novelty to warrant publication', which are strong criticisms in academic peer review. However, it's not entirely negative as the reviewer acknowledges a 'thorough experimental section'. The politeness score is 0 (neutral) because the language is neither particularly polite nor rude. The reviewer uses direct, matter-of-fact language without personal attacks or overly courteous phrasing. Phrases like 'Unfortunately' and 'does not provide enough novelty' deliver criticism in a professional, neutral tone typical of academic reviews."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper 'lacks any novelty/contribution', criticizes the purpose and clarity of the proposed pipeline, points out inaccuracies in the authors' claims, and describes the presentation as 'marginal'. There are no positive comments to offset these criticisms. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. Phrases like 'I fail to see' and 'Actually, in my opinion' come across as somewhat condescending. The reviewer also doesn't offer any constructive feedback or suggestions for improvement, which contributes to the overall negative tone.""]"
"['\\clarity & quality\nThe paper is easy to follow and self-contained. \nHowever, the motivation for minimizing the upper bound is not so clear for me. \nAs far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. \nIn the introduction, the author just mentioned that ""the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.""\nFor me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.\n\n\\originality & significance\nAlthough the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.\n\nHowever, it is hard to evaluate the usefulness of the proposed method from the current experiments.\nIt seems that there are two merits about the proposed method as above.\nThe only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.\nAbout the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.\nSo I cannot tell whether the proposed objective is really useful to learn the deep generative models.\nI think further experimental results are needed to validate the proposed method.\n\n\\Question\nIn page 4,  the variance of the p(y|x) and p_\\theta(y|z) are set to be the same. What is the intuition behind this trick? \nSince this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.', 'This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The second contribution might be the spread f-divergence for distributions having different supports. Even though theoretically sound, I believe that the presented experimental results are not strong enough to support the effectiveness of the proposed techniques. Detailed comments are listed below.\n\n1) Notations are confusing, especially in Section 3 when introducing the SPREAD f -DIVERGENCE.\n2) I cannot find on arXiv the reference “D. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.” So I am not sure whether you can take credit from the “spread f-divergence” or not.\n3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? (ii) how to estimate log p(y)? What is the influence?\n4) In the paragraph before Sec 4.2, how the sigma of the spread divergence is annealed?\n5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? The current experimental results barely show any advantage.\n\nMinors:\n1) Page 6, under Figure 3. The statements of “KL (moment matching)” and “reverse KL (mode seeking)” are not consistent with what’s stated in Sec 2.2 (the paragraph under Eq (3)). \n2) “RKL” and “JS” are not defined. Forward KL and standard KL are both used in the paper.\n\n\n', ""The paper proposes a method for training generative models with general f-divergences between the model and empirical distribution (the VAE and GAN objectives are captured as specific instantiations). The trick that leads to computational tractability involves utilizing a latent variable and optimizing the f-divergence between joint distributions which is an upper bound to the (desired) f-divergence between the marginal distributions. Distribution support issues are handled by convolving the data space with a blurring function. Empirical results on image datasets demonstrate that the additional training flexibility results in qualitatively different learned models. Specifically, optimizing the reverse KL (and/or Jensen-Shannon divergence) leads to sharper images, perhaps at the loss of some variance in the data distribution.\n\nI liked the simplicity of the core idea, and appreciated the exposition being generally easy to follow. The application of upper bounds to general f-divergences for training generative models is novel as far as I know. My two issues are with the practicality of the implementation and evaluation methodology, both potentially affecting the significance of the work. Regarding practicality, it appears the training details are specific to each experiment. This begs the question of how sensitive the results are to these settings. Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method. Moreover, the subjective nature of the results in the real dataset experiments makes it difficult to judge what the increased flexibility in training really provides - although I do note that the authors make this same point in the paper. Finally, given that model training is the paper's focus, an explicit discussion of computational cost was missed.\n\nEven with these issues, however, I believe the paper makes a contribution to the important problem of fitting expressive generative models. In my opinion, a more rigorous and thorough experimental exploration would increase the value, but the paper demonstrates that training with alternative f-divergences is feasible.""]","[-20, -30, 50]","[50, 50, 75]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('easy to follow', 'new and interesting'), they express several concerns and criticisms. They state that the motivation is not clear, the experimental evidence is weak, and further validation is needed. The overall tone suggests the paper needs significant improvements.\n\nThe politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'As far as I understood' and 'I think', which soften their criticisms. The reviewer also balances negative points with positive ones, acknowledging the paper's strengths before discussing its weaknesses. However, the score is not higher because the review is direct in its criticisms without excessive hedging or overly polite language."", ""The sentiment score is -30 because while the reviewer acknowledges the paper's theoretical soundness, they express significant concerns about the experimental results and missing analyses. The overall tone is more critical than positive, but not entirely negative. The politeness score is 50 because the reviewer uses professional and respectful language throughout, avoiding harsh criticism and using phrases like 'I believe' to soften their critique. They also provide specific, constructive feedback. However, the review is not overly polite or praising, maintaining a neutral professional tone."", ""The sentiment score is 50 (slightly positive) because the reviewer expresses appreciation for the paper's core idea and its contribution to the field, while also noting some issues. The reviewer states they 'liked the simplicity of the core idea' and believes the paper 'makes a contribution to the important problem of fitting expressive generative models.' However, they also raise concerns about practicality and evaluation methodology, which tempers the overall positive sentiment.\n\nThe politeness score is 75 (quite polite) because the reviewer uses respectful and constructive language throughout. They begin by acknowledging the paper's strengths and novel contributions. When discussing issues, they use phrases like 'it would have been nice to see' and 'in my opinion,' which soften the criticism. The reviewer also balances critiques with positive remarks, demonstrating a fair and courteous approach. The tone remains professional and supportive, even when suggesting improvements.""]"
"['This paper provide a modification on the classical LSTM structure. Specifically, it reformulate the forget gate with a monotonically decreasing manner, using sinusoidal function as the activation function. \n\nHowever, both the motivation and experimental results on such modification are not convincing enough. \n\n1. While there are many heuristic guesses in sec3, important supports of these guesses are missed. For example, Figure 2 is designed to provide supports for the claim that we need controlled forget gates.  However, all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*, which provides limited insight on what will happen in the real cases. While the reformulation in sec4 is based on the observations in Figure 2, it is important to plot the real cell propagation after the reformulation, and see whether the real observation meets the conceptual observations in Figure 2.\nBTW, Plots in Figure 2 only account for LSTMs\' propagation within 3 steps, but in real cases there are way more steps. \n\n2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed. But it isn\'t straightforward to get the relations between abrupt shrinkage and sudden growth on forget gates and the expressive power of the vanilla-LSTM. Also, it\'s hard to say the monotonic propagation is more interpretable because we don\'t know what\'s the meaning of such propagation on the behaviors of LSTMs in applications. \n\n3. The reformulation in sec 4, especially the formula for the forget-polar input p_k, looks heavily hand-crafted, without experimental supports but statements such as ""we ran numerous simulations"", which is not convincing enough. \n\n4. Experiments are applied on MNIST and Fashion-MNIST. While both datasets are not designed in nature for sequential models like LSTMs. There are better datasets and tasks for testing the proposed reformulation.   e.g. sentence classification, text generation, etc.  No explanation on the choice of datasets.  In addition, the difference between vanilla-LSTM and DecayNet-LSTM is small and it\'s hard to say it isn\'t marginal. Maybe larger-scale datasets are needed. \n\n5. Lacking of explanation on specific experimental settings. E.g. training all methods for *only one epoch*, which is very different from the standard practice.  \n\n6. More qualitative interpretations for real cell states in both vanilla LSTM  and DecayNet-LSTM are needed. Only conceptual demonstration is included in Figure 2. ', 'This paper analyses the internal dynamics of an LSTM, focusing on the cell state as being the most important component, and analyses what directly influences the contents of the cell state using difference equations. The authors note that at any timestep, the output cell state is the sum of (previous cell state * forget) and (input gate * input). The former can only shrink or maintain the cell value, which the authors label \'catch\' and the latter can increase the magnitude, labelled \'release\'.\n\nThe authors show that for a single neuron, with chosen values for the forget gate and inputs, consistent growth or consistent shrinking of the cell state can be observed. When the forget is large, say 0.9, the input gate can be anywhere in the range 0.5, 1.0] and still produce growth in the cell value. For forget gate = 0.25, an even larger range of input gate values all produce a shrinking cell value.\n\nDue to the forget gate seemingly having a stronger effect than the (input gate * input) component, the authors propose to hard wire the forget gate to produce a continuous and monotonic decrease, producing the DecayNet. The rate of this decay is controlled by a learned function of the input and previous hidden state, with some shifting in order to maintain a monotonic decrease. Each forget neuron will decrease at a different rate through the processing of a sequence, leading to sections of the cell state which will decay slowly and sections which will decay quickly.\n\nThe authors perform two sets of experiments. The second is sequence classification with the standard \'pixel by pixel\' permuted sequential MNIST, in which they show a new SOTA with using Recurrent Batch Norm combined with DecayNet. They also demonstrate a DecayNet with fewer parameters producing roughy the same median performance as a normal LSTM but with lower variance.\n\nThe first experiment is described as ""image classification"", with MNIST and Fashion-MNIST. This section is unclear to me, I had initally assumed that the data would be fed in one pixel at a time, but due to the presence of the other experiments I presume this is not the case. It is not clear what the \'time\' dimension is in how the RNNs are applied here, if not through some ordering of pixels. If the entire image is presented as a flattened input, and the time dimension is iterating through the dataset, then there is no reason to use an RNN here. More detail must be added here to make it clear exactly how these RNNs are being applied to images - the text says the softmax layer is produced from the final hidden state, but without the information about how the different hidden states are produced for a given training example this not meaningful. I can imagine that both tasks are pixel by pixel, and the only difference is whether to apply the permutation.. but that is my guesswork.\n\nIn general I find this paper an interesting idea, reasonably well communicated but some parts are not clear. All the experiments (as far as I can tell) work on fixed length sequences. One advantage of an LSTM is that can run onnline on arbitrary length data, for example when used in a RL Agent. In those circumstances, does learning a fixed monotonic delay on the forget gate make sense? I would guess not, and therefore I think the paper could be more explicit in indicating when a DecayNet is a good idea.\n\nThere are definitely tasks in which you want to have the forget gate drop to zero, to reset the state, and then go back up to 1 in subsequent timesteps to remember some new information. Presumably the monotonic delay would perform poorly.\n\nIs DecayNet appropriate only when you have fixed length sequences, where the distribution of \'when does relevant information appear in the input\' is fixed? These questions make me doubt the generality of this approach, whereas ""this reformulation increases LSTM modelling power ... and also yields more consistent results"" from the abstract reads like this is a strictly better LSTM. A much wider variety of experiments would be required to justify this sentence. \n\nIt would be interesting to see some diagrams of forget gate / cell state changes throughout a real task, ie a graph with `k` on the x axis. The presentation of the new forget gate in ""System 2"" is clear in terms of being able to implement it, but it\'s not intuitive to me what this actually looks like. The graphs I suggest might go a long way to providing intuition for readers.\n\n\n\nOverall while I like the spirit of trying to understand and manipulate LSTM learning dynamics I am recommending reject. I do not think the paper sufficiently motivates why a monotonic decay should be good, and while the new SOTA on permuted MNIST is great, I\'m concerned that the first experiments are not reproducable, as detailed previously in this review. All hyperparameters appear to be present, so this paper would be reproducable, except for the NIST experiments.\n\n\nGeneral recommendations for a future resubmission:\n\n* Clarify description of first MNIST experiments, and how they are different from permuted MNIST.\n* Experiments on a wider variety of canonical RNN tasks - Penn Treebank is an obvious contender.\n* Some mention of in what situations this is obviously not a good model to use (RL?)\n* More intuition / visualisations as to what the internal dynamics inside DecayNet look like, vs normal LSTM.\n* Devote less space to the initial dynamics analysis, or modify to be representative of a real task. This part was interesting on first read, but the only thing I think it really proves is \'when we artificially choose the input values things get bigger or smaller\'. The important thing is, what actually happens when training on a task that we care about - if the same catch and release dynamics are observable, then that makes the idea more compelling.\n\n\nNotes and suggestions:\nI feel the notation would be clearer if instead of k = 1 .. D, this index was t = 1 ... T. This makes it cleare that s_k is not the k\'th item in the array, but rather than whole activation array at a specific time. The notation \\sigma_t could then be replace with \\tanh.\n\n""We replace \\sigma_t with sin for an ergodic delay over time"": as this is a new gate for the forget gate, should this be \\sigma_s?\n\nOne DL rule of thumb heard relatively often is to simply initialise LSTM forget gate biases to 1, to ""remember more by default"". As this is a (much simpler) way of trying to influence the behaviour of the gate, and it anecdotally improves data efficiency, it is worth mentioning in the paper.', 'This paper performs an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate. Based on these outcomes they reformulate the functions (interpreted as differential equations) to add a decay-behaviour in the forget gates, finally called DecayNet-LSTM.\n\n\nThe theoretical analysis in this paper is very welcome and goes beyond observations which we made in the past, i.e., we often saw similar behavior in our experiments and as the authors also state in Section 5, there have been previous observations and approaches. In 2016, I have seen an idea called Random Activation Preservation (RAP) (https://ieeexplore.ieee.org/abstract/document/7487778 ) which just randomly ""resets"" the gates. However, they only show empirical outcomes, not a sophisticated analysis as in this paper.\n\nIn the experiments it is shown, that the DecayNet-LSTM performs similarly, or sometimes better than simple LSTM on standard tasks. On more difficult tasks, such as Perm-SeqMNIST, a state-of-the-art performance is achieved.\n\nMinor comments:\nPlease note, it should be Long Short-Term Memory (with hyphen between short and term)\nYou call the contribution DecayNet; And in the paper sometimes refer to it as DecayNet-LSTM; Maybe there could also be a DecayNet-GRU, ... If you, instead of ""reformulation"", would clearly write that DecayNet is an addition to the LSTM architecture, it might be more clear.']","[-60, -40, 80]","[20, 60, 70]","[""The sentiment score is -60 because the review is predominantly critical. While it acknowledges the paper's contribution in the first paragraph, the rest of the review lists several significant concerns and shortcomings, indicating a generally negative sentiment. The reviewer uses phrases like 'not convincing enough', 'limited insight', and 'heavily hand-crafted', which contribute to the negative score. However, it's not entirely negative, as the reviewer does offer constructive feedback and suggestions for improvement.\n\nThe politeness score is 20 because the reviewer maintains a professional and respectful tone throughout. They use neutral language and avoid personal attacks or harsh criticism. Phrases like 'However,' and 'It's hard to say' soften the critique. The reviewer also offers specific suggestions for improvement, which is a polite way to provide feedback. However, the score is not higher because the review lacks overtly polite language or praise, and the criticism, while professionally delivered, is quite direct."", ""The sentiment score is -40 because while the reviewer finds the paper interesting and acknowledges some positive aspects (e.g., 'interesting idea', 'reasonably well communicated', 'new SOTA on permuted MNIST is great'), they ultimately recommend rejection and express several significant concerns. The overall tone is more negative than positive, with phrases like 'I am recommending reject' and 'I do not think the paper sufficiently motivates why a monotonic decay should be good'. The politeness score is 60 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They acknowledge the paper's strengths and use phrases like 'I like the spirit of trying to understand and manipulate LSTM learning dynamics' while providing detailed feedback. The reviewer maintains a professional tone, even when pointing out issues, and offers specific recommendations for future resubmission, which is considerate and helpful."", ""The sentiment score is 80 (positive) because the reviewer expresses a very positive view of the paper, praising its theoretical analysis as 'very welcome' and noting that it 'goes beyond observations which we made in the past'. The reviewer also highlights the paper's achievements, including state-of-the-art performance on difficult tasks. The politeness score is 70 (polite) due to the reviewer's respectful and constructive tone throughout. They offer praise where due and provide minor suggestions for improvement without harsh criticism. The use of phrases like 'Please note' and the overall supportive language contributes to the polite tone. The reviewer also acknowledges the authors' work in the context of previous research, showing respect for their contribution to the field.""]"
"['This paper describes a two-stage encoder-decoder model for semantic parsing. The model first decodes a cross-domain schema (CDS) representation from the input utterance, then decodes the final logial form from both the utterance and CDS. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset, but is still behind the traditional slot-filling models (Goo et al., 2018).\n\nMy main concern is that it is unclear to me how CDS (cross-domain schema) can be generalized to the other semantic parsing datasets, e.g., the Overnight dataset (Wang et al., 2015), which also contains multiple domains. \n\nI think it would be nice to have some details about the CDS in the paper. For example, I’m wondering 1) how is this CDS designed? 2) how are the CDS annotations derived from the target output? \n\nThere are other details missing regarding the comparisons and the evaluation metrics. In 4.2, the authors mentioned “We use accuracy as the evaluation metric’’, does “accuracy” mean full logical form accuracy or accuracy on execution results?\n\n* More minor comments:\nIn the first paragraph of Section 3, “irrelevant to domain’’ -> “domain-general’’ or “domain-agnostic’’?\n\nIt will be nice to write something more specific than “explore more ways to make it work better” in the future work.\n\nThis paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).\n\n* Missing references:\nNeural semantic parsing over multiple knowledge-bases, Herzig and Berant, ACL 2017 <- This paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.\n\n(Concurrent) Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing, Herzig and Berant, EMNLP 2018 \n', 'This is a wonderful paper as it seems to have brougth Semantic Role Labelling (SRL) in the context of DL and in the context of voice search. \nResults are interesting but the paper has some major limitations. In fact, the paper totally disregard the work on Semantic Role Labelling and on languages for expressing the general meaning of language in terms of relations and in terms of concepts. \n\nThe first limitation is on the key idea. The key idea of the paper seems to be the existence of an intermediate representation language to encode meaning for utterances. Yet, this intermediate language seems to be the final language with the same relation types (for example, SearchAction) and without representations for the involved concepts (Type that becomes alternatively Film or Weather according to the target final language). This seems to be SRL where the first step is to recognize the relation and, then, the second step is to recognize the roles even if roles are slot filler types in the case of this papers.\n\nThe second limitation is on how the intermediate language has been choosen. What is the relation with FrameNet or VerbNet? Why the authors have not choosen something similar? What are the limitations of these two resources that have forced the authors to disregard them?\n\nMinor problems\n====\n- Why there are not spaces between characters and opening brackets?\n- ""compositional graph based .... language"" is a really large noun compound\n', 'Overall Score 3\n\nThis paper introduces “Cross domain schemas” (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the user’s high-level intent. Also introduced is a model which leverages CDS to improve semantic parsing of utterances to a meaning representation language (MRL). This model first parses an utterance to CDS, then uses an encoding of the CDS jointly with the utterance encoding to decode a meaning representation. By treating different intents as separate domains, the authors construct a multi-task learning setup for CDS and MRL parsing. Results are provided for the Snips dataset of virtual assistant queries. \n\nUnfortunately, this paper fails to sufficiently explain its main proposal, the CDS. The stated goal is to explicitly define the cross-domain features that would otherwise be implicitly learned by the parameters of a neural network, yet no explicit definition is given. No rough quantification of how many or what percent of features appear across domains is provided. Rather, significant time and space is given to describing a fairly unsophisticated two-decoder model for inserting the mysterious CDS representation into the final decoding task. \n\nThe paper ignores standard semantic parsing datasets (GeoQuery and ATIS) due to their size and scope. However, comparable models (Goo et al. 2018) are trained and tested on ATIS. Moreover, an evaluation on a small, unseen target domain would be the perfect justification for the kind of cross-domain learning proposed here. \n\nInstead, this paper opts only to evaluate on the recent Snips dataset. This dataset seems to be best suited to evaluating intent classification and slot filling (intent-slot), but the current work fails to improve over what Goo et al. 2018 report on this data. In the current work, the Snips dataset is used to evaluate MRL parsing, where the CDS model shows improvements over other seq2seq models. However, since MRL can be parsed from intent-slot format by predefined rules, it is uncertain whether the CDS model outperforms the Goo et al. model at even the task of MRL parsing (no such comparison is provided). \n\nOverall, the paper suffers from some clarity issues especially regarding the definition and value of CDS. The model provided may be slightly original but is quite similar to the model of Dong and Lapata 2018. The significance of this work is questionable due to the poor comparison with recently released baseline models for the more common intent-slot task. \n\nPros\n\nIntroduces “Cross Domain Schemas” (CDS) for semantic parsing, which help improve robustness of semantic parsers by allowing models to learn patterns in one domain for use in another. \n\nThrough the use of CDS, train semantic parsers in a multi-task learning setup\n\nCons\n\nCDS is not described in sufficient detail. In particular, the possible actions and attributes are not defined. \n\nThe model is described as “multi-task learning”, however all tasks are parsing requests made of a virtual assistant. \n\nResults on standard data for semantic parsing such as GEO or ATIS are not reported.\n\nThe model does not appear to improve the results on the Snips dataset compared to the paper that introduces this dataset. Thus, the value of CDS is difficult to judge. \n\nNo per-domain analysis of the impact of CDS is provided.']","[-20, -20, -60]","[50, 50, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper, they express several concerns and point out missing details and errors. The overall tone suggests the paper needs significant improvements. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, offering constructive criticism and suggestions rather than harsh criticism. They use phrases like 'I think it would be nice' and 'It will be nice to', which contribute to a respectful tone. The reviewer also provides specific, helpful feedback and additional references, which is considerate and helpful to the authors."", ""The sentiment score is slightly negative (-20) because while the reviewer starts by calling it a 'wonderful paper' with 'interesting' results, they quickly point out 'major limitations' and spend most of the review detailing these issues. The overall tone suggests more criticism than praise. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, even when pointing out flaws. They use phrases like 'seems to be' and 'Why have the authors not chosen' instead of more direct criticisms. The reviewer also balances negative feedback with positive comments, which contributes to the polite tone. However, the politeness doesn't reach the highest levels as the criticism, while respectfully phrased, is still quite direct and extensive."", ""The sentiment score is -60 because the review is predominantly negative. The reviewer states the paper 'fails to sufficiently explain its main proposal', 'ignores standard semantic parsing datasets', and 'suffers from some clarity issues'. They question the significance of the work and highlight several cons. However, it's not entirely negative as they do mention some pros, hence not the lowest possible score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'Unfortunately' and 'Overall' to soften criticisms, and provide both pros and cons. The language is not overtly polite, but it avoids rudeness, striking a neutral to slightly polite tone typical of academic reviews.""]"
"['This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. \n\nThe hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs.\n\nIn a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize ""shape"" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images).\n\nAdditional analysis involving visualization methods to further explain why shape features were ignored would have been a plus– with bonus points for providing a heuristic to determine the ""shapelessness"" of a convolution kernel.', ""The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision – they don’t encode shape-bias like human vision. Towards this goal, the authors modified the training data with ‘shortcut’ features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task.\n\nInvestigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. \n\nThe paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting.\n\nNonetheless, there are significant drawbacks in the presented work:\n\n1.\tThe experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Let’s make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.)\n\nThe best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. \n\n2.\tDue to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so:\n\na.\tWe tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on ‘nopix’ case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed.\n\nDetails: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: https://file.io/qiziAK. The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-à-vis the results in the paper. The ‘same’ setting didn’t achieve 100% either though it got close - achieving 98.4%. \n\nSummary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality.\n\nI encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including ‘shape’-features and submit to a future conference.\n\nRevision: Updated my rating to acknowledge that the reproducibility issue is addressed."", ""Humans leverage shape information to recognize objects. Shape prior information helps human object recognition ability to generalize well to different scenarios. This paper aims to highlight the fact that CNNs will not necessarily learn to recognize objects based on their shape. Authors modified training images by changing a value of a pixel where its location is correlated with object category or by adding noise-like (additive or Salt-and-pepper) masks to training images. Parameters of such noise-like masks are correlated with object category. In other words if one learns noise parameters or location of altered pixel for each object category, they can categorize all images in the training set. This paper shows that CNNs will overfeat to these noise based features and fail to correctly classify images at test time when these noise based features are changed or not added to the test images.  \n\nDataset bias is a very important factor in designing a dataset (Torralba et al,. 2011). Consider the case where we have a dataset of birds and cats. The task is image classification. All birds' images have the same background which is different than cats' background. As a result the network that is trained on these images will learn to categorize training images based on their background. Because extracting object based features such as shape of a bird and bird's texture is more difficult than extracting background features which is the same for all training images. \n\nAuthors have carefully designed a set of experiments which shows CNNs will overfeat to non-shape features that they added to training images. However, this outcome is not surprising. Similar to dataset design example, if you add a noise pattern correlated with object categories to training images, you are adding a significant bias to your dataset. As a result networks that are trained on this dataset will overfeat to these noise patterns. Because it is easier to extract these noise parameters  than to extract object based features which are different for each image due to different viewpoints or illumination and so on. \n\nThis paper would have been a stronger paper if authors had suggested mechanisms or solutions which could have reduced dataset bias or geared CNNs towards extracting shape like features.  \n\nAntonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '11).""]","[20, -50, -20]","[60, 50, 50]","[""The sentiment score is slightly positive (20) because while the reviewer acknowledges the value of the work and its clever experiments, they also point out that it's not groundbreaking and offers a null result. The positive aspects (adding to literature, clever experiments) are balanced against the limitations (lack of novelty, absence of proposed solutions). The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's contributions while offering constructive criticism. They use phrases like 'cleverly answer these questions' and 'there is value in the work presented', which maintain a polite tone even when discussing limitations. The reviewer also offers suggestions for improvement without being demanding or harsh."", ""The sentiment score is -50 because while the reviewer acknowledges the paper's relevance and well-designed experiments, they point out significant drawbacks and state that 'the submission is not of publishable quality.' The review starts positively but becomes increasingly critical, ending with a recommendation to revise and resubmit. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They offer constructive feedback and encourage the authors to improve their work for future submission, which is a polite approach to negative feedback. The reviewer also uses phrases like 'I encourage the authors' and 'please let me know,' which contribute to a polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the careful design of experiments and the importance of the topic, they also express that the outcome is not surprising and suggest that the paper would have been stronger with additional solutions. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the authors' efforts, and provides constructive criticism without harsh wording. The reviewer maintains a professional tone, offering both positive comments and areas for improvement in a balanced manner.""]"
"['This paper presents a method to train NNs as black box estimators of the commitor function for a physical, statistical mechanical, distribution. This training is performed using samples from the distribution. As the committor function is used to understand transitions between modes of the distribution, it is important that the training samples include points between modes, which are often extremely low probability. To address this concern, this paper draws MCMC samples at a high temperature, and then uses importance weights when training the committor function using these samples. Overall -- this seemed like a good application paper. It applies largely off-the-shelf machine learning techniques to a problem in physics. I don\'t have enough background to judge the quality of the experimental results.\n\nI had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in high-d. The variance of importance sampling estimates typically increases exponentially in the dimensionality of the problem, so I suspect this technique as presented would fall apart quickly if pushed to higher dimensions. All experiments are on problems with either 9 or 10 (effective) degrees of freedom, which from the ML perspective at least is quite low dimensional, and which is consistent with this exponentially poor scaling. There are likely fixes to this problem -- e.g. the authors might want to look into annealed importance sampling*.\n\nmore specific comments:\n\n""and dislocation dynamics"" -> ""dislocation dynamics""\n\n""One can easily check"" -> ""One can check"" :P\n\neq 5 -- this is very sudden deep water! Especially for an ML audience. You should either give more context for the Kolmogorov backward equation, or just drop it. (The Kolmogorov formulation of the problem is not used later, and for an ML audience describing the task in terms of it will confuse rather than clarify.)\nwhat is \\Delta q? Does that indicate the Laplacian? Not standard ML notation -- define.\n\nsimilarly, define what is intended by \\partial A and \\partial B (boundary of the respective regions?)\n\neq. 9 -- nit -- recommend using a symbol other than rho for regularization coefficient. visually resembles p, and is rarely used this way. lambda is very common.\n\neqs 10/11 -- include some text motivation for why the definition of chi explicitly excludes the regions inside A and B.\n\neq 14: cleverly formulated!\n\neq 14 / eq 20:\nfactor of 1000 is very fast! corresponds to an epsilon of O(1e-3). You need to make sure that training samples are generated in the epsilon width border around A and B, otherwise the effect of chi will be invisible when training q_theta. So it seems like epsilon should be chosen significantly larger than this. Might want to include some discussion of how to choose epsilon.\n\n* Totally incidental to the current context, but fascinatingly, annealed importance sampling turns out to be equivalent to the Jarzynski equality in nonequilibrium physics.', 'The paper proposes to apply neural network to compute the committor function arose from physics, which looks an interesting application problem by employing machine learning algorithms. Typically, I know very well that the BG distribution usually has multi-modes which makes the sampling difficult extremely. The authors then employ the importance sampling for possibly explore the whole variable space. It seems to me the only possible contribution is parameterize the committor function by using a neural network.\nThe committor function is parametrized by using a neural network. My first concern is the training data. How would you collect the training data? It is well-known that a neural network works best when there are plenty of training data. Presumably when you are collecting the data, you are basically calculate the committor function and so that you may be able to directly solve the variational problem. \nImportance sampling: I would not consider the importance sampling as a big deal concerning the contribution of the paper. You could employ a series of importance distributions which could result in many more samples. Have you also looked at the Uniform distribution?\nThe paper targets high-dimensional problem. However, in the experiments, the problems do not look really like high-dimensional problems.\nSome notations need to be clarified, for example, the Nabla, the Delta, and as well as the dot operator.\n', 'In response to the authors\' rebuttal, I have increased my ratings accordingly. I strongly encourage the authors to include those ablative study results in the work. I also strongly recommend an ablative study on importance sampling so as to provide more quantitative results, in addition to Fig. 4. Finally, I hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.\n\n=================================\nThis paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space. The basic idea is using a variational formulation with Dirichlet-like boundary conditions to learn a neural committor function. The authors claim to improve a previous neural network based method by i) using a clever parameterization of the neural committor function so that it approximately satisfy the boundary condition; ii) bypassing the difficulty of rare events using importance sampling; and iii) using collective variables as feature engineering.\n\nGenerally I feel this paper is well written and easy to understand, without requiring too much background in physics and chemistry. The application is new to most people in the machine learning community. However, \nthe main contributions of this paper are empirical, and I found the experiments not very convincing. Here are my main concerns:\n\n1. There is almost no ablation study. The parameterization of committor function satisfies the Dirichlet boundary condition, which is aesthetically pleasing. However, it\'s unclear how much this improves the regularization used in the previous method. Similarly, without importance sampling, will the results actually become worse? What changes if the collective variables are removed? There is even no comparison with the previous neural network based method on computing committor functions, though the authors cited it.\n\n2. In the experiment on extended Mueller potentials, authors use the FEM results as the ground truth. However, it is not clear how accurate those FEM solutions are. Without this being clarified, it is unclear to me that the RMSE and MAE results in Table 1 are meaningful. Maybe try some simpler problem where the committor functions can be computed exactly?\n\n3. In experiments the authors often argue that results will improve when networks become deeper. However, all network architectures used in the paper are narrow and shallow when viewed from the perspective of modern deep learning. If the authors want to stress this point, I would expect to see more experimental results on neural network architectures, where you vary the depth of the network and report the change of results.\n\n4. ""Then we use the result as the initial network to compute the committor function at T = 300K"" => Did you first train a neural committor on samples of T = 800K and use its weights as initialization to the neural committor for T = 300K? Please clarify this more.\n\n5. Finally, I think the importance sampling technique proposed in this paper can be improved by other methods, such as annealed importance sampling. The largest dimension tested in this paper is only 66, which is still fairly small in machine learning, and I don\'t expect the vanilla importance sampling can work in higher dimensions.', 'This paper looks at the problem of computing committor functions, which is defined as the probability a state first visits a local minimum of the energy landscape in Langevin dynamics. The authors motivates the problem well by explaining why this is difficult to compute. Khoo et al, 2018 uses a deep network to variationally approximate this function. The major contribution of this paper is several improvements to the techniques of Khoo et al. I will comment on each of the improvements in turn. \n\nIn section 3.1 the authors proposes that instead of using optimization to satisfy boundary condition, it might be better to parameterize the function to satisfy the boundary condition. This contribution seems incremental. Eq.(9) guarantees satisfaction of the boundary conditions when lambda is large enough, so I imagine that lambda is not very difficult to pick. Therefore, the practical reason to use the more sophisticated parameterization is unclear. The new proposal removes a hyper-parameter lambda, but introduces new hyper-parameters epsilon and the exact smoothing for the two functions X_A, X_B.\n\nThe contribution in 3.2 seems interesting. The authors replace the original sampling function, which has high variance, with importance sampling. It seems that importance sampling is very well suited for this problem, and the authors found a very natural and reasonable proposal distribution. This method is generally interesting for estimating the expectation of any random variable with respect to a Boltzmann distribution.\n\nIn section 3.3 the authors proposes to work on a feature space. This is interesting to audience interested in the specific applications. But for machine learning this is a standard procedure, so has limited methodology novelty. \n\nOne issue of this paper is limited audience in ICLR. It seems much more appropriate to submit to statistical physics, material science or other relevant communities. I am not capable to judge the significance of this paper to those communities. As an application paper, the proposed methods are somewhat incremental; I only find section 3.2 interesting to a broader audience. \n\nWriting:\nI like the writing. Everything symbol is defined before use, and the notation is clean and unambiguous. I can easily follow the author’s arguments down to the minor details. \n\nA minor improvement to section 2 is to first explain the shortcoming of Eq.(5), then introduce Eq.(6)(7). \n\nEq.(11) can be better explained. The definition doesn’t look like a smooth function, and it takes some time to figure out what the authors mean here. \n\nMinor comments:\nA, B \\in \\Omega should be A, B \\subset \\Omega in Section 2\n']","[50, -20, 20, 20]","[70, 50, 60, 80]","[""The sentiment score is 50 (slightly positive) because the reviewer starts by describing the paper as 'a good application paper' and praises its clever formulation. However, they also express a 'major concern' about the method's scalability to higher dimensions, which tempers the overall positivity. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offers constructive criticism, and even provides helpful suggestions (e.g., looking into annealed importance sampling). The use of phrases like 'I don't have enough background to judge...' shows humility. The reviewer also softens criticism with phrases like 'nit' and uses smiley faces ':P', indicating a friendly tone. However, it's not perfectly polite due to some direct criticisms and the use of phrases like 'very sudden deep water!'"", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'interesting', they express several concerns and criticisms. They question the data collection method, the significance of the importance sampling, and the dimensionality of the experiments. These criticisms outweigh the initial positive remarks. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, such as 'It seems to me' and 'My first concern is', which softens the criticisms. They also use phrases like 'I know very well' to acknowledge the authors' expertise. The reviewer maintains a professional tone without using harsh or dismissive language, even when pointing out issues."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges improvements in their ratings based on the authors' rebuttal and encourages further enhancements. However, they also express several concerns about the paper's experiments and methodology. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, employing phrases like 'I strongly encourage' and 'I hope the authors can consider,' while also providing constructive criticism. The reviewer maintains a professional tone, balancing positive feedback with areas for improvement, without using harsh or rude language."", ""The sentiment score is slightly positive (20) because while the reviewer acknowledges some interesting contributions, particularly in section 3.2, they also note that some aspects are incremental and question the paper's suitability for ICLR. The overall tone is constructive but with reservations. The politeness score is high (80) as the reviewer uses respectful language throughout, offers specific praise (e.g., 'I like the writing'), and frames criticisms constructively. They provide detailed feedback and suggestions for improvement, which is considerate and helpful to the authors.""]"
"['The paper is extremely difficult to read. There are too many concepts introduced at once, casual comments mixed with semi-formal statements. The theorems sound interesting, the implications are grand and of interest to ICLR, but the proofs are impossible to follow. As such, I am not in a position to make a recommendation. \n\nI strongly recommend the authors to split the paper into multiple parts with clear-cut statements in each, with clear and detailed proofs, and submit to appropriate journals / conferences. \n', 'The paper provides a new framework ""S-System"" as a generalization of hierarchal models including neural networks. The paper shows an alternative way to derive the activation functions commonly used in practice in a principled way. It further shows that the landscape of the optimization problem of neural networks has nice properties in the setting where the number of input/hidden units tending to infinity and the neurons satisfy certain diversity conditions.\n\nOverall, the paper presents super interesting ideas that can potentially lead to a deeper understanding of the fundamentals of deep learning. However, for a general reader it is a hard-to-follow paper. Without a full understanding of the various domains this paper presents ideas from, it is hard to verify and fully understand the claims. I believe the paper would be better appreciated by an audience of a mathematical journal. As an alternative, I would encourage the readers to split the paper and possibly simplify the content by using a running example (more concrete than the one of MLP used) to explain the implications as well as assumptions.\n\nA clearer, more accessible presentation is necessary so that a non-expert can understand the paper\'s results. Thus, I vote to reject. \n']","[-60, -30]","[20, 50]","[""The sentiment score is -60 because the reviewer expresses significant difficulty in reading and understanding the paper, stating it's 'extremely difficult to read' and the proofs are 'impossible to follow'. However, they do mention that the theorems sound interesting and have implications of interest to ICLR, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they use relatively polite language. They 'strongly recommend' rather than demand changes, and they acknowledge the potential value of the work. The reviewer also explains their inability to make a recommendation, showing consideration for the authors. The language is professional and constructive, albeit direct about the paper's shortcomings."", ""The sentiment score is -30 because while the reviewer acknowledges the paper's interesting ideas and potential for deeper understanding, they ultimately recommend rejection due to the paper's complexity and lack of accessibility. The positive aspects (interesting ideas, potential impact) are outweighed by the criticisms (hard to follow, need for clearer presentation). The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They offer constructive suggestions for improvement, such as splitting the paper or using a running example, which demonstrates a polite and helpful approach. The tone is professional and considerate, even when recommending rejection.""]"
"['This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by  angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca.\n\nThis paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference).\n\nIn addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.', 'Paper summary: The authors propose layer rotation speed as a measure of layer-wise training speed and introduce the Layca optimizer as a means of enforcing uniform layer rotation speed throughout the network. They show empirically that layer rotation speed is linked to the generalization performance of deep neural networks and that weight decay induces uniform layer rotation speeds.\n\n\nDetailed comments:\n\nOverall, I felt that the paper introduced some interesting ideas but I was not left convinced that layer rotation speed is the correct measure of layer training speed. I hope that the authors can clarify this based on my questions and comments below.\n\n1) In the introduction you refer to input and feedback signals to a layer, I assume this refers to the forward and backward pass. As I understand it, this intuition and the findings of Figure 1 do not immediately relate to the notion of layer rotation speed during training. Could you clarify what you mean by ""the input and feedback signals that a layer receives could also influence the generalization ability induced by the layer\'s training"", to me this statement seems obvious as input+feedback signals contains training entirely.\n\n2) In Figure 1 you show that when training one chosen layer and keeping the others fixed, if the chosen layer is deeper into the network the test accuracy is worse. I wonder to what extent this might be remedied by initialization. For example, one might expect that when sampling random square matrices there are some very small eigenvalues which ""kill"" information in the forward pass. If we train a layer deep in the network it may have access to less information from the data than one earlier on. Whereas training an earlier layer could allow this layer to shift mass into the parts of the eigenspace which are well represented (so-to-speak) in the future layers. Have you thought about this at all? One simple way to evaluate this would be to initialize the weights to be random orthogonal matrices, ensuring that the eigenvalues are equal. With that said, I thought that this was an interesting experiment with a fairly surprising outcome!\n\n3) In related work you discuss the vanishing and exploding gradients problems in terms of layer-level training speed. I think that another relevant research direction may be dynamical isometry [1] which solves this problem by restricting all singular values of the Jacobian matrix to be close to 1. These ideas may also be relevant when discussing Layca and layer-rotation.\n\n4) I found section 3.1. a little unconvincing. It is not obvious to me that layer rotation speed is necessarily a good measure of training speed. In fact, there are many updates which have large cosine distance (as you define it) but do not change the network function (for example, permuting the weight matrices in fully-connected networks). Why is the rotation defined through a vectorization of the weight matrix as opposed to e.g. the polar decomposition? Is this a computational issue? Similarly, in section 3.2 you liken Layca to optimization on a manifold but I am not convinced that this makes sense for matrices which inherently have some structure (e.g, perhaps the Stiefel manifold would be more meaningful).\n\n6) Figure 2 shows that uniform rotation leads to improved test accuracy. But could it be the case that controlling the effective learning rate is sufficient (and layer rotation is one way to achieve this)? For example, we might take the sign of the update and use this to ensure that each weight matrix has the same effective learning rate (something like [2]). Do you expect this would have a similar effect? If not, what is unique about layer rotation that provides good test accuracy?\n\n7) You claim that SGD and adaptive methods with weight decay works without taking extra care to control the layer-rotation rate, as weight decay provides a similar effect. Firstly, you use weight decay and L2 regularization interchangeable, could you be explicit about exactly which you mean (see e.g. [3]). Assuming you mean weight decay (and not L2 regularization), then this could also be due to the effective learning rate ([4,5,6]) which may have some interaction with layer rotation rate (i.e. Figure 4). In summary, I would have liked to see an explanation for why weight decay leads to uniform rotation speeds.\n\n8) If I understand correctly, Figure 5 shows 5 tasks and reportedly 5 optimization schemes - each on a different task? It seems more reasonable to compare these on the same task.\n\nOverall I felt that the paper had some interesting contributions and a fairly comprehensive empirical study. However, I do not feel that the paper gives adequate attention to the notion of effective learning rate induced by weight decay and I was not totally convinced that the way layer rotation speed is defined is the correct way.\n\nMinor comments:\n\n- A lot of white space and a large caption for Figure 1.\n- Section 4 opens with ""monitor and control"", but I think the latter is really presented in section 4 and not section 3.\n- I think a diagram of the projection step of the Layca algorithm would be informative (for 2D weight vector).\n- Why does `5` appear in equation 1? Is this an arbitrary choice?\n- Some of the lighter colors in e.g. Figure 2(b) made some lines hard to read when printed. I do not believe that this affected the image significantly.\n\nClarity: The paper is well written and is easy to understand. Some of the figures in the experiments are a little cluttered and the lighter colors can be hard to see (e.g. Fig 2(b)), but this is minor.\n\nSignificance: The paper presents an interesting view point but I am not convinced that it offers as strong an explanation for these phenomena as other approaches. I believe with some more clarification the results could become more significant. My review score hinges mostly on the interaction between layer rotation speed and the effective layer-wise learning rate.\n\nOriginality: To my knowledge, the ideas are presented in the paper are original. In particular, this is a novel way to characterize layer-wise training speed.\n\n\nReferences:\n\n[1] Pennington et al. ""Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"" https://arxiv.org/abs/1711.04735\n[2] Bernstein et al. ""signSGD: Compressed Optimisation for Non-Convex Problems"" https://arxiv.org/abs/1802.04434\n[3] Loshchilov et al. ""Fixing Weight Decay Regularization in Adam"", https://arxiv.org/pdf/1711.05101.pdf\n[4] Laarhoven, ""L2 Regularization versus Batch and Weight Normalization"" https://arxiv.org/abs/1706.05350\n[5] Hoffer et al. ""Norm matters: efficient and accurate normalization schemes in deep networks"" https://arxiv.org/abs/1803.01814\n[6] Anonymous, ""Three Mechanisms of Weight Decay Regularization"" https://openreview.net/forum?id=B1lz-3Rct7   (Another ICLR 2019 submission)', 'Pros:\nOverall, this is a nice empirical paper with a reasonably extensive set of experiments. It is interesting that, among networks that train to ~100% with Layca, the best generalizing ones tend to have balanced training between layers (Fig. 2), and that tuned SGD does not generalize as well as Layca (Fig. 4). I think this paper’s focus on discrepancies in training & generalization originating from layers of a deep network is an interesting and important topic of study that warrants further empirical and theoretical investigation from the community. I think the work already has some interesting results and will encourage further investigation.\n\nCons:\n--Would appreciate greater discussion of the originality of the results; in particular, a more upfront discussion (which is currently concisely presented in the supplementary) regarding algorithms that are similar to Layca when less crucial steps are dropped, e.g. Yu et al 2017 and Ginsburg et al 2018.\n--After reading the paper, I don’t feel especially convinced that rotation (of the flattened weight matrix) is the best quantity to analyze training dynamics of a single layer. Could there be greater discussion & motivation for this, and in particular, relationship to work where weights are parameterized using orthogonal matrices, or even orthogonal initialization?\n\nSome minor comments:\n--Would have appreciated a discussion of the learning rate schedule (as well as other experimental details, e.g. loss function used and what role this plays) and whether networks with lower learning rates would need to be trained longer.\n--Greater discussion of why the first and last layer(s) do not experience the same rotation rate as other layers and if there would be better generalization if they did.\n']","[-20, -20, 60]","[50, 60, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's 'impressive theme' and insights, they also point out significant limitations. The reviewer states that 'the contribution of this paper is limited' and that it 'lacks the discussion of how much the layer-level training speed is important.' They also criticize the figures as 'quite hard to read.' However, the criticism is balanced with positive comments, preventing a more negative score. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, even when offering criticism. They use phrases like 'It is better to' and 'I strongly recommend' rather than harsh or demanding language. The reviewer also acknowledges the paper's strengths before discussing its weaknesses, which is a polite approach to criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting ideas and contributions, they express significant doubts and criticisms about key aspects of the paper. The reviewer states they are 'not left convinced' about the main measure proposed, has concerns about the methodology, and feels some explanations are inadequate. However, they do note positive aspects like 'interesting contributions' and 'comprehensive empirical study', preventing a more negative score. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions, and includes positive comments alongside critiques. They use polite phrases like 'I hope that the authors can clarify' and 'I would have liked to see', maintaining a constructive tone even when expressing doubts."", ""The sentiment score is 60 (positive) because the reviewer starts with praising the paper as 'nice' and 'interesting', highlighting its strengths and potential impact. They mention that the paper has 'some interesting results' and will encourage further investigation. However, it's not extremely positive as they also provide some criticisms and suggestions for improvement. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, even when providing criticisms. They use phrases like 'Would appreciate' and 'Would have appreciated' when suggesting improvements, which is a polite way of giving feedback. The reviewer also balances positive comments with constructive criticism, maintaining a professional and courteous tone throughout the review.""]"
"['This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough. The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection. On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.\nThe paper is written in a way that makes following it a bit difficult, for example, the experimental setups. Also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.). The contributions of the method could also be underlined more clearly in the abstract and introduction. The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments. \nThe idea of selective sampling for self-training is promising and the investigated questions are interesting. As far as I understand, the main contribution of this paper is the use of separate ""selection network"" to estimate the confidence of predictions by ""classification network"". However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information. For example, imagine the case of binary classification. If the selection network predicts 0 in come cases, it can be used to improve the result of ""classification network"" by flipping the corresponding label. How can you interpret such a thought experiment? One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds. Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of ""classification network"" is greater than some threshold? Finally, could you show a plot of top-1 prediction of ""classification network"" vs score of ""selection network"" and elaborate on that?\nThen, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes. Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled. Ablation study shows that the use of the ""selection network"" strategy does not improve the results without these heuristics. It would be interesting to see how these heuristics would do without ""selection network"", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG. In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.\nIt is very valuable that the experimental results include many recently proposed methods. Besides, the settings are described in details that could help for the reproducibility of the results. However, I have a few concerns about the results. First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3). Besides, as the base classifier is different for various baselines, it is hard to compare the methods. Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2). How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice? Another important parameters is the number of iterations of the algorithm. How was it chosen? Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes? What would happen if you use random class splits or split animal classes (like in a more realistic scenario)? \nTo conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.\n\nSome questions and comments:\n- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\n- In the training procedure of ""selection network"" of Sections 3.1, do you use the same datapoints to train a ""classification network"" and ""selection network""? If it is the case, how do you insure that the ""classification network"" does not learn to fit the data perfectly and thus all labels s_i are 1?\n- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?\n- What do you mean in section 3.3 by ""if one class dominates the dataset, the model tends to overfit""?\n- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.\n- Figure 3: wouldn\'t the plot of accuracy vs amount of data be more suitable here?\n- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\n- Can you explain the sentence ""To prevent data being added suddenly, no data was added until 5 iterations""?\n- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?', 'This is an novel, interesting paper on an important topic: semi-supervised learning.\nEven though the proposed approach seems to have significant potential, the experimental\nis somewhat disorganized,  and it also includes some weak claims that should be removed. \n\nFor example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).  In this reviewer\'s opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.\n \nIn 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?\n\nThe explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.\n\nYou should also show the performance of regular SSL methods in the setup on Table 4.\n\nLast but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;\nhowever, you do not provide any evidence for it, so you should avoid making such claims.   \n\nOther comments:\n- on page 2, the two terms classification & selection network appear ""out of the blue;"" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.\n- figures 2 & 3 should be a lot larger in order to be readable\n- 4.1.2 top of page 7: claims such as ""SST could have obtained better performance"" have no place in such a paper; you could instead make a note about the method being ""prohibitively CPU intensive for the time being""\n- lower on the same page you say: ""SST may get better performance"" - see above\n', 'Summary:\nIn the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.\n\nStrong points:\nIt is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.\n\nWeak points:\n1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey. Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter. Therefore, the technical contribution of this paper is moderate.\n\nZhu, Xiaojin. ""Semi-supervised learning literature survey."" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.\n\n 2) The writing is poor and hard to follow. First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing. For example, what is the number_iterations in the experiments? How are they chosen or what\'s the specific stopping criteria? From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations. The descriptions of the datasets used are not clear, e.g., the number of classes for each data. Second, many typos and grammar errors need to fix, e.g., ""the proposed SST is suitable for lifelong learning which make use..."", ""the error 21.44% was lower than"" 18.97?\n\n3) The overall performance of the proposed SST in the experiments is not convincing and not promising. First, the labeled data portion is fixed and is relatively high compared to most standard semi-supervised learning settings. Second, SST itself is only comparable with or even worse than the state-of-art methods. Combining SST with other existing techniques can help. However, the additional cost is expensive. Further demonstrations are necessary for the proposed SST method.']","[-30, -20, -60]","[50, 50, -20]","['The sentiment score is slightly negative (-30) because while the reviewer finds the topic interesting and acknowledges some positive aspects, they express several significant concerns about the methodology, experimental evaluation, and clarity of the paper. The reviewer states that the paper is difficult to follow, the writing needs improvement, and they are not convinced by the experimental results or the necessity of the proposed approach. However, they do note some positive elements like the interesting problem and reasonable intuitions.\n\nThe politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They phrase criticisms as suggestions for improvement or questions rather than harsh judgments. The reviewer also acknowledges positive aspects and the potential value of the work alongside the critiques. The language used is respectful and focused on the content rather than making personal comments about the authors.', ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'novel' and 'interesting' on an 'important topic', they also point out several significant issues with the experimental setup, weak claims, and areas needing improvement. The overall tone suggests the paper needs substantial revisions. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'in this reviewer's opinion' and 'it would be helpful', which maintain a polite tone while delivering feedback."", ""The sentiment score is -60 because the review is predominantly negative. While it acknowledges a 'good idea' in the first point, the majority of the review focuses on weak points and criticisms. The reviewer describes the technical contribution as 'moderate', the writing as 'poor and hard to follow', and the overall performance as 'not convincing and not promising'. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. Phrases like 'the writing is poor' and 'many typos and grammar errors need to fix' come across as somewhat harsh. The reviewer does not use particularly polite or encouraging language, but also doesn't resort to overtly rude expressions.""]"
"['This paper formulates feature attribution from a feature selection perspective, and compares EFS (Exclusive Feature Selection) and IFS (Exclusive Feature Selection), which shows IFS is a better fit for feature attribution.\n\n[+] The paper is well-structured and the proposed approach is clearly presented.\n[-] It would helpful if the author could discuss the time complexity of proposed methods and compare the running time with baseline methods in evaluation.\n[-] My major concern on this paper is the significance, as the contribution of the paper seems to be very limited.\n    1) Formalizing the feature attribution problem as a feature selection problem is straightforward. IFS and EFS are just Forward and Backward stepwise feature selection, which are classic feature selection schemes. Applying them to feature attribution/saliency map does not seem to have much technical contribution.\n    2) One claimed contribution of this paper is that existing feature attribution methods can be viewed as approximation of IFS and EFS. However, this contribution also seems to be minor. As many feature selection methods are known to be approximation of backward or forward stepwise feature selection, it is straightforward to show the connection between other feature attribution methods and IFS/EFS.\n\nIn conclusion, I would recommend to reject this paper due to the limited novelty and technical contribution.\n', 'In this paper, the authors study the feature attribution problem as feature selection (exclusive and inclusive).  The authors go through previous work, provide definitions and attempt to answer questions that are relevant to this task.  The authors provide several experiments in order to empirically evaluate which of the two feature selection approaches is better suited for feature attibution.  Although this is a good review, and the motivation is sound, I think that much more ellaboration, and experiments on more than 200 images, would be required to reach definitive conclusions.', 'The authors formalize the feature attribution problem as a feature selection problem and they demonstrate that several existing feature attribution methods can be interpreted as approximation algorithms for Exclusive Feature Selection and Inclusive Feature Selection.\n\n- The authors claim that IFS is better suited as the formalization for the feature attribution problem and EFS has several unfavourable properties. Although they did exhaustive experiments to show this, it is not clear to the reviewer. \n\n- Also, it is more interesting if the authors can show how we use IFS in real applications.\n']","[-70, 20, -20]","[20, 50, 50]","[""The sentiment score is -70 because the review is predominantly negative. While it starts with a positive comment about the paper's structure, the majority of the review focuses on significant concerns and limitations, ultimately recommending rejection due to 'limited novelty and technical contribution'. The politeness score is 20 because the reviewer uses relatively neutral language and provides specific feedback, avoiding harsh or rude phrasing. They acknowledge positive aspects before presenting criticisms, which is a polite approach. However, the overall tone remains professional rather than overtly polite, hence the modest positive score."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's good review, sound motivation, and relevant questions. However, they express concerns about the need for more elaboration and experiments, which prevents a higher positive score. The politeness score is moderately positive (50) as the reviewer uses respectful language, acknowledging the paper's strengths before offering constructive criticism. They avoid harsh or dismissive language, instead suggesting improvements in a professional manner. The phrase 'I think' also softens the critique, making it more polite."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the authors' work in formalizing the feature attribution problem, they express skepticism about the authors' claims regarding IFS being better suited than EFS. The reviewer also suggests that the authors' conclusions are not clear despite exhaustive experiments, and requests more information on real-world applications of IFS. These critiques indicate a somewhat negative sentiment, though not strongly so. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They avoid harsh criticism and instead phrase their concerns as observations ('it is not clear to the reviewer') and suggestions ('it is more interesting if...'). This approach maintains a respectful tone while still conveying their reservations about the work.""]"
"['The paper proposed a RNN with skip-connection (external memory) to past hidden states, this is a slightly different version of the TARDIS network. The authors experimented on PTB and a temporal action detection method.\n\nNovelty:\n\nI dont see a lot of novelty to the method. The authors proposed a method very similar to TARDIS, the difference seems to be that MMARNN does not use extra usage vectors for reading from previous memory, but this is not a fundamental difference between MMARNN and Tardis.\n\nShortcomings of the paper:\n\n1. The experiments seem rather weak. The authors experimented on PTB and temporal action detection method. It is not clear why authors experimented with PTB, this is not a task with long-term dependencies, I do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used\n\n2. The model uses a single past hidden state, it is not clear to me why this is better than using  a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past. The authors should cite ""Sparse attentive backtracking"" (https://arxiv.org/abs/1809.03702) at NIPS 2018. SAB is very related in that it also propagate gradients to a few hidden states in the memory. The difference is that SAB used a few hidden states from the past/ memory instead of one; another difference is that it propagates gradients locally to the selected hidden states/ memory slots.\n\n3. The paper only demonstrated experimental results on PTB and temporal action prediction. I think it would make the paper a lot stronger if the authors experimented with a variety of  different tasks. Tasks that requires long term dependencies can really demonstrate the strength of the model (copy and adding tasks).\n\n4. If the authors could run the model on copy and adding tasks, I would be curious to see if the model is picking the ""correct"" timestep in the memory / past.\n\npost rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes.\n', 'This paper introduces a memory-augmented RNN (MARNN) which aims at being lightweight and   differentiable. In a nutshell, authors propose to augment a LSTM-type architecture with several memory cells. At each time-step, MARNN retrieves one memory cell, updates his state, and updates the memory cell content. To learn the retrieval operation that requires discrete addressing,  authors rely on the Gumbel-Softmax. Authors evaluate their approach on PennTreeBank character level modelling where they demonstrate competitive performances. They also report state-of-art performance on the Thumos dataset. The paper is overall clear and pleasant to read. \n\nAuthors highlight that MARNN is more lightweight compared to existing memory networks. MARNN can indeed retrieves only memory cells at inference. However,  since MARNN uses a Gumbel-Softmax to train the discrete addressing scheme, it is it not clear if there is any advantage in term of memory and computation of MARNN relatively to other network during training? It would be nice to compare the computation time/memory usage of MARNN with other memory augmented network such as TARDIS, NTM or Memory Network during training and inference. \n\nAnother claim is that MARNN can possibly boost training speed by reducing the lengths of TBTT.  But MARNN also haves a training time overhead as showed in Figure 2.  How does the overall training time/performances of MARNN with TBPTT of 50 compared to a LSTM with TBPTT of 100/150?\n\nThe writing can be sometime a bit imprecise. For instance authors say that MARNN “learns better representations that many hierarchical RNN structure”. I agree that MARNN outperforms in term of accuracy, however, it is not clear what the author are referring to by “better representation” of the MARNN hidden state? Performance gain of MARNN could also be due to the external memory which allows  to retain more information of the input? In addition, it would be nice to precise which type of hierarchical RNN structure MARNN does (or doesn’t) outperform. Another claim is that MARNN can “easily learn long-term dependencies”. While this is reasonable, I am unsure that the empirical evaluation support this.  It would be nice to show how the gradients backpropagated through time behave in practice to support this claim? \n\n\nMemory-augmented network are a very important research directions and the MARNN architecture is interesting. However, it is not entirely clear to me what is the main advantage of MARNN relatively to other memory networks network such as TARDIS, NTM or Memory Network for training and/or inference. Although authors do compare with TARDIS, further comparison with the other networks and in term of computation time and memory could help clarify those points. \n', '\nSummary:\n\nThis paper introduces a new RNN architecture with external memory for sequence modeling. The proposed architecture (MARNN) is a simplification of TARDIS (Gulcehre et al., 2017). It uses the similar reader-writer tying mechanism, gates to control information flow from previous hidden state and memory. However, it has a simpler addressing mechanism. Authors show results in Character level PTB and a temporal action detection/proposal task.\n\nMajor comments:\n\nMARNN looks like a simplification of TARDIS architecture with Gumbel softmax. The major difference between the two architectures is the addressing mechanism.\n\n1.\tCan the authors clearly differentiate MARNN vs TARDIS?\n\n2.\tAuthors compare against TARDIS only in character level PTB which is actually a task which does not require very long term dependencies. It would be better if authors consider more tasks and directly compare against the TARDIS addressing mechanism to prove that the proposed addressing mechanism is indeed better.\n\n3.\tAuthors should consider more tasks, to show the efficiency of the proposed architecture.\n\n4.\tThe name of the model seems to be too generic. NTM, TARDIS, DNC with recurrent controller can be considered as memory augmented RNN. Please change the name.\n\n']","[-30, 50, -20]","[20, 80, 50]","[""The sentiment score is -30 because the reviewer expresses several criticisms and shortcomings of the paper, indicating a generally negative sentiment. However, it's not extremely negative as the reviewer acknowledges some positive aspects and mentions that they raised their score after the authors addressed some concerns. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or rude expressions. They provide constructive criticism and suggestions for improvement rather than outright dismissal. The slightly positive politeness score reflects the reviewer's effort to maintain a courteous tone while still conveying their concerns about the paper's limitations."", ""The sentiment score is 50 (slightly positive) because the reviewer begins with a neutral to positive tone, stating that the paper is 'overall clear and pleasant to read' and acknowledges the competitive performance of the proposed method. However, they also raise several questions and concerns about the claims made in the paper, which balances out the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and phrases their concerns as suggestions or questions rather than direct criticisms. They use phrases like 'It would be nice to...' and 'I am unsure that...' which maintain a polite tone while still addressing potential issues with the paper."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contribution, they express several major concerns and suggest significant improvements. The review starts with a neutral summary but then lists multiple critical points, indicating that the paper needs substantial work. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns, framing them as suggestions or questions rather than harsh criticisms. The reviewer also acknowledges the authors' work by summarizing it accurately at the beginning. The language is formal and constructive, avoiding any personal attacks or overly negative phrasing.""]"
"['This paper suggests a continuous-time framework consisting of two coupled processes in order to perform derivative-free optimization. The first process optimizes a surrogate function, while the second process updates the surrogate function. This continuous-time process is then discretized in order to be run on various machine learning datasets. Overall, I think this is an interesting idea as competing methods do have high computational complexity costs. However, I’m not satisfied with the current state of the paper that does not properly discuss notions of complexity of their own method compared to existing methods.\n\n1) “The computational and storage complexity for (convex) surrogates is extremely high.” The discussion in this paragraph is too superficial and not precise enough.\na) First of all, the authors only discuss quadratic models but one can of course use linear models as well, see two references below (including work by Powell referenced there):\nChapter 9 in Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.\nConn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.\nI think this discussion should also be more precise, the authors claim the cost is extremely high but I would really expect a discussion comparing the complexity of this method with the complexity of their own approach. As discussed in Nocedal (reference above) the cost of each iteration with a linear model is O(n^3) instead of O(n^4) where n is the number of interpolation points. Perhaps this can also be improved with more recent developments, the authors should do a more thorough literature review.\nb) What is the complexity of the methods cited in the paper that rely on Gaussian processes?\n(including (Wu et al., 2017) and mini-batch (Lyu et al., 2018)).\n\n\n2) “The convergence of trust region methods cannot be guaranteed for high-dimensional nonconvex DFO”\nTwo remarks: a) This statement is incorrect as there are global convergence guarantees for derivative-free trust-region algorithms, see e.g.\nConn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.\nIn chapter 10, you will find global convergence guarantees for both first-order and second-order critical points.\nb) The authors seem to emphasize high-dimensional problems although the convergence guarantees above still apply. For high-order models, the dimension does have an effect, please elaborate on what specific comment you would like to make. Finally, can you comment on whether the lower bounds derived by Jamieson mentioned depend on the dimension.\n\n3) Quadratic loss function\nThe method developed by the authors rely on the use of a quadratic loss function. Can you comment on generalizing the results derived in the paper to more general loss functions? It seems that the computational complexity wouldn’t increase as much as existing DFO methods. Again, I think it would be interesting to give a more in-depth discussion of the complexity of your approach.\n\n4) Convergence rate\nThe authors used a perturbed variant of the second-order ODE defined in Su et al. 2014. The noise added to the ODE implies that the analysis derived in Su et al. 2014 does not apply as is. In order to deal with the noise the authors show that unbiased noise does not affect the asymptotic convergence. I think the authors could get strong non-asymptotic convergence results. In a nutshell, one could use tools from Ito calculus in order to bound the effect of the noise in the derivative of the Hamiltonian used in Lemma 1. See following references:\nLi, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.\nKrichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous\nand discrete time. In Advances in neural information processing systems, pages 2845–2853.\nOf course, the above works rely on the use of derivatives but as mentioned earlier, one should be able to rely on existing DFO results to prove convergence. If you check Chapter 2 in the book of Conn et al. (see reference above), you will see that linear interpolation schemes already offer some simple bounds on the distance between the true gradient of the gradient of the model (assuming Lipschitz continuity and differentiability).\n\n5) Noise\n“The noise would help the system escape from an unstable stationary point in even shorter time”\nPlease add a relevant citation. For isotropic noise, see\nGe, R., Huang, F., Jin, C., and Yuan, Y. Escaping from saddle points-online stochastic gradient for tensor decomposition.\nJin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456,\n\n6) Figure 2\nInstead of having 2 separate plots for iteration numbers and time per iteration, why don’t you combine them to show the loss vs time. This would make it easier for the reader to see the combined effect.\n\n7) Empirical evaluation\na) There are not enough details provided to be able to reproduce the experiments. Reporting the range of the hyperparameters (Table 2 in the appendix) is not enough. How did you select the hyperparameters for each method? Especially step-size and batch-size which are critical for the performance of most algorithms. \nb) I have to admit that I am not extremely familiar with common experimental evaluations used for derivative-free methods but the datasets used in the paper seem to be rather small. Can you please justify the choice of these datasets, perhaps citing other recent papers that use similar datasets?\n\n8) Connection to existing solutions\nThe text is quite unclear but the authors seem to claim they establish a rigorous connection between their approach and particle swarm (“In terms of contribution, our research made as yet an rigorous analysis for Particle Swarm”). This however is not **rigorously** established and needs further explanation. The reference cited in the text (Kennedy 2011) does not appear to make any connection between particle swarm and accelerated gradient descent. Please elaborate.\n\n9) SGD results\nWhy are the results for SGD only reported in Table 1 and not in the figure? Some results for SGD are better than for P-SHE2 so why are you bolding the numbers for P-SHE2?\nIt also seem surprising that SGD would achieve better results than the accelerated SGD method. What are the possible explanations?\n\n10) Minor comments\n- Corollaries 1 and 2 should probably be named as theorems. They are not derived from any other theorem in the paper. They are also not Corollaries in Su et al. 2014.\n- Corollary 2 uses both X and Z.\n- Equation 5, the last equation with \\dot{V}(t): there is a dot missing on top of the first X(t)\n“SHE2 should enjoy the same convergence rate Ω(1/T) without addressing any further assumptions” => What do you mean by “should”?\n- There are **many** typos in the text!! e.g. “the the”, “is to used”, “convergeable”,... please have someone else proofread your submission.\n', 'Overall, I could potentially be persuaded to accept this paper given a relatively favorable comparison to some other blackbox optimization algorithms, but I have some serious issues about clarity and some technical details that seem wrong to me (e.g., the inclusion of L-BFGS as a ""derivative free"" baseline, and the authors\' method outperforming derivative based methods at optimizing convex loss functions).\n\nI\'d like to start by focusing on a few of the results in section 5.2 specifically.\nIn this section, you compare your method and several baselines on the task of training logistic regression and \nSVM models. Given that these models have convex loss functions, it is almost inconceivable to me that methods like L-BFGS and SGD (at least with decent learning rates) would perform worse than gradient free optimization algorithms,\nas both L-BFGS and SGD should clearly globally optimize a convex loss. I am also generally confused by the inclusion\nof L-BFGS as an example of a derivative free optimization problem. Are you using L-BFGS with search directions\nother than the gradient or something as a baseline? I think the exact setup here may require substantially more explanation.\n\nThe primary other issue I\'d like to discuss is clarity. While I think the authors do a very good job\ngiving formal definitions of their proposed methods, the paper would massively benefit from some additional\ntime spent motivating the authors\' approach. As a primary example, definition 2 is extremely confusing. I felt it wasn\'t as well motivated as it could have been given that it is  the central contribution of the paper. You reference an ""exploration process"" and an ""exploitation process"" that ""were shown in Eq. 4,"" but equation four is the next equation that directly jumps in to using these two processes X(t) and Y(t). These two processes are very vaguely\ndefined in the definition. For example, I understand from that definition that Y(t) tracks the current min value, but even after reading the remainder of the paper I am still not entirely sure I understand the purpose of X(t). Perhaps \nthe paper assumes a detailed understanding on the readers\' part of the work in Su et al. (2014), which is cited \nrepeatedly throughout the method section?\n\nTo be concrete, my recommendation to the authors would be to substantially shorten the discussion in the paper\nbefore section 3, provide background information on Su et al., 2014 if necessary, and spend a substantially \nlarger portion of the paper explaining the derivation of SHE2 rather than directly presenting it as an ODE\nthat immediately introduces its own notation. In the algorithm block, the underlying blackbox function\nis only evaluated in the if statement on line 9 -- can the authors explain intuitively how their surrogate \nmodel evolves as a result of the Y_{t} update?\n\nIn addition to these concerns, some of the claims made in the method section seem strange or even wrong to me,\nand I would definitely like to see these addressed in some way. Here is a list of a few concerns I have\nalong this line:\n\n- A few of the citations you\'ve given as examples of derivative free optimization are confusing.\nYou cite natural gradient methods and L-BFGS as two examples, but natural gradient descent involves preconditioning\nthe gradient with the inverse Fisher information matrix, and is therefore typically not derivative\nfree. You give Gaussian process surrogate models as an example of a convex surrogate, but GPs\nin general do not lead to convex surrogates save for with very specific kernels that are not\noften used for Bayesian optimization.\n\n- In the background, it reads to me like you define GP based Bayesian optimization as a quadratic\nbased trust region method. This seems strange to me. Trust region methods do involve quadratic surrogates,\nbut my understanding is that they are usually local optimization schemes where successive local quadratic \napproximations are made for each step. GP based Bayesian optimization, by contrast, maintains a global\nsurrogate of the full loss surface, and seeks to perform global optimization.\n\n- Equation 3 defines the squared norm \\frac{1}{2}||X-Y||^{2}_{2} as the ""Euclid[ean] distance"".\nBased on the following derivatives, I assume this is intended to be kept as \nthe squared Euclidean distance (with the 1/2 term included for derivative simplicity).\n', 'Derivative-free optimization is not a novel domain and your work could benefit from some accepted benchmarking practices. For instance, you can consider the Black-Box Optimization Benchmarking (BBOB) Workshop and its COCO platform which was used to test many optimization algorithms including the ones mentioned in the paper. \nBenchmarking on BBOB problems would probably reveal that your algorithm fails on non-separable ill-conditioned problems and even on problems like Rosenbrock (e.g., compared to BOBYQA). The results for other algorithms can be downloaded, you don\'t need to rerun them. BBOB\'s computational budget can be as low as 1 function evaluation. \n\nExtended review of Update 17 Nov:\nI would like to note that I liked the fact that you used several optimization algorithms in your comparison. To my best understanding, several algorithms shown in Figure 3 (e.g., BOBYQA and L-BFGS) would benefit from restarts and it is fairly common to use restarts when the computational budget allows it (it seems to be the case for Figure 3).\n\nThe results shown in Figure 4 are hard to trust because it does not seem that we observe mean/median results but probably a single run where the results after 1 iteration are drastically different for different algorithms. For instance, after one iteration BOBYQA only tests its second DOE point. Here, again, the issue is that one iteration for BOBYQA is 1 function evaluation while it is several (10) function evaluations for other algorithms. In that scenario, it would be more fair to run BOBYQA with 10 different initializations as well. \nI don\'t understand ""Due to the restriction of PyBOBYQA API, we can only provide the function evaluation of the final solution obtained by BOBYQA as a flatline in Figure 4"". At the point when your objective function (which is not part of PyBOBYQA API) is called, I suppose you can log everything you need. \n']","[-30, -50, -30]","[50, 20, 20]","[""The sentiment score is -30 because while the reviewer acknowledges the paper as 'interesting', they express significant dissatisfaction with its current state, particularly regarding the lack of proper discussion on complexity compared to existing methods. The review lists numerous critical points and corrections, indicating a generally negative sentiment. However, it's not entirely negative as the reviewer sees potential in the idea. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout, offering specific suggestions for improvement and citing relevant literature. They use polite phrases like 'I think' and 'Can you please justify' rather than making blunt criticisms. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a neutral-to-slightly-positive tone in terms of politeness."", ""The sentiment score is -50 because the reviewer expresses several serious concerns and issues with the paper, including clarity problems and technical details that seem incorrect. However, they do mention they 'could potentially be persuaded to accept this paper', which prevents the score from being more negative. The politeness score is 20 because the reviewer uses generally polite language and offers constructive criticism, but doesn't go out of their way to be overly polite. They use phrases like 'I'd like to start by focusing on...' and 'To be concrete, my recommendation would be...', which maintain a professional and respectful tone. However, they also use some direct language when pointing out issues, such as 'seem strange or even wrong to me', which keeps the politeness score from being higher."", ""The sentiment score is -30 because the review is generally critical, pointing out several shortcomings in the authors' approach and suggesting improvements. However, it's not entirely negative as the reviewer acknowledges some positive aspects ('I liked the fact that you used several optimization algorithms'). The politeness score is 20 because the language is professional and constructive, offering specific suggestions for improvement without being harsh. The reviewer uses phrases like 'I would like to note' and 'To my best understanding', which maintain a respectful tone. The criticism is presented as helpful advice rather than blunt criticism, contributing to the slightly positive politeness score.""]"
"[""The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks. The probabilities are approximated with variational autoencoders. During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.\n\nThis paper focuses on the second part, with a different model. Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification. This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.  This pre-trained model is then incorporated into the neural net for MNIST classification.  The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model. This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)\n\nThe authors compare to the work of Schott for one type of attack. It would be nice to see more detailed experiments as done in Schott.\n\nQuestions:\n1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.\n2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?\n3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.\n4- Could you please add the found J_h's to the appendix. This architecture reminds me of the good old MRFs for image denoising. Could it be that what we are seeing is the attack being denoised?\n\nI am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott. \n\nThanks in advance. I will re-adjust the review rating following your reply.\n\n\n\n"", 'Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks. They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor. Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it. The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser. \n\nOne question which is not addressed is the reason for only one RBM layer. In ""Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning"" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST. Attacking CRBMs is highly relevant and should be included as a baseline.\n\nThe only set of experiments are comparisons on first 500 MNIST test images. If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits. Authors should clarify the justification behind experimenting only on \'first 500 test images\'. \n\nFurthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input. Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here. The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers. Although their network was resilient toward white box attacks they suffered from black box attacks. The boundary method on MNIST could be  weaker than a black box attack.  ', 'This paper is clearly written and in an interesting domain. The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks. However, there are some key issues with the paper that are not clear.\n\nThe first is regarding the structure of the paper. The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks. The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs. It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.  Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).\n\nIn a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:\nsee paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf \n\nIt would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.\n\nIt is also unclear how the calculation of relative entropy ""D"" was performed in figure 3. Obtaining the normalized marginal density in a BM is very challenging due to the partition function.\n\nThe second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM. This is a very good point, however the paper do not compare or contrast with existing methods. For example, it is curious to see how denoising Auto encoders would perform. In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf\n\n- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box. Defending against black box attacks is considerably easier than defending against white-box attacks.\n\nIn summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage  of the proposed MF BMs in increasing robustness against adversarial attacks.']","[-20, -20, -20]","[60, 50, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the work and its comparison to previous research, they express puzzlement and request several clarifications and additional experiments. The phrase 'I don't yet understand' indicates some skepticism about the approach. However, it's not strongly negative as the reviewer seems open to changing their opinion based on the authors' response.\n\nThe politeness score is moderately positive (60) due to the respectful tone throughout the review. The reviewer uses polite language such as 'Could you please...', 'Thanks in advance', and expresses looking forward to the answers. They also offer to re-adjust the review rating after receiving a reply, which shows consideration for the authors. The questions are framed as requests rather than demands, contributing to the overall polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novel approach, they raise several concerns and suggest improvements. The review points out missing elements, questions the experimental setup, and recommends additional experiments, indicating a somewhat critical stance. However, it's not entirely negative as the reviewer shows interest in the work and offers constructive feedback. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, uses neutral language, and frames criticisms as suggestions or questions rather than direct attacks. The reviewer acknowledges the authors' work and provides detailed, constructive feedback without using harsh or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is 'clearly written and in an interesting domain', they also point out 'key issues' and areas where the paper could be improved. The reviewer suggests that 'more experiments could be added' and that certain aspects are 'unclear', indicating that the overall sentiment is somewhat critical but not entirely negative. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh criticism. Phrases like 'It would make the paper much stronger if...' and 'It would be very interesting to...' demonstrate a polite and encouraging tone. The reviewer also balances critique with positive comments, such as 'This is a very good point', which contributes to the polite tone.""]"
"['The main difficulty of neuroevolution---requiring a huge number of simulations for high dimensional problem---is addressed in this paper by introducing VAE to reduce the state space dimensionality and using a rather shallow controller network. This idea itself is very promising, however, it has been introduced in (Ha and Schmidhuber, 2018).  Still, there seems to be differences in how to gather histories and how to use them. Nevertheless, the differences are not well described in the text. The effect of the modification is not evaluated on experiments.', 'Summary: They propose ""Sample Efficient Deep Neuroevolution"" (SEDN) model and experiment on Atari games. In this model, they use a Variational Encoder (VAE) to encode state frames into a latent vector, and use an LSTM to encode the current latent vector and action to predict the next latent space. A policy network (trained using CMA-ES) takes the latent space, and hidden state of the RNN as an input, and outputs an action to execute.\n\nThe strengths of this paper is that it is clearly written. They even explain details of RL, background of evolution strategies, motivation of using CMA-ES (and also the algorithm itself, which is no small ordeal), so it might a good background review paper of the literature. The experimental setup is relatively easy to understand. I suggest putting Algorithm 3 before Table 1 since presenting the algorithm before results may be more natural.\n\nThat being said, there are issues with this work that needs to be addressed before publication. I will list the issues and some suggestions I have, in order to help make this work better, hopefully good enough for acceptance:\n\n1) The authors cited [1] a few times in the paper, but actually their approach of using a VAE to compress frames into a latent, an LSTM to predict the next latent, and a CMA-ES trained network for the policy is precisely what is proposed in [1] (which had experiments that trained on the actual environment, like in this work, and also the generated environment). This paper reads like they have proposed the setup, and lacks clarity as to which parts are their contribution, and which parts are prior work, which I believe is important for a paper submitted to an academic venue. Not to say at all that there\'s no contribution or originally in this work - there are many, but I feel they should list out which bits are their contribution, and which bits are prior work more clearly. Doing so will make this paper and their contributions stronger.\n\nIn my opinion, their contributions are: Expanding on the approach of [1] to study on a larger set of environments (the Atari suite), where they also incorporated an iterative training loop (described Algorithm 3) that was not used in [1]. Also, unlike [1], they used a multi-layer policy network, and also explained and rationalized the intuition behind the choice of CMA-ES. I think by listing out the contributions, and separating them from previous work, the paper will be much stronger.\n\n2) The results are not terribly strong. They achieved good results on 7 games out of 50 using 10M frames. To me, that\'s actually not a deal breaker, since research is not a SOTA game, but I would like to see a more detailed analysis of why the algorithm works, and when it fails so people know what future work needs to be done to address this. I\'m also not convinced that using CMA-ES would have an advantage over A3C (with the same latent / hidden features going into A3C as inputs), so perhaps the author might achieve better results if A3C was used to train the policy network (or not, but would be nice to see this experiment). It would offer more insight if we know what kind of terminal scores can be achieved using this algorithm, if it were allowed to train for 1B frames like the other 2 setup. Finally, if the author was able to show that training inside a generated environment, even for pre-training before going back to the actual environment, helps sample efficiency, that would be a very interesting result to me.\n\nI\'m assigning a preliminary score of 5 for this work for now, but if the author address point (1) to my satisfaction I will revise the score to +1 points, and if the author is able to achieve much better results, or address items in point (2) to my satisfaction, I will revise the score by +1 or potentially +2 points, so the final score of this work may lie in the range of 5 -> 8. I feel the author should be able to improve the paper to get a score of 6-7 in the end, at least from me. Good luck!\n\n[1] https://arxiv.org/abs/1803.10122', ""This paper proposes a combination of Evolutionary methods and variational representation learning to improve the sample efficiency of RL methods.\nThey train a VAE on environment frames, as well as an action-conditioned Dynamics model to predict the next frames, and these form the representations fed into a policy network which is trained through ES.\n\nOverall, I find the problem setting interesting, and they try to tackle Atari games instead of simpler domains.\nThe use of CMA-ES instead of NES is a good improvement, and the way they motivate using VAE representations to obtain manageable representation sizes is well put forward.\n[Edit: as mentioned by the other reviewers, this extension isn't as novel, given Ha et al's work, hence that reduces my confidence about accepting this paper further...]\n\nHowever, this paper suffers from several issues in its current state:\n1.\tIts presentation is overly detailed about known literature. Section 2 goes in low-level details which are not necessary. It covers ES methods even though a citation to Salimans et al. 2017 would have been sufficient. Section 2.1 is a really complete coverage of CMA-ES, which should really just be a citation of the actual paper again or should be in the Appendix, this doesn’t warrant 1.5 pages of the main text.\n2.\tThe actual model presentation is too succinct and split into Section 2.2 and Section 3 (network architectures and parameters). It is never clear how many parameters are optimized by CMA-ES (I counted ~8200 parameters if the MLP of size 256 x 32 x n_a is used). Algorithm 3 however was extremely clear and helpful to fully understand the method.\n3.\tThere is no clear evaluation of the performance of the VAE representations and of the RNN dynamics model. Did they actually learn to represent anything at all? Figure 4 is not sufficient in providing evidence supporting this. \u2028Compare this to Higgins et al. 2017, which used VAEs which represented enough information to perform at the same performance as non-variational representations.\n4.\tThis feeds into the biggest issue with the current results:\u2028The proposed method works rather badly, obtaining worst performance than ES on 35 out of 51 games (68%). On most of these games, the proposed method does not seem to be able to get off the ground at all. \u2028Why is that the case? Obviously if the VAE+RNN do not represent the games well enough, the performance will be bad. Did the policy learning with CMA-ES converge well? (seeing learning curves might help)\u2028The fact that no gradients are passed back from the Policy to the VAE/RNN clearly emphasises that issue (The policy only affect the data on which the representations are periodically retrained on).\n\nIn conclusion, even though I feel this paper tries to tackle an interesting problem, the results are not sufficient to support them as of now.\n\nTypos:\n-\t“Donates” instead of “denotes” in a few places.\n\nReferences:\n-\tHiggins et al., 2017: https://arxiv.org/abs/1707.08475 \n""]","[-20, -20, -50]","[20, 70, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the promise of the idea, they point out that it's not entirely novel and that the paper lacks sufficient description of differences from previous work and experimental evaluation. The politeness score is slightly positive (20) as the reviewer uses neutral language and acknowledges the potential of the idea, but doesn't use overtly polite phrases. They present criticisms in a matter-of-fact manner without harsh language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out significant issues that need to be addressed before publication. The reviewer states that 'there are issues with this work that needs to be addressed before publication' and mentions that the results 'are not terribly strong'. However, the reviewer also offers constructive feedback and expresses hope for improvement, which prevents the score from being more negative. The politeness score is relatively high (70) because the reviewer uses respectful language throughout, offers constructive criticism, and provides detailed suggestions for improvement. They use phrases like 'I suggest', 'I feel', and 'Good luck!', which contribute to a polite tone. The reviewer also acknowledges the potential of the work and offers a path for the authors to improve their score, which is considerate and encouraging."", ""The sentiment score is -50 because while the reviewer finds the problem setting interesting and acknowledges some positive aspects (e.g., use of CMA-ES, tackling Atari games), they ultimately conclude that the paper 'suffers from several issues' and the results are 'not sufficient to support them as of now'. The reviewer lists multiple significant problems with the paper, including overly detailed literature review, unclear model presentation, lack of evaluation for key components, and poor performance of the proposed method. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They acknowledge positive aspects before presenting criticisms, use phrases like 'I find' and 'I feel' to soften critiques, and provide specific suggestions for improvement. The language is not overly formal or deferential, but it remains respectful and focused on the paper's content rather than making personal comments about the authors.""]"
"['The paper introduces a novel architecture for low-resource named entity tagging: a dual adversarial transfer network, in which fusion between high- and low-resource, or high- and low-noise data is achieved via also novel resource-adversarial discriminator.\n\nThe model is interesting, novel, clearly exposed in sufficient detail, and warrants publication as such. The idea to unify representation differences and data imbalance under one model is noteworthy.\n\nI find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.\n\nThis unfair account of related work would not trouble me as much if it weren\'t coupled with an experiment in ""low-resource"" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.\n\nThis experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is poor scholarship in my submission, regardless of the clever model.\n\nI do let the clever model upvote my review, but not beyond borderline.\n\nMinor:\n- the use of ""lingual"" as noun is rather off-putting, at least to me', '<Summary>\nAuthors propose the new “DATNet” for the NER task, which extends the base neural model for NER (Bi-LSTM+CRF sequence model with input represented with CharCNN-word embeddings) with the following two main components: (1) GRAD: a language (or resource) adversarial discriminator with adaptive weights that regularize source-target data imbalance, and (2) additional adversarial training approaches that perturb input samples in the embeddings space.\n\nThe paper reports big improvement over their baseline approaches without having to rely on other auxiliary or hand-crafted features. The experiment is performed for various low-resource scenarios (varying training data size). \n\n<Comments>\n- While the idea of applying “dual adversarial” approaches is new in the context of NER, the technical novelty of each component is limited. GRAD, for example, is rather a minor modification of Language Adversarial Discriminator (Kim et al., 2017) with a scalar weight parameter on loss. The empirical superiority of the proposed method (GRAD) over normal AD approaches cannot be claimed either -- for this authors need to report quantitative results of the (Base + AT + F/P-transfer with AD -- or e.g. \\alpha fixed at 0.5 or at some other rate), which I believe is missing in all figures and tables. Visualization of resulting feature distribution (Figure 3) is interesting to look at, but that alone does not suffice. There is no technical novelty in applying adversarial training either, except that it was used in the context of NER.\n\n\n- Authors use both source and target data to train their “base model” as well (“... we exploit all the source data and target data ...“), e.g. presumably by merging the source and target dataset as well as their embedding matrices, etc. It is important that authors report if there ever is a negative transfer case (e.g. a base model trained with just target data may outperform models trained with source+target data) at varying resource scenarios -- especially at sufficiently-resourced cases.\nIf by any chance their “base model” refers to in-domain training / in-domain testing results on target, the aforementioned baseline (naive merge of source and target data) is obviously an important baseline to report. I suggest that authors provide these details or clarify. (The confusion comes mainly because some of the SOTA results the authors quote are in-domain training / in-domain evaluation results on respective languages, and some are cross-domain results -- yet they are all under “cross-lingual/domain” columns in Table 2).\n\n- It would be interesting to report the learned optimal \\alpha value for each different setting (at varying r or training size) to see if the authors’ intuition is met. On a related note, from the manuscript alone, it is not entirely clear if \\alpha is a learnable parameter or a tunable hyper parameter -- by context I believe it is a model parameter. If they are tuned, authors need to report these values.\n\n<Nit> \n- Section 3.2.3, “... (GRAD) to enable adaptive weights for [each sample]” → I think it reads better with [each resource] or [each source and target], unless you meant \\alpha_i for each sentence.\n- Section 3.2.5, “... recently, adversarial samples are [wisely] incorporated” → [widely]\n- Fonts for figures could be bigger. ', 'The authors propose a new architecture Dual Adversarial Transfer Network for addressing low-resource NER. They achieve a new SOTA on low resource language. The authors compare a base-line with two alternatives based on variants of GANs.  \n\nThe results go beyond SOTA for low resource NER which seems a solid contribution. The paper is well and clearly written and I would be able to replicate the experiments. I wonder if this is a new architecture that works well for one task or if it could be applied to other tasks too. This would strengthen the approach and paper quite a bit. Could the method for instance be applied to other labeling tasks: POS tagging, morphological features. This would increase the potential impact substantially.  \n  \nIn low resource scenarios often methods work that do stop working at a some point with more resources. For this methods the boundary when this effect occurs would be interesting to explore; Figure 2 goes into this direction which is a quite nice study but the boundary is not  explored further; by using for instance the English NER data. Additionally, the performance on the English data set would indicate what the method could perform in comparison to current SOTA for normal resource setting - you could use some of the low resources in addition. The English data set was used but only to exploit it for the transfer learning. \nTable 2 is a good overview on SOTA. I really wonder about the variance of the results of the system which can be depending on the network quite large. Why not running repetition test, this  would enable the authors to report variance and statistically significance between the baseline and their other systems. \nI wonder also how more standard exploitation of additional data would do such as Bert, ELMO or older methods such as up-training - this would help to get a more complete picture and strength the paper further. \n\nThe paper could be stronger by applying the method to other task too as stated the authors - this is a ‘new architectures’ (for NER) which triggers the question and does it generalize to other tasks? In the conclusion there is even the claim as a statement! that this can be generalized to other NLP task without actually trying. I think, this can not be claimed in the conclusions without pursuing this in some other task and I suggest to tune this down.   \n\nOverall:\nNicely and clear written paper containing innovative elements. The results look strong too me but due to the lack of variance and stat sig., I am not sure if they are really super strong. The paper could be stronger by applying the method to other task too as stated ‘new architectures’ which triggers the question if the method generalizes to other tasks?   \n']","[-20, -30, 60]","[50, 50, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty and interest of the model, they express significant concerns about the experimental setup and representation of related work. The reviewer states the paper 'warrants publication' but also calls aspects 'poor scholarship'. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positives before critiquing, and phrases criticisms as personal opinions ('I find', 'troubles me'). They avoid harsh language, though some phrases like 'poor scholarship' are more direct. The reviewer maintains a professional tone overall, balancing critique with recognition of the paper's merits."", ""The sentiment score is -30 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'big improvement over their baseline approaches'), they raise several significant concerns about the novelty and methodology. The reviewer points out limitations in technical novelty, missing baselines, and unclear details, indicating an overall negative sentiment. However, the criticism is not extremely harsh, hence a moderately negative score. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'I suggest' and 'It would be interesting to' which maintain a polite tone. The reviewer also balances criticism with positive observations. While not overly formal or deferential, the language is consistently polite and appropriate for academic discourse."", ""The sentiment score is 60 (positive) because the reviewer expresses overall positive sentiment towards the paper, praising its clear writing, innovative elements, and strong results. They mention it's a 'solid contribution' and 'nicely and clear written paper'. However, they also suggest several improvements, which prevents the score from being higher. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I wonder' and 'I suggest' which are polite ways of offering feedback. The reviewer also acknowledges the paper's strengths while providing areas for improvement, maintaining a balanced and courteous tone.""]"
"['The paper investigates different machine learning approaches to model and\npredict the return on property investments, in particular with respect to eager\nand lazy learning techniques. The authors evaluate those different techniques on\na dataset of properties in Virginia. They conclude that lazy techniques provide\nbetter performance than eager ones.\n\nThe paper is a purely empirical study that does not introduce any novel\nmachine learning or evaluation techniques. The authors use the off-the-shelf\nWEKA toolbox. The results are not clear, given that only a single data set was\nused to evaluate the different approaches, and general recommendations cannot be\nmade.\n\nThe paper is not well written and the descriptions do not convey what the\nauthors have done very well. An example of this is Figure 3, which purports to\nshow the average rent (or rent distribution?) for different housing types. There\nare multiple categories in there that are not valid housing types (""Make Me\nMove"") and the rents shown are incorrect (e.g. more than a million for a town\nhouse). It is also unclear why the average rent for a single family house is\napproximately 4 times as much as for a town house.\n\nThe problems with Figure 3 are exemplary of the paper; the other issues are too\nnumerous to list.\n\nIn summary, this paper should be rejected.', ""Quality: your paper provides a comprehensive evaluation of a number of methods. However, I am not convinced by the choice of methods or the need for them to be compared as such.\n\nClarity: the paper is unclear in a number of places.\n\nOriginality: as far as I am aware, the paper is novel.\n\nSignificance: As stated above, I do not believe you have made the case that we need a comparison of these methods on this dataset.\n\nMore detailed comments:\n\n1. The distinction between lazy and eager learning is not one that I find particularly helpful. Perhaps this is my own ignorance. I checked two standard ML textbooks (Bishop, Murphy) and neither mentions this distinction. As the crux of your paper is a comparison between these methods, you need to be very upfront about why this dichotomy is a useful one. And as your paper is very application-oriented, the justification needs to be related to that application.\n\n2. Your data is, fundamentally, spatial (actually spatiotemporal). Thus it would be appropriate to consider models that specifically account for spatial structure, e.g.  Gaussian processes.\n\n3. Do you have a novel methodological or theoretical contribution? A comparison of methods on an important dataset is no doubt useful, but I don't think ICLR is the right venue."", 'The authors compare a collection of machine learning models to predict the expected rental income from an investment property. The dataset they use to train their model is fairly small (around 4K transactions). In addition to using house specific features the authors use other macro features, such as, walk score etc. Using this the authors compare a set of machine learning models and report their findings. \n\nWhile the work presented in the paper is informative, I feel there are a number of issues with the paper, making it unsuitable for publication in ICLR. Some of them include: \n- There is no new novel model or technique proposed in the paper. It is essentially a collection of experiments run on some dataset with reported findings. \n- The dataset used is really small. Making the generalizability of the results somewhat questionable. \n- Lastly, even though there is nothing technically wrong with the presented work, I feel that ICLR is not the best venue for such works. Perhaps a data science conference, such as, KDD/WSDM might be much better suited.']","[-80, -50, -50]","[-20, 20, 20]","[""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper should be rejected, criticizes the methodology, writing quality, and results presentation. They mention numerous issues and provide no positive feedback. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. They use phrases like 'purely empirical study that does not introduce any novel techniques' and 'the problems... are too numerous to list,' which come across as somewhat impolite. The reviewer doesn't offer constructive feedback or soften their criticisms, leading to a slightly negative politeness score."", ""The sentiment score is -50 because the reviewer expresses significant concerns about the paper's choice of methods, clarity, and significance, while acknowledging its novelty. The reviewer is 'not convinced' by key aspects of the paper and doesn't believe the authors have made a strong case for their work. However, some positive aspects (like novelty) prevent the score from being lower. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I am not convinced' and 'Perhaps this is my own ignorance' rather than harsh criticism. The reviewer also provides specific, constructive feedback. However, the overall critical nature of the comments prevents a higher politeness score."", ""The sentiment score is -50 because while the reviewer acknowledges that the work is 'informative', they ultimately conclude it is 'unsuitable for publication in ICLR'. They list several significant issues with the paper, indicating a generally negative sentiment. However, it's not extremely negative as they do recognize some value in the work. The politeness score is 20 because the reviewer uses relatively neutral language and frames their criticisms constructively (e.g., 'I feel there are a number of issues'). They also offer alternative suggestions for publication venues, which is helpful. The tone is professional and not personally critical, but it's not overtly polite either, hence a slightly positive score.""]"
"['This paper describes a meta-RL algorithm through imitation on RL policies. While the paper builds nicely up to the core part, I find essential details missing about the imitation setup. By glancing at previous BC papers (some of which are cited), the quantity for supervised imitations, etc., were clearly defined. \n\nIt will be useful for this reviewer if the authors can provide more clarity in explaining the BC task involved in their algorithm.', 'This work addresses the problem of learning a policy-learning-procedure, through meta-learning, that can adapt quickly to new tasks. This work uses MAML for meta-learning, and with this choice, the problem can be broken down into two loops: \n\n1) inner loop: adapting a policy \\pi_phi based on unseen rollouts, where initial parameters phi were provided by the meta-trainer in the outer loop \n2) outer loop: the meta-trainer tries to learn parameters phi on batches of tasks that provide good initial parameters \n\nIn prior work on meta-reinforcement learning via MAML, both the outer as well as inner objective attempt to minimize a RL objective, leading to an algorithm that has very high sample-complexity. This work uses imitation learning for the outer loop procedure, to significantly decrease sample-complexity.\n\nTechnical Contribution:\n-----------------------\nThe idea of using imitation learning for reinforcement learning is well explored in the literature, and so using this idea in itself is not real contribution. There are several issues with the presentation of this work, that make it incredibly difficult to identify a technical contribution:\n\n1. overreaching statements without details to backup: you are writing the paper as if you are learning a ""RL algorithm"" that can be used to quickly learn new tasks. your manuscript does not really provide a description for this ""algorithm"". After re-reading several other papers I concluded that what you mean is that you learn an initial set of policy parameters that can quickly adapt to new related tasks and an update rule with which you update these parameters. However, standard MAML uses SGD as an update rule so there is really nothing to be learned here. Unfortunately, your paper provides zero detail on these claims of learning a ""RL procedure"", so for now I have to assume that you are simply learning a good initial set of policy parameters through meta-learning. If that is the case, then using imitation learning in this setting is really not novel, this has been done by a lot of other people before (you\'re just using MAML to learn ""better"" initial parameters).\n2. you\'re technical section (section 4) provides some details on the technical challenges of using demonstrations to perform the outer loop optimization step. Unfortunately, you are not putting your work in the context of existing work ([1], [2]), that discuss and address the importance/issue of sampling in meta-rl with MAML. So it\'s impossible to know whether there is any new insight here\n\nExperimental Evaluation:\n-------------------------\nThe experimental evaluation is very ""thin"", other than the original MAML-RL and pure imitation learning no other more recent baselines ([1], [2]) have been compared to. And only 2 relatively simple simulation settings are tested. \n\nSummary:\n-----------\nVery minor contribution, a manuscript that is lacking important details and does not relate it\'s technical section to existing work, with very thin evaluation. \n\n\n[1] The Importance of Sampling in Meta-Reinforcement Learning, NIPS 2018\n[2] CONTINUOUS ADAPTATION VIA META-LEARNING IN NONSTATIONARY AND COMPETITIVE ENVIRONMENTS, ICLR 2018', ""The paper presents a meta-RL method extends previous work on meta-RL by including an imitation learning step. It is mentioned that the behaviour closing part of this extended algorithm can come from a teacher or some other source. Since this extension is the major contribution, it must be discussed in more detail. I also don't understand why \n\nMy second problem with the paper is reproducibility. The purpose of OpenAI is comparability and reproducibility of algorithms. It is not sufficient to simply state that you have used TensorFlow. We need information about the architecture, etc so that the results can be reproduced. Also, since you use OpenAI, the score for each experiment should be compared to the best scores known from the website so that the performance of the new algorithm can be compared to others."", 'This paper proposes a meta-learning algorithm for reinforcement learning that incorporates expert demonstrations. The goal is to reduce the sample complexity of meta-RL algorithms in the validation phase. The paper provides a good discussion of the background literature. Experimental results are provided on multi-goal planning problems for three prototypical simulated systems, namely, a 2D point-mass robot, a 7-DOF manipulator and a quadruped crawler.\n\nThe theoretical and practical contributions of this paper are minor. The authors propose a straight-forward combination of MAML, importance-weighted policy gradients across the inner-outer loop and off-policy supervised learning of the expert demonstrations, all standard techniques in reinforcement learning and meta-learning. The experimental section is unconvincing and lacking in details. I however find the approach well-motivated and pertinent. Demonstrations on a real robotic platform, where the improved sample complexity is essential, would make this paper much more impressive.\n\nDetailed comments:\n\n1. Can you compare the different algorithms using the number of rollouts as the X-axis? The narrative provides this information but it is difficult to judge the performance based on Figs. 2 and 3.\n2. It is unfair to compare MRI (the approach of this paper) with MAML which does not have access to expert demonstrations for validation tasks. The improved sample complexity is thus directly coming from demonstrations. It is difficult to compare MRI and MAML-RL/Imitation on an equal footing. Perhaps the validation tasks could be significantly harder, e.g., sub-goals in the planning problems, or one could consider a large number of validation tasks.\n3. It seems the improvement over MAML-RL/Imitation in Fig. 3 is minor. Why is this so?']","[-20, -70, -30, -20]","[50, -20, 20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper 'builds nicely up to the core part', they also express concerns about 'essential details missing' and request more clarity. This indicates a mixed but slightly critical view. The politeness score is moderately positive (50) as the reviewer uses respectful language, framing their critique as a request for clarification ('It will be useful if the authors can provide more clarity') rather than a direct criticism. The tone is professional and constructive, avoiding harsh or rude language."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the contribution is 'very minor', the manuscript lacks important details, doesn't relate to existing work, and has a 'very thin evaluation'. The reviewer also points out several issues with the presentation and claims made in the paper. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism and somewhat dismissive language. Phrases like 'overreaching statements without details to backup' and 'incredibly difficult to identify a technical contribution' come across as harsh. The reviewer doesn't use overtly rude language, but the tone is more critical than polite, especially in the summary section."", ""The sentiment score is slightly negative (-30) because the reviewer points out several issues with the paper, including the need for more detailed discussion of the main contribution and problems with reproducibility. However, it's not extremely negative as the reviewer acknowledges the paper's contribution and provides constructive feedback. The politeness score is slightly positive (20) as the reviewer uses neutral language and offers specific suggestions for improvement without using harsh or rude phrasing. The reviewer maintains a professional tone throughout, even when pointing out shortcomings in the paper."", ""The sentiment score is slightly negative (-20) because while the reviewer finds the approach 'well-motivated and pertinent', they also state that the 'theoretical and practical contributions of this paper are minor' and that the 'experimental section is unconvincing and lacking in details'. The reviewer suggests significant improvements, indicating overall dissatisfaction with the current state of the paper. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, acknowledges positive aspects of the paper, and provides constructive feedback. They use phrases like 'good discussion' and 'well-motivated' to soften criticism, and their suggestions are framed as opportunities for improvement rather than harsh criticisms.""]"
"['This paper proposes to learn a manifold of deep generative models using a pre-trained VAE. To generate samples with desired features, this paper proposes to learn an embedding of each feature in the hidden space using VAE. Then the learned hidden space is used to train a GAN.\n\nHowever, the method in this paper and main contributions are not clearly represented. I can hardly understand the motivation of this paper. In the introduction part, this paper mentions “large scale of latent space” lots of times, but does not make it clear that why a large latent space hinders the deep generative models. In Fig.1, it demonstrates that for some manifold, L2 distance cannot be applied directly. However, for most DGMs, the hidden space is defined in Euclid Space, and L2 distance is a valid distance for them. \n\nIn Sec. 3, the method is not presented clearly and the notation is confusing. In Sec. 3.2.1, Eqn (8) is not an objective function and it is confusing how to optimize the generator using it. In Sec. 3.2.2, the notation is really confusing and I can hardly understand the proof the Theorem 2.\n\nThe experimental results are not solid where no well-known metrics, such as Inception Score, FID, are used to evaluate the generated samples. For compression rate, the size of bottleneck has not been mentioned above, and the experimental setting of each baseline is not ignored which makes the experimental results incomparable.\n\nOverall, this paper is not a qualified paper for ICLR.\n', 'The paper proposes a generative model that combines VAE and GAN. The main idea of the paper is to replace the standard normal distribution used in VAE with a normal distribution centered at a feature representation of the input image. In other words, the prior distribution is data adaptive. The paper compares the proposed generative model to DCGAN and EBGAN for image generation quality using the CelebA dataset and reports better human preference score.\n\nOverall, the paper is poorly written with incorrect technical descriptions and vague expositions.  The two baselines (DCGAN and EBGAN) are also quite out-dated. Beating these two baselines are insignificant, particularly there are GAN methods that  can generate high quality images without an encoder. For example, the Progressive GAN by Terro et. al. (ICLR 2018), SNGAN by Miyato et. al.(ICLR 2018), and GAN with zero-center gradient penalty by Mescheder et. al. (ICML 2018). The paper also fails to give a literature overview of effort in combining VAE and GAN. For example, Zhiting et. al. ICLR 2018 and Liu et. al. NIPS 2017.\n\nTechnical errors\n\n- In the related works section, the paper states that VAEs and GANs are both based on maximum likelihood. This statement is incorrect as GANs are based on distribution matching. \n\nVague exposition\n\n- In Section 2, the paper states that ""Larsen et al. (2015) used both VAE and GAN in one generative model. As they just mixed two models and did not analyzed a latent space, so that the manifold of data was hidden to us."" Isn\'t the data manifold in this case a multivariate Gaussian distribution. The paper fails to explain what it means by the sentence.\n\n- In Section 3.1, the paper states that ""Since any supervision is not in training process, the manifold constructed is hidden to us."" Again, the reviewer fails to understand what the paper means.\n\n- In Section 3.2.1, the paper states that ""it is not efficient to pre-train the G , because it depends on the parameters of the D."" This sentence is confusing. Isn\'t pretraining just meaning using a pretrained decoder weight to initialize G?\n\n\n\n\n\n', '* Pros\n- addresses an interesting problem\n- gives a nice approach to the problem\n- attempts to give some theoretical justification for the approach\n\n* Cons\n- I generally understand the approach, but details were not clear to me (specifics given below)\n- Sections 3.2.1 and 3.2.2 (the theoretical section), I found particuarly hard to follow.\n- The visual results are not particularly compelling, tbhough I suppose the panel liked them better than the competitor methods (Table 1).  For example ""inverse pale skin"" and ""pale skin"" in figure 5 do not convince me that the model understands skin. The skin and background seem to be changing colors together.  Might be worth including examples of the competitor approaches to show that they are even worse.\n\n* Comments and Questions\n- Throughout, you seem to assume binary-valued features, without ever explicitly stating this.  Would be helpful to state explicitly.\n- Would be useful to specify the codomain of the discriminator D(x) -- from the objective function, seems to be a value in (0,1).\n- In Section 3.2, you say: ""Besides, we add the encoder for LSC-VAE into LSC-GAN to make sure that the generated data actually have the desired features.  The encoder projects back to latent space so as to be trained to minimize the difference between latent space where data is generated and the space where the compressed data is projected.""  This seems like a fundamental change to training, much more than just initializing a GAN with a VAE-GAN. I think this should be elaborated on.  For example, what happens if you don\'t include the term with the encoder in (5).  Moreover, what goes wrong if you just try to train everything jointly using (5) without the VAE-GAN initialization step?\n- I have a very difficult time understanding 3.2.1, both the text and the equations.  e.g. what are the z_i\'s in (8)?  In 3.2.1, you say ""we pre-train G with the decoder of LSC-VAE"" -- this ""pretraining"" is what you refer to as initialization previously, I think?  Which seems also what section 3.1 is about?\n- I think some clarification would be nice in Section 3.2.2.  The conclusion of the section is that the ""proposed learning process is valid and efficient.""       What do you mean by ""valid"" and ""efficient""?  Perhaps you can explain that a bit more in words at the beginning of the section.  It\'s not entirely clear where your theory section connects to the objective function in (5). I don\'t really follow your argument for LSC_VAE being a good initializer in this section. In what sense is theorem 2 demonstrating ""efficiency""?  \n- In Theorem 1, you write p_data \\approx p_G.  I guess p_data is some unknown data generating distribution, rather than the empirical distribution of a training set?   I\'ve also never seen \\approx 0 used in formal mathematical statements and proofs especially when we\'re talking about \\approx 0 at infinitely many points.  Can this be elaborated on?  \n- In the end of section 3 intro paragraph you say ""The decoder of LSC-VAE is used in the generator (G) of LSC-GAN in the second phase."" By ""used"" do you mean used as the initialization of the generator G, when we switch to the LSC_GAN training?  Seems like it, but could be made more clear.\n- In Section 3, second paragraph, you say ""In the first phase, LSC-VAE is trained to project data into a specific location of latent space according to its features"".  It\'s not clear whether or not this ""projection step"" (which I guess is also called encoding step or the inference step depending on the context?) uses the explicit feature values in this step, or can only use the input (e.g. the image).  I really have the same question for the decoder/generator: does it explicitly use the feature values, or does it depend only on the latent variables?  My guess is that in both cases the feature values are not depended on directly, but I think this could be made more clear, one way or the other.\n- how did results vary when you deviate from using 20 latent dimension per feature?\n- You say ""As shown in Fig. 4, the change between images is natural so that we can say that the latent space of LSC-GAN is a manifold."" --- maybe a linear manifold?\n- Footnote 2 on page 4: This is confusing.  You are [basically arbitrarily] defining the conditional distribution on the latent space for any feature setting.  How can any particular distribution be ""correct"".\n- In the equations in (5), you\'re taking expectations over z_i, but don\'t you need to have an expectation over i (the feature assigments) as well?  Do you use the same feature distributions as you have in the training data?  Should be clarified.\n- Also, in (5), the expectation over z_i applies to the first term, as well as the z_i in G(z_i) in the second KL divergence term, right?  \n- In VAE the encoder typically produces the parameters of the Q distribution on the latent space.  What distribution does Q have and how are you parameterizing it?  Indendent Gaussians on each coordinate, each with its own mean and standard deviation?  or what? If you are allowing the encoder to take the feature values as input (which I don\'t think you are, but am not entirely sure of), does the encoder have to learn the means for each feature setting, or are you explicitly building those feature-based offsets into the encoder?\n']","[-80, -70, -20]","[-20, -20, 60]","[""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that they 'can hardly understand the motivation', finds the method and contributions 'not clearly represented', and concludes that 'this paper is not a qualified paper for ICLR'. There are no positive comments to balance these criticisms. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. Phrases like 'I can hardly understand' and 'the notation is really confusing' could be phrased more diplomatically. However, the reviewer does maintain some level of professional language, preventing the score from being lower."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states the paper is 'poorly written with incorrect technical descriptions and vague expositions', criticizes the outdated baselines, and points out several technical errors and vague explanations. There are no positive comments about the paper's content or contributions. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. Phrases like 'poorly written', 'incorrect technical descriptions', and 'fails to explain' are direct criticisms without much softening language. However, the reviewer does provide specific examples and explanations for their criticisms, which prevents the score from being even lower."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting problem', 'nice approach'), they express several significant concerns and criticisms. The cons outweigh the pros, with issues raised about clarity, theoretical justification, and the quality of visual results. The politeness score is moderately positive (60) as the reviewer maintains a professional and constructive tone throughout. They use polite language like 'would be helpful', 'would be useful', and phrase criticisms as questions or suggestions rather than direct attacks. The reviewer also balances negative feedback with positive observations, which contributes to the overall politeness.""]"
"['The authors propose a method to generate predictions under fairness constraints. The main idea is to take linear fairness constraints, and replace them with weak squared penalties plugged into the objectives, which supposed to help in cases where the loss function is not convex. The penalty coefficients are chosen by cross-validation, and the effectiveness of this approach is demonstrated empirically.\n\nIn Sec. 3.1, the authors point out several shortcomings of using linear penalties (using Lagrange multipliers) for non-convex losses. These seem valid. Sec. 3.2, however, is not clear on why exactly replacing the linear penalties with quadratic penalties solves these issues. I\'m hoping the authors can clarify the following points:\n\n1) The authors note that, for quadratic penalties, \\lambda->0 means no constraints, and \\lambda->\\infty means hard constraints. Isn\'t this also true for linear constraints?\n\n2a) Why do linear penalties have unique \\lambda_k for each constraint k, but the quadratic objective has only a single \\lambda for all constraints?\n\n2b) Why can CV over \\lambda be used for quadratic constraints - what is the justification? And, more importantly, why *can\'t* it be used with linear constraints? If it can, then this should be one of the baselines compared to in the experiments.\n\n3) What is the criterion optimized for by CV - accuracy or the constraints? Different parts of the paper give different answers to this question. For example, ""... may be easily determined via standard hyperparameter optimization methods"" vs. ""tuning \\lambda to directly satisfy the desired fairness metrics"". Or even more unclear - ""choose \\lambda ... so that the final solution gives the desired fairness-accuracy trade-off"". How is the desired trade-off defined?\n\n4) If there is a trade-off between fairness and accuracy, and no clear-cut criterion for evaluation is pre-defined, then the evaluation procedure should compare methods across this trade-off (similarly to precision-recall analysis).\n\n5) The authors differentiate between cases where the loss is either convex or non-convex. This is confusing - most losses are convex, and non-linearity appears when they are composed with non-linear predictors. Is this the case here? If so, the fairness constraints are no longer linear, and they\'re quadratic counterpart is no longer quadratic. It would be helpful if the authors specify where the non-linearity comes from, and what they assume about the loss and predictors.\n\n6) Why is it important to show that the quadratic constraints can be written as an expectation? Isn\'t the square of an expectation always an expectation of pairs? How does the double summation/integral effect runtime?\n\n7) It would be helpful if the authors differentiate between loss/constraints over the entire distribution vs. over a given sample set.\n\n\n', 'In this paper, the authors propose a method for optimizing quadratic penalties over non-convex loss functions. The authors motivate their approach by showing the complexity of training non-convex models with linear constraints and proposing a simpler method to introduce these constraints as regularization terms. Finally, they show how their proposed method compares to alternative solutions on a series of benchmarks.\n\nUse of double sampling method for estimating the loss and second order penalty appears to be novel, however, there is no discussion of the implications of using this method to train non-convex models --- one would suspect that use of double sampling may make the gradient descent susceptible to high variance. Simulation results in the paper only demonstrate how the use of the loss changes the solution but there is no discussion or experiments on complexity of training models that use this approach. This is particularly important because authors are claiming their method does not suffer from complexity issues which other methods suffer, but this claim is not supported by any evidence. For example, how many iterations were necessary to train the model? How sensitive is the training to initial conditions and changes in hyper parameters?\n\nAuthors motivate their method by pointing out the limitations of using fairness constraints in non-convex models, however, they don’t provide-sufficient evidence for why non-convex models are actually useful in their experiments --- the datasets they used are small and models that are actually convex may perform just as good or nearly as good as highly non-linear, non-convex models which authors are trying to use.\n\nOverall, I would recommend the authors to improve the presentation by providing more context for the use of double sampling method and other relevant works in this area (at least showing the impact of using double sampling on training). Moreover, given the datasets and scope of questions related to fairness they need to provide better experiments that motivate the use of their method (compared to simpler methods) or consider other problems where their approach could be more useful. Given these considerations I believe this paper does not meet the standards for publication.', ""In this paper, the authors propose a method to generate predictions under fairness constraints by optimizing quadratic penalties over non-convex loss functions. The main idea is to replace the linear fairness constraints by the second-order penalty. Meanwhile, an efficient method is derived to compute the gradients associated with the second-order penalties in stochastic mini-batch settings. Finally, the experimental results show the effectiveness of their proposed method empirically. \n\n(1) The authors argued that their experimental results in a more practical training procedure in non-convex, large-data settings. However, in Section 6, the sample size of the data-sets they used are small and the loss functions of learning models are convex. I think they need to provide more experimental results to make their proposed method more convincing. \n\n(2) It is a little difficult to follow the motivation and contributions of this paper. I would recommend the authors to improve the presentation by providing more context for the use of double integral or sampling method and other mostly relevant works in this area. \n\n(3) From the optimization viewpoint, the second-order penalty in Eq. (3) is convex with respect to d. Why replacing the linear penalties with quadratic penalties to solve the shortcomings of using linear penalties or Lagrange multipliers. Isn't the square of an expectation always an expectation of pairs? In other word, Eq. (4) is always equivalient to Eq. (7) without additional conditions?\n\n""]","[-20, -70, -20]","[60, 20, 50]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the authors' proposed method and its potential benefits, they raise several critical questions and points of confusion about the approach. The review is not overtly negative, but it does highlight multiple areas that need clarification or improvement.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I'm hoping the authors can clarify' and frame their critiques as questions or requests for clarification rather than direct criticisms. The language is constructive and focused on improving the paper rather than dismissing it.\n\nThe review begins with a neutral summary of the paper's content, followed by a series of specific, detailed questions and suggestions. This structure, combined with the polite phrasing, indicates a thorough and fair review process, even though the reviewer has identified several areas of concern."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer points out several significant shortcomings of the paper, such as lack of discussion on training complexity, insufficient evidence for claims, and inadequate experiments. The review concludes by stating that the paper does not meet publication standards. However, it's not entirely negative as the reviewer acknowledges some novel aspects of the work. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I would recommend the authors to improve' and provide specific suggestions for improvement, which is constructive. The language is not overly harsh or rude, but it's also not excessively polite, hence a slightly positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the authors' proposed method and its effectiveness, they express several concerns and recommendations for improvement. The reviewer points out limitations in the experimental results, difficulty in following the paper's motivation and contributions, and questions about the methodology. These critiques suggest the reviewer is not fully convinced by the paper in its current form.\n\nThe politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I think they need to provide,' 'I would recommend,' and frame their concerns as questions or suggestions rather than harsh criticisms. The language is respectful and focuses on improving the paper rather than attacking the authors' work.\n\nThe scores are not at extremes because while there are criticisms, they are presented constructively, and the reviewer does acknowledge some positive aspects of the work. The politeness score is higher than the sentiment score because the reviewer's tone remains courteous even when expressing concerns about the paper's content.""]"
"['This paper introduces two methods of adjusting the overconfidence error for predictions on novel data. The ensemble distillation approach is to penalize the distillation loss on a potentially unlabeled general dataset.  The second approach (NCR) detects the novelty first and reweigh the prediction based on the familiarity to training data. \n\n*stationarity*\nFrom the statistical perspective, the overconfidence of extrapolation can kick in from two sources: a)the epistemic uncertainty.  The point estimation of softmax ignores the uncertainty of prediction at all.  A full Bayesian approach will remedy this though computationally impractical.  b) the generative distribution p(y|x) might not be identical on training data and test data.  To see the difference, if the training sample size goes to infinity, the uncertainty in a) will go to zero, but b) may still exist.  Section 3 assumes the invariant p(y|x) in novel data.  But theoretically, both methods do not require such invariance?\n\nSlightly related here, there can be novel data for classification, and in principle, there can also be novel data for novelty detection?  That will make NCR fail.\n\n*why the distillation helps uncertainty adjustment*\nI am not convinced how the g-distillation works for this task.  In the extreme case if the ensemble model itself is totally wrong for novel data and the unlabeled general data used in training, how can I learn any extra uncertainty information from that noise? To be fair, when the temperature goes high enough, the ensemble will make uniform prediction and then the distillation loss is merely a loss function that enforces uniformity.  If I replace the ensemble softmax by a uniform prior for unlabeled general data, do I achieve the same effect?  That is essentially the same regularization as method 2, except g-distillation is on logit scale.  \n\n*robustness-accuracy tradeoff*\nThe experiments do not reveal too much robustness-efficiency conflict, as the new methods still perform good enough on familiar dataset. Indeed they can be even better than the baseline in E99 loss. Does it suggest the over-confidence is even a concern for familiar data/ iid data?\n\nIn general, the paper is well-written and well-motivated. It would be more interesting to make some theoretical explanation why/when this simple approach works.   I would recommend a weak accept at this point.\n', 'The authors proposed two methods to deal with estimating classification confidence on novel unseen data distributions. The first idea is to use ensemble methods as the base approach that helps in identifying uncertain cases and then using distillation methods to reduce the ensemble into a single model mimicking behavior of the ensemble. They propose a generalization of this idea, that is to also perform distillation on a more generic unsupervised data distribution (than the supervised one that is used in training the ensemble). It is not clear whether this distribution should overlap with the novel distribution as a requirement or not. The second idea is to use a novelty detector classifier and weight the network output by the novelty score. \nMy major concern is that the comparison doesn\'t seem to be sufficiently comprehensive. The main method that is used to compare against is (Kendall & Gal, 2017), in which the main aim seems to be reducing uncertainty and improving generalization error under i.i.d. assumptions. This is different from the main focus of the paper, which is to better estimate classification confidence on novel data distributions. It seems that other approaches, such as ""Calibration methods"" (Guo et al. 2017) are better aligned with the focus of the paper, and should be considered instead. \nMy other concern is that the novelty seems to be marginal: either extending distillation methods in a very natural form, or weighting the network output using a novelty detector. ', 'The paper proposes two ideas for reducing overconfident wrong predictions:\n- Method 1: “G-distillation” of an ensemble with extra unsupervised data\n- Method 2: Novelty Confidence reduction (NCR) using novelty detector\n\nThe paper is well-written and was a pleasure to read. In particular, I really enjoyed reading the introduction and related work. My main concern is that some of the contributions claimed were already shown in previous work (see method 1 below for details), and the novelty feels a bit limited. That said, I like the simplicity of the method and think that the extensive experiments on a variety of datasets and architectures is useful to the community.\n\nMethod 1:\n- The paper claims “Draw attention to a counter-intuitive yet important problem of highly confident wrong predictions when samples are drawn from a unknown distribution that is different than training” as one of the contributions. Note that previous work has already shown that single models are overconfident on unknown classes and ensembles are less overconfident, e.g. see Section 3.5 of the paper: \nSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\nhttps://arxiv.org/pdf/1612.01474.pdf\n- If I understand correctly, the key difference is that the proposed method 1 also uses ensemble prediction on unlabeled data for distillation, which could make the distilled model more robust. Ensembling on unlabeled data for robustness does seem novel to me, however, the text needs to be updated to clarify the novelty.\n\n\nMethod 2:\n- By off-the-shelf, do you mean a pre-trained network released by ODIN? Or did you train ODIN-based novelty detector on your dataset? \n- There was a recent paper that proposed to reduce confidence on novel inputs, which might be worth discussing:\nReliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors\nhttps://arxiv.org/pdf/1807.09289.pdf\n\n\nMinor issues:\n- Figures 3,4 are a bit small and hard to see\n']","[20, -50, 20]","[60, 0, 80]","[""The sentiment score is slightly positive (20) because while the reviewer points out some concerns and areas for improvement, they ultimately recommend a 'weak accept' and acknowledge that the paper is 'well-written and well-motivated'. The overall tone is constructive rather than critical. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions rather than direct attacks, and concludes with positive remarks. The reviewer maintains a professional and courteous tone, even when expressing doubts or requesting clarifications."", ""The sentiment score is -50 because the reviewer expresses major concerns about the paper's comparison methodology and novelty, which indicates a negative sentiment. However, it's not extremely negative as the reviewer acknowledges the authors' proposed methods and doesn't outright reject the paper. The politeness score is 0 (neutral) because the reviewer uses professional and objective language without being particularly polite or rude. They directly state their concerns without using overly harsh language or personal attacks, but also without using explicitly polite phrases or softening their criticisms."", ""The sentiment score is 20 (slightly positive) because while the reviewer expresses enjoyment in reading the paper and appreciates its simplicity and extensive experiments, they also raise concerns about the novelty of some contributions. The overall tone is constructive but with reservations. The politeness score is 80 (quite polite) due to the reviewer's use of positive language ('well-written', 'pleasure to read', 'really enjoyed'), constructive criticism, and respectful suggestions. The reviewer maintains a professional and courteous tone throughout, even when pointing out potential issues.""]"
"['The authors proposed a generalized adaptive moment estimation method(Game). Compared to the existing methods AMSGrad and PAdam, the new method Game tracks only two parameters in iteration and hence saves memory. Besides, they introduced a additional tuning parameter $q$ to track the q-th moment of the gradient and allow more flexibility. The authors also provided the theoretical convergence analysis of Game for convex optimization and smooth nonconvex optimization. Their experiment shows Game may produce better performance than AMSGrad and PAdam with a little bit sacrifice of convergence speed. Game is a promising alternative method for training large-scale neural network.', ""Pros:\n1. The algorithm saves about 1/3 memory consumption compared with AMSGrad.\n2. The authors give the proof that the generalized algorithms have the same convergence rate with weaker assumptions.\n\nCons:\n1. All the experiments are based on CNN. There are no results based on modern deep neural networks such as Residual Nets and Dense Nets, where it is obvious to see Adam suffers from poor generalization. \n\n2. Algorithms like SGD with momentum and Adam should be included for comparison.\n\n3. This framework introduces two more hyper-parameters p and q, which makes it more difficult for practitioners to tune.\n\nAlthough this framework has proven convergence in both convex and non-convex smooth cases, the experimental evidence is limited. In addition, the proof strategy is not novel enough, Theorem 1 is similar to Theorem 4 in AMSGrad paper and Theorem 2 is similar to Theorem 3.3 in Zhou et al's paper."", ""Summary\n------\n\nThe authors propose an adaptation of the Adam method, with the AMSGrad correction and an additional parameter to p to exponentiate the diagonal conditioning matrix V (Padam).\n\nThe proposed method changes two aspects: first, there is no need to retain two version of the rescaling matrix v, where amsgrad and Padam keeps the last monotone \\hat v)t and non-monotone version v_t. Secondly, a new parameter q is introduced, that replaces the q=2 in the moment estimation phase of (P)Adam.\n\nA regret analysis is proposed in the convex case, while a vanishing bound on the gradient is derived in the non-convex smooth case.\n\nReview\n------\n\nAlthough improving optimization methods is certainly important for the machine learning community, the reviewer have strong concerns about this paper.\n\nFirst of all, the paper is hard to read as it contains too many approximations. What does 'SGD is known to work reasonably well regardless of their problem structure' means ? Same thing for 'Its performance deteriorates when the gradients are dense due to a rapid decay of the learning rates.' The authors uses many times elliptical discourse to detail the course of their analysis, which is non informative: for instance, 'one can easily derive the upper bound expression', and 'It is not difficult to conclude that when G_t [...]'. This level of writing is not professional. Some completely irrelevant argument are proposed to justify the method: 'For instance the extension from l_2 norm to l_p norm and generalization from Cauchy-Schwart to Holder inequality'.\n\nThe reviewer has interrogations about the relevance of the proposed algorithm. The additional parameter q needs to be tuned, which carries only the promise of further overfitting. I would have been convinced by an sequence of experiment where q is set automatically by considering a validation set, and then tested on a left out test set. However, the authors report only the results for the best q, with non significant differences (and not quantified, there is no result tables). Using q=2 at least made sense from the point of view of empirical Fisher matrix approximation.\n\nThe review also have several concerns aout the correctness of the proposed arguments. First, the major argument of memory usage stems from 1) a miscalculation and 2) a misunderstanding of memory bottlenecks in deep learning. 1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. In contrast, the proposed model do not track v_{t-1}: this amounts to a memory saving of 20% considering all model related parameters. 2) more importantly, the most important memory usage in deep leaning comes from the activations that need to be kept in memory during the forward pass to perform the backward pass. Even the biggest model are less than 1GB, and most of the memory used during training is dedicated to intermediary activations. This makes the major argument of the paper less convincing, and misleads the reader.\n\nSecond, even when disregarding the slightly abusive assumptions over the iterate sequences, that are common in the adaptive stochastic optimizers community, I think that the bound proposed in theorem 1 is non informative, as the second term behaves like T sqrt(T) assymptotically, due to the presence of 1 / \\alpha_t. This does not show the convergence of averaged regret R_T / T.\n\nRegarding the experiment section, I am afraid that testing a new optimizer over MNIST and CIFAR is not enough to show the relevance of the method for the whole deep learning community. An eperiment over a non-toy dataset (eg ImageNet), and on non computer-vision dataset (eg from NLP) would be a minimum, besides the overfitting concern described above.\n\nIn conclusion, it is the reviewer's opinion that significant rework in term of presentation and strong improvement of the experiment section to make the case for the Game optimizer.\n\nMinor comments\n------------\nTable 1: what do you bound when you compare results ? I think there is a typo in Zhou et al. result: 1/2 should read p.\n\nEq (1): it is rather surprising to use x_t as the model parameters in the ICLR community. \n\np 7: the dimension d could be larger than T when training large-scale neural networks: how does it relate to comparing sqrt(dT) to (dT)^s ?""]","[80, -20, -80]","[50, 50, -20]","[""The sentiment score is 80 (positive) because the reviewer expresses a favorable view of the proposed method (Game). They highlight its advantages over existing methods, such as memory efficiency and flexibility. The reviewer also notes that Game shows better performance in experiments and describes it as a 'promising alternative method'. The politeness score is 50 (somewhat polite) because the language used is professional and respectful, without any negative or critical remarks. However, it doesn't contain explicitly polite phrases or compliments, maintaining a neutral, objective tone typical of scientific reviews. The reviewer presents the information in a straightforward manner without using overly formal or particularly courteous language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they express more concerns ('Cons') and conclude that the experimental evidence is limited and the proof strategy is not novel enough. The overall tone suggests that the reviewer is not fully convinced by the work. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They present both pros and cons objectively, without using harsh or dismissive language. The use of 'Although' to introduce criticisms softens the negative feedback, maintaining a respectful tone."", ""The sentiment score is -80 because the reviewer expresses 'strong concerns' about the paper and lists multiple significant issues, including poor readability, questionable relevance, potential incorrectness, and insufficient experiments. The overall tone is highly critical, with very few positive remarks. The politeness score is -20 because while the reviewer maintains some level of professional language, there are instances of blunt criticism ('This level of writing is not professional') and dismissive language ('completely irrelevant argument'). The reviewer does not use overtly rude language, but the tone is often harsh and lacks the softening typically found in more polite academic discourse. The review also lacks positive reinforcement or encouragement, which contributes to its somewhat impolite tone.""]"
"[""The papers studies estimators of gradients taken from expectations with respect to the distribution parameters. The paper has studied two main types of estimators, Finite Difference and Continuous Relaxation. The paper made several improvements to existing estimators. \n\nMy rating of the paper in different aspects (quality 6, clarity 8, originality 6, significance 4). \n\nPros: \n1. The paper has made a nice introduction of FD and CR estimators. The improvements over previous estimators are concrete -- it is generally clear to see the benefit of these improvements. \n\n2. The first method reduces the running time of the RAM estimator. The second method (IGM) reduces the bias of GM estimator. The first improvement avoids many function evaluations when the probability is extreme. The second improvement helps to correct bias introduced by continuous approximation of \\zeta_i itself. \n\nCons: \n1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper. \n\n2. All these improvements are not very significant considering a few previous papers on this topic. Some arguments are not rigorous. (see details below)\n\n3. A few important papers are not well discussed and omitted from the experiment section. \n\nDetailed comments\n\n1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.  \n\n2. In the equation 7 and above: what does 4 mean? When beta \\neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)\n\n3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]? \n\n4. In the PWL development, the paper argues that we can choose alpha_i \\approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.\n\n"", 'The paper proposed a modification to RAM that allows us to trade decreased computational cost for increased variance. It also proposes an improved continuous relaxation (ICR) estimator to reduce the bias of CR, which is extended to categorical variables.\nThe proposed piece-wise linear relaxation (PWL) can be considered as the inverse CDF of the random variable is very interesting. The ICR estimators can also be extended to categorical variables. \nThe paper is well written. I have some questions:\n1.\tHow does the dimension of the variables affect the bias and variance of the proposed estimator?\n2.\tDose the proposed estimators applicable to hierarchical models with multi-discrete latent variables?\n3.\t What’s the performance of the proposed method compared with the others in terms of running time?\n', 'After revision:\nThe authors have addressed all points in my review. Although I will not be increasing the score, these fixes certainly increase the confidence of my evaluation and I think it deserves to be accepted.\n\n====================\n\nSummary: This paper analyzes finite-difference and continuous relaxation gradient estimators for discrete random variables and from their analysis develop improvements to these existing methods. They empirically demonstrate the improvement by evaluating the gradient estimators on toy tasks and an autoencoding task.\n\nWriting: I found this paper very well written and explained. It covered an extensive background concisely while introducing all necessary ideas to understand the contributions of the paper.\n\nComments: Overall, I found the ideas presented in this paper interesting and novel, and results sufficiently strong to support the ideas. Though the contributions are not groundbreaking, they will certainly be useful to researchers in this space. I have some minor comments relating to notation and related work.\n\n- I found the notation in Section 3.2 to be a little confusing, namely that $\\zeta$ appears as both a random variable and a continuous function (that takes in one variable in the paragraph after eq11, but takes in two variables in eq15). I understand that the authors may have done this to suppress extra notation, but I found this section harder to understand than the rest due to this choice. There is also a small typo in eq2 where the $\\phi$ from $l_\\phi$ is dropped.\n\n- I think it would be useful to compare IGSM and PWL against a score-function gradient estimator (maybe REBAR, given the similarity in experiment setup). The authors do contextualize the line of work concerning score-function gradient estimators. However, since SF estimators are unbiased but high variance and the authors aim to reduce bias at the cost of variance, I think evaluating SF baselines will better contextualize the tradeoffs made in this paper.']","[-20, 70, 70]","[60, 80, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), there are more 'Cons' listed, and the overall tone suggests room for improvement. The reviewer mentions that the improvements are 'not very significant' and some arguments are 'not rigorous'. The politeness score is moderately positive (60) as the reviewer maintains a professional tone throughout, using phrases like 'nice introduction' and 'clear to see the benefit'. They also frame criticisms constructively, such as 'A few important papers are not well discussed' rather than using harsh language. The reviewer provides detailed comments and asks questions, showing engagement with the paper rather than dismissing it outright."", ""The sentiment score is 70 (positive) because the reviewer expresses interest in the paper's proposed modifications and improvements, describing them as 'very interesting'. The overall tone is appreciative of the paper's contributions and its writing quality. The reviewer also poses questions, which indicates engagement with the content rather than criticism. The politeness score is 80 (polite) due to the respectful and professional language used throughout. The reviewer acknowledges the paper's merits ('well written') and frames their inquiries as questions rather than demands or criticisms. The use of phrases like 'I have some questions' further contributes to the polite tone. The absence of negative or harsh language also supports this high politeness score."", ""The sentiment score is 70 (positive) because the reviewer states that the authors have addressed all points, expresses increased confidence in their evaluation, and recommends acceptance. The overall tone is positive, with praise for the paper's writing, explanation, and contributions. The politeness score is 80 (polite) due to the reviewer's respectful and constructive language throughout. They offer praise where due and frame their suggestions and critiques in a considerate manner. The reviewer maintains a professional and courteous tone, even when pointing out minor issues or suggesting improvements.""]"
"['Given an MDP <S, A, T, R>, the paper suggests to learn both the optimal Q function of that MDP (denoted Q^+), but also that of the MDP <S, A, T, -R> (denoted Q^-). The basic idea is that min_a Q^-(s, a) could be a good action for the initial MDP. Based on this idea, the authors propose to combine Q^+ and Q^- with a linear combination in order to obtain what they call a hybrid policy. \n\nThe proposed idea is indeed interesting and I find the experimental results surprising. It is not clear to me why the policy obtained from Q^- does better than Q^+. Theoretically, this should not happen: if we have the exact optimal Q function, Q^-, for <S, A, T, -R>, the policy defined by argmin_a Q^-(s, a) in every state s may be suboptimal in <S, A, T, R>. Is there a good conjecture/explanation for why the policy induced by Q^- works so well in 2(a) and (b)?\n\nThe authors chose to report the results using off-line training, which seem to favor their proposition. What are the results for on-line training?\n\nIn the experimental part, I think the authors should also report the results of the method that consists in learning two Q^+ and combining them with an average. This baseline would help understand if the good performance of hybrid policies really comes from learning Q^-.\n\nObtaining hybrid policies faces one important issue, which is the need to perform two actions in the environment in a given state, one for Q^+ and the other for Q^-. Therefore, the proposition seems to be doable only when one has access to a simulator.\n\nThe writing is generally clear, but the paper should be checked for typos.', 'This paper proposes that, in addition to learning the normal action values, an RL agent should also learn the action values for an alternate “inverse” problem consisting of the same transitions as the original MDP and the negative of the original rewards. The intuitive argument is that the values for the inverse problem clearly identify what actions should not be taken. Results are presented on OpenAI Gym problems in which the new method performed better than conventional methods. \n\nThe paper is not yet ready for publication for many reasons. First, the idea is not presented clearly, and it is not clear why it ever could be sensible. The inverse problem has a different solution than the base problem. Its solution would appear to have an arbitrary relationship to the base problem’s solution. The two optimal policies may choose different actions, as suggested in the text, but this is not necessarily true; in some states the two policies may choose the same action. I don’t see how anything can be said in general, and no significant theoretical results are presented. (They do prove a form of convergence on the inverse problem, but this is not a new result; the inverse problem is just another problem and needs no new result.)\n\nThe new hybrid method is never fully explained (e.g., the reader has to guess at what Q^H is). But by combining the solutions to the base and inverse problems in some way, it seems inevitable that the final optimal policy would be changed. Suppose the function approximation is completely successful and the correct values are exactly found. Then those for the base problem would give the optimal policy. Any alteration of them by the correct action-value function for the inverse problem could only make them worse and could only cause them to produce worse (or the same) behavior. There is no room for improvement and so this technique could only make things worse asymptotically if the function approximation is completely successful. This is the only thing that I see that can be clearly said about the new technique, and of course it is not a good thing.\n\nThere are results presented, but they are not well done; they provide no significant evidence for any conclusion. The methods are not completely presented, and the results seem to be for a single run, in which case any relative ordering of the methods could be obtained.\n\nGenerally, the paper is unfortunately poorly written. The grammar is not good. The notation is unnecessarily complex and confusing. The citations are made in an unusual, poor way.', 'The paper proposes a method for improving the stability of reinforcement learning with value function approximation, e.g., deep Q-learning. The key idea is fitting a Q function to rewards, fitting another Q function to negative rewards, then estimating Q values using a linear combination of the two Q functions. The method is applied to DQN, double DQN, and on-policy actor-critic on the CartPole, Mountain Car, and Pendulum tasks in OpenAI Gym.\n\nThe writing isn\'t clear, especially in the introduction. Phrases like ""risky"", ""badness of a state"", and ""inverse policy"" are used without definition.\n\nThe experiments only test one value of \\lambda. Since \\lambda is the one hyperparameter that controls the degree to which the inverse rewards influence the Q value estimates, I think it is critical to test the performance of the proposed method under various values of \\lambda (e.g., by sweeping the unit interval in increments of 0.1).\n\nOne of the central claims is that the proposed Q value estimator gives more accurate estimates of returns than the estimators used in previous deep Q-learning methods. However, the experiments never compare the predicted Q values to the true values, as is done in [3].\n\nThe experiments only evaluate the proposed method on the CartPole, Mountain Car, and Pendulum tasks, which have very small action spaces. I suspect the benefit of the proposed method will be smaller in environments with a larger number of possible actions, since the inverse policy may fail to accurately estimate the values of actions that are neither the best nor the worst at any given time. \n\nOne of the central claims is that the proposed method improves the stability of Q-learning, but it is unclear how many random seeds were tested in Figure 2 and Table 2. It appears that only the data from one training run was used, and the reported standard deviations are computed using the last 10% of episodes in that single training run. Furthermore, the curves are smoothed using a moving average with a window size of 100 episodes. Together, these two details make it extremely difficult to evaluate the claim that the proposed algorithm is more stable, and also makes it difficult to evaluate the significance of the differences between the method\'s performance and the baselines. [1] shows how results on a small number of random seeds tend to not be reproducible.\n\n[1] https://arxiv.org/pdf/1709.06560.pdf\n[2] http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf\n[3] https://arxiv.org/pdf/1509.06461.pdf']","[20, -80, -30]","[60, -20, 20]","[""The sentiment score is slightly positive (20) because the reviewer finds the idea interesting and the experimental results surprising. They acknowledge the potential of the proposed method but also raise several questions and concerns. The overall tone is constructive rather than dismissive. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions, and acknowledges the positive aspects of the work. They use phrases like 'I find' and 'I think' to soften their critiques, and offer constructive suggestions for improvement rather than harsh criticism."", ""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'not yet ready for publication for many reasons,' criticizes the clarity of the idea, questions its sensibility, and points out lack of significant theoretical results. The reviewer also criticizes the explanation of the new method, the quality of the results presented, and the overall writing quality. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. Phrases like 'unfortunately poorly written,' 'grammar is not good,' and 'notation is unnecessarily complex and confusing' are particularly blunt criticisms. The reviewer does not attempt to soften the criticism or offer encouragement, which contributes to the somewhat impolite tone."", ""The sentiment score is -30 because while the reviewer acknowledges the paper's proposal of a new method, they express several significant concerns and criticisms. These include unclear writing, limited experimental testing, lack of comparison to true Q values, and potential issues with the stability claims. The tone is more negative than positive, but not entirely dismissive. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or personal criticisms. They offer constructive feedback and suggestions for improvement, such as testing various values of λ and comparing predicted Q values to true values. The reviewer also cites relevant literature to support their points, which adds to the polite and professional tone. However, the score is not higher as the review doesn't include any explicitly positive or encouraging statements.""]"
"[' This paper proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN). And then the landscape state is utilized to training the neural network by using the standard quantum amplitude amplification method.   \n\nAlthough this idea is interesting, I trend to reject this submission as I think its presentation is unclear and the technical detail is a little difficult to follow. So, the correctness and soundness of this work is difficult to verify. I urge the authors to revise their draft to provide more and clearer technical details.\n\nDetailed comments and questions:\n\nCould the authors further point out that what the scope of the binary features are, {0, 1} or {-1, +1}? To my understanding, it should be {-1, +1}, or the corresponding variables are always +1. In addition, the construction of the “multiplying values by binary weights” module implies that the value should take -1 or +1, rather than 0 or 1. However, at the bottom of page 5, the authors claim that the binary values take +1 and 0. Could the authors clearly explain the term “parameter” and “value”?\n \nCould the author further explain how to construct the majority activation function?\nIn the part of “calculating accuracy”, the authors mention that “running the QBNN with the weights in superposition for each point in the training set separately” and “there are N qubits containing the prediction of the QBNN”. To my understanding, there are 3 qubits representing the 8 weights, several qubits representing the input values, and N qubits representing the predictions. But how to construct the final landscape state to be optimized with these qubits?\n \nCould the author explain intuitively the main idea of the amplitude amplification method? Specifically, what is the relation between the qubits representing parameters and the qubits presenting prediction results?\n \nDuring the amplitude amplification process, the probabilities change periodically. How to select the best number of steps k in advanced if we do not known the best parameter? Or how to judge if the training is success?\n \nOverall speaking, I think this paper is interesting. However, the presentation is unclear and I suggest the authors to revise their draft by providing more technical details.', 'Review of ""Neural Network Cost Landscapes as Quantum States""\n\nPaper summary:\n\nThe paper proposes a new algorithm ""quantum amplitude amplification""\nfor training and model selection in binary neural networks. (in which\nboth weights and activations are restricted to the set -1, 1)\n\nSection 2 references related work and gives some motivation, that some\nquantum algorithms scale better (in terms of big-O notation) than\nclassical algorithms.\n\nSection 3 explains the basics of quantum computing (qubits and quantum\ngates).\n\nSection 4 explains the proposed method. There are two toy\nproblems. The binary neural network has 8 weight parameters. There are\nhelpful Figures 1-2 which explain the network structure and the\nquantum circuit.\n\nSection 5 explains the results of using the proposed method in a\nquantum computer simulator (not an actual quantum computer). On the\ntwo toy problems the paper observes quadratic speedups with respect to\na brute force search.\n\nComments:\n\nA strong point is that the paper is very well-written and easy to\nunderstand. \n\nHowever there are several weak points which should be addressed before\npublication. Major weak points are (1) only (noiseless?) toy data sets\nare used, (2) some terms in the paper are unclear/undefined, and (3)\nresults are unconvincing.\n\nIt is not clear that this article should be published in the machine\nlearning literature. One of the hallmarks of machine learning is a\nfocus on algorithms for real data sets / problems. In contrast the\nfocus of this paper is quantum computations on toy data /\nproblems. Maybe this paper would be better suited for publication in\nthe quantum computation literature?\n\nThe toy problems are explained in section 4.2. Is there any noise or\nare these noiseless simulations? How does your model/algo perform as a\nfunction of the noise level? How many data points did you simulate\nfrom the model? (e.g. what is the number of observations in the training set?)\n\nThe paper uses the terms ""cost landscape"" and ""meta-cost landscape""\nwithout explicitly defining them. Equations should be added to clarify\nthese terms.\n\nResults could be made more convincing by\n1. using a real quantum computer.\n2. using real data rather than toy data.\n3. adding error bars or confidence intervals to Figures 4-5.\n4. using a more appropriate baseline -- why not try the algorithms mentioned in section 2.1?\n\nFigure 3 could be clarified by providing ticks and labels on the x\naxes.\n\n', 'Neural Network Cost Landscapes as Quantum States\n\nThe authors describe a method where a deep learning framework can be quantised, this is done by considering the two state form of a Bloch sphere/qubit and mapping binary neural network. onto the quantum object creating a quantum binary neural network\n\nI have to say I liked the paper, it is indeed novel and I haven’t seen this anywhere else. More then that, it addresses the quantum aspects of deep learning which has only recently started getting so much attention rather the regular machine learning algorithms. And such I think it’s a good fit for ICLR.\n\nWith that I have a few concerns/ nitpicking issues I would like the authors to address if possible\n1. While authors show basically a discreet network (much like Soudry’s work) there has also been recently a show of continues variable networks (https://arxiv.org/pdf/1806.06871.pdf), how does your scaling compare to that ? Could one think of this is the continuum limit (albeit you showed a *very* small system) of your model ?\n2. As a general style remark there is a lot of introduction on quantum information and I wondering if authors could just reference classical text such as Isaac Chuang, Michael Nielsen or for the CS flavour Classical and Quantum Computation  ? I would have liked to see all 3.1,3.2 maybe in an appendix and a lot more details on the experiments and setup for example \n3. At the end of section 3.2 authors mention the lack of correspondence principle in some quantum systems, I would be happy for a refinement in the aspect of quantum computing that is true and also in general quantum mechanics but there is a huge body of work and an entire field dedicated to just that, settling the difference in correspondence principle and it’s meaning (for example Chaos and the semiclassical limit of quantum mechanics (is the moon there when somebody looks?) by Michael Berry. While this is defiantly far from a deal breaker I would be happy for a bit more clarification on subtle difference. My guess is that authors are referring to the fact that an essential part of quantum computing is the lack of correspondence that can also be taken advantage of for quantum parallelism/ quantum speed ups ?\u2028Same for the last paragraph of 3.1.\n4. In the method section  all of this procedure is described for a fault tolerant machine, can you say something about coupling to error correction codes for near term quantum devices ?\n5. On the same note, there is a feeling that non linearities are swept under the rug, do you have a hunch how can one use non linear activation functions ? (General question, you don’t really have to answer and can take the fifth )\n6. In the problem statement (4.2)  x is in fact the binary representation of the qubit state ?\n7. In your method Fig. 2, should there be some sort of measurements ?\n8. Can you elaborate what exactly goes into the U gates and how are they constructed in Fig.2 ? Or is it some  oracle model that can do all the actions \n9. You report a quadratic speedup, is there some relationship to random walk  is this in fact a case of a RW search over the parameter space ?\n']","[-30, -50, 70]","[60, 50, 80]","[""The sentiment score is -30 because while the reviewer finds the idea interesting, they 'trend to reject this submission' due to unclear presentation and difficulty in verifying the work's correctness. This indicates a generally negative sentiment, but not extremely so due to the acknowledgment of the interesting concept. The politeness score is 60 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'Could the authors further explain...' and 'I suggest the authors to revise...', which are polite ways of requesting clarification and improvements. The reviewer also acknowledges the interesting aspects of the paper, which adds to the politeness. However, it's not extremely high on the politeness scale as it's a professional review maintaining some critical distance."", ""The sentiment score is -50 because while the reviewer acknowledges some strong points ('very well-written and easy to understand'), they also highlight several major weak points and suggest that the paper might not be suitable for publication in its current form or in the intended field. The overall tone leans towards negative, but not extremely so.\n\nThe politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'should be addressed' and 'could be made more convincing' rather than harsh criticisms. The reviewer also starts by acknowledging a strong point before moving on to criticisms, which is a polite approach. However, the score is not higher because the review is direct in its criticisms without excessive softening language."", ""The sentiment score is 70 (positive) because the reviewer explicitly states 'I liked the paper' and describes it as 'novel' and a 'good fit for ICLR'. They also mention that it addresses an important and timely topic. The score is not higher because the reviewer does raise several concerns and requests for clarification. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, frames their concerns as requests ('I would like the authors to address'), and uses phrases like 'I would be happy for' when asking for clarifications. The reviewer also acknowledges that some of their points are 'nitpicking' and not 'deal breakers', showing consideration for the authors' work. The tone is consistently constructive and collegial.""]"
"['This paper proposes a novel parameter-efficient generative modeling approach that is based on the Monge-Ampere equation. In the proposal, a feed-forward neural network is trained as an ODE integrator which solves (2) and (3) for a fixed time interval $[0,T]$, so that the distribution $p(x,t)$ at time 0 is a simple base distribution such as a Gaussian, and that at time $T$ mimics the target distribution.\n\n[pros]\n- The proposal provides a parameter-efficient approach to generative modeling, via parameter sharing in the depth direction.\n- I think that the idea itself is quite interesting and that it is worth pursuing this direction further.\n\n[cons]\n- The Applications section is not convincing enough to demonstrate usefulness of the proposal as an approach to generative modeling.\n- How the gradient-based learning in the proposal behaves is not discussed in this paper.\n\n[quality]\nHow the gradient-based learning in the proposal behaves is not discussed. I understand that the non-convex nature of the loss function poses problems already in the conventional back-propagation learning of a multilayer neural network. On the other hand, in the proposal, the loss function (e.g., (4)) is further indirectly parameterized via $\\varphi$. It would be nice if the parameterization of the loss in terms of $\\varphi$ is regular in some sence.\n\n[clarity]\nDescription of this paper is basically clear. In the author-date citation style employed in this paper, both the author names and publication year are enclosed in parentheses, with exception being the author names incorporated in the text. This paper does not follow the above standard convention for citation and thus poses strong resistance to the reader. For example, in the first line of the Introduction section, ""Goodfellow et al. (2016)"" should read ""(Goodfellow et al., 2016)"".\n\n[originality]\nThe idea of considering the Monge-Ampere equation in its linearized form to formulate generative modeling seems original.\n\n[significance]\nIn the experiment described in Section 4.1, it is not clear at all from the description here whether the learned system is capable of successfully generating MNIST-like fake images, which would question the significance of the proposal as a framework for generative modeling. It is well known that the KL divergence $D(P\\|Q)$ tends to put more penalty when $P$ is large and $Q$ is small than the opposite. One can then expect in this experiment that it tolerates the model, appearing as $Q$ in $D(P\\|Q)$, to put weights on regions where the data are scarce, which might result in generation of low-quality fake images. It would be nice if the authors provide figures showing samples generated via mapping of Gaussian samples with the learned system.\nAlso, in the experiment described in Section 4.2, I do not see its significance. It is nice to observe in Figure 4 that the loss function approaches the true free energy as well as that the snapshots generated by the model seem more or less realistic. My main concern however is regarding what the potential utilities of the proposal are in elucidating statistical-physical properties of a system. For example, it would be nice if the proposal could estimate the phase-transition point more easily and/or more accurately compared with alternative conventional approaches, but there is no such comparison presented in this paper, making the significance of this paper obscure.\n\nMinor points:\n\nThe reference entitled ""A proposal on machine learning via dynamical systems"" would be better cited not as ""E (2017)"" but rather as ""Weinan (2017)"".\n\nPage 6, line 10: the likelihoods of these sample(s)\n\n----Updated after author feedback----\nUpon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.\nI still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold.', 'This paper proposes a continuous-time gradient flow as an alternative normalizing flow. The formulation is motivated from optimal transport and the Monge-Ampere equation. Symmetry constraints motivated by the use of a potential function can be enforced during training.\n\nI\'m surprised [1] was mentioned only for their backpropagation scheme. Much of this paper is similar to theirs, such as Eqs (2) and (3) being the ""instantaneous change of variables"" in [1], the mention of efficient reversibility, experimenting with forward and reverse KL objectives, and parameter efficiency. \n\nGiven the different angle of approach in this work, I\'m willing to believe some of this is independently done. This work contains interesting derivations and a different parameterization, with enough contributions to potentially be interesting in its own right. However, I firmly believe in proper attribution and believe [1] should at least be mentioned in the introduction and/or theoretical background. \n\nPros:\n - The potential function and the resulting gradient flow parameterization is interesting.\n - Parameterizing a potential function motivates some symmetry constraints.\n - Interesting application of normalizing flows to the Ising model.\n \nCons:\n - Paper presentation needs some work on clarity.\n - Relation to existing work needs to be clarified.\n - Experiments lack ablations and proper comparisons. e.g. the effect of using symmetry constraints, the effect of using a gradient flow parameterization.\n - If I understood correctly, the symmetry ""constraints"" are really data augmentation during the training phase, rather than hard constraints on the model.\n\nMain questions:\n- It seems the potential function plays a similar role to the negative log-likelihood in [2]. \n- Does having symmetry constraints lead to a better model when the constraints are justified? ie. can you provide comparisons for the Ising model experiment in 4.2?\n- What are the set of constraints you can specify using a potential function? Permutation of the variables is very briefly mentioned in the experiment section, but this could be clarified much earlier. \n- I may have missed this, but what exactly are the symmetry conditions that were used in the experiments?\n- It seems that the proposed permutation constraints could be part of the training algorithm rather than the model. How different would it be if you permute the data samples and use an existing normalizing flow algorithm? ie. can you provide comparisons where randomly permuted data samples are also used during training with existing algorithms?\n- Since you used a fixed-step size solver, do you have some guarantees, theoretical or empirically, that the numerical integration has low error? e.g. what is the reconstruction error from doing a forward and reverse pass, and what would the error be if compared to a much smaller step size?\n\nMinor:\n- The potential function is parameterized directly but is not integrated to infinity. Since the resulting gradient flow is time-invariant, how this would affect the expressivity of the flow? Could a time-variant potential function be used?\n- Eq (5) is also the Liouville equation, which I think should be mentioned.\n- MNIST digits have completely black backgrounds, so I don\'t understand why Figure 3 samples have grey backgrounds. Could this have something to do with numerical issues in reversing the numerical integration?\n- It\'s awkward that Figure 3 contains the loss over training for Monge-Ampere flows but only the final loss for the rest. Table 1 sufficiently summarizes this figure, so unless you can show the loss over training for all methods I think this figure is redundant.\n- Equations are referenced both with and without parenthesis. It\'d be best if this is consistent across text.\n- There are quite a few grammar mistakes, especially around important digression. (e.g. top of page 3 ""experienced by someone travels with the fluid"" -> ""experienced by someone traveling with the fluid"".)\n- Please use citep and citet properly. Many references should be done using citep (with brackets around the author-year), when the author is a not a part of the sentence.\n\n[1] Chen, Tian Qi, et al. ""Neural Ordinary Differential Equations.""\n[2] Tabak, Esteban G., and Eric Vanden-Eijnden. ""Density estimation by dual ascent of the log-likelihood.""', 'Summary:\nThis paper introduces a continuous-time flow, which is motivated from continuous-time gradient flow in the Monge-Ampere equation in optimal transport theory. They relate the resulting generative model to a dynamical system with a learnable potential function that controls a compressible fluid (representing the probability density), towards the target density. The resulting set of differential equations for the evolution of the samples and the density is solved through a fourth-order Runge-Kutta ODE solver. By imposing symmetries on the scalar potential function, symmetries of the resulting target distribution can also be enforced, a property that is desirable in many applications.\n\nThe scalar potential is modeled using a fully connected MLP with a single hidden layer. Forward propagating of samples requires obtaining the gradient of the scalar potential (output of MLP) with respect to its input (the sample). Forward propagation of the log density requires computation of the Laplacian (not the hessian) of the scalar potential. Both of these quantities can easily be computed with automatic differentiation (in O(D) where D is data dimension). The potential is kept constant over time, although this is not necessary.\n\nThe proposed method is evaluated on density estimation for MNIST, and variational inference with respect to the Boltzmann distribution of the 2D Ising model at the critical temperature. \nOn MNIST, comparison is done with respect to MADE, MAF and realNVP. Monge-Ampere flows outperforms the baselines. On the variational inference task one baseline is used, and the result is compared to the exact known free energy. Monge-Ampere flows are reported to approximate the exact solution to comparable accuracy as a baseline. As the authors show that they can easily enforce symmetries, it would be very informative to see the performance of Monge-ampere flows with and without these symmetries enforced on for instance the Ising model. Have the authors looked at this?\n\nIt is not clear from the paper how much the ODE solvers used in the forward pass, as well as backpropagating through it with respect to model parameters, will influence the run time. I suspect the training time of models like MAF to be significantly shorter than that of Monge-Ampere flows. For sampling, the comparison would also be interesting. Where sampling from MAF is O(D), with D the data dimension, sampling from the Monge-Ampere flows requires propagating through an ODE solver. Can the authors comment on the runtimes for these settings?\n \nThe experimental validation is not extensive, but the proposed method is well motivated and as far as I can tell original. It is a useful contribution to the field of normalizing flows/invertible networks. The ability to easily enforce symmetries into the density seems to be promising and could lead to interesting future work on permutation invariant systems.\n\nSee below for comments and more questions:\n\nQuality\nThe paper is well structured. The experimental validation is not extensive, and perhaps even on the low side.\n\nClarity\nThe paper is overal clearly written. One small nuisance is that the citations are not in brackets in sentences, even if they are not part of the actual sentence itself. This interrupts reading. It would be greatly appreciated if the authors could change this. The authors leave out some details with regards to the experiments, but with code available this should be sufficient for reproducibility. \n\nOriginality\nTo my knowledge the idea of using the Monge-Ampere equation for continuous normalizing flows is new. Note that it is also significantly different from a concurrent ICLR submission entitled ‘Scalable Reversible Generative Models with Free-form Continuous Dynamics’, which also discussed continuous normalizing flows with ODE’s.\n\nSignificance\nThis work is of use to the research community. The method is memory efficient and appears to perform well. Especially the ability to enforce symmetries seems very appealing. If the authors can comment on the runtime in comparison to other flow methods, both in terms of training time and sampling, this would enable a better view on practical use. \n\nDetailed questions/comments:\n\n1. In Fig. 3a, the train and test error during training are shown to follow each other very closely. How long was the model trained, and did the train and test curve at some point start to diverge?\n2. In Section 4.2, the results are said to be of comparable accuracy as the baseline by Li & Wang. It would be informative to actually state the result of Li & Wang, so that the reader can judge too if this is comparable. \n3. Out of curiosity, did the authors also consider using other activation functions that have higher order derivatives, such as tanh?\n\n\n***** EDIT ******\n\nI thank the authors for their clarifications. They have sufficiently answered my questions/comments, so I will stick with my score.\n\n']","[-20, -20, 60]","[60, 60, 80]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('quite interesting', 'worth pursuing'), there are significant criticisms and concerns raised. The reviewer points out that the applications section is not convincing, the gradient-based learning behavior is not discussed, and the significance of the experiments is questioned. The updated section at the end further downgrades the rating, indicating increased dissatisfaction. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positives before presenting criticisms, and using phrases like 'It would be nice if' when suggesting improvements. The tone is professional and constructive, even when expressing concerns."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting derivations', 'enough contributions to potentially be interesting'), they express significant concerns about the paper's originality, clarity, and experimental rigor. The reviewer points out several 'cons' and asks many critical questions, indicating overall dissatisfaction with the current state of the paper. However, the score is not deeply negative because the reviewer sees potential in the work and offers constructive feedback. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms as suggestions for improvement. They use phrases like 'I'm willing to believe' and 'I may have missed this', which show consideration for the authors. The reviewer also provides detailed, constructive feedback, which is a polite way to help improve the paper. However, the score is not extremely high as the criticism, while politely phrased, is still direct and extensive."", ""The sentiment score is 60 (moderately positive) because the reviewer expresses that the paper is a 'useful contribution', 'well motivated', and 'original'. They praise aspects like the ability to enforce symmetries and note that the method performs well. However, they also point out some limitations, such as the experimental validation being 'not extensive' and request more information on runtimes. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms constructively (e.g. 'It would be greatly appreciated if...'), and shows engagement by asking thoughtful questions. They also thank the authors for clarifications in the edit section. The tone is professional and courteous throughout, without any harsh or rude phrasing.""]"
"['This paper proposes a new approach to sequential learning by introducing an adversarial memory unit for each new task and uses EWC as a regularizer for training other parts of the network on the new task.\nThe memory units are trained with Fast Gradient Sign Method to increase the loss, and they are connected to the next layer with weights trained to decrease the loss. \nIt shows superior performance than EWC and the plain gradient descent baseline on disjoint MNIST/CIFAR10 and EMNIST. The authors also share their experience with EWC, which provides useful feedbacks to the community.\n\nThe proposed adversarial memory unit is novel to the best of my knowledge. However, its motivation is not quite intuitive to me, and the authors fail to provide persuading explanations. My major concern is whether it is better to take the adversarial direction rather than the direction that decrease the loss for the memory units.\n\nTo support their ideas, the authors mentioned the paper ""Adversarial Reprogramming of Neural Networks"" and said this paper\'s ""adversarial program"" is formed by choosing the ""intersection of adversarial subspaces"" as in their paper. However, they (Elsayed et al. 2018) are actually finding such adversarial programs in the direction of decreasing the loss, which is contrary to finding the ""intersection of adversarial subspaces"". \nThe authors also want to support the pros of adversarial memory units by comparing against ""Gradient"" memory units that are trained to decrease the loss with the experiment shown in Figure 2. However, Figure 2a seems problematic to me, so I am not sure whether the authors are doing their experiments correctly. I think the experimental conditions for FGSD and Gradient are different, which makes the comparison meaningless. We can see that the network\'s accuracy with Adversarial memory unit on task 1 is a constant when the network is trained on task 2 and 3, because the network\'s weights (except memory units and their weights for task 2 and 3) and task 1\'s memory units are fixed, as described in the experimental setting for ""AD"". The accuracy on task 1 with Gradient memory units is changing when the network is trained on task 2 and 3, which means either the network\'s weights are changing or the memory unit is changing. \n\nAs a result, I don\'t think this paper will be accepted until the authors provide further explanations and results to support the adversarial memory unit, or clarify my misunderstandings in the comments above.', ""This paper proposes a novel continual learning method that stores intersection of adversarial subspaces into long-term memory units for each task, which is used used to characterize the given task at future tasks. This adversarial memory network requires supposedly less number of parameters for each task to store, compared to methods that stores explicit examples. The authors validated the proposed model on three datasets for continual learning, on which it obtains good performance when networks trained with plain gradient descent and elastic weight consolidation suffers from catastrophic forgetting.\n\nPros\n- The idea of using adversarial subspaces to characterize a task is a novel idea which seems to work to some degree.\n\nCons\n\nExperimental validation is lacking in many aspects. \n\n- First, while the proposed method requires additional memory storage and parameters, it is not comparing against any of the existing work that increases network capacity or storing a small subset of the original dataset. To list a few that seems relevant, [Yoon et al. 18] proposes a network that can dynamically expand its capacity with minimal number of units per layer, and [Nguyen et al. 18] proposes to store a small subset called CoreSet that well-represent the task-specific dataset. To show that the proposed method is indeed effective in terms of accuracy over number of parameters, the authors should compare against such baselines with additional parameters. The increase in the network capacity reported in the paper seems quite large (over 60% for AD+EWC) and thus its effectiveness is questionable without such comparative study. \n\n- Their implementation of EWC seems suboptimal as it is only applied to fully connected layers, and thus the EWC baseline performs much poorly than what are reported in many of the previous work, and performs comparable to PGD. Since EWC baseline is crippled the only message that is remaining is that the proposed method works better than simple PGD. \n\n- The reported results using the proposed method shows some performance degradation on earlier tasks, which seems large considering the difficulty of the tasks. Again, the authors should compare against recent methods on continual learning so that the readers can understand how good these reported performances are.\n\n- It is difficult to understand why storing adversarial subspaces helps, since there is no visualization or illustrations that provide intuitive explanations. \n\nIn sum, while the proposed model seems novel, its motivation is unclear and it is difficult to assess the effectiveness of the proposed method due to lack of experimental validation. Thus I recommend the rating of reject for this paper, until the authors provide additional experimental results for proper assessment of the method's effectiveness."", 'The paper is about a new method for training neural networks in the continual learning setting, where tasks are presented in a sequential manner (and data from the previous task cannot be revisited). The method proposes a new architecture that adds task-parameters parameters to prevent catastrophic forgetting.\n\nTo my understanding, the paper proposes a modification to EWC in which the capacity of the network is augmented after a new task is added. Unlike similar methods (like Progressive networks, see bellow), this augmentation is input agnostic. It acts as a correction of the model parameters such that the new task can be easier to train while still maintaining the \'normal parameters\' close to the ones of the initial task (as in EWC). I find this idea interesting and certainly worth publishing. In my view, the paper cannot be published in its current state. With the current presentation it is very difficult to understand what is being proposed. Please correct me if I misunderstood the work. \n\nThe writing of the manuscript needs significant improvement. I read it carefully several times and I am still not sure of how exactly the model is trained. I had to read the paper by Elsayed et al 2018, to have an idea of what could have been proposed here. As I mentioned, the paper has novel and interesting ideas, but it would be greatly improved with some important re-writing. Please find bellow some questions.\n\n- In the second to last paragraph of page two, the authors say that: instead of adding a perturbation that would force the network to perform a misclassification, tune it using ""the input\'s own correct class to assist correct classification"". If the gradients are computed with respect to the correct class of a given input, why is this called an adversarial perturbation? \n\n- Elsayed maintain the parameters of the first task fixed and train a fixed input-agnostic correction that can be added to the input such that a second task can be trained (with a re-mapping the outputs). Applying Elsayed et al 2018 to the continual learning setting, the model should only learn correction for task 2 (and 3). How do the authors compute the corrections for task 1? Computing a correction requires having access to the training data.\n\n- The authors use the FGSM method to compute ""adversarial perturbations"". This method was proposed as a proxy for performing gradient descent to minimize the computational load required for finding adversarial examples. In this application, unlike the case of adversarial perturbations, the memories don\'t need to be constrained to be smaller than a given epsilon. What is the motivation of using this method? How do you explain the difference in the results.\n\n- Having mentioned this, both W_task and M_task are updated by minimizing the same loss function (ignoring the difference of using FGSD or not). In that case, why is it needed to have a factorized form W_task * M_task instead of a single bias?\n\n- Throughout the paper the authors say that the long term memory lies on the ""intersection of adversarial subspaces"". It is not clear at all why this should be the case. The authors do not explain adversarial subspaces corresponding to which model.\n\n- The authors should cite the Progressive networks as this is a very related work. Unlike progressive networks, this work proposes and augmentation that is input agnostic which is interesting. https://arxiv.org/abs/1606.04671\n\n- With EWC, once the model is trained, one does not need to know the task being evaluated at test time. This is not the case in the proposed model. This should be clarified. Also, when having many tasks mapping to the same input, the fair way of comparing to EWC would to have a different head per task. This baselines should be included.\n\n- What are the task specific functions g_taskA and g_taskB?\n\n- Adding an explicit algorithm, the exact loss functions used should help clarifying the proposed method.\n\n- The paper would be stronger if more complex tasks would be added.']","[-50, -70, -50]","[20, 20, 50]","[""The sentiment score is -50 because the reviewer expresses significant concerns about the paper's methodology and conclusions. While they acknowledge some positive aspects (e.g., novel approach, superior performance), the overall tone is critical, stating that the paper shouldn't be accepted without major revisions. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'to the best of my knowledge' and 'I think,' which soften the criticism. They also acknowledge positive aspects before presenting their concerns. However, the language isn't overly polite, maintaining a direct and critical stance where necessary."", ""The sentiment score is -70 because the review is predominantly negative. While the reviewer acknowledges the novelty of the idea, they list several major concerns about the experimental validation and effectiveness of the method. The review concludes with a recommendation to reject the paper. The politeness score is 20 because the reviewer uses professional language and provides specific feedback, but the overall tone is critical. They begin by noting a pro of the paper and use phrases like 'seems to work to some degree' which soften the criticism slightly. However, the bulk of the review is focused on the paper's shortcomings, presented in a direct manner without much cushioning language."", ""The sentiment score is -50 because while the reviewer finds the idea interesting and worth publishing, they state that 'the paper cannot be published in its current state' and that it needs 'significant improvement'. This indicates a generally negative sentiment, though not extremely so due to the recognition of the paper's potential. The politeness score is 50 because the reviewer uses respectful language throughout, such as 'Please correct me if I misunderstood the work' and 'Please find below some questions'. They also acknowledge the paper's merits while providing constructive criticism. The reviewer maintains a professional tone without being overly formal or informal, striking a balance that is polite and appropriate for academic discourse.""]"
"['The paper presents two variational inference frameworks for generative models of knowledge graphs. Such models are based respectively on latent fact model and latent information model.\nThe authors argue that with the presented framework the underlying probabilistic semantics can be discovered. Experiments show performances comparable with state-of-art approaches.\n\nUnfortunately, the paper seems to me not clear and rather incomplete in its actual form.\nOverall, the proposal is novel. I cannot decide about significance because results do not outperform those of other approaches. To this extent, the authors should better discuss the results, explaining in more detail why this approach should be used instead of others (scales better, is faster, etc.).\n\nIn the abstract, it is asserted that one can discover underlying probabilistic semantics, but in the corpus of the paper this aspect is not described or mentioned in detail.\nSimilar problem for the reference to von-Mises distribution. This distribution is just named, it is said that the framework can handle such a distribution, but a reference to a paper and/or a short paragraph to explain the sentence are missing. This statement now results to be just information disconnected by the rest of the paper.\n\nIn a similar way, many other points suffer from a poor organization in my opinion.\nWhen describing LIM an error is introduced here that is then copied and pasted throughout the paper: in the productory on p, p is in R not in E. This is a simple typo, but the fact that it is repeated so many times, also in the proof, gives me the feeling that the paper was written at the last moment.\nFigures 1 and 2 are never referred.\n\nFormula 6 must be better explained. If I have not lost something, n is the number of labeled triples, s_c is undefined, b_c is the probability of s_c to be equal to 1, the index i is never used. The paper lacks information here.\n\nAs regards the experimental part, some results are shown in subsection 4.3 called link prediction, others in section 5 called link prediction analysis. This organization does not seem to me to be really optimal. I would suggest creating an experimental section.\nMoreover, the tests should be better explained, the tables are shown without specifying how they are built and how the values are collected. Information is provided in the appendix but could be included in the paper as the maximum limit is of 10 pages (8 suggested but I think an extra half page can be used).\nThe knowledge bases used should be at least cited, I know that freebase and wordnet are well-known but somewhere, in the description of the test, the name should be included. Also to specify the characteristics of the versions (WN18 vs WN18RR). Moreover, what does the value -257 in column 1, row 4 means?\nThen, it is said that Table 1 shows improvements for ComplEx, but such improvements are rather low, is there a way to prove their significance? Otherwise, I would say that the performance is the same for WN18.\nTables 1 and 2 contain cells with \'-\' value, what does it mean?\n\nDiscussion about table 3 is incomplete in my opinion. First of all, the ""proportion"" column should be described. Also, on one hand, it is true that the _member_meronym is the least accurate and prominent but the most problem may come from _hypernym, which is the most prominent and the accuracy is also low. This fact is highlighted for table 4 but not for table 3.\n\nMinor issues\n- sec 3: references to Miao et al. must be enclosed by brackets\n- sec 4.3: ""We believe this *is* due to ...""\n- sec 5.2: what is Model A? Also, the sentence seems incomplete.\n\nPros:\n- Novel approach\n\nCons\n- Test results are not convincing\n- The paper is not mathematically sound\n- The paper needs to be re-organized', '* The paper proposed a neural variational inference framework for knowledge graph embedding. The paper proposed two models (Latent Fact Model and Latent Information Model) where the neural variational inference is carried out, with competitive results on standard datasets (WN18 and FB15K).\n\n* I am not fully convinced of the advantage of this variational inference approach compared with the optimization approach used in TransE, TransG, DistMult and ComplEx. As can be seen in Table 1, the best performance on WN18-RR and FB15K-257 are obtained without variational inference. In Table 2, performance of the variational inference approach is not as good as other approaches under the MR or Raw Hits metrics. Moreover, performance on FB15K is not reported in Table 2, which makes the result not as complete as Table 1.\n\n* It seems that the main difference between Latent Fact Model and Latent Information Model is the way prior is imposed on h. The authors may want to explain in plain language the differences and the motivation behind that.\n\n* It is unclear how the (\\tau,y) labeled triples are generated, especially for the negative examples with y=0. Is it obtained by randomly corrupting the triples in the knowledge base, as done in other work? It would be better to make this point clear.', '\nThis work proposed two variational embedding methods for knowledge graph inference tasks. The experiments show slight improvements compared to other variational embedding methods, e.g., KG2E, TransG, and slight improvement on the WN18 dataset compared with the non-variational method. On the other hand, both the embedding method and the training of variational models used in this work are already well developed. Thus, this work doesn’t show too much novel contribution. However, the reviewer really appreciate the visualization provided in Fig. 4. \nMinor issues:\n1)\tNotation of the KL divergence is not conventional\n2)\tThere are some mistakes of indices for predicates, e.g., in Eq. 7, 8.\n']","[-60, -20, -20]","[20, 50, 50]","[""The sentiment score is -60 because the review is predominantly negative. The reviewer states the paper is 'not clear and rather incomplete', points out numerous issues with organization, clarity, and mathematical soundness, and concludes that 'test results are not convincing'. The few positive comments (e.g., 'the proposal is novel') are outweighed by the criticisms. The politeness score is 20 because while the reviewer is critical, they use relatively polite language such as 'Unfortunately', 'I would suggest', and 'in my opinion'. They also acknowledge some positive aspects. However, some phrases like 'poor organization' and 'the paper lacks information' are more direct, preventing a higher politeness score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions and competitive results, they express significant doubts about the advantages of the proposed approach compared to existing methods. The reviewer points out that the best performance is achieved without the proposed variational inference approach and that some results are incomplete or underperforming. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'I am not fully convinced' and 'It seems that' which maintain a polite tone while expressing concerns. The reviewer also provides specific recommendations for clarification and improvement, which is a courteous approach in academic peer review."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some improvements and appreciates the visualization, they also point out that the work doesn't show much novel contribution. The overall tone suggests mild disappointment with the novelty of the work. The politeness score is moderately positive (50) as the reviewer uses respectful language, acknowledges positive aspects ('really appreciate'), and frames criticisms constructively as 'minor issues'. The reviewer maintains a professional tone throughout, avoiding harsh language or personal attacks.""]"
"['This paper centers around efficient estimation of the kernel function for the Hawkes process and relaxation of the “linearity” assumption in the original Hawkes process. They rely on a classical sparse generalized linear model using the wavelet basis set and Hawkes loss function to estimate a shallow kernel function. This approach is opposite to the deep function estimation approach which does not rely on a predefined basis set [e.g. see [Du et al, 2016]]. However, it can have an advantage that the learned functions are interpretable, thought the authors never demonstrate it in the paper.\n\nGiven this view of WRNs (an unfortunate coincidence with WideResNets), we understand how LSTM2 outperforms LSTM1 in the results. However, the results tables do have peculiar numbers too. For example, why the Goodman-Kruskal gammas for H. Poisson are exactly -1? Why is it always pointing in the wrong direction? There are other observations in the results table that the authors have listed without much explanation. For example, in Section 5, what is the reason for “The WRN-PPL method excelled particularly in tasks with many target occurrences”?\n\nAnother example is the arguments in the discussion section about the use-case of rate functions. For example, the authors state: “ For example, the rate prediction for the individual denoted in green in Figure 5 (right) suggests that individual may have skipped, missed, or rescheduled 5 to 6 appointments over the last decade.” How did the authors conclude this claim? What is the clinical significance of missing or rescheduling 5-6 appointments in the context of A1c prediction?\n\nWriting can be seriously improved (basically the paper is not ready in the current state). For example, only in Section 6, the authors have introduced the full name of WRN-PPL after using it many times before. \n\nThe motivation for this paper is misleading. There have been several works on “Deep Cox” and “Deep Hawkes” models. I don’t see the novelty in the authors’ contribution in defining the clinical risk. Especially Fig. 3 (left) is already known and does not add much value.\n\nOverall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models. Unfortunately, the authors do not clearly state how many training data points they have. They have a vague statement: “798,818 timestamped events in a study population of 4,732 individuals”, but it does not say exactly how many training examples they have.', '\n[PROS]\n\n[originality]\n\nThe paper proposed to construct the intensity function of a point process using wavelet, in order to improve its expressiveness, e.g. allowing non-additivity.  \n\nThe authors did extensive experiments to investigate their model performance compared to many appropriate baselines, on both synthetic and real-world datasets. \n\n[CONS]\n\n[clarity]\n\nThe major drawback of this submission is its clarity. The paper is vague at various important points in both method and experiments, thus leaving their correctness and soundness undetermined. \n\nIn the method section, the authors did not specify a well-defined and self-consistent notation system. This makes the paper really hard to understand. For example, one may be easily confused with things like: \n1) How q, q(g(t), t_i), g(t), g_{es}, g_{d} are connected and distinguished? \n2) The function g_{es} maps from t to R, then is t a space or a variable? \n3) The state s seems crucial in the function g_{es}, but why it is only mentioned one time in the paper? How it is defined and how it is used?\n4) How the Hadamard product is applied to two matrices of different sizes? \n5) j=1 is time dimension and j=2 is time and value dimension, then why j=1 is not part of j=2? Time is needed in both cases and it seems natural that the associated parameters are shared. \n6) Figure-3 has t_i in figure, t’ in caption, but the text in main paper mentions t_0 for Figure-3. How are they related? Are they actually the same?\n\nThe most confusing part is the censoring distance c. Its introduction around eqn-(2) suggests that c > 0 and the censoring section clearly mentions that. But c is also used to denote the forecast distance, which is clearly < 0 according to Figure-6 and Figure-7. What’s worse, there is also a term called forecast censoring distance. What are the relationships among these terms? If they are all the same, then is the c actually a model parameter or an evaluation control knob? Such things are very important to clarify. \n\nMoreover, the paper did not clearly explain how the model is trained in each case, especially for (multi-)forecasting. In details:\n1) What is the training objective?.\n2) What is the optimization method for this objective?\n3) How is it implemented and would the code be released?\n\nIt is good that the experimental section lists many appropriate baseline models and multiple evaluation metrics, but it is not clear how they are used. For example:\n1) Fourier methods and Hawkes process do not deal with the value v, then how are they fairly compared to the proposed model which takes v into account as in eqn-(2)? \n2) How is the Goodman-Kruskal gamma exactly computed? On all the instances of the held-out set? What is exactly the rank in this case?\n3) The authors also leave out the positiveness constraints of a Hawkes process to incorporate inhibitory interactions, but how the positivity of the intensity function is ensured in this case? \n\n[quality and significance]\n\nThe method is well-motivated and innovative. But details of the model and experiments are very unclear, so its overall soundness is hard to judge. For example: \n1) The authors claim that, compared to neural models, their model has the advantage of the interpretability (for small datasets), but they also have neural components in their model. So why their model is more interpretable than others (e.g. Mei and Eisner 2017 as they cited) is not clear to me. \n2) It is not clear why the interpretability is associated with the size of the dataset (quote `remains interpretable for small data sets’). What’s worse, the interpretability seems the only advantage of the model over other neural models (please correct me if I am wrong). If this edge could not scale up to large datasets, then does it mean on large datasets, a neural model should always be preferred over this model, because they are supposed to be more expressive? \n', 'The authors of this paper propose a point process model that uses wavelet reconstruction to capture complicated dependencies between events. They motivate this approach using experiments in the medical domain, which show how certain dependencies between events is better captured by their proposed model. The primary contribution of the paper is novel and could be useful in medical settings where predicting occurrence of important events such heart attacks could be challenging with alternative methods.\n\nA few recommendation for improving the paper besides a few typos that are present in the manuscript (such as first sentence of section 3 where  ""depicted"" is redundant): 1) A more substantive discussion of the challenges one may encounter while training the proposed model and elaborate on the complexity of the inference procedure in comparison with alternatives. For example, training vanilla Hawkes model is relatively easy and efficient, even though it may not perform as good as the proposed model. 2) Presentation of the model in section 3 lacks sufficient explanation and heavily relies on high level remarks on how the model is developed, a more detailed explanation (even including how the wavelet coefficients are computed) could be far more useful than the schematic view of the network architecture presented in Figure 1.\n\nOverall, I think this paper provides a novel contribution for modeling event data specifically for medical data. The ideas are well presented and experiments provide insights in how the proposed model can be useful for forecasting particular medical events. The overall recommendation is to accept the paper, however, I hope authors would address the concerns provided in the previous paragraph.']","[-50, -50, 70]","[20, 20, 80]","[""The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper, such as peculiar numbers in results tables, unexplained observations, and misleading motivation. However, it's not entirely negative as it acknowledges some positive aspects, like the potential advantage of interpretable functions. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and avoid personal attacks. They use phrases like 'can be seriously improved' rather than harsh language. The reviewer also balances criticism with some positive comments, which contributes to a more polite tone overall."", 'The sentiment score is -50 because the review is predominantly critical, with a long list of concerns and unclear aspects of the paper. The reviewer acknowledges some positive aspects (originality, extensive experiments) but these are outweighed by the numerous issues raised, particularly regarding clarity. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and frame their concerns as questions or observations rather than harsh judgments. They also acknowledge the positive aspects of the paper at the beginning. The reviewer maintains a constructive tone, suggesting areas for improvement rather than outright dismissing the work.', ""The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, noting its novel contribution and usefulness in medical settings. They recommend acceptance and provide constructive feedback for improvement. The politeness score is 80 (polite) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames their recommendations as suggestions rather than demands. They use phrases like 'I hope authors would address' and 'could be far more useful,' which maintain a courteous tone while providing feedback.""]"
"['This paper proposes an alternative search procedure for A* sampling that, in contrast to the original optimistic search, doesn\'t rely on (possibly difficult-to-find) bounds for the log-probability function.\n\nThe first major issue with this paper is clarity. The preliminary section describing the Gumbel process and A* sampling is very difficult to understand (despite my being quite familiar with A* sampling). The authors use undefined notation frequently throughout the introduction and refer to it in abstract terms. There are also numerous errors -- for example, when describing the ""bottom up"" approach to generating a Gumbel process, the authors suggest perturbing all points in the input space by independent Gumbel noise (which would result in Gp(S) = infty almost surely when omega is uncountable). \n\nThe description of the main contributions in section 4 is equally unclear. This section starts by suggesting that sampling the next node to investigate according to the probability that it contains the maximum is reasonable, and then presents a lemma about regret in a bandit setting where the sampler never splits the subset. This lemma does not apply to the actual sampler proposed in the paper, so it is not clear why it is included. Section 4.2 is also very unclear -- I am not certain how both w and script Y are defined, nor why we need an ""unbiased estimator of Y"" (a random variable?) when we can simply sample from Y directly. As the definition of w is unclear, the purpose of 4.3 is unclear as well.\n\nThe other major issue is more fundamental -- I am not convinced the sampler is correct. The algorithm simply terminates after some finite horizon (rather than having a conclusive proof of termination via branch and bound as in the original A*). There is no proof or argument included in the paper regarding this. Any proposed sampling algorithm must be correct to be acceptable in ICLR.\n', "" It seems a good paper,  containing with interesting ideas.\nHowever, the state-of-the-art discussion about sampling methods is formed by just one sentence. I suggest to add more references about adaptive rejection sampling and MCMC, discussing a bit more their differences, and the differences and connections with A* sampling. Also direct methods should be discussed. For this reasons, I also suggest to add references to related books as Devroye's sampling book  and other, more recent books devoted to describe independent random sampling methods."", ""Summary: This paper introduces a probability matching approach for optimizing Gumbel processes, i.e. the extension of the Gumbel-Max trick to more general measure spaces. The basic idea is to use a more refined subset selection mechanism as compared to A* Sampling, but at the cost of being able to guarantee an exact sample. Instead, the authors study the algorithm's best Gumbel so far as a function of the time budget. \n\nQuality: This is an interesting and natural idea, and the experiments support the author's claim that it improves over A* Sampling in the regimes that they consider. The claims and proofs look correct to me.\n\nOriginality: This idea has not been explored in the literature, and is a sensible one to consider when having an exact sample is not necessary. \n\nClarity: The comparison to A* Sampling is a bit apples-to-oranges and the paper lacks some clarity overall. In particular, the authors should be much more clear that their termination condition necessarily differs from A* Sampling's. In A* Sampling the termination condition guarantees that the algorithm has found the true optimum, but in PM-A* the authors consider only a fixed budget of time --- terminating whether or not the algorithm has found the true optimum. The difference is a necessary feature, because otherwise I'm fairly sure A* Sampling is optimal (It has to push the upper bound of every subset below the true maximum, so if the term. cond. is not currently satisfied the subset with the current max. upper bound must eventually be visited before termination. Since it is the only subset where that is necessarily true, taking it is the optimal choice.) More comments below.\n\nSignificance: I think this is interesting work, but I would recommend the authors consider some examples that clarify where these ideas might impact that problems that the ICLR audience would be interested in.\n\nOverall: The paper's clarity problems get in the way of its contribution. I think it is an interesting direction, but the authors are not clear enough about the correctness of the algorithm (it is not guaranteed to return an exact sample) to recommend acceptance. If this and other issues are fixed, it would be a good paper.\n\nPros:\n- Solid contribution to the literature on Gumbels.\n- Beginnings of a regret analysis.\n- Uniform performance gains in comparison to A* Sampling (in the regime considered).\n\nCons:\n- Lacks clarity in comparisons to A* Sampling.\n- Writing could clarify significance as well.\n\nSpecific questions / suggestions:\n\n- One of the important omissions of the paper is the following question. What is the distribution of PM-A*'s output at time T for a fixed T? It won't be the target distribution in general, but it may be close. This applicability of this work will be greatly improved by that analysis.\n\n- In general the figures and experiments need more details. In particular,\n(1) In figure 2a, why are there two sets of lines? Different values of T?\n(2) You need to report T throughout.\n(3) What did you take as Y? This is a critical part of the method, and you should report the estimator used.\n\n- In A* Sampling for two subsets without splitting algorithm,  shouldn't you be removing ALL previous Xi from  be S_i? i.e. S_i \\ {Xi_1, Xi_2, ... Xi_{k-1}} or something like that.""]","[-70, 50, -20]","[-20, 75, 60]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer points out major issues with clarity, uses phrases like 'very difficult to understand', mentions 'numerous errors', and expresses doubt about the correctness of the proposed sampler. The lack of positive comments and the fundamental concerns raised indicate a strongly negative sentiment. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite direct and critical. Phrases like 'The first major issue' and 'I am not convinced' are blunt and could be perceived as somewhat impolite in academic discourse. However, the reviewer maintains a professional tone overall, avoiding personal attacks or overly harsh language, which prevents the score from being extremely negative."", ""The sentiment score is 50 (slightly positive) because the reviewer starts by saying 'It seems a good paper, containing interesting ideas,' which is positive. However, the rest of the review focuses on improvements needed, indicating a balanced view. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, such as 'I suggest' instead of demanding changes. They also begin with a positive comment before offering constructive criticism. The review is professional and courteous, offering specific recommendations for improvement without harsh criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting idea and solid contribution, they express significant concerns about clarity issues and the paper's failure to guarantee an exact sample. The reviewer states that these problems 'get in the way of its contribution' and prevent them from recommending acceptance. However, the score is not deeply negative because the reviewer sees potential in the work if issues are addressed.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They balance criticism with praise, acknowledging the paper's strengths ('solid contribution', 'interesting direction') while clearly stating areas for improvement. The language used is respectful and focuses on the work rather than the authors personally. The reviewer offers specific suggestions for improvement, which is a polite and helpful approach in academic review.""]"
"['The underlying motivation for the paper is really interesting and cuts straight to the heart of Deep Learning and strives to unravel the key understanding that we are still to a large extent missing.\n\nWhen it comes to clarity and organization I find the paper a bit ""messy"" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions. Especially given the very specific nature of the topic I miss a strong and clear path through the paper. Unfortunately the paper leaves me with the distinct feeling that there are still a lot of work needed to be able to tell the story about the problem under study. Having said that the paper does contain several individual findings. Having said that I find the ideas leading up to what the authors refers to as ""gradient starvation"" to be really interesting and that would be a great clear idea to focus on.\n\nA few concrete questions/comments:\nCan you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.\n\nGiven the very specific nature of the topic treated in the paper I find the title of the paper largely misleading. The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.\n\nIn Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?\n\nThe final sentence in Section 2 is highly speculative and I find this hard to believe without solid backing. The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.\n\nTiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.', 'The authors study properties of the learning behavior of non-linear (ReLu) neural networks. In particular, their main focus is on binary classification for the linear-separable case, when optimization is done using gradient descent minimizing either binary entropy or hinge loss.\n\nThere are 3 main results in the paper:\n1) During learning, each neuron only activates on data points of one class: hence (due to ReLu), each neuron only updates its weights when seeing data points from that class. The authors refer to this property as ""Independent modes of learning"", suggesting that the learning of parameters of the network is decoupled between the two classes.\n2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape: slow improvement at the beginning, followed by a period of fast improvement, followed by another plateau.\n3) Most frequent features, if discriminative, can prevent learning of other, less frequent, features.\n\nApart from the assumption H1 of linear separability of the data (which I don\'t mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"". \n\nEven for a shallow net, the authors are essentially assuming that the first layer of weights W is such that each row w is already a hyperplane separating the two classes after initialization (wx > 0 for all x belonging to one class and wx\' < 0 for x\' in the other class). In other words, at initialization, the first layer is already correctly classifying all data points. This is of course an extremely stringent assumption that doesn\'t hold in practice (eg, the probability of such an initialization shrinks to zero exponentially in the number of dimensions and in the number of neurons).\n\nBecause of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.\n\nPros:\n - Authors consider a non-linear (ReLu) neural network, as opposed to the analysis of Save et al which only considers linear nets.\n - The fundamentally different behavior between Hinge and binary entropy loss is interesting, and worth analyzing further.\n - Sigmoidal shape of classification error as a function of number of iterations is inline with what is seen in practice. However, I believe the assumptions needed to show this point force the analysis to only characterize learning close to convergence.\n\nMinor Cons (apart from major concern above):\n - Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn\'t \\bar{u}_t logarithmic (non-linear) in t? \n - Theorem 3.2: Even if strong, I don\'t mind the assumption on a dataset merely consisting of two (weighted) data points. I would suggest to simulate this case without putting any condition on the initialization of the weights (ie, without assumptions H1-H2), and compare the empirical shape of the classification error with the one you obtain analytically in Figure 2 Right.\n- Theorem 3.2 Interpretation: unfinished sentence ""We can characterize the convergence speeds more quantitatively with the""\n- Theorem 4.1: Can you give an intuition or lower/upper bounds for u(t) for the Hinge case, to make evident its difference from the binary entropy case (where u(t) ~ log(t))\n- Gradient starvation, Kaggle experiment: I\'m not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator. What I\'m trying to say is that ""gradient starvation"" is a more general problem that really doesn\'t have to do with gradient descent. Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.\n\n', 'The authors study the learning dynamics of deep neural networks, which is of fundamental importance but lacks understanding. The authors study several dynamics like activation independence, gradient starvation, which gives new insights. However, the assumption is too strong.\n\nThere are two main results in the paper:\n1) Through learning, the neurons activates of one class. \n2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape.\n\nHowever, there are two strong assumptions: 1. the two data are perfectly separable by linear classifier. 2.  H2 assumes ""at the beginning of training data points from different classes do not activate the same neurons"". This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied. It sounds to me this assumption implicitly suggests that the algorithm is already ALMOST CONVERGENT. \n\nIf this assumption cannot be weakened, I don\'t think the paper can be accepted.\n']","[-30, -20, -50]","[50, 60, 20]","[""The sentiment score is slightly negative (-30) because while the reviewer acknowledges the interesting motivation and some individual findings, they express concerns about the paper's clarity, organization, and the limited scope of the results compared to the broad claims. The reviewer uses phrases like 'a bit messy,' 'missing a strong and clear path,' and 'misleading title,' which contribute to the negative sentiment. However, the reviewer also highlights positive aspects, which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'I find,' 'Can you explain,' and 'I am missing' rather than making blunt criticisms. They also balance negative feedback with positive observations, showing respect for the authors' work while providing constructive criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they express a major concern about the assumptions made in the paper, which they believe significantly limit the applicability of the results. The reviewer states that the assumptions are 'extremely stringent' and 'don't hold in practice', indicating a significant criticism of the paper's methodology. However, the review is not entirely negative, as it does highlight some strengths of the paper.\n\nThe politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I believe' and 'I'm not too convinced' rather than making blunt criticisms. The reviewer also balances their critique with positive comments ('Pros') and offers constructive suggestions for improvement. The language is formal and objective, avoiding personal attacks or harsh language."", ""The sentiment score is -50 because while the reviewer acknowledges the importance of the topic and some new insights provided, they express significant concerns about the strong assumptions made in the paper. The reviewer states that if these assumptions cannot be weakened, the paper should not be accepted, indicating a generally negative sentiment. However, it's not entirely negative as they do recognize some positive aspects of the work. The politeness score is 20 because the reviewer uses relatively neutral language and doesn't use harsh or rude expressions. They present their criticisms in a professional manner, stating their concerns clearly without being overly critical or dismissive. The slightly positive score reflects the reviewer's effort to acknowledge the paper's contributions before presenting their concerns.""]"
"['The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The loss of the bandit is binary: zero if the validation loss decreases and 1 otherwise. In the experiments, the Exp3 bandit algorithm is run with Adam and Adagrad on MNIST, CIFAR-10, and CIFAR-100. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.\n\nThe idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know.\n\nThe paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.\n\nIt is not clear how results scale with the number and choice of the grid values.\n\nI would have liked to see a more thorough investigation of the impact of the bandit loss on the experiments. It is true that as far as the theory is concerned, any bounded loss is OK. But I practice I would expect that a graduated loss (e.g., signed percentage of change in validation loss), would be more informative.', ""This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. Its results suggest that RMGD faster than MGD with grid search, and generalizes better.\n\nThe paper is well written. The idea itself is a simple and relatively straightforward application of bandits. The paper has some merits as it proposes an efficient and theoretically sound method to replace grid search in MGD.\n\nOne result that stands out is that RMGD achieves better results than the best performing batch size. The authors may want to discuss this in more depth. This may be due to the fact that the problem is inherently contextual: each epoch is different from other epochs, and may require a better-suited bach-size. Maybe contextual bandits would be a good candidate to try.\n\nComments:\n- offer some analysis or explanation of the surprising results\n- add equation numbers for ease of reference\n- in 4.1, why did you use this particular probability update? motivate/explain this choice.\n- appendix A: Specify that <> is dot product.\n                        introduce Beta\n                        briefly explain mirror descent\n                        why is beta z >= -1? My sense is that it is >= 0. can it be negative?\n                        explain, motivate or cite the equation following beta z >= -1\n\nI am pretty familiar wit bandit literature. Less so with GD literature. The paper's hybrid approach, although simple, exposes interesting questions. I tend towards accepting the paper.\n"", 'The authors consider the problem of determining the minibatch size for SGD by first fixing a set of candidate sizes, and then learning a distribution over those sizes using a MAB algorithm. A minibatch size is first sampled from the distribution, then one training epoch is performed. A validation error is then computed, and if it is lower than that of the last epoch, the cost of the minibatch is taken to be zero (otherwise one), and the distribution is updated. This is Algorithm 1.\n\nIn Section 4.2, they prove a regret bound, but I don’t think that regret is really the correct notion, here (although it’s very close). This is a subtle point, so I’ll set up some notation. Let w(b_1, ..., b_t) be the result at the tth epoch, if the batch sizes b_1, …, b_t were used at the 1st through tth epochs. Let y(w,b) be 0 if training one epoch starting at w with batch size b would improve the validation error, and 1 otherwise.\n\nThey show (unnumbered inequality on the middle of page 5) that \\sum_t y(w(b_1,...,b_{t-1}),b_t) is close to \\sum_t y(w(b_1,...,b_{t-1}),b^*), where b_t is the batch size that was chosen at time t, and b^* is the best fixed batch size. The key point here is that the comparator (the second sum) starts each epoch at the result that was found by their adaptive algorithm, *not* what would have been found if a batch size of b^* had been used from the beginning.\n\nIn other words, their result does *not* show that their algorithm is close to outperforming a fixed choice of batch size (for that to hold, the comparator would need to be \\sum_t y(w(b^*,...,b^*),b^*)). What they show is similar, but subtly different. They don’t put too much weight on this theoretical result, and in fact don’t even explicitly claim that the comparator in this result is that for a fixed choice of batch size, so really this is a minor issue, but I think that this is something that should be clarified, since it would be easy for a reader to draw an incorrect conclusion.\n\nWith that said, their approach is well-motivated, and their experiments seem to show consistent small improvements in performance. I don’t think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn’t be much more computationally expensive than using a fixed minibatch size. Furthermore, their approach is potentially more robust, since you can presumably be less careful about choosing the set of candidate minibatch sizes, than you would be for choosing only one. So while the experiments don’t show a big improvement, their proposal has other benefits.']","[20, 60, 50]","[50, 80, 80]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty of the paper's approach and its potential benefits, stating that it 'allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size.' However, the reviewer also points out some limitations and areas for improvement, which prevents a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout and offers constructive criticism without harsh or dismissive comments. The reviewer suggests improvements and expresses personal preferences ('I would have liked to see...') in a respectful manner, maintaining a collegial tone throughout the review."", ""The sentiment score is 60 (positive) because the reviewer expresses that the paper has merits, is well-written, and proposes an efficient and theoretically sound method. The reviewer also tends towards accepting the paper. However, it's not extremely positive as there are some suggestions for improvement. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive feedback, and frames suggestions as questions or opportunities for the authors to expand on their work. The reviewer acknowledges their own expertise limitations and uses phrases like 'The authors may want to discuss' instead of making demands."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the merits of the authors' approach, noting it is 'well-motivated' and shows 'consistent small improvements in performance'. They also highlight potential benefits like robustness and computational efficiency. However, they point out some theoretical issues and state that the performance improvements are not 'totally conclusive', balancing the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively where appropriate. They frame criticisms constructively, using phrases like 'I don't think' and 'I think that this is something that should be clarified' rather than making blunt statements. The reviewer also takes care to explain their points in detail, showing respect for the authors' work.""]"
"['This work proposes an approach for explicitly placing information in a subset of the latent variables. The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information. \n\nExperiments focus on learning global information. The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.\n\n# Approach seems limited.\n - This approach seems very limited, as there must exist a known transformation that removes the desired information. Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)\n - Can this approach learn multiple factors as opposed to just two? \n - What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)\n\n# More ablations or experiments with comparable settings would be desirable.\n - What is the choice of beta in the beta-VAE training objective? Apart from 1.2, this isn\'t mentioned. My concern here is that beta might be affecting the result more than the proposed training algorithm. Can the proposed approach perform just as well without a modified objective? Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper. (e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)\n - Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?\n\n# Related work.\nThere are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:\n- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.\n- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2]. I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder. Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?\n\n[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. ""Transforming auto-encoders."" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.\n[2] Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" Advances in neural information processing systems. 2015.\n\n---- Update since rebuttal ----\n\nI thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters. I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement. More experiments based on different transformations that the authors have mentioned would make this a stronger contribution. The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation. ', 'The paper proposes a method to disentangle latent variables for certain factors of interest in an image by considering the original input image and a transformation of the image where information about the factors of interest is removed. The generative process is then modeled by having two latent variables --  the first responsible for generating the transformed image whereas both latent variables are responsible for generating the original input image. This inductive bias naturally enforces that the second latent variable will not model the information which the first needs to reconstruct the transformed image, due to the VAE objective penalizing redundancy in information present in the latents. The paper demonstrates this in one setting where the transformation is random shuffling of image patches, which should remove the global information of the original input image.\n\nThe methodology of the paper was concise and easy to follow. The simple inductive bias presented in the paper for disentangling local and global information is very interesting. It is not obvious that shuffling image patches at a particular scale would lead to complete loss of global information, but the paper does show results on SVHN and CIFAR10 for which global information is sufficiently disentangled. The results for digit identity clustering were great for showing the correlation between their learnt global information and label information.\n\nThe paper introduced their model as a general purpose strategy for placing desired information in latent variables using auxiliary tasks, but focus was directed to the global vs local line of analysis. While giving examples for what kind of information can be removed, the authors mentioned that color to gray-scale might be one possibility.  It would have been interesting to see this and other possibilities explored in the paper. I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.\n\nIt is mentioned in the paper that having a single inference network for the posterior as opposed to the factorized one is conceivable. I would be curious to see an analysis of how that works out as compared to the separate encoders case.\n\nOverall, the paper has a novel idea which is well motivated and executed in terms of experiments.', 'This paper proposed a new training framework to disentangle global structures from local structures \nbased on Variational Autoencoders (VAEs). They first generate a transformed image by shuffl\x0fing the \npatches of the original image to destroy the global structures. The training task forces the model to\nreconstruct the original image and shuffled images from different latent variables, thus separating \nglobal long-range structural correlations and local patch-wise correlations .\n\nInstead of adjusting the objective function or model structure, the paper proposed a new and simple\ntraining framework to disentangle the global and local structures, which is novel. \n\nThe experiment results are good on SVHN. Some visual inspection experiments on CIFAR10 are performed. \nThe plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it. The rest experiments \nare all based on SVHN, which is too simple. \n\nMore experiments based on other types of data sets with clear global structures such as faces or stop signs will \nbe more convincing.\n\nIn the digit dataset, the local and global structures are relatively easy to separate. However, in Table 1, the \nperformance of VAE+Auxiliary is not better than two of the other methods.\n\nThe idea in this paper is novel but experiments do not seem to be enough. More experiments on datasets \nwith clear global and local structure separations with careful analyses are required to make the paper stronger.']","[-30, 80, -20]","[50, 70, 50]","[""The sentiment score is -30 because the review is generally critical, pointing out limitations and suggesting more experiments. However, it's not entirely negative as it acknowledges the work's potential and offers constructive feedback. The politeness score is 50 because the reviewer uses respectful language, offers suggestions rather than demands, and thanks the authors for their rebuttal. The tone is professional and constructive, even when critical. The reviewer uses phrases like 'Can the authors provide...', 'would be desirable', and 'I thank the authors for clarifying', which contribute to the polite tone. However, it's not extremely polite, as it maintains a direct and critical stance throughout."", ""The sentiment score is 80 (positive) because the reviewer expresses a generally positive view of the paper. They describe the methodology as 'concise and easy to follow', the idea as 'very interesting', and the results as 'great'. The reviewer also states that the paper has a 'novel idea which is well motivated and executed'. While they do suggest some areas for improvement or expansion, these are presented as opportunities rather than criticisms. The politeness score is 70 (polite) because the reviewer uses respectful and constructive language throughout. They acknowledge the strengths of the paper and frame their suggestions for improvement in a positive manner, using phrases like 'It would have been interesting' and 'I would be curious to see'. The tone is professional and courteous, without any harsh or dismissive language."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty of the approach, they express significant concerns about the limited experiments and the need for more convincing results. The reviewer states that 'The idea in this paper is novel but experiments do not seem to be enough,' indicating a mix of positive and negative sentiments, but leaning towards the negative. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive criticism without harsh or rude phrasing. They acknowledge the positive aspects ('The experiment results are good on SVHN') while politely suggesting improvements ('More experiments... are required to make the paper stronger').""]"
"['The paper claims to propose a computationally efficient algorithm for training deep CNNs by making assumptions about the distribution of data. The authors argue that (i) they don\'t make very simplistic assumptions about the data generating distribution as some other papers do, and (ii) their algorithm resembles the actual methods that are used for training deep models and shows some surprising properties of SGD.\n\nThroughout the paper, the authors make a number of assumptions which seem arbitrary at times; not much justifications are provided. The authors claim that their assumptions are not as simplistic as assuming e.g., the inputs are sampled from Gaussian distribution. Unfortunately this is highly unclear: while the ""assumptions"" themselves are complex, the combination of those assumptions may make the problem solution trivial. While proving a lower bound to address this issue may be hard, at least the authors should try to illuminate more why the solution is not trivial (e.g., why a linear classifier doesn\'t work, etc.)\n\nDespite the claims, I find the proposed algorithm very far from the usual SGD-based training methods; this is not a problem per se but I don\'t think that the result illuminates on the effectiveness of SGD (as the authors suggest). The proposed algorithm is a greedy layer-wise method that in each level does a clustering and also trains a ""linear"" CNN with SGD. So the hardness of end-to-end training of a deep network does not show up. Furthermore, it is not clear for training a linear CNN the SGD is even needed.\n\nI suggest that the authors name each of the assumptions and clearly say which ones are assumed for which result. Here are some of the assumptions that the authors talk about.\n\n1_ The data is generated by the following recursive procedure: First a small ""high-level image"" is generated from a distribution, G_0. The ""pixels"" of this high-level image are supposed to encode semantic classes, e.g., sky or ground. In the next step, each of these high-level pixels are turned into a small (lower-level) image. Therefore, we will have a more refined image after the second step. (each semantic class (e.g., sky) has a corresponding distribution that generates the smaller lower-level image (e.g., uniform over 4 possible types of skies)). This procedure continues recursively until we have the final image.\n\n2_ G_0 is ""linearly separable"".\n\n3_ Semantic classes defined in the model are different enough from each other\n\n4_ {F_c} corresponding to semantic classes are linearly independent \n\n5_ Patch Orthonormality (apparently not assumed everywhere) \n\n\nit appears that if one assumes all of 1-5, then the problem becomes trivial (linearly separable). The authors then say that we don\'t want to make assumption 5 for this reason; still, the problem solution may be trivial (authors should at least intuitively justify why it isn\'t )\n\nHere are some more uses of the word ""assumption"".\n\n6_ ""For simplicity of analysis, we assume only the first layer of the network is trained"".\n\n7_ ""We assume the algorithm [KMEANS++] returns a mapping [...] such that [...]"" \n\nThe experiments do not seem conclusive. Only a few experiments have been done. I think the acquired results for CIFAR-10 are below the usual ones using CNNs, and the effects of various hyper-parameters may have interfered.\n\n--\nAfter reading the authors\' response, I still think the way that the contributions are depicted (e.g., a justifying the effectiveness of SGD) are inaccurate/unsupported. \n\nFurthermore, although the authors\' suggest that they have tested a linear classifier and observed that the data is not linearly separable, more explanations/intuitions are needed about the assumptions that are made throughout the paper.', 'After the rebuttal: I appreciate the authors\' effort to revise the paper. The revision made clear that the data produced by the proposed generative model is not linearly separable in general while the theory (Theorem 2) still holds.\n\nI am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence. The fact that the classification algorithm motivated by the generative model can do as well as a similar-sized ConvNet does not quite support that the generative model itself is good -- getting a good classifier is still an easier task than getting a good generative model. \n\n=====================\n\nThis paper proposes a new generative model for natural images. Based on the architecture of the generative model, a “layer-wise clustering” algorithm for image classification is proposed and theoretically shown to converge to an optimal classifier. Experimentally, the algorithm is shown to have similar performances as a baseline CNN on CIFAR-10.\n\nThe main novelty of this paper is the proposed hierarchical generative model and the associated algorithm. One interesting feature is that the network obtained by this algorithm is entirely linear except for the ReLU-pool part. However, the ReLU-pool does not serve as a typical nonlinearity / pooling I believe; rather it sounds to me like a specially tailored step for the theoretical results, which under the “patch orthonormality” assumption is guaranteed to recover the previous layer. Therefore, it surprises me a little bit that the algorithm actually works reasonably well on CIFAR-10. However, as the baseline it compares with is still below ""typical"", I do want to see if this algorithm can be scaled up to match the performance of more complicated (at least pre-ResNet) models such as VGG.\n\nThe theoretical result looks appealing, but I feel like the magic more or less comes from the strong assumptions. In particular, in expectation the output image is just a *linear* operator on the initial (m_0 x m_0 x C_0) one-hot semantic variable. Also, the patch orthonormality assumption implies that intermediate semantics can be perfectly recovered by the (clustering + conv with centroids + ReLU-pool) step, as we are just recovering a partition of a group of orthonormal vectors.', 'The paper first puts forward a generative model for labelled images. The generative model is hierarchical and interesting although a bit complicated. They then show that when there is only one latent layer (i.e., two overall layers) in the generative model, the latent layer can be learned by gradient descent under a linear convolutional model. Inspired by this, the authors present an algorithm for the general case which involves using the two-layer algorithm iteratively to learn each individual layer of the full model. There is a theoretical result proving that this algorithm works. I find the theoretical results interesting. \n\nIt must be said though that the generative model is quite complicated and somewhat unrealistic. The theoretical results are proved under additional stringent assumptions. For example, Theorem 1 applies to Gradient Descent applied to the population loss as opposed to the actual SGD. Also, the GD in practice here is with respect to both K and W. But the analysis is restricted to the setting where W is fixed. Is it possible to prove a version of Theorem 1 that applies to the actual SGD? Further when Theorem 1 is invoked in the proof of Theorem 2 (specifically in the proof of Lemma 8), the fact that Theorem 1 applies to population loss is glossed over? I also fear that the assumptions in Theorem 2 may be too strong. The fact that one can find orthonormal patches in each layer together with the assumption that the images in the final layer are linearly separable might mean that there is some sort of linear separability overall? It would be good if the authors can clarify this. The conclusion of Theorem 2 (that the algorithm learns a hypothesis with zero error) seems too strong to me (perfect classification is usually possible only under clear separability assumptions). \n\nThe paper is also slightly hard to read with too many assumptions of various kinds floating around.\n\n']","[-60, -20, 20]","[20, 60, 50]","[""The sentiment score is -60 because the review is predominantly critical. The reviewer expresses skepticism about the paper's claims, points out several issues with the methodology and assumptions, and states that the experiments are not conclusive. The reviewer also mentions that the results are below usual standards. However, it's not entirely negative as the reviewer acknowledges some potential in the approach, hence not scoring at the extreme negative end.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I suggest' and 'I find' rather than making blunt statements. The reviewer also provides specific recommendations for improvement, which is constructive. However, the score is only slightly positive because the review doesn't go out of its way to be particularly polite or encouraging, maintaining a mostly neutral, academic tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('I appreciate the authors' effort', 'The theoretical result looks appealing'), they express several concerns and maintain their original evaluation. They point out limitations in the experimental evidence and question the practical implications of the theoretical results. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and using phrases like 'I appreciate' and 'I feel like'. They provide constructive criticism without harsh language, maintaining a professional tone. The reviewer also offers suggestions for improvement, which is a polite way to provide feedback."", ""The sentiment score is slightly positive (20) because the reviewer finds the theoretical results interesting and acknowledges the paper's contributions. However, they also express several concerns and criticisms, which temper the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while constructively pointing out areas for improvement. They use phrases like 'It must be said though' and 'I fear that' to soften their criticisms, and they ask questions rather than making blunt statements. The review maintains a professional tone without being overly formal or deferential.""]"
"['\nUPDATE:\n\nI have read the authors’ response and the other reviews.  While the authors have made some improvements, my core criticism remains – the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.\n\nThe authors also seemed to have missed the point of the Kearns & Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.  \n\n\nSummary:\n\nThe paper attempts to link the known mismatch between infinite horizon MDP values and finite trajectory sums to the problem of exploration.  Trajectories in environments requiring exploration (mountain car and a number-line walk) are shown and the effects of changing trajectory lengths and initial values are discussed.  Potential solutions to the problem are proposed though the authors did not deem any of the solutions satisfactory.\n\n\nReview:\n\nThe paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.  Specifically, several issues seem to be studied at once here (including finite-horizon MDPs, function approximation, and exploration), relevant work from the exploration and RL community is not cited, the early experiments do not reach a formal theoretical claim, and the proposed solutions do not appear to adequately address the problem).  These issues are detailed below.\n\nFirst, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories.  While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior.  While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.\n\nThe paper also fails to cite much of the relevant work on these topics.  For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with  epsilon derived from the discount factor (see “Near-Optimal Reinforcement Learning in Polynomial Time”).  In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehl’s thesis “Probably Approximate Correct Exploration in Reinforcement Learning” ).  Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.\n\nIn addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point.  No theorem or proof is given that compactly describes which exact problem the authors have uncovered.  Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.\n\nFinally, several of the references are only ArXiv pre-prints.  Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers).\n', 'The paper addresses the problem of truncating trajectories in Reinforcement Learning. The scope is right for ICLR. \n\nThe presentation is pretty good as far as the English goes but suffers from serious problems on the level of formulating concepts. Overall, I believe that the issue addressed by the paper is important, but I am not sure whether the approach taken by the authors addresses it.\n\nI have the following complaints.\n1. The paper suffers from a fundamental confusion about what problem it is trying to address. There are two straightforward ways to formulate the problem. First, we can formulate the task as solving an MDP with infinite trajectories and ask the question of what we can learn training from finite ones. The learned policies would then be evaluated on the original MDP (in practice using trajectories that are, say, an order of magnitude longer or, for a simple MDP, analytically). Second, we can consider the family of episodic MDPs parametrised by trajectory length T and ask the question of what we can learn by training on some values of T and evaluating on others. These problems are similar, but not the same and should be carefully distinguished. Right now, the introduction reads like the authors were trying to use the first approach but Section 4 reads like they are doing the second: ""If during training we just used times up to T, but deployed the agent in an environment with times greater than T"". Either way, the concept of bias, which appears throughout the paper, isn\'t formally defined anywhere. I believe that a paper that claims to address bias should have an equation that defines it as a difference between two clearly defined quantities. It should also be clearly and formally distinguished which quantities are deterministic and which are random variables. The introduction seems to (implicitly?) define bias as a random variable, section 3.1 seems to talk about ""bias introduced by initializing the network"". As the paper stands now, the working definition of bias used by the authors seems to be that some quantity is vaguely wrong. I do expect a higher standard of clarity in a scientific paper,\n\n2. The paper confounds the problem of learning the value function, specifying the initial estimates of the value function and exploration. The analysis of exploration is entirely informal and suffers from the lack of clear problem formulation as per (1). Of course, one can influence exploration by initialising the value function in various ways, and this may respond differently to different truncations (different values of T), but I don\'t see how it is related to the ""bias"" problem that the paper is trying to address. In any case, I wish the authors either provided a formal handle on exploration or shift the focus of the paper and remove it,\n\n3. I don\'t see what hypothesis the experiments are trying to test. Clearly, if I train my agent on a different MDP and test it on a different one, I get a mismatch. The lack of clear definitions as per (1) comes back with a vengeance.\n\n4. Section 1.1 seems to exist purely to create a spurious impression of formality, which bears little relevance for what the paper is actually about. RL, as traditionally formulated, uses discrete time-steps so the Brownian motion model developed in this section doesn\'t seem very applicable - it is true that it is a limiting case of a family of discrete chains, but I don\'t see how this produces any insights about RL - chains are easy to simulate, so why not test on a chain directly? In any case, the result shown in Section 1.1 is entirely standard, can be found in any textbook on stochastic processes and was likely introduced purely to cover for the lack on any substantial theory that comes form the authors.\n\nTo summarize: if the authors address these points, there is a possibility that the ideas presented in this draft may somehow lead to a paper at some later point. However, I feel that the required changes would be pretty massive and don\'t see how the authors could make it during the ICLR revision phase - the problems aren\'t details or technicalities but touch the very substance of what the paper is trying to do. Basically, the whole paper would need to be re-written.\n\nanother minor point:  sloppy capitalization in section 2.1\n\n===================\n\nFor reasons outlined in my comment below, I updated the score to 5 (for the new heavily updated version).\n', ""Paper summary: This paper focuses on the case where the finiteness of trajectories will make the underlying process to lose the Markov property and investigates the claims theoretically for a one-dimensional random walk and Wiener process, and empirically on a number of simple environments. \n\nComments: The language and the structure of the paper are not on a very good scientific level. The paper should be proofread as it contains a lot of grammatical mistakes. \n\nGiven the assumption that every state's reward is fixed, the theoretical analysis is trivial.\n\nThe comparison of policy gradient methods is too old. The authors should look for more advanced methods to compare.\n\nThe experimental environment is very simple in reinforcement learning tasks, and the authors should look for more complex environments for comparison. The experiment results are hard to interpret. \n\n\n\nQ1: In the theoretical analysis, why should the rewards for each state be fixed?\n\nQ2:Why use r_t – (V(s_t)-\\gammaV(s_{t+1})) as the advantage function?\n\nQ3: What does the “variant” mean in all figures? \n\nTypos: with lower absolute value -> with lower absolute values \n""]","[-70, -80, -50]","[20, -20, -20]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper 'fails to tackle' the issues 'in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.' They also mention multiple shortcomings such as not citing relevant work, lack of formal theoretical claims, and inadequate proposed solutions. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'The paper brings up a number of important issues' before critiquing, and provide detailed explanations for their criticisms rather than making blunt negative statements. However, the language is not overtly polite or complimentary, hence the relatively low positive score."", ""The sentiment score is -80 because the review is highly critical and negative. The reviewer expresses serious concerns about the paper's fundamental approach, clarity, and experimental design. They state that the paper 'suffers from serious problems' and that 'the whole paper would need to be re-written.' The reviewer does not recommend acceptance in its current form. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism and somewhat dismissive language. For example, phrases like 'I do expect a higher standard of clarity in a scientific paper' and describing a section as existing 'purely to create a spurious impression of formality' come across as somewhat rude. However, the reviewer does acknowledge the importance of the topic and provides detailed feedback, which prevents the score from being even lower."", ""The sentiment score is -50 because the review is generally negative, pointing out several issues with the paper such as poor language, grammatical mistakes, trivial theoretical analysis, outdated comparisons, and overly simple experiments. However, it's not entirely negative as it does provide some constructive feedback and questions for improvement. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite blunt and critical without much effort to soften the criticism. Phrases like 'not on a very good scientific level' and 'trivial' come across as somewhat harsh. The reviewer does ask some clarifying questions at the end, which slightly improves the politeness, but overall the tone remains rather brusque.""]"
"['Authors present a new LSTM architecture, i.e. reduced gate convolutional LSTM. Authors use only one trainable gate, i.e. the forget gate which leads to less trainable parameters. They demonstrate the superiority of ti in two datasets, the moving MNIST and KITTI. Their results show that their architecture performs better than others in more accurately predicting next-frame. The paper is clearly written and the evaluation is based on other proposed similar architectures. For the moving MNIST they compared their model against the vanilla convLSTM with three gates and no peephole connections, based on previous work which has shown that it exceeds the accuracy performance of other LSTM based approaches. \nThe results show that the training time is reduced and the standard error alike. The only limitations is that more databases could have been used to demonstrate the enhancement in performance.', 'The authors propose a variant to convLSTMs with convolutional peephole units (as opposed to the original Hadamard product) and tied gates. The description of the model is confusing, the authors don\'t offer a strong justification for the proposed approach, some of the technical choices seem flawed.\n\nIt is also obviously false that removing an LSTM gate does not incur in a reduction of trainable parameters: ""There were other attempts to design smaller recurrent gated models [...] based on either removing one of the gates or the activation functions [...] these models had no significant reduction in trainable parameters"".\n\nThe model description is difficult to follow, but from what I can gather the model depiction is flawed. In particular, the proposed model bases on the idea to use tied gates to reduce the number of parameters. f^{(t)}, i.e., the value that governs the input, forger and output gates, is derived via a sigmoid. When the sigmoid is zero, the network will hence set all the gates to zero, ignoring the input, forgetting the memory and suppressing the output, which is hardly a behaviour one could ever want. Unfortunately, almost half of the sigmoid spectrum will lead to such behavior. This seems to be a poor modeling decision.\n\nThe nodes in Figure 1 seem to suggest that two consecutive non-linearities are applied in a row, if W_fc is clamped to zero, as reported by the authors, there is no reason to specify it, the corresponding part of the equation can be removed and the narrative simplified. In sec3, the meaning of “net input image” and “network gate image value” is unclear. Also the description of the equation is hard to follow, e.g., the square bracket notation is eventually explained only after 8 lines of text.\n\nAt the beginning of Sec4 err_1 and err_2 are defined as the difference between the predicted frame and the target frame, and vice versa. This error is then fed to the “rgcLSTM input arranger unit and to the next higher layer”. By my understanding the error of one layer is fed in the next as an input. I wonder if such error is provided also at inference time, giving a clear guidance to the network to produce the correct output exploiting privileged information. This could also explain why the training process was completed in only one epoch.\n\n- Typos:\n*Intro: More important → more importantly\n* page5: ReL -> ReLu', ""This manuscript proposes replacing the three gates in the standard LSTM with one gate to reduce the number of parameters and the computation time. The proposed reduce-gate convolutional LSTM is applied in PredNet to predict next frames of a video. \n\nThe main contribution of this paper is proposing an efficient convolutional LSTM. Although the number of the parameters and the training time in the experiments support this statement, the description in the paper is very confusing. \n\n1) In the standard LSTM, the cell state c^{t-1} is not an input for the computation of the three gates and the cell state's candidate. That is, in Eq(15) 2\\kappa and 2n should be \\kappa and n. Compared to Eq(14), it may not show that the standard LSTM has more parameters than rgcLSTM.\n\n2) Eq(5) and Eq(6) are not consistent. If I_g = I_f, the coefficient before \\kappa should be 2; otherwise, the input update shouldn't include c^{t-1}. \n\n3) The intuition of having one gate instead of three is not very clear in the paper. Mixing all the functions, i.e., input, forget, and output filters, does provide freedom for learning but also introduces the learning complexity. Decoupling a complex function into a couple of simpler functions is a common way to narrow down the searching space. This is exactly what the standard LSTM does. The authors may want to provide reasonable arguments to explain intuitively why using one gate is better. \n\nThe model performance comparison in the experiments would be fairer if let the models converge. Perhaps, the standard LSTM is just suffering the gradient-vanishing issue and using ResNet design, for example, might improve the performance. Similarly, the rgcLSTM has fewer parameters as shown in the experiments. A possible explanation for its demonstrated better performance could be that it's less suffering the vanishing gradient. But, this doesn't indicate it's better than the standard LSTM. In summary, experimental results are good to verify the goodness of a model, but the theoretical support or intuitions are more important to understand the benefits that we can gain by making this change to the standard LSTM. This part is not clear from the paper. ""]","[80, -80, -20]","[50, -20, 50]","[""The sentiment score is 80 (positive) because the reviewer praises the paper's clarity, the authors' demonstration of their architecture's superiority, and the improved performance in terms of training time and standard error. The only criticism is minor, suggesting more databases could have been used. The politeness score is 50 (slightly polite) as the reviewer uses neutral, professional language without overly formal or polite phrases, but also avoids any rudeness or harsh criticism. The review focuses on the paper's merits and provides a balanced, constructive assessment."", ""The sentiment score is -80 because the review is highly critical of the paper. The reviewer points out multiple flaws in the model description, justification, and technical choices. They use strong negative language like 'obviously false', 'flawed', and 'poor modeling decision'. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite harsh and dismissive. They directly challenge the authors' claims and understanding without much softening of the criticism. However, they do provide some constructive feedback and point out typos, which prevents the score from being even lower."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the main contribution of the paper, they express several concerns and criticisms about the clarity of the description, inconsistencies in equations, and lack of intuitive explanation for the proposed changes. The reviewer also suggests that the experimental results may not fully support the claimed benefits of the new model. However, the score is not deeply negative as the reviewer does recognize some positive aspects and provides constructive feedback. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout, offering specific suggestions for improvement and explaining their concerns in detail without using harsh language. The reviewer balances critique with acknowledgment of the paper's contributions, which contributes to the polite tone.""]"
"['The authors propose to study what they call weak contraction map. The idea may have its merits, but in the present form, it is not acceptable.\n\nNotably, the key definition of the paper, that is that of the weak contraction mapping (starting with ""Then a mapping T : X → X is called weak"") is incomplete, because it uses a \\mathcal{R}, which is never defined. This makes it hard to evaluate any of the results. \n\nFurther, there is no clear separation between text and theorems. Worse, the theorems are not self-contained. E.g. what could perhaps be Theorem 2 (Starting with ""x* is a fixed-point of T in X0."") does not define x*.\n\nWhile I cannot be certain because of the reasons stated above, the authors seem to have had in mind something like the Kakutani theorem:\nhttps://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem\nwhich they don\'t cite. Their assumptions on the map are weaker than that of Kakutani (upper hemicontinuity), which makes me a bit doubtful as to whether the statements could be proven, even if made precise. ', 'This paper proposes an approach to zeroth order optimization based on the Banach fixed point theorem for contractive maps. They define a ""weak contraction map,"" argue that it will have a unique fixed point, and use this to propose a zeroth order optimization algorithm which iteratively identifies sublevel sets of the objective until convergence to the optimum.\n\nAt each iteration $t$, the $f(x_t)$-sublevel set of $f$ is found using a root-finding algorithm, and the next point $x_{t+1}$ is calculated by averaging a collection of points on the boundary of the sublevel set.\n\nMy main concern about this paper is that the optimization algorithm works neither in theory nor in realistic practical scenarios. There are two main issues:\n(1) Identifying the sublevel sets requires solving equations of the form $f(x) = L$, which is just as hard as optimizing $f$ in the first place! In many realistic scenarios, e.g. machine learning problems, you know what the minimum value of the function is, so you could just solve $f(x) = f^*$ and be done in one step! Even if you don\'t know the optimal value, you could do some version of binary search. Also, for the two or three dimensional problems with relatively simple expressions that the authors experimented on, finding roots might be possible, but for higher dimensions or more complicated functions, finding these roots would require numerical optimization--which is the problem we are trying to solve in the first place.\n(2) The authors seem to imply that this algorithm would work for any $f$, however, consider the function in 1 dimension $f(x) = 1$ for all $x \\neq x^*$, and $f(x^*) = 0$. For this function, the $f(x_t)$-sublevel sets are the entire domain until $x_t = x^*$. It is unclear what ""points on the contours"" would mean in this case, but whatever those contours are, the algorithm would never converge on this function because the function value of $x_0, x_1, ...$ would all be the same, so the contours would remain the same. This function might seem a little ridiculous, but continuous or even Lipschitz version of this counterexample could be constructed by smoothing things out around $x^*$, and a function such as this could be obfuscated by writing it down with a long, complicated expression making it hard to identify $x^*$ by inspection.\n\nThere are some typos/typesetting issues. It seems that all of the theorem and lemma statements in Section 2 are missing the bold ""Theorem"" and ""Lemma"" heading. In the paragraph after equation (2), ""weak contraction mapping"" is defined twice, I believe the first definition should be just a ""contraction mapping.""', 'This paper considers self-maps of metric spaces where the range is strictly smaller than the domain.  Under this condition this tries to show that such a map has a fixed point.  Now the paper suggests that such a ""weakly contractive"" map has a fixed point and tries to use such maps to find the global minima of functions.  \n\nEven if all the proofs in this paper were right I do not see what this has anything to do with learning and why such a paper has been submitted to ICLR! This paper should probably be submitted to an optimization journal!\n\nThe basic proofs here are completely unclear. Like in Lemma 1.2, its not even clear what the variable ""x"" is in the limit! The limit is being taken over the sequence index as far as I can see. Top of page 3 tries to describe an algorithm which can leverage weak contraction to get the global minima if it exists. But this description is hardly making any sense to me. I don\'t see how the function to be optimized is being used to define the weakly contractive ""T"" map in the paragraph just below the proof of Lemma 1.4. (How is that parameter ""h"" even chosen in the definitions of X^{>} and X^{\\leq}?) \n\nWithout a clear pseudocode there is almost nothing concrete in the paper to judge correctness by. The experiments are all set-up on standardized functions which have nothing to do with learning setups. So the relevance of the experiments is completely unclear, let alone the fact that the description is too muddled up.  \n\nAlso the notation used in the paper is highly non-standard and that makes reading very difficult. For example ""D"" seems to be the symbol for diameter of the metric space. So D is a property of the metric space (X,d) and its not a part of the definition of the metric-space as the weird notation ""(X,d and D)"" seems to suggest!  Also the definition 2 is ambiguous because it uses a {\\cal R} which doesnt seem to have been defined anywhere!\n\n']","[-70, -70, -80]","[-20, 20, -40]","[""The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper is 'not acceptable' in its current form and points out several significant issues, including incomplete definitions, lack of clarity, and potential lack of originality. The politeness score is -20 because while the reviewer isn't overtly rude, the language is quite direct and critical without much attempt to soften the criticism. Phrases like 'not acceptable' and 'Worse, the theorems are not self-contained' come across as somewhat harsh. The reviewer does acknowledge that the idea 'may have its merits', which prevents the scores from being even lower. The reasoning behind the scores is based on the overall negative tone, the direct criticism, and the lack of positive or encouraging comments."", ""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's approach, stating that the optimization algorithm 'works neither in theory nor in realistic practical scenarios.' They provide detailed explanations of two major issues, which form the bulk of the review. The negative sentiment is clear, but not entirely harsh, as the reviewer does provide constructive feedback and acknowledges some aspects of the paper.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'My main concern' and 'It is unclear' rather than more aggressive language. The reviewer also provides detailed explanations for their concerns, which is a courteous way to give feedback. However, the score is not higher because the review is predominantly critical and doesn't include many positive remarks or encouragement.\n\nThe reviewer also points out some minor typographical issues at the end, which is helpful but delivered in a neutral tone."", ""The sentiment score is -80 because the reviewer expresses strong negative opinions about the paper. They state that they don't see the relevance to learning or ICLR, suggest it should be submitted elsewhere, criticize the proofs as unclear, and find the experiments irrelevant. The lack of positive comments and the repeated use of negative phrases indicate a highly negative sentiment. The politeness score is -40 because while the reviewer doesn't use explicitly rude language, their tone is quite harsh and dismissive. Phrases like 'hardly making any sense to me' and 'there is almost nothing concrete in the paper to judge correctness by' are particularly blunt and could be seen as impolite in academic discourse. The reviewer also doesn't offer any constructive feedback or soften their criticisms, which contributes to the overall negative politeness score.""]"
"['Strength: \n\nThe proposed approach is architecture-independent: the attack is constructed only from the dataset.\n\nWeaknesses: \n\nPaper is not sufficiently well written for a venue like ICLR. \nAttack has very low success rate.\nTo the exception of Figures 4 and 5, many experiments are conducted on MNIST.\n\nFeedback: \n\nExperimental results show that the attack is able to degrade a classifier’s performance by inserting perturbations that are computed on the data only. However, there are no baselines included to compare adversarial evasion rates achieved here to prior efforts. This makes it difficult to justify the fairly low success rate. In your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?\n\nFurthermore, strong conclusions are made from the results despite the lack of supporting evidence. For instance, on P10, the attack is said to “also explains why adversarial examples can be universal.”. However, not only does the attack achieve less success than universal adversarial examples would (so it cannot explain all of them) but also does it not share any characteristics with the way universal adversarial examples are crafted. Drawing such a strong conclusion thus most likely needs a lot more supporting evidence. \n\nSeveral directions would improve the content of the paper: \n\n* Complete existing experimental results by being more systematic. For instance, in Section 3.1, measurements are only performed on one pair of MNIST images. Without studying a significant portion of the test set of two datasets, it is very difficult to draw any conclusions from the limited evidence.\n* Perform a human study to have all perturbed images labeled again. Indeed, because of the ways images are perturbed here, it is unclear how much perturbation can be added without changing the label of the resulting input. \n* Study how existing adversarial example techniques modify internal representations. This would help support conclusions made (e.g., about universal perturbations---see above). \n* Rewrite the related work section to scope it better: for instance, Sabour et al. in Adversarial Manipulation of Deep Representations and Wicker et al. in Feature-Guided Black-Box Safety Testing of Deep Neural Networks explore adversaries operating in the feature space. This will also help build better baselines for the evaluation.\n\nAdditional details: \n\nTLDR: typo in the first word\nP1: The following definition of adversarial examples is a bit restrictive, because they are not necessarily limited to vision applications (e.g., they could be found for text or speech as well). “Adversarial examples are modified samples that preserve original image structures” \nP1: The following statement is a bit vague (what is obvious impact referring to?): “Experiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks.”\nP1: References do not typeset properly (the parentheses are missing: perhaps, the \\citep{} command was not used?)\nP2: What is the motivation for including references to prior work in the realm of image segmentation and more generally-speaking multi-camera settings in the related work section? \nP2: Typo in “ linear vibration”\nP2: It remains difficult to make a conclusion about humans being robust to the perturbations introduced by adversarial examples. For instance, Elsayed et al. at NIPS 2018 found that time-constrained humans were also misled by adversarial examples crafted to evade ML classifiers: see Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.\nP2: Prior work suggests that the following conclusion is not entirely true: “Most of these kinds of examples are generated by carefully designed algorithms and procedures. This complexity to some extent shows that adversarial examples may only occupy a small percentage for total image space we can imagine with pixel representations.” For instance, Tramer et al. in ICLR 2018 found that adversarial subspaces were often large: “Ensemble Adversarial Training: Attacks and Defenses”.\nP2: Others have looked at internal representations of adversarial examples so the following statement would be best softened: “To the best of our knowledge, this paper should be the first one that discusses adversarial examples from the internal knowledge representation point of view.”. See for instance, Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning by Papernot and McDaniel.\nP3: Could you add pointers to support the description of human abstraction and sparsity? It reads a bit speculative as is, and adding some pointers would help relate the arguments made to relevant pointers for readers that are less familiar with this topic. \nP3: What is the motivation for including the discussion of computations performed by a neural network layer-by-layer in Section 2?\nP4: Given that saliency maps can be manipulated easily and are only applicable locally, it appears that Figure 1 is too limited to serve as sufficient evidence for the following conclusion: “This, in some way, proves the point that the knowledge storage and representation of current neural networks are not exactly sparse prototype based.”\nP5: The error rate reported on MNIST is quite low (45%). Even using the Fast Gradient Method, one should be able to have the error rate be as high as 90% on a standard CNN.\nP7: Would you be able to provide references to backup the following statement? “This is a network structure that is very common.”\nP10: How does the discussion in Section 4.2 relate to the attack described in the submission?\n', '**First of all, this paper uses 11 pages**\nSubmission instruction is ""There will be a strict upper limit of 10 pages.""\n\nThe readability of the manuscript should be improved. \n\nI\'m not convinced why Chapter 2 motivates Chapter 3. I think Ch. 2 and Ch. 3 are different stories. \n\n\n ', 'The paper discusses two ways of constructing adversarial examples (images) using PCA+knn in the input space. Compared to the litterature on adversarial examples, the modifications proposed by the authors are clearly visible to the human eye and the resulting images do not seem natural (see Figure 4 and 5). The authors acknowledge this difference between their work and the state-of-the-art (e.g., ""Modified images are sometimes visible but still can keep original structures and information, this shows that adversarial images can be in more forms than small perturbations"", Section 4.1), but it remains unclear why generating such images would be interesting in practice.\n\nThe algorithm for generating adversarial examples from nearest neighbors and PCA is reasonable. It seems simple and fairly easy to implement. However, it does not seem to be competitive with the current litterature for generating adversarial examples. An important point of the authors is that their method constructs ""adversarial"" samples without taking into account the specific structure of neural networks (more generally, without any knowledge of the classifier). This claim would have more practical impact if the method was shown to fool more algorithms/types of models than usual approaches (e.g., fast gradient sign). But there is no comparison to the state-of-the-art, so it is unclear in what situation the method should be interesting.\n\nI found the motivation based on knowledge representation rather confusing, and I found no clear arguments for the PCA nor the k-nn approach. The write-up uses statements that are vague or not properly justified such as ""For human beings, both abstraction and sparsity can be achieved with hierarchical storage of knowledge. This can be similar to object-oriented programming"" (why is object-oriented programming relevant here?, is there any justification and formal statement for the first claim (e.g., a reference)?), ""Neural networks store learned knowledge in a more hybrid way"", ""In summary, human beings can detect different objects as well as their transformations at the same time. CNNs do not separate these two."" (there is no clear experiment proving that there is no ""separation"" because it is unclear what the DeepDream visualization of Figure 1 effectively proves). The equations do not really help (e.g., X_2 is supposed to be a vector if I understand correctly, what is X_2^{-1}?). Overall, I think the paper would gain a lot by making the motivation part more formal.\n\nIn conclusion, the authors seem to depart from the idea that adversarial examples should a) fool the network but b) still feel natural to humans. There is no clear motivation for generating such unnatural adversarial examples, and there is no clear application scenario where the algorithm would generate ""better"" adversarial examples than usual methods.\n\nminor comments:\n\n* please add spaces before \\cite commands \n']","[-50, -60, -70]","[20, -20, 20]","[""The sentiment score is -50 because the review highlights several major weaknesses and suggests significant improvements, indicating overall negative sentiment. However, it does acknowledge some strengths, preventing an extremely negative score. The politeness score is 20 because the reviewer uses polite and constructive language throughout, offering suggestions rather than harsh criticisms. They use phrases like 'could you clarify' and 'would improve' which maintain a respectful tone. However, the directness of some critiques prevents a higher politeness score. The reasoning is based on the balance of positive and negative comments, the specific language used, and the overall tone of the review."", ""The sentiment score is -60 because the review is predominantly negative. The reviewer points out multiple issues: the paper exceeds the page limit, has poor readability, and lacks a clear connection between chapters. There are no positive comments to balance these criticisms. The politeness score is -20 because while the language isn't overtly rude, it's quite blunt and lacks any softening phrases or constructive feedback. The reviewer states problems without offering solutions or using polite language to cushion the criticism. The use of 'I'm not convinced' and the abrupt style of feedback contribute to the slightly impolite tone."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer criticizes the paper's approach, stating that the adversarial examples are clearly visible and unnatural, unlike state-of-the-art methods. They also question the practical relevance and competitiveness of the proposed method. The reviewer finds the motivation confusing and poorly justified, and concludes that there's no clear application scenario where this method would be better than usual methods. However, it's not entirely negative as the reviewer acknowledges that the algorithm is reasonable and fairly easy to implement.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone throughout. They use phrases like 'I found' and 'I think' to soften their criticisms, and provide specific suggestions for improvement. The reviewer also acknowledges some positive aspects of the paper. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.""]"
"['Summary:\nThis paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as “ultrafine-grained” class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of “class embedding” for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.\n\nComments:\nArguably, the primary contribution of this paper is the introduction of a new “ultrafine-grained” classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it’s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It’s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.\n\nThe model introduced is interesting, but its novelty is limited. It’s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an “embedding” is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.\n\nOther Comments:\n-Since you ran a cross-validation, you should add confidence intervals to your reported numbers\n-One additional dataset detail I was hoping to see that you didn’t provide is the mean/standard deviation of the number of instances per class,\n-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well\n-The title of your paper is somewhat misleading - it’s hard to argue that the form of class embedding you use is a “deep” class embedding since it’s just a matrix of parameters that are learned during training.\n\nOverall, I’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.\n\n[1] Ma, Xuezhe, and Eduard Hovy. ""End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.""\n\n\nREVISION:\nThe other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.\n', ""This paper tackles the problem of estimating pairwise potentials when the number of labels is large. Two modifications are proposed: one is to factorize the matrix for pairwise potentials, and the other is to approximate the log likelihood objective with the MEMM objective.\n\nThe problem and the proposed approach are well motivated. It is particularly useful to draw the connections between MEMM and piecewise-pseudolikelihood.\n\nThe major weakness of the paper is whether the approximations are necessary. It is hard to see why approximating the log likelihood with MEMM is necessary, because inference and computing the gradients of the log likelihood have the same computational complexity. So the authors could have trained the model with the log likelihood.\n\nRegardless, it is still valuable to compare MEMM and log likelihood for training CRFs. However, the authors fail to show how well MEMM approximates the log likelihood. For example, the authors can compare the solutions when optimizing with the gradients of log likelihood and the with the gradients of MEMM. It is especially important to compute the training log likelihood for the two solutions, as it tells us how well MEMM approximates the log likelihood. This is also true for the low-rank approximation of the pairwise potentials. The authors fail to compare the case with low-rank approximation and the case without. It is important to evaluate the training error first with both methods as they share the same objective. This type of comparison should be apply to batch normalization as well.\n\nApproximating the pairwise potentials with matrix factorization is also not novel.  See the list below. (The list is by no means exhaustive. Please see the citations therein.)\n\nDense and low-rank Gaussian CRFs using deep embeddings\nChandra et al., ICCV 2017\n\nEfficient SDP inference for fully-connected CRFs based on low-rank decomposition\nWang et al., CVPR 2015\n\nNeural CRF parsing\nDurrett and Klein, ACL 2015\n\nFinally, some of the claims made in the paper (listed below) should be more careful.\n\np.4\n\nthe likelihood function, therefore, is log-linear and concave.\n--> concave in what?\n\nthe scoring function is still concave, ...\n--> concave in what?\n\nthe objective function is no longer linear or concave with respect to the network parameters, ...\n--> what are the network parameters?\n\nbut deep learning training techniques have been shown to yield good results ...\n--> this argument is weak. the key is point out that SGD is used, plus SGD has been shown to work well on many matrix factorization problems. see the paper below.\n\nOnline learning for matrix factorization and sparse coding\nMairal et al., JMLR 2010\n\np.5\n\nthe test time inference uses a global normalization ... avoids the label bias problem.\n--> the partition function is not even computed when using Viterbi. I'm also not sure how this avoids the label bias problem.\n\nwhitening the inputs to each layer may also prevent converging into poor local optima.\n--> this is a hand-wavy claim. it would be best if the authors can provide citations to the claim.\n"", 'This paper proposed to tackle a large-scale fine-grained object classification problem by approximated CRF. The main motivation is to exploit the spatial conference of object labels to reduce noises in the instance-wise prediction. To this end, the task is formulated by sequential inference problem using CRF. To speed up training, several techniques are applied such as factorized pairwise-potential and approximation of CRF objective.  \n\nAlthough the paper presented a reasonable idea for their particular problem (i.e. classification of products in the store display), the significance of the work is quite limited as the same idea is not generally applicable to other settings (e.g. there is no strong spatial correlation of labels in general images). Also, the performance improvement over the instance object classification is not significant as shown in Figure 5 (Unary vs. Approximate factorized). Due to the limited significance and impact of the work, this reviewer suggests a rejection of this paper. \n']","[-20, -20, -70]","[50, 50, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (interesting dataset, outperforming baselines), they express significant doubts about the novelty and broader impact of the work. The review concludes by stating the paper is not ready for acceptance, indicating an overall negative sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive feedback, and frames criticisms as suggestions (e.g. 'I would have liked to see...'). The reviewer also acknowledges the interesting aspects of the work alongside the criticisms. However, the tone remains professional rather than overtly friendly, hence the score is not higher."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well motivated', 'valuable to compare'), they express significant concerns about the necessity and novelty of the approach, and point out several weaknesses. The reviewer also provides a list of specific criticisms and requests for additional comparisons and clarifications. The politeness score is moderately positive (50) as the reviewer uses professional language throughout, acknowledges strengths, and frames criticisms as suggestions for improvement rather than harsh judgments. The reviewer uses phrases like 'it is still valuable' and 'it would be best if' which maintain a constructive tone. However, the review doesn't go out of its way to be overly polite or complimentary, keeping the score from being higher."", ""The sentiment score is -70 because the reviewer suggests rejection of the paper due to limited significance and impact. They acknowledge the reasonable idea but emphasize its limited applicability and insignificant performance improvement. The politeness score is 20 because the reviewer uses professional language and acknowledges some positive aspects before critiquing, but the overall tone is critical. They use phrases like 'reasonable idea' and 'although' to soften the criticism, but ultimately deliver a negative assessment without excessive courtesy.""]"
"[""# Summary of the paper\n\nThis paper proposes an embedding of directed graphs based on the SVD of a normalized adjacency matrix. This embedding is shown to be equivalent to the spectral embedding of a co-citation graph, which is more complex to calculate. Interestingly, the proposed approach does not require the *explicit* representation of this graph. Moreover, the paper also shows that distances of the embedded vectors are proportional to mean commute times of a forward--backward random walk in the original graph. A suite of experiments is run on graphs from KONECT.\n\n# Review\n\nThis is a well-written paper, which I enjoyed reading. The extension of embeddings to the case of directed graphs is significant and warrants a detailed exploration.\n\nThe principal issues I see with this paper are as follows:\n\n- The originality or scope of the contribution is not clear\n- The experimental section is uncompelling\n- Several relevant works appear to have been ignored\n\nOverall, I like the way the paper treats the subject. In particular, I appreciate the fact that proofs are explained well; additionally, code is provided, which will increase reproducibility. This is uncommon and praiseworthy!\n\nAs for the originality of the paper, I find it hard to judge the scope of the contribution. The paper is extremely well written and employs a very pedagogical treatment of the subject, which I appreciate. Yet, it is hard for me to judge the utility and novelty of the proposed method in light of Section 8, where the paper shows that a spectral embedding of the undirected variant of the graph leads to essentially the *same* eigenvectors (up to renormalization and permutations). To prove that a new method is more effective, this point should be emphasized more:\n\n1. In a sense, I would see the results from Section 8 as the equal to what 'Laplacian Eigenmaps' (LE) yields. This needs to be stressed, and analysed in an experimental section.\n\n2. I understand that the order of the singular vectors is different, so embeddings that use only parts of them will be different. However, a convincing experiment should assess the differences. For example, in which regime for $d$ (number of used vectors) will the new method be surpassed by the old one? Is there such a regime? Ideally, this will be answered in the form of an asymptotic theorem; it could also be a larger experiment, though (to simulate the conditions in practice).\n\n3. I understand that the new approach has a lower run-time, because the SVD is more efficient than eigendecomposition. However, what about a simple baseline algorithm that uses SVD for the *undirected* graph of the input data? This should be simple to accomplish, and would be a way to ascertain the benefit of using edge directions.\n\n  To my understanding, LE should be this embedding, but from the table, I can see that its runtime is a lot worse than the novel method. What causes this? The fact that eigendecomposition is used instead of SVD?  If so, an additional SVD-based approach should be implemented.\n\nThis brings me to the experimental section. Here, the paper demonstrates the superiority of the new embedding based on evaluating modularity of a set of different clusterings of larger graphs, obtained using $k$-means. I have several concerns about this:\n\n1. Modularity has problems with larger networks because only a small part of the network will be used in its configuration.\n\n2. Since the embeddings cannot be easily compared due to missing ground truth information, other metrics should be employed. Here are a few, which are often used  by the community. See 'Is there a best quality metric for graph clusters?' by Almeida et al. for more details and a description of their shortcomings:\n    - Silhouette coefficient\n    - Coverage\n    - Conductance\n\n  Different ones should be evaluated here in order to show the behaviour of the new embedding. Do the embeddings differ if the modularities are similar?\n\n3. How do the results change for different values of $d$? I find it hard to disentangle such a discussion from instabilities in $k$-means, but to my understanding of the method, tuning $d$ means that more or less information is used from the singular vectors.\n\n   This could also be quantified in a proof (about asymptotic behaviour) but an experiment would be equally fine.\n\nConcerning the bibliography, or the treatment of prior works, there are some issues:\n\n- There appear to be some missing references of earlier works that used SVD or variants in order to cluster graphs or embed them:\n\n  - Drineas et al.: 'Clustering Large Graphs via the Singular Value Decomposition'\n  - Malliaros and Vazirgiannis: 'Clustering and Community Detection in Directed Networks: A Survey'\n\n- Likewise, the use of pseudo-inverse Laplacians has a lot more papers attached to it (these are only a few that are relevant):\n\n  - Ho and Dooren: 'On the pseudo-inverse of the Laplacian of a bipartite graph'\n  - Gutman and Xia: 'Generalized inverse of the Laplacian matrix and some applications'\n\n# Suggestions for improvement\n\n- In some sense, this work can be seen as an extension of Laplacian eigenmaps to the directed case. The paper needs to be more clear about these extensions with respect to prior work. In Section 2, it is claimed that 'our main contribution is a proper normalization'. This strikes me as a rather small contribution in light of the experimental section, as outlined above.\n\n- I am also hesitant to speak about a better interpretability of the mean commute time. I agree that it is nice to know that the distance permits such an interpretation in terms of random walks, but what is the impact of knowing the MCT? It is not only used in the embedding insofar as one obtains a vector representation.\n\n- Section 5 is then the standard way of defining random walks based on a Laplacian matrix, and the correspondence to the pseudo-inverse of the Laplacian is shown. This is mathematically interesting, but appears to me to be in line with previous research.\n\n- The section about co-citation graphs should make it more clear that 'successors' are to be taken in terms of the original graph and the directionality of edges. Since this is a standard definition in the domain of network analysis I would suggest citing a textbook here.\n\n- In Section 6, the paper could give more details about random walk concepts such as 'stochastic', 'stationary distribution' etc., as it would make the paper more accessible (I am familiar with these concepts but since the writing of the paper is of high quality in the other sections, I am convinced this would improve its impact, and attract more readers).\n\nTypos & grammar issues:\n\n- 'in terms of random walk' --> 'in terms of random walks'\n- 'equivalent to build' --> 'equivalent to building'\n- 'with corresponding unitary matrix' --> 'with a corresponding unitary\n  matrix'\n- 'square Euclidean distance' --> 'squared Euclidean distance'\n- 'equivalent to consider' --> 'equivalent to considering'\n- 'irreductible' --> 'irreducible'\n- 'and provide generally' --> 'and provides generally'\n- 'in low dimension' --> 'in a lower dimension'\n\nFurthermore, the bibliography should employ consistent capitalization and journal names for articles."", 'The paper studies embeddings of directed graphs based on SVD. It proposes an interpretation of the embedding obtained from the normalized adjacency matrix in terms of the forward-backward random walk on the graph. In such random walk odd steps are taken using the edges of the graph while even steps are taken using reverse edges. Such an interpretation seems to be a trivial extension of the work for undirected graphs. \n\nFor example, consider bipartite graphs, in such graphs the forward-backward random walk can be seen as a standard random walk on the graph with directions removed. Indeed, if the walk starts in part A then remove all edges from B to A and make A to B edges undirected.\n\n', 'The paper discusses how to embed nodes of a graph to a vector space using singular value decomposition of the normalized adjacency matrix. The material in this paper are standard literature in graph theory and spectral analysis. The paper contains a well-written literature review. The contributions and not new enough for a publication.']","[-20, -50, -50]","[70, 0, 20]","[""The sentiment score is slightly negative (-20) because while the reviewer expresses some positive aspects ('well-written paper', 'enjoyed reading', 'pedagogical treatment'), they also raise several significant concerns about the paper's originality, experimental section, and missing references. The overall tone suggests that major revisions are needed. The politeness score is relatively high (70) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms constructively. They use phrases like 'I appreciate', 'This is uncommon and praiseworthy!', and provide detailed suggestions for improvement. The reviewer maintains a professional and courteous tone even when pointing out weaknesses in the paper."", ""The sentiment score is -50 because the reviewer's tone is somewhat negative. They describe the paper's contribution as a 'trivial extension' of existing work, which implies that the novelty and significance of the research are limited. The reviewer also provides a counterexample (bipartite graphs) to challenge the paper's approach, further indicating a critical stance. However, the review is not entirely negative, as it does acknowledge that the paper studies embeddings and proposes an interpretation, which prevents the score from being lower. The politeness score is 0 (neutral) because the language used is neither particularly polite nor rude. The reviewer states their critique directly without using overly harsh language or personal attacks, but also without any notably courteous phrases or positive reinforcement. The tone is professional and matter-of-fact, focusing on the content rather than on pleasantries or rudeness."", ""The sentiment score is -50 because the review is generally negative, stating that the contributions are 'not new enough for a publication.' However, it's not entirely negative as it acknowledges a 'well-written literature review' and that the paper discusses standard material in the field. The politeness score is 20 because the language is professional and not overtly rude, but it's also not particularly warm or encouraging. The reviewer uses neutral language to convey criticism, such as 'standard literature' and 'not new enough,' rather than harsh or dismissive terms. The slight positive politeness comes from the acknowledgment of the well-written literature review.""]"
"['This paper proposes an autoencoder architecture and training procedure for producing high-quality reconstructions and realistic interpolations. A ""generator"" autoencoder is trained to fool a ""discriminator"" autoencoder. The generator tries to minimize its own reconstruction error and minimize the reconstruction error of the discriminator when fed with interpolated latent vectors of real datapoints. The discriminator autoencoder has three losses, corresponding to minimizing reconstruction error on real datapoints and maximizing reconstruction error on the generator\'s output on both real datapoints and interpolated outputs. The authors also propose a loss which encourages the distances between real datapoints and their corresponding latent vectors to be similar, as well as a heuristic procedure for stabilizing GAN training. Qualitative results are shown on CelebA.\n\nWhile the results look nice, the paper is not fit for publication in its current form. At a high level, the issues include a lack of convincing experimental verification of the method, a generally contradictory and confusing description of the methods, and frequent factual errors or mischaracterizations. Here I will try to describe many of the issues I found while reading the paper:\n- Experimental results are only given on CelebA which is a dataset with a very strong and easy-to-model structure. The experimental results are completely qualitative. No effort is made to provide a quantitative proof of claims such as ""the reconstructions are less blurry"" or ""the interpolations are higher quality""; only a few examples are shown. The experiments are not even described in the text, and many of the figures are unreferenced. No ablation studies are done to determine the importance of different loss terms, such as L_dist. No mention is given to how hyperparameters like alpha should be chosen (and in fact, the value given for it ""1^{-4}/2"" is nonsense; 1^{-4} is just 1). No results for a baseline autoencoder (i.e., just optimizing reconstruction loss) are given.\n- At a higher level, no effort is given to argue why interpolation is a useful characteristic to try to encourage. There are no downstream applications proposed or tested. Earlier models, such as VAEGAN, also give reasonable reconstructions and good interpolations. Why is GAIA better? On what problem would I use GAIA and achieve better results apart from making nice-looking interpolations of people\'s faces?\n- Definitions are often unclear or contradictory. For example, the generator autoencoder is alternatingly treating as taking input X and taking input Z. I believe what is meant is that the generator consists of two networks which compute Z = encoder(X) and X = decoder(Z). Instead, the paper just switches between G(Z) and G(X) wherever convenient. Similarly, the equation for \\delta_Disc is different in Algorithm 1 and in the equation in 2.2. Interpolation, arguably one of the core parts of the model, is described as ""interpolations are Euclidean interpolations between pairs of points in Z, sampled from a Gaussian distribution around the midpoint between Zg1en and Zg2en."" I assume the mean of this Gaussian is the midpoint; what is its covariance? Etc.\n- All autoencoders are not generative models, and in particular GAIA is not a generative model. There is no generative process. It does not estimate a data distribution. A VAE is a generative model which an autoencoder-like structure, but this does not make all autoencoders generative models.\n- GAIA is described as encouraging ""convex latent distributions"" and a convex set is defined in the text as ""A convex set of points is defined as a set in which the line connecting any pair of points will fall within the rest of the set."" A convex set is not defined in terms of lines; it\'s defined in terms of convex combinations of points within the set. In the paper, only lines between points are considered. Claiming that the latent space is ""convex"" in the sense of purple blobs in B is not done - you would need to take a convex combination of multiple latent vectors and decode the results.\n\nThis is an incomplete list of the issues with this paper. The paper would need significant changes before publication.', '\nUpdate:\n\nI’d like to thank the authors for their thoroughness in responding to the issues I raised. I will echo my fellow reviewers in saying that I would encourage the authors to submit to another venue, given the substantial modifications made to the original submission.\n\nThe updated version provides a clearer context for the proposed approach (phychophysical experimentation) and avoids mischaracterizing GAIA as a generative model.\n\nDespite more emphasis being put on mentioning the existence of bidirectional variants of GANs, I still feel that the paper does not adequately address the following question: “What does GAIA offer that is not already achievable by models such as ALI, BiGAN, ALICE, and IAE, which equip GANs with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates?” To be clear, I do think that the above models are inadequate for the paper’s intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but I believe this is a question that is likely to be raised by many readers.\n\nTo answer the authors’ questions:\n\n- Flow-based generative models such as RealNVP relate to gaussian latent spaces in that they learn to map from the data distribution to a simple base distribution (usually a Gaussian distribution) in a way that is invertible (and which makes the computation of the Jacobian’s determinant tractable). The base distribution can be seen as a Gaussian latent space which has the same dimensionality as the data space.\n- Papers on building more flexible approximate posteriors in VAEs: in addition to the inverse autoregressive flow paper already cited in the submission, I would point the authors to Rezende and Mohamed’s “Variational Inference with Normalizing Flows”, Huang et al.’s “Neural Autoregressive Flows”, and van den Berg et al.’s “Sylvester Normalizing Flows for Variational Inference”.\n\n-----\n\nThe paper title summarizes the main claim of the paper: ""adversarial training on latent space interpolations encourage[s] convex latent distributions"". A convex latent space is defined as a space in which a linear interpolation between latent codes obtained by encoding a pair of points from some data distribution yields latent codes whose decoding also belongs to the same data distribution. The authors argue that current leading approaches fall short of producing convex latent spaces while preserving the ""high-dimensional structure of the original distribution"". They propose a GAN-AE hybrid, called GAIA, which they claim addresses this issue. The proposed approach turns the GAN generator and discriminator into autoencoders, and the adversarial game is framed in terms of minimizing/maximizing the discriminator’s reconstruction error. In addition to that, interpolations between pairs of data points are computed in the generator’s latent space, and the interpolations are decoded and treated as generator samples. A regularization term is introduced to encourage distances between pairs of data points to be mirrored by their representation in the generator’s latent space. The proposed approach is evaluated through qualitative inspection of latent space interpolations, attribute manipulations, attribute vectors, and generator reconstructions.\n\nOverall I feel like the problem presented in the paper is well-justified, but the paper itself does not build a sufficiently strong argument in favor of the proposed approach for me to recommend its acceptance. I do think there is a case to be made for a model which exhibits sharp reconstructions and which allows realistic latent space manipulations -- and this is in some ways put forward in the introduction -- but I don’t feel that the way in which the paper is currently cast highlights this very well. Here is a detailed breakdown of why, and where I think it should be improved, roughly ordered by importance:\n\n- The main reason for my reluctance to accept the paper is the fact that its main subject is convex latent spaces, yet I don’t see that reflected in the evaluation. The authors do not address how to evaluate (quantitatively or qualitatively) whether a certain model exhibits a convex latent space, and how to compare competing approaches with respect to latent space convexity. Figure 2 does present latent space interpolations which help get a sense of the extent to which interpolates also belong to the data distribution, however in the absence of a comparison to competing approaches it’s impossible for me to tell whether the proposed approach yields more convex latent spaces.\n- I don’t agree with the premise that current approaches are insufficient. The authors claim that autoencoders produce blurry reconstructions; while this may be true for factorized decoders, autoregressive decoders should alleviate this issue. They also claim that GANs lack bidirectionality but fail to mention the existing line of work in that direction (ALI, BiGAN, ALICE, and more recently Implicit Autoencoders). Finally, although flow-based generative models are mentioned later in the paper, they are not discussed in Section 1.2 when potential approaches to building convex latent spaces are enumerated and declared insufficient. As a result, the paper feels a little disconnected from the current state of the generative modeling literature.\n- The necessity for latent spaces to ""respect the high-dimensional structure of the [data] distribution"" is stated as a fact but not well-justified. How do we determine whether a marginal posterior is ""a suboptimal representation of the high-dimensional dataset""? I think a more nuanced statement would be necessary. For instance, many recent approaches have been proposed to build more flexible approximate posteriors in VAEs; would that go some way towards embedding the data distribution in a more natural way?\n- I also question whether latent space convexity is a property that should always hold. In the case of face images a reasonable argument can be made, but in a dataset such as CIFAR10 how should we linearly interpolate between a horse and a car?\n- The proposed model is presented in the abstract as an ""AE which produces non-blurry samples"", but it’s not clear to me how one would sample from such a model. The generator is defined as a mapping from data points to their reconstruction; does this mean that the sampling procedure requires access to training examples? Alternatively one could fit a prior distribution on top of the latent codes and their interpolations, but as far as I can tell this is not discussed in the paper. I would like to see a more thorough discussion on the subject.\n- When comparing reconstructions with competing approaches there are several confounding factors, like the resolution at which the models were trained and the fact that they all reconstruct different inputs. Removing those confounding factors by comparing models trained at the same resolution and reconstructing the same inputs would help a great deal in comparing each approach.\n- The structure-preserving regularization term compares distances in X and Z space, but I doubt that pixelwise Euclidian distances are good at capturing an intuitive notion of distance: for example, if we translate an image by a few pixels the result is perceptually very similar but its Euclidian distance to the original image is likely to be high. As far as I can tell, the paper does not present evidence backing up the claim that the regularization term does indeed preserve local structure.\n- Figures 2 and 3 are never referenced in the main text, and I am left to draw my own conclusions as to what claim they are supporting. As far as I can tell they showcase the general capabilities of the proposed approach, but I would have liked to see a discussion of whether and how they improve on results that can be achieved by competing approaches.\n- The decision of making the discriminator an autoencoder is briefly justified when discussing related work; I would have liked to see a more upfront and explicit justification when first introducing the model architecture.\n- When discussing feature vectors it would be appropriate to also mention Tom White’s paper on Sampling Generative Networks.', 'Update:\n\nI appreciate the effort put by the authors into improving the paper. The revised draft is much better than the initial one. But I agree to AnonReviewer2 in that the degree to which this paper has to be modified goes beyond what the review process (even at ICLR) assumes. It is wrong to submit a very unfinished paper and then use the review period to polish it and add results. This incurs unnecessary extra load on the review process.\n\nThe added 2D results are toy-ish and somewhat confusing (I am not sure I understand what the meshgrids are and what do they tell us). Generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation. The paper should have more results on complex datasets, like for instance ImageNet or LSUN or CIFAR or so, and should have comparisons to existing VAE-GAN hybrids, like VAE-GAN. Also, since a lot of the authors’ motivation seems to come from psychophysics, showing some application to that might be a good way to showcase the value of the method (although this may not go well if submitting to machine learning conferences). \n\nI encourage the authors to further strengthen the paper and resubmit to another venue.\n\n-----\n\nThe paper proposes a model for image generation that combines an autoencoder with a generative adversarial network (GAN). The GAN is used to enforce that interpolations between latent vectors of two samples from the training set are decoded to realistic images. The method is applied to attribute manipulation and interpolation of face images.\n\nPros:\n1) A simple and reasonable formulation\n2) Visually good reconstruction of samples and convincing interpolation between samples on the CelebA-HQ dataset.\n3) Good qualitative facial attribute manipulation results on the CelebA-HQ dataset.\n\nCons:\n1) Experimental evaluation is very limited. There is just one dataset and only qualitative results. This is unacceptable: the method should be evaluated on more datasets and there should be quantitative results. I do realize it is not trivial to get quantitative, but it is possible. For instance, a user study can always be performed. But I believe one could also come up with simpler-to-measure metrics for at least some of the reported tasks. There is no comparison to other methods for facial attribute manipulation (for instance, StarGAN).\n2) There is no ablation study. To which extent is each of the model components important? For instance, the interpolation adversarial loss, the discriminator/generator balancing term, the network architecture (autoencoder discriminator)?\n3) As I understand, it is impossible to randomly sample directly from the model, only interpolate/modify existing images. This is a difference from most of prior work. It should be discussed clearly.\n4) Related work discussion is quite brief and misses some relevant work, for instance Adversarial Autoencoders (Makhazani et al., ICLR 2016, somewhat related) or Adversarially Constrained Autoencoder Interpolations (Berthelot et al., arxiv 2018, it’s concurrent, but could be good to discuss).\n5) Writing is not of very high quality. There are typos, grammatical issues, and questionable statements. The manuscript should be significantly improved. Specific comments:\n- The structure is quite strange. There is no separation between the method and the experiments, the related work comes in very late and is very brief. This is all not critical, but confusing.\n- A typo in the title: should be “encourages”\n- Second sentence of the introduction should be supported with evidence (for instance references)\n- “Two unsupervised neural network based algorithms, the Autoencoder (AE; Hinton & Salakhutdinov\n2006) and Generative Adversarial Network (GAN; Goodfellow et al. 2014), are at present the most popular tools in generative modeling.” - Vanilla autoencoders are not very popular tools for generative modeling. Variational autoencoders and some other flavors are.\n- “Unsupervised neural network approaches are often ideal generative models because they require little or no tweaking of network architectures to handle entirely new datasets.” I do not really get this sentence. What is the alternative to unsupervised generative models? Why do unsupervised approaches not require tweaking? (In my experience, they very well benefit from tweaking.)\n- “… a lack of certain constraints on the generative capacity of current neural-network based generative models make it challenging to infer structure from their latent generative representations.” What does “a lack of certain constraints” mean? There are some constraints, for instance the latent space is usually forced to correspond to a fixed distribution. Moreover, there is a lot of work on disentangling that also aims to find structure in latent spaces (for instance, InfoGAN).\n- “and promotes convexity in the model’s generative capacity.” What is convexity in the capacity? I do not think this is grammatical.\n- In 1.1 the mathematical notation seems wrong. Does X really denote a set? In what follows it seems that X is used interchangeably for three different things: a sample from the dataset, the set of training samples, and the space the samples come from.\n- “bidirectionality of adversarially generated data.” What is bidirectional data?\n- “AEs tend to produce blurry images due to their pixel-wise error functions (Larsen et al., 2015)” Perhaps this was intended to refer to VAEs. AEs can generate perfectly sharp images if given enough capacity. \n- “method more greatly resembles the original data than other GAN-based methods” Method does not resemble data\n- “Due to their exact latent-variable inference, these architectures may also provide a useful direction for developing generative models to explore latent-spaces of data for generating datasets for psychophysical experiments.” This is mentioned a few times, but never supported\n- Acknowledgements should not be in the review version (can violate anonymity)\n\n5) Minor: Why is a Gaussian around the midpoint used for interpolations? Why not all convex combinations of two, or possibly more, samples? \n\nTo conclude, the paper presents quite good qualitative results on the CelebA-HQ dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation. The paper cannot be published in its current form. ']","[-80, -30, -50]","[-20, 60, 20]","[""The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'not fit for publication in its current form' and lists numerous significant issues with the paper, including lack of experimental verification, confusing descriptions, and factual errors. The politeness score is -20 because while the reviewer maintains a professional tone overall, the language used is quite direct and critical. Phrases like 'the paper is not fit for publication' and 'This is an incomplete list of the issues with this paper' are blunt and could be perceived as somewhat impolite. However, the reviewer does attempt to provide constructive feedback and explains their reasoning, which prevents the score from being even lower."", ""The sentiment score is slightly negative (-30) because while the reviewer acknowledges some improvements and thanks the authors for their thoroughness, they still express significant concerns about the paper. The reviewer states they cannot recommend acceptance due to insufficient arguments supporting the proposed approach and several unaddressed issues. However, the tone is not entirely negative, as they see potential in the problem being addressed. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, begins by thanking the authors, and frames criticisms constructively as suggestions for improvement rather than harsh judgments. They also offer helpful references and answer the authors' questions directly. The reviewer maintains a professional and courteous tone even while expressing concerns."", ""The sentiment score is -50 because while the reviewer acknowledges some improvements and positive aspects ('The revised draft is much better', 'Visually good reconstruction', 'Good qualitative facial attribute manipulation results'), the overall tone is critical and recommends rejection ('encourage the authors to further strengthen the paper and resubmit to another venue'). The reviewer lists numerous significant issues with the paper, including limited experimental evaluation, lack of ablation studies, and poor writing quality. The politeness score is 20 because the reviewer uses generally polite language ('I appreciate the effort', 'I encourage the authors') and provides constructive feedback, but also includes some blunt criticisms ('It is wrong to submit a very unfinished paper', 'This is unacceptable'). The reviewer maintains a professional tone overall while clearly communicating the paper's shortcomings.""]"
"['The paper addresses the problem of pixel-wise segmentation of lanes from images taken from a vehicle-mounted camera. The proposed method uses multiple passes through encoders decoders convnets, thereby allowing extract global features to inform better local features, and vice versa. Only qualitative baseline comparisons are presented by manually comparing the output of the network to reported results of other methods in [Pan et al.2017].\nIt is unclear to me if the proposed multiple encoder-decoder network is a novel architecture, or a known architecture applied to a novel use case. In case of the former, more details should be given on the design of the network, how it is trained, etc. for reproducibility. The biggest problem however is the subjective manual comparison to existing methods, which the authors do in favor of a quantitative comparison using well-understand objective metrics. While they point out problems with evaluating segmentation with conventional accuracy metrics, no attempt is made to make a better objective measure. We are left to judge the results on only a few selected example frames. \nIt is also unclear how the method and evaluation strategy compares to methods which predict lanes as splines or other parameterized functions. E.g. see surveys on existing approaches, and discussion of different evaluation strategies, e.g. ""Recent progress in road and lane detection: a survey"" [Hillel et al.2014] and ""Visual lane analysis and higher-order tasks: a concise review"" [Shin, 2014].\nThroughout the paper, various fuzzy and unclear statements are made (see detailed comments below). The paper would be in a better shape if more time is spend to improve the writing, provide more details on the method, and extend the experiments.\n\nPros:\n+ multiple encoder-decoder stages could be beneficial for lane segmentation\n\nCons:\n- lacking evaluation and comparison to baseline methods\n- missing details on proposed network architecture, making it hard to reproduce\n- unclear what colors in figures for qualitative evaluation represent: are individual lanes also distinguished?\n\nBelow are more detailed comments and questions:\n* Abstract\n\t* ""the capability has not been fully embodied for"" → Fuzzy statement, I don\'t understand what this means.\n\t* ""In especial"" → check grammar\n* Sec 1.: Introduction\n\t* ""the local information of a lane such as sharp, edges, texture and color, can not provides distinctive features for lane detection"" local edges are not distinctive for lanes? Possibly local edges alone are not sufficient, but various lanes detection approaches rely on edge extraction as features. This statement therefore seems too strong.\n\t* ""End-to-end CNNs always give better results than systems relying on hand-crafted features."". It is not possible to say that one type of classifier categorically better than another. The \'best\' classifier depends on the problem at hand, valid assumptions that can be made, and the amount of training data avaiable, among others. For instance, ""How Far are We from Solving Pedestrian Detection?"" [Zhang,CVPR16] demonstrates that CNNs do not always give better results than hand-crafted features for some tasks and datasets. The paper should be more careful with such strong statements.\n\t* ""Highly hand-craft features based methods can only deal with harsh scenarios."". I don\'t understand, is this statement intended as an argument against hand-crafted features? Isn\'t it good to deal especially with harsh scenarios?\n\t* ""but less explored on Semantic Image Segmentation due to strong prior information is needed."" CNNs are extensively used for semantic image segmentation, e.g. see the well-known Cityscapes benchmark.\n\t* ""recent methods have replaced the feature-based methods with model-based methods."". Not sure why the paper call CNNs ""model-driven methods"", but refer to the earlier classical methods with highly designed representations (Kalman filter, B-snakes, ...) as ""feature-based methods"". This seems diferent from what I typically see, where CNNs are referred to as \'data-driven methods\', and the classical methods as ""model-driven"".\n* Sec 1.2: Contributions\n\t* ""First, reduced localization accuracy due to the weak performance of combining the local information and global information effectively and efficiently"". Instead of presenting a first contribution, the paper presents a problem. Do the authors mean that they ""tackle the problem of reduced localization accuracy ..."" ? That would still not make this contribution very concrete though ...\n\t* ""We make our attempts to rethink these IoU based methods."" → Please argue in favor of your new method. An in-depth comparison of evaluation methods, and why some metrics fail or could be redesigned would be good. However, the paper currently fails to present a new metric, and convince that it tackles shortcomings of established metrics.\n\n* Sec 2.: Multiple Encoder-Decoder Nets\n\t* Figure 2: Is this the first paper to propose this multiple encoder-decoder net? Or is the idea taken from other work, and is the novelty to apply it to this problem? If this general architecture was already proposed (for semantic segmentation?), please add citations and discuss it as related work. If this network design is completely novel, I would expect more details on how the network is constructed (e.g. dimensions of each layer, non-linear activation function used, batch normalization, strides, etc.). \n\t* ""the following loss function:"". Since it is a binary classification problem, and not a regression problem, why not use a (binary) cross entropy loss instead of a mean squared error?\n\n* Sec 3: Experiments\n\t* Figure 3: What is the ""Baseline"" method ? Where are the references to the other works, or is the reader required to read [Pan\'2017] to understand your figures?\n\t* Figure 3: How are the colors in these figures determined? Is this also an instance segmentation problem? From your methodology section I though only binary classification was considered. Do you do some post-processing to separate individual lanes? I find this confusing, as I thought that the task was limited to binary segmentation.\n\t* ""Recent works evaluated ..."" please cite the works you refer to.\n\t* ""we have compared more than 500 probmaps of each level nets manually and count the accuracy of these probmaps as shown in figure 7."" So if I understand correctly, instead of using an objective evaluation metric, you have reverted to manual labor to visually judge lane detection quality. This is not really a metric, and not really a solution that \'rethinks IoU based methods.\' Problems of your approach is that it is unclear on what criteria results are judged, your evaluation is not objectively reproducible by others, and does not scale well for novel future evaluations. Why is this even needed? E.g. why not use some chamfer distance or Gaussian smoothing of the edge map if you want to evaluate near coverage instead of hard boundaries? Or, fit a function through the boundary, and evaluate distance (in meters) to true lane. I find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking.\n\t* Figure 6: What are the Ground Truth images of each row ? E.g. in the fourth row from the top, should the right-most yellow lane be present or not? As it stands, I can\'t interpret the columns and see which x times is visually \'better\'.\n* Sec 3.4:\n\t* ""To improve ability of the network, we propose a small quantity of channel to reduce overfitting by considering inter-dependencies among channels."" To improve relative to what? Where are the results comparing large amounts vs small amount of channels? Note that Figure 8 is not referred to in the text, and confusingly compares ""18 layers"" to ""1 layers"". Do you mean channels instead of layers? And, how many channels were to obtained the results in the preceding sections?\n', '\n# Summary\nThis work deals with a computer vision task specific to autonomous vehicles, namely detection of lane markings on the road. The authors propose an encoder-decoder CNN architecture  (typically used for semantic segmentation) for which a parts of the encoder and the encoder are instanced several times (with different weights) in order to better capture the semantic and spacial information from intermediate feature maps. The method is tested on the recent Lane Detection dataset (Pan et al.) and report qualitative results.\n\n\n# Paper strengths\n- the paper deals with a topic of interest for the autonomous driving community\n- the authors identify a flaw in the IoU accuracy evaluation metric for lane detection\n\n# Paper weakness\n- The paper could be written better. It seems unfinished and some additional proof-reading is necessary to correct the multiple typos across the paper\n\n- There are no quantitative results and no comparisons with baselines and related works, making it difficult to evaluate the performances of this work.\n\n- The authors mention that the current IoU accuracy evaluation metric is flawed on some cases and then propose scanning manually results from 500 prediction maps and report results on that. There is no baseline or other related method considered for this evaluation. Also the original dataset from Pan et al. has ~88k train images, ~9.5k validation images and ~35k test images. It is not clear from which set where the 500 images taken and how representative they are for the entire dataset\n\n- The structure of the proposed architecture is not clear, in particular the merging of the feature maps across multiple encoders. From the diagram in Figure 2 b) it seems that the feature maps are transferred to the neighbour encoder/decoder branch similarly with RNNs. Is this right or only the features maps from the first branch are transferred? Furthermore, the merging is done by element-wise addition or concatenation?\nGiven the application domain, an important aspect is the computational complexity of the proposed architectures. I would welcome such an analysis in the current work as well.\n\n- I enumerate a few other unclear aspects and improvable points in the paper:\n  + why standard encoder-decoder architecture are limited to small receptive field (cf. Section 2), while this approach is not and can use large kernels? Do the authors use kernels of different sizes in the branch? This problem is usually addressed with dilated convolutions or parallel sub-networks of different kernel sizes like in Inception. \n  + there is no description of the task and of the meaning of the 4 lane markings. \n  + in Section 1 the authors mention that ""CNNs show the robust capacity to capture object localization on image classification ... and object detection, but less explored on semantic image segmentation due to strong prior information is needed."". This is not true as semantic segmentation is addressed by means of CNNs since several years already by most of community.\n  + why the label of lane pixels is set to 0.5? Such problems are typically addressed with binary-cross entropy and the output of the sigmoid is used as classification score at test time.\n\n# Recommendation\nThis paper deals with an interesting task, but it\'s still in a rather draft state. There are several shortcomings of the work that I\'ve mentioned above. I recommend the submission for rejection.\n', 'The authors propose a neural network architecture for lane detection in on-road driving. The architecture consists of multiple encoder-decoder stages. This is motivated by a need to overcome limitations of traditional CNNs with respect to considering high-level context while providing pixel-level accuracy. The paper additionally claims contributions in the analysis of the performance of various instantiations of this architecture as well as in considering limitations of the popular IoU metric. Experiments are reported on the publicly available CULane dataset.\n\nThe paper addresses a topical problem in autonomous driving and ADAS but is currently let down by a severe lack of accessibility. Much of the evidence corroborating the principal claims of the submission appears to be missing. While a number of CNN architectures do struggle to provide pixel-level segmentation accuracy particularly for objects of certain geometries there exists a whole host of literature regarding attempts to remedy this. The SegNet family of works as well as many works leveraging the now established skip-connection u-net architecture is missing entirely. The related works  section does list a number of recent relevant work but does not succeed in putting this into context given the approach proposed here. This is also not remedied in the experimental evaluation as almost no benchmarking to the established state of the art is performed. The evaluation itself is mainly qualitative and does not serve to convince the reader that the approach offered here is beneficial. For example, how does one choose between the different solutions offered in Fig. 8? When it comes to the quantitative evaluation some important detail appears to be missing. For example, what is a probmap and how is accuracy on these computed to arrive at Fig 7? Much of the experimental detail is also left unclear. \n\nThe submission also requires extensive spell and grammar checking. This would significantly improve accessibility, though much remains to be done to make the science case more convincing. \n\nOverall this makes the originality and significance of the work difficult to judge. As it stands I can not recommend publication.']","[-60, -70, -70]","[20, 20, 20]","[""The sentiment score is -60 because the review is predominantly critical, pointing out several major flaws in the paper such as lack of proper evaluation, missing details, and unclear statements. However, it's not entirely negative as it acknowledges some potential benefits of the proposed method. The politeness score is 20 because while the reviewer maintains a professional tone and offers constructive criticism, there are instances of direct criticism without much softening language. The reviewer uses phrases like 'The biggest problem however is...' and 'It is unclear...' which are direct but not impolite. The reviewer also provides a balanced view by listing both pros and cons, and offers detailed suggestions for improvement, which contributes to the slightly positive politeness score."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer recommends rejection, lists several weaknesses, and uses phrases like 'unfinished', 'difficult to evaluate', and 'unclear aspects'. The few positive points mentioned are outweighed by the criticisms. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer constructive feedback. They use phrases like 'I would welcome' and 'I recommend' which maintain a respectful tone. The reviewer also acknowledges the interesting aspects of the work before detailing the shortcomings, which adds to the politeness."", ""The sentiment score is -70 because the reviewer expresses significant concerns about the paper's accessibility, lack of evidence, missing literature review, and inadequate experimental evaluation. The reviewer concludes by stating they cannot recommend publication, indicating a strongly negative sentiment. However, it's not entirely negative as they acknowledge the paper addresses a topical problem.\n\nThe politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone. They use phrases like 'the paper addresses a topical problem' and offer constructive feedback. However, the politeness is somewhat limited by direct criticisms such as 'let down by a severe lack of accessibility' and 'requires extensive spell and grammar checking'. The reviewer attempts to balance negative feedback with some positive acknowledgments, resulting in a slightly positive politeness score.""]"
"['The author proposed a novel classification procedure of discerning, maximum response, and multiple check (DMM) to improve accuracy of mediocre network and enhance feedforward network. The author used a toy model with randomly scattered points around four centres to demonstrate the dynamics of decision boundary construction and show how and why the filter training had worked when a multiclass problem is reduced to a pseudo binary classification. The literature review is rather short, some discussion on how the proposed work advances the latest deep neural networks should be added. The experiments are carried out with a simple CNN, it will be better if the author demonstrate its integration to more state-of-art network and make a comparison to their performance.  ', 'The paper discusses a method to increase accuracy of deep-nets on multi-class classification tasks by what seems to be a reduction of multi-class to binary classification following the classical one-vs-all mechanism. I fail to see any novelty in the paper. The problem of reducing multi-class to binary classification has been studied thoroughly with many classical papers like:\n\n1. Usual one-vs-all - this paper does the same thing as one vs all in my opinion even though this technique is known for a decade or so. \n2. Weighted One-Against-All - http://hunch.net/~jl/projects/reductions/woa/woa.pdf\n\nand more sophisticated techniques like:\n\n3. Alina Beygelzimer, John Langford, Pradeep D. Ravikumar. Error-Correcting Tournaments. CoRR, abs/0902.3176, 2009.\n4. Erin L. Allwein, Robert E. Schapire, Yoram Singer, Pack Kaelbling. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Journal of Machine Learning Research, 113—141, 2000.\n5. Thomas G. Dietterich, Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263—286, 1995.\n\nIn my opinion the methods in [1,2,3,5] above can be used with any binary learners and therefore deep-networks. This paper makes no effort in comparing with any of these well-known papers. Moreover the experiments do not show any gain in state of the art performances in the data-sets used, as experiments are done with toy-networks. Further some rules for selecting the class is discussed in Section 3. There are many known rules for generating probability scores in one-vs-all classification and the relations to these are not discussed. \n\nTherefore, I fail to see any novelty in this paper (theoretical or empirical). ', ""Unfortunately I don't understand what this paper is about. Please assign to another reviewer.""]","[20, -80, -50]","[50, -20, 0]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the novel aspects of the author's work and its potential benefits. However, they also point out areas for improvement, such as expanding the literature review and conducting more comprehensive experiments. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive feedback without harsh criticism. They use phrases like 'it will be better if' and 'should be added', which suggest improvements in a polite manner rather than demanding changes. The review maintains a professional tone throughout, balancing positive acknowledgment with suggestions for enhancement."", ""The sentiment score is -80 because the reviewer expresses strong negative opinions about the paper, stating they 'fail to see any novelty' multiple times and criticizing the lack of comparison with existing methods. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and dismissive, particularly in phrases like 'I fail to see any novelty' and 'This paper makes no effort in comparing with any of these well-known papers.' The reviewer doesn't use any softening language or acknowledge potential positives, which contributes to the slightly negative politeness score."", ""The sentiment score is -50 because the reviewer expresses inability to understand the paper, which is a negative response, but not extremely harsh. It's not the lowest possible score as they don't criticize the paper directly, just state their lack of comprehension. The politeness score is 0 (neutral) because the language is neither particularly polite nor rude. The reviewer states their position plainly without using overly courteous language or any impolite expressions. The brevity of the response also contributes to its neutral tone in terms of politeness.""]"
"['\nIn this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed “generalized” BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. \n\nI have the following three main comments about the paper. \n1)\tOnly deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. \n2)\tBecause the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. \n3)\tOnly one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that “the technique presented can be extended to more layers with additional notation”. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. \n4)\tThe authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \\lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. \n5)\tAssumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. \n6)\tFollow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. \n', 'The authors propose a momentum based approach for batch normalization and provide an asymptotic convergence analysis of the objective in terms of the first order criterion. To my understanding, the main effort in the analysis is to show that the sequences of interest are Cauchy. Some numerical results are reported to demonstrate that the proposed variant of BN slightly outperforms BN with careful adjustment of some hyper parameter. The proposed approach is incremental, and the theoretical results are somewhat weak.\n\nThe most important issue is that the zero gradient of the objective function does not imply that it attains an (even local) minimum point. As for the 2-layer case, the objective function can be nonconvex in terms of the weight parameters with stationary points being saddle points, it is crucial to understand whether an iterative algorithm (GD or SGD) converges to a minimum point rather a saddle point. Thus, the first order criterion alone is not enough for this purpose, which is why extensive studies are carried out for nonconvex optimization (e.g., using both first and second order criteria for convergence [1]) and considering the specific structure of neural nets [2].\n\nThe analysis is somewhat confusing. The authors assume that the objective of interest have stationary points (\\theta*, \\lambda*), and also show that the sequence of the norm of gradient convergence to zero, with the \\lambda^(m) converges to \\bar{\\lambda}. What is the relationship between \\lambda* and \\bar{\\lambda}? It is not clear whether they are the same point or not. Moreover, since there is no converge of the parameter, it is not clear what the convergence for the \\lambda imply here, as we also discussed above that the zero gradient itself may mean nothing.\n\nIn addition, the writing need improvements. Some statements are not accurate. For example, on page 3, after equation (2), the authors state “The deep network …”, though they mentioned it is for a 2-layer net. Also, more explicit explanation and definitions are necessary for notations. For example, it is clearer to define explicitly the parameters with \\bar (e.g., for \\lambda) as the limit point. \n\n[1] Ge et al. Escaping from saddle points—online stochastic gradient for tensor decomposition.\n[2] Li and Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation.', 'The paper introduces a modification of batch normalization technique. Original\nbatch normalization normalizes minibatch examples using their mean and standard deviation. \nThe proposed version of batch normalization, called diminishing batch normalization, normalized \nexamples in the current minibatch using  mean and standard deviations that are weighted average \nof mean and standard deviation from the current and all previous minibatches. The authors prove convergence of \nbatch gradient descent with diminished batch normalization. Also, the authors show empirically\nthat Adagrad optimization with diminishing batch normalization can find a better local minimum than\nAdagrad optimization with original batch normalization.\n\nThe idea of diminishing normalization is very sound. However I was not convinced that it gives empirical advantage.\nThe paper says that Table 1 shows ""the best result obtained from each choice of \\alpha^m "". Probably the numbers in \nthis table were obtained using some particular choice of the number of epochs. Unfortunately I didn\'t find in the \npaper any details about the choice of the number of epochs. If we choose the number of epochs that minimize validation\nloss, then according to Figures 4(a) and 3(a), if \\alpha^m=1 then the validation loss is minimized around epoch 55\nand corresponding test error should be less than 2.2%. But the corresponding top left entry in Table 1 has error 2.7%. \n\nAdditional technical remarks:\n1. The abstract says ""we also show the sufficient and necessary conditions for the step sizes and diminishing weights to ensure the convergence"". I didn\'t find necessary conditions in the paper.\n\n2. The authors claim that they are not aware of any prior analysis of batch normalization. The papers at https://arxiv.org/abs/1805.11604 and \nhttps://arxiv.org/abs/1806.02375 , published initially in 5-6/2018, provide interesting theoretical insights on batch normalization.\n\n3. Sentence after equation (2): change from D_1 to D.\n\n4. Usually batch normalization is applied before non-linear activation. According to equations 3-6, the paper applies \nbatch normalization after the nonlinear activation. My understanding is that convergence proof relies on the former architecture. Does section 5 use the former or the latter architecture? \n\n5. I am not sure that fully connected neural network is an efficient architecture for MNIST dataset. I would like to \nto see experiments with CNN and MNIST. ']","[-50, -50, -20]","[20, 20, 60]","[""The sentiment score is -50 because while the reviewer acknowledges the authors' work, they express significant doubts and criticisms. The review lists six main comments, most of which highlight limitations or issues with the paper. Phrases like 'the reviewer has doubt' and 'it is not clear' indicate a negative sentiment. However, it's not entirely negative as the reviewer does describe the work objectively at the start. The politeness score is 20 because the language used is generally professional and constructive. The reviewer uses phrases like 'I have the following comments' and 'Please specify' which are polite ways of offering criticism. However, the tone is not overly warm or encouraging, maintaining a neutral to slightly positive level of politeness typical in academic reviews."", ""The sentiment score is -50 because the review is generally critical of the paper. The reviewer states that the approach is 'incremental' and the theoretical results are 'somewhat weak'. They also point out several important issues with the analysis and methodology. However, it's not entirely negative as they acknowledge some positive aspects like numerical results demonstrating slight outperformance. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'To my understanding' and 'It is not clear' rather than making blunt accusations. They also provide constructive feedback and references for improvement. The language is more formal and objective than overtly polite, but it avoids rudeness or harsh criticism."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the soundness of the idea, they express skepticism about the empirical advantages and point out several issues with the paper. The reviewer states, 'I was not convinced that it gives empirical advantage' and raises concerns about the presentation of results and missing details. The politeness score is moderately positive (60) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'The idea... is very sound' and frame criticisms as observations or questions rather than direct attacks. The reviewer also provides specific, constructive feedback for improvement, which is a polite way to address shortcomings in academic writing.""]"
"[""In this paper, the authors present two methods, Sequential and Parallel-FEED for learning student networks that share architectures with their teacher.\n\nFirstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.\n\nThe paper could do with some further grammar/spell checks.\n\nIt isn't clear to me where the novelty lies in this work. Sequential-FEED appears to be identical to BANs (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. Parallel-FEED is just an ensemble of teachers; please correct me if I'm wrong.\n\nThe experimental results aren't convincing. There aren't any fair comparisons. For instance, in table 6 a WRN-28-10(sFEED) after 5 whole training iterations is compared to a WRN-28-1(BAN) after 1. It would be good to run BAN for as many iterations. A comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the ImageNet experiments. Furthermore, if one isn't interested in compression, then Table 4 indicates that an ensemble is largely preferable.\n\nThis work would benefit from a CIFAR-10 experiment as it's so widely used (interestingly, BANs perform poorly on CIFAR-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.\n\nIn summary I believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.\n\nPros:\n- Simple method\n- Largely written with clarity\n\nCons:\n- Method is not very novel\n- No compared thoroughly enough to other work"", 'In summary, I think this paper contains some reasonable results based on a reasonable, moderately novel, idea, but unfortunately, it is not yet ready for publication. Reading it made me rather confused. \n\nGood things:\n- The main idea is sensible, though distilling into the same architecture (sFEED) is not that novel. I think the pFEED is probably the more novel part.\n- The numerical results are quite good.\n- It\'s a fairly simple method. If others reproduced these results, I think it would be useful.\n\nProblems:\n- Some parts of the paper are written in a way that makes the reader confused about what this paper is about. For example the first paragraph. Some motivations I just did not understand.\n- Some parts of the paper are repeating itself. For example ""introduction"" and ""related works"". The section on related work also includes some quite unrelated papers.\n- The references in the paper are often pointing to work that came much later than the original idea or some pretty random recent papers. For example the idea of model compression (or knowledge distillation) is much older than Hinton et al. I believe it was first proposed by Bucila et al. [1] (which the authors mention later as if knowledge distillation and model compression were very different ideas), it definitely doesn\'t come from Kim et al. (2018). Learning from intermediate representations of the network is at least as old as Romero et al. [2]. Compression into a network of the same architecture is definitely older than Furnarello et al. (2018). It was done, for example, by Geras et al. [3]. The paper also cites Goodfellow et al. (2016) in some pretty random contexts. I don\'t want to be too petty about references, but unfortunately, this paper is just below a threshold that I would still find acceptable in this respect.\n- The comparison in Table 6 would make more sense if the same architectures would be clearly compared. As it is, it is difficult to be certain where the improvement is coming from and how it actually compares to different methods.\n\nTypos: Titap X, ResNext, prarphraser.\n\nReferences:\n[1] Bucila et al. Model Compression. 2006.\n[2] Romero et al. FitNets: Hints for Thin Deep Nets. 2014.\n[3] Geras et al. Blending LSTMs into CNNs. 2016.', ""I do not necessarily see something wrong with the paper, but I'm not convinced of the significance (or sufficient novelty) of the approach. \n\nThe way I understand it, a translator is added on top of the top layer of the student, which is nothing but a few conv layers that project the output to potentially the size of the teacher (by the way, why do you need both a paraphraser and translator, rather than making the translator always project to the size of the teacher which basically will do the same thing !? )\nAnd then a distance is minimized between the translated value of the students and the teacher output layer. The distance is somewhat similar to L2 (though the norm is removed from the features -- which probably helps with learning in terms of gradient norm). \n\nComparing with normal distillation I'm not sure how significant the improvement is. And technically this is just a distance metric between the output of the student and teacher. Sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and I do not see this as being fundamentally different, or at least not different enough for an ICLR paper.\n\nSome of the choices seem arbitrary to me (e.g. using both translator and paraphraser). Does the translator need to be non-linear? Could it be linear? What is this mapping doing (e.g. when teacher and student have the same size) ? Is it just finding a rotation of the features? Is it doing something fundamentally more interesting? \n\nWhy this particular distance metric between the translated features? Why not just L2? \n\nIn the end I'm not sure the work as is, is ready for ICLR.\n""]","[-70, -30, -50]","[20, 20, 0]","[""The sentiment score is -70 because the reviewer recommends rejection and points out several significant issues with the paper, including lack of novelty and unconvincing experimental results. The overall tone is critical, though not entirely negative as the reviewer acknowledges some pros. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer constructive feedback. They use phrases like 'it would be a good idea' and 'please correct me if I'm wrong,' which maintain a level of politeness. However, the overall critical nature of the review prevents a higher politeness score."", ""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('Good things'), the overall tone is critical and indicates the paper is not ready for publication. The reviewer expresses confusion and points out several problems, which outweigh the positive elements. The politeness score is 20 because the reviewer uses relatively polite language, starting with positive points and using phrases like 'I think' and 'unfortunately'. However, some criticisms are quite direct, preventing a higher politeness score. The reviewer also provides constructive feedback and references to improve the paper, which contributes to the slightly positive politeness score."", ""The sentiment score is -50 because the reviewer expresses skepticism about the significance and novelty of the approach, and ultimately concludes that the work is not ready for ICLR. However, they do not completely dismiss the paper, noting that they 'do not necessarily see something wrong with the paper.' This indicates a negative sentiment, but not extremely so. The politeness score is 0 because the language used is neutral and professional. The reviewer raises concerns and questions without using rude or overly polite language. They maintain a matter-of-fact tone throughout, focusing on the technical aspects of the paper rather than personal opinions or emotions.""]"
"[""This paper presents an analysis of the inverse invariance of ReLU networks. It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments. They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope. They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.\n\nThe paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together. The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting. The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network. However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.\n\nI have several questions for the authors:\n- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?\n- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability. Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.\n- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?\n\nIn conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space. The experiments are not very convincing or illustrative of the theoretical results in my opinion. It is not clear how those observations can affect practical algorithms and this is something I hope the author can address."", 'The paper has two distinct parts. In the first part (section 2) it studies the volume of preimage of a ReLU network’s activation at a certain layer as being singular, finite, or infinite. This part is an extension of the work in the study of (Carlsson et al. 2017). The second part (section 3) builds on the piecewise linearity of a ReLU network’s forward function. As a result, each point in the input space is in a polytope where the model acts linearly. In that respect, it studies the stability of the linearized model at a point in the input space. The study involves looking at the singular values of the linear mapping. \n\nThe findings of the paper are non-trivial and the implications potentially interesting. However, I have some concerns about the study.\n\nThere is a key concern about the feasibility of the numerical analysis for the first part. That is, a layer-by-layer study can have a computational problem where the preimage is finite at each layer but can become infinite by the mapping of the preceding layers. In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.\n\nAs for the second part, the authors mention the increase in the dimensionality of the latent space in the current deep networks. However, this observation views convolutional networks as MLPs. However, there is more structure in a convolutional layer’s mapping function. The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.\n\nAll in all, while there are some concerns and the contributions are not entirely novel, the reviewer believes the findings of the paper is generally non-trivial and shed more light on the inner workings of the ReLU networks and is thus a valuable contribution to the field.', '\n\n\n\nReview\n\nThis paper discusses invariances in ReLU networks. The discussion is anchored around the observation that while the spectral norm of neural networks layers (their product bounds the Lipschitz constant) has been investigated as a measure of robustness of nets, the smallest singular values are also of interest as these indicate directions of invariance. \n\nThe paper consists mostly of a theoretical analysis with little empirical support, focusing on a property of matrices called omnidirectIonality. The definition given seems weird — an A \\in R^{m \\times n} is omnidirectional if there exists a unique x \\in R^n such that Ax \\leq 0. \n\nIf there is a *unique* x then that x must be 0. Else if there were a nonzero x for which Ax \\leq 0, then A(cx) also \\leq 0 for any positive scalar 0 and thus x is not unique. Moreover if x must be equal to 0 Ax \\leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright? Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0? Also perhaps better to use the curly sign for vector inequality. \n\nOverall the paper, while interesting is unacceptably messy. \nThe first two pages have no paragraph breaks!!! This means either that the author are separating paragraphs with \\\\ \\noindent or that they have modified the style file to remove paragraph breaks to save space. Either choice is unreadable and unacceptable. The paper is also littered with typos and vague statements (many enumerated below under *small issues*). In this case, they add up to make a big issue. \n\n\nThe notation at the top of page 4 — see (1) and (2) — comes out of nowhere and requires explanation. |_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?\n\nUltimately this paper focuses on the question on whether the pre-image of a ReLU layer can be concluded (based on the post-image) to be a singleton,  a compact polytope, or if it has infinite volume. The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?). \n\nUltimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score. \n\n\nSmall issues\n\nThe following is a *very* incomplete list of small bugs found in the paper:\n\n“From a high-level perspective both of these approaches” --> missing comma after “perspective”\n\n""as well as the gradient correspond to the highest\npossible responds for a given perturbation"" --> incomprehensible ""corresponding?"" ""possible responds?"" do you mean ""response"", and if so what is the precise technical meaning here?\n\n""analyzing the lowest possible response"" what does ""response\' mean here?\n\n""We provide upper bounds on the smallest singular value"" -- the singular value of what? This hasn\'t been stated yet.\n\n""reverse view on adversarial examples"" --- what this means isn\'t clear from the preceding text.\n\n""we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights"" -- what does ""mechanisms"" mean here?\n\nNotation section -- need a sentence here at the beginning, can\'t just have a section heading followed by bullets. \n\n""realated""\n']","[20, 50, -70]","[70, 75, -20]","[""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's interesting and original analysis, well-written content, and correct proofs. However, they also point out some weaknesses, such as disconnected structure and unconvincing experiments. The overall tone is constructive rather than overtly negative or positive. The politeness score is relatively high (70) as the reviewer uses respectful language throughout, poses questions politely, and offers constructive criticism. They acknowledge the paper's strengths before discussing areas for improvement and use phrases like 'I hope the author can address' rather than making demands. The review maintains a professional and courteous tone throughout."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the non-trivial findings and potential interesting implications of the paper, while also expressing some concerns. The overall tone is balanced, recognizing both strengths and weaknesses. The final paragraph indicates that despite concerns, the reviewer believes the paper is a valuable contribution to the field.\n\nThe politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing concerns as requests for comments or clarification rather than direct criticisms. Phrases like 'I would like the authors to comment on' and 'All in all' contribute to a courteous tone. The reviewer also acknowledges the paper's contributions and value, which adds to the overall politeness of the review."", ""The sentiment score is -70 because the review is predominantly negative. The reviewer states the paper is 'unacceptably messy', 'unreadable', 'littered with typos and vague statements', and 'falls well below the standards of exposition'. They do mention the paper is 'interesting' twice, which prevents the score from being even lower. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and critical. Phrases like 'unacceptably messy' and the extensive list of errors and issues convey a lack of politeness. However, the reviewer does offer to re-evaluate if improvements are made, which slightly mitigates the overall negative tone.""]"
"[""The reinforcement learning tasks with sparse rewards are very important and challenging. The main idea of this work is to encourage intra-life novelty. The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode. \n\nHowever, the results are not enough to be accepted to ICLR having a very high standard. In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C. There are some RL algorithms reported to be better than A2C. For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017. \n\n=================================================================================================\nI've read the rebuttal. I updated my score but still not vote for accept. \n\nThis paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments.\n"", 'Summary:\nThe authors look at the problem of exploration in deep RL. They propose a “curiosity grid” which is a virtual grid laid out on top of the current level/area that an Atari agent is in. Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game. The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).\n\nThe authors argue that this method enables better exploration and they obtain an impressive score on Montezuma’s Revenge (MR). \n\nReview:\nThe paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm. The algorithm itself seems to work well and some of the results are convincing. I am a bit worried about the fact that the agents have access to their history of locations (“the grid”). The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.\n\nThe authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly. Only removing the grid access made results on MR very unstable. However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there. \n\nI was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.\n\nThe future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage. \n\nNits/writing feedback:\n- There is no need for such repetitive citing (esp paragraph 2 on page 2). Sometimes the same paper is cited 4 times within a few lines. While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.\n- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting\n\n####\nRevision:\n\nThe rebuttal does little to clarify open questions:\n1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.\n2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.\n3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn\'t the key aspect of their algorithm. This seems contradictory.', 'This paper proposes use of intra-life coverage (an agent must visit all locations within each episode) for effective exploration in Atari games. This is in contrast of approaches that use inter-life coverage or curiosity metrics to incentivize exploration. The paper shows detailed results and analysis on 2 Atari games: Montezuma’s Revenge and Seaquest, and reports results on other games as well.\n\nStrengths\n1. Intuitively, the idea of intra-life curiosity is reasonable. The paper pursues this idea and provides experimental evidence towards it on 2 Atari games. It is able to show compelling improvements on the challenging Montezuma’s Revenge game.\n\nWeaknesses\n1. The two primary comparison points are missing:\n1a. Comparison to other exploration methods. A number of methods that use state visitation counts (also referred to as diversity, eg. [A,B]), or prediction error (also referred to as curiosity, eg [C]) have been proposed in recent years. It is important to place the contributions in this paper in context of these other works. A number of these references are missing and no experimental comparison to these methods has been made. \n\n1b. Comparison between inter and intra life curiosity. One of the central motivation is the utility of intra-life curiosity vs inter-life curiosity, yet no comparisons to this effect have been provided.\n\n2. Additionally, the paper employs a custom way of computing coverage (or diversity). It is in terms of location of agent on the screen, as opposed to featurization of the full game screen as used in prior works. It is possible that a large part of the gain comes from the clever design of the space for computing intrinsic exploration reward. The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward). More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for. The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.\n\n3. I will encourage investigation on a more varied set of tasks. Perhaps, also using some MuJoCo environments, or 3D navigation environments. Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid. Additionally, all of these are still on Atari.\n\n[A] Diversity is All You Need: Learning Skills without a Reward Function Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine\n\n[B] EX2: Exploration with Exemplar Models for Deep Reinforcement Learning Justin Fu, John D. Co-Reyes, Sergey Levine\n\n[C] Curiosity-driven Exploration by Self-supervised Prediction Deepak Pathak, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell International Conference on Machine Learning (ICML), 2017\n\n']","[-20, -40, -20]","[50, 20, 60]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the topic and the main idea, they express that the results are not sufficient for acceptance at ICLR. They also request additional comparisons and express uncertainty about judging the paper fairly. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges their own limitations in evaluating the paper, and suggests deferring to other reviewers' comments. They also mention reading the rebuttal and updating their score, showing engagement with the authors' response. The language is professional and constructive, avoiding harsh criticism while still providing clear feedback."", ""The sentiment score is -40 because while the reviewer acknowledges some positive aspects of the paper (extensive introduction, sensible algorithm, convincing results), they express significant concerns about the method's advantage and its generalizability. The revision section further emphasizes these concerns, indicating that the rebuttal did not adequately address the reviewer's questions. This suggests an overall negative sentiment, though not extremely so.\n\nThe politeness score is 20 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I was wondering' and 'I think' to soften criticisms, and they acknowledge positive aspects of the work. However, the language is not overly polite or deferential, maintaining a neutral to slightly positive politeness level. The revision section is more direct in its criticisms but still maintains a professional tone.\n\nOverall, the review balances constructive criticism with acknowledgment of the paper's strengths, but leans towards a negative assessment, particularly after the revision."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they spend more time detailing weaknesses and areas for improvement. The reviewer points out several major issues, including lack of comparisons to other methods and limited scope of experiments. However, they do recognize the paper's intuitive idea and improvements on a challenging game. The politeness score is moderately positive (60) as the reviewer uses professional and constructive language throughout. They offer specific suggestions for improvement rather than harsh criticism, and use phrases like 'I will encourage' and 'It is important to' which maintain a respectful tone. The reviewer also acknowledges the paper's strengths before discussing weaknesses, which is a polite approach to feedback.""]"
"['Pros:\n- The paper generalizes upon past observations by Ott et al. that NMT models might decode ""copies"" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in ""training set predictions."" It\'s unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel.\n- The paper draws a connection between the observed degradation and ""label bias"", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding.\n- The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing.\n- The paper is very well written. The analysis throughout the paper is easy to follow and convincing.\n\nCons:\n- Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor.\n\nComments/questions:\n- In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$.\n- To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row.\n- In Section 5, when describing the ""Discrepancy gap"" constraint, you say that you ""modify Eq. 3 to include the constraint"", but I suspect you meant that you modify Eq. 1 to include this constraint.\n- In Table 4, why didn\'t you tune $\\mathcal{M}$ and $\\mathcal{N}$ separately for each beam width?', '\nThis work does extensive experiments on three different text generation tasks and shows the relationship between wider beam degradation and more and larger early discrepancies. This is an interesting observation but the reason behind the scene are still unclear to me. A lot of the statements in the paper lack of theoretical analysis. \n\nThe proposed solutions addressing the beam discrepancies are effective, which further proves the relationship between beam size and early discrepancies. My questions/suggestions are as follows:\n* It’s better to show the dataset statistics along with Fig1,3. So that readers know how much of test set have discrepancies in early steps.\n* It is not right to conduct your analysis on the test set. You have to be very clear about which results are from test set or dev set.\n* All the results with BLEU score must include the brevity penalty as well. It is very useful to analyze the length ratio changes between baseline, other methods, and your proposal.\n* The example in Sec. 4.6 is unclear to me, maybe you could illustrate it more clearly.\n* Your approaches eliminate the discrepancies along with the diversity with a wider beam. I am curious what if you only apply those constraints on early steps.\n* I suggest comparing your proposal to the word reward model in [1] since it is also about improving beam search quality. Your threshold-based method is also kind of word reward method.\n* In eq.2, what do you mean by sequence y \\in V? y is a sequence, V just a set of vocabulary.  What do you mean by P (y|x;{y_0..y_t}). Why the whole sequence y is conditioned on a prefix of y?\n\n[1] Huang et al, ""When to Finish? Optimal Beam Search for Neural Text Generation"" 2017', ""This paper addresses issues with the beam search decoding algorithm that is commonly applied to recurrent models during inference. In particular, the paper investigates why using larger beam widths, resulting in output sequences with higher log-probabilities, often leads to worse performance on evaluation metrics of interest such as BLEU. The paper argues that this effect is related to ‘search discrepancies’ (deviations from greedy choices early in decoding), and proposes a constrained decoding mechanism as a heuristic fix. \n\nStrengths:\n- The reduction in performance from using larger beam widths has been often reported and needs more investigation.\n- The paper views beam search decoding through the lens of heuristic and combinatorial search, and suggests an interesting connection with methods such as limited discrepancy search (Harvey and Ginsberg 1995) that seek to eliminate early ‘wrong turns’. \n- In most areas the paper is clear and well-written, although it may help to be more careful about explaining and / or defining terms such as ‘highly non-greedy’, ‘search discrepancies’ in the introduction. \n\nWeaknesses and suggestions for improvement:\n\n- Understanding: The paper does not offer much in the way of a deeper understanding of search discrepancies. For example, are search discrepancies caused by exposure bias or label bias, i.e. an artifact of local normalization at each time step during training, as suggested in the conclusion? Or are they actually a linguistic phenomenon (noting that English, French and German have common roots)? As there are neural network methods that attempt to do approximate global normalization (e.g. https://www.aclweb.org/anthology/P16-1231), there may be ways to investigate this question by looking at whether search discrepancies are reduced in these models (although I haven’t looked deeply into this).\n\n- Evaluation: In the empirical evaluation, the results seem quite marginal. Taking the best performing beam size for the proposed method, and comparing the score to the best performing beam size for the baseline, the scores appear to be within around 1% for each task. Although the proposed method allows larger beam widths to be used without degradation during decoding, of course this is not actually beneficial unless the larger beam can improve the score. In the end, the evidence that search discrepancies are the cause of the problems with large beam widths, and therefore the best way to mitigate these problems, is not that strong.\n\n- Evaluation metrics and need for human evals: The limitations of automatic linguistic evaluations such as BLEU are well known. For image captioning, the SPICE (ECCV 2016 https://arxiv.org/abs/1607.08822) and CIDEr (CVPR 2015 https://arxiv.org/abs/1411.5726) metrics show much greater correlation with human judgements of caption quality, and should be reported in preference (or in addition) to BLEU. More generally, it is quite possible that the proposed fix based on constraining discrepancies could improve the generated output in the eyes of humans, even if this is not strongly reflected in automatic evaluation metrics. Therefore, it would be interesting to see human evaluations for the generated outputs in each task.  \n\n- Rare words: The authors reference Koehn and Knowles’ (2017) six challenges for NMT, which includes beam search decoding. One of the other six challenges is low-frequency words. However, the impact of the proposed constrained decoding approach on the generation of rare words is not explored. It seems reasonable that limiting search discrepancies might also further limit the generation of rare words. Therefore, I would like to suggest that an analysis of the diversity of the generated outputs for each approach be included in the evaluation.\n\n- Constrained beam search: There is a bunch of prior work on constrained beam search. For example, an algorithm called constrained beam search was introduced at EMNLP 2017 (http://aclweb.org/anthology/D17-1098). This is a general algorithm for decoding RNNs with constraints defined by a finite state acceptor. Other works have also been proposed that are variations on this idea, e.g. http://aclweb.org/anthology/P17-1141, http://aclweb.org/anthology/N18-1119). It might be helpful to identify these in the related work section to help limit confusion when talking about this ‘constrained beam search’ algorithm.  \n\nMinor issues:\n- Section 3. The image captioning splits used by Xu et al. 2015 were actually first proposed by Karpathy & Li, ‘Deep visual-semantic alignments for generating image descriptions’, CVPR 2015, and should be cited as such. (Some papers actually refer to them as the ‘Karpathy splits’.)\n- In Table 4 it is somewhat difficult to interpret the comparison between the baseline results and the constrained beam search methods, because the best results appear in different columns. Bolding the highest score in every row would be helpful.\n\nSummary:\nIn summary, improving beam search is an important direction, and to the best of my knowledge the idea of looking at beam search through the lens of search discrepancies is novel. Having said, I don't feel that this paper in it's current form contributes very much to our understanding of RNN decoding, since it is not clear if search discrepancies are actually a problem. Limiting search discrepancies during decoding has minimal impact on BLEU scores, and it seems possible that search discrepancies could just be an aspect of linguistic structure. I rate this paper marginally below acceptance, although I would encourage the authors to keep working in this direction and have tried to provide some suggestions for improvement.   ""]","[80, -20, -20]","[90, 50, 60]","[""The sentiment score is 80 (positive) because the review starts with a list of 'Pros' that are extensive and detailed, highlighting several positive aspects of the paper. The reviewer uses phrases like 'very well written', 'easy to follow', and 'convincing', indicating a strong positive sentiment. There is only one brief 'Con' mentioned, which slightly reduces the overall positive score. The politeness score is 90 (very polite) because the reviewer uses respectful and constructive language throughout. Even when pointing out areas for improvement, the tone remains professional and helpful. The reviewer offers suggestions rather than criticisms, using phrases like 'consider using' and 'you might compress'. The overall tone is supportive and aimed at improving the paper, demonstrating a high level of politeness in academic discourse."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the work as 'interesting' and the proposed solutions as 'effective', they also express that 'the reason behind the scene are still unclear' and that 'a lot of the statements in the paper lack of theoretical analysis'. This indicates some reservations about the depth of the work. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, framing criticisms as suggestions ('It's better to...', 'I suggest...') and asking questions rather than making blunt statements. They also acknowledge positive aspects of the work before offering critiques. However, the review doesn't go out of its way to be overly polite, maintaining a professional tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they ultimately rate it 'marginally below acceptance' and express significant concerns about the paper's contributions and evaluation. The reviewer states that the paper doesn't contribute much to understanding RNN decoding and the results seem quite marginal. However, they do encourage the authors to continue working on the topic. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, offers constructive criticism, and provides detailed suggestions for improvement. They balance critiques with acknowledgments of the paper's strengths and use phrases like 'I would like to suggest' and 'it might be helpful' when offering recommendations, which maintains a polite tone.""]"
"['The manuscript proposes a method for unsupervised learning with unknown class number k. The problem is classical and important. The proposed method is interesting and novel, but the experiments are not convincing. In detail, it did not compare other methods in the experiments. \nPros: clear description and novelty of the method\nCons: insufficient experiments. ', 'This paper develops an unsupervised classification algorithm using the idea of CycleGAN. Specifically, it constructs a piece-wise linear mapping (the Connector Network) between the discriminator network and the generator network. The learning objective is based on the cycle-consistency loss. Experiments show that it can achieve reasonable loss. This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. However, the paper is not in a good shape for publication in its current form.\n\nFirst, the paper is not well written and many of the key ideas are not clear. It devotes more than half of the pages to the review of the preliminary materials in Sections 2-3 while only briefly explained the main algorithm in Section 4. Many of the details are missing. For example, why L1-loss is used in (5)-(7) in Algorithm 1? What is the “random-walk Laplacian matrix L_{sym}” (never defined)? More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification. However, the ideas (regarding the footprints and footprint mask etc.) are totally unclear. It is assumed that all different classes have equal probabilities. In this setting, it is unclear (in its intuition) why it is possible to assign a cluster index to its true class labels. What is the key signal in the data that enables the algorithm to relate different clusters to their corresponding classes, especially when all classes have equal probability? Furthermore, it is not clear why the mapping from H to Z can be written as a sum of C_1,…,C_k in Proposition 3.2. If the final mapping is piece-wise linear, how can it be written as a sum of linear mappings? Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix? (Only linear mapping can be expressed as a matrix.)\n\nSecond, the experiment is insufficient. None of the experiment details are presented in the paper. Only the final accuracy of 0.874 is given without any further details. What is the model architecture and size? More experimental analysis should be presented. For example, there are many different hyperparameters in the algorithms. How are the \\lambda_D, \\lambda_G chosen when there is no labeled validation set? How sensitive is the algorithm to different model architecture and model size? Furthermore, none of the baselines results are presented and compared against.\n\nA lot of related works are missing. There have been a lot of emerging works related to unsupervised classification recently, which should be discussed and compared:\n[1] G. Lample, L. Denoyer, and M. Ranzato.  Unsupervised machine translation using monolingual corpora only. ICLR, 2018.\n[2] M. Artetxe, G. Labaka, E. Agirre, and K. Cho.  Unsupervised neural machine translation. ICLR, 2018.\n[3] Y. Liu, J. Chen, and L. Deng.  Unsupervised sequence classification using sequential output statistics. NIPS, 2017\n[4] A. Gupta, A. Vedaldi, A. Zisserman. Learning to Read by Spelling: Towards Unsupervised Text Recognition. arXiv:1809.08675, 2018.\n[5] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato. Phrase-based & neural unsupervised machine translation. EMNLP 2018.\n\nThe presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and English usage issues as well as other unclear statements. Just to name a few examples below:\n-\t(1st paragraph of Introduction): “…imagine the learned objectconstruct…”\n-\tThe last paragraph in Section 1 is not in the right position and should be placed somewhere else in the introduction.\n-\tIn the first paragraph of Section 2.2, “one of the clustering algorithm” should be “one of the clustering algorithms”.\n-\tIn the first paragraph of Section 3, it is not clear what it means by “we can make the tuples (Z,X,H) for the whole dataset”.\n-\tAt the end of the first paragraph of Section 3, there is a missing reference in “network in section()”.\n-\tIn the third paragraph on page 4, there is a grammar issue in “H and Z have greater than convexity than X…” and in “it allows the linear combination on these two manifolds in the feature spaces H and Z are and”.\n-\tIn the first paragraph of Section 3.2, it is not clear what it means by “two feature spaces will be trained by the cycle consistency loss to obtain the tuples (Z,X,H) with the correct permutation, where all of elements is in the same-class manifold and shares same learned features.”\n-\tIn the first paragraph of page 5, “cycle-consistency loss z C(D(G(z))) and backward cycle consistency loss x G(C(D(x)))” does not read well. It sounds like z is the loss C(D(G(z))) and x is the loss G(C(D(x)))?\n-\tTypo in Figure 4: “a shows” should be “(a) shows”.\n', '------------------------------------------\nSummary\n------------------------------------------\nThis paper performs unsupervised classification where the number of classes is unknown. The main idea is to use the CycleGAN framework such that one can reconstruct the original image by first moving to latent space that represents another class (via the connector network), then moving back to the original latent space and going back into image space using a generator. Experiments are conducted on MNIST and CIFAR.\n\n------------------------------------------\nEvaluation\n------------------------------------------\nThe paper tackles an important problem: namely, unsupervised classification (i.e. clustering). I think the use cycle-consistency loss in an auxiliary latent space is quite clever. However, experimental results are lacking. Unsupervised clustering (even when number of classes is not known) is a very well studied problem in machine learning. The authors should compare against at least a few reasonable baselines.\n\n------------------------------------------\nPresentation\n------------------------------------------\nI found the presentation to be somewhat wanting. Section 3 is extremely confusing and in my opinion, not well-motivated. For example, why is self-expressivity important? Why can we assume propositions 3.1 and 3.2?\n\n']","[-20, -60, -20]","[50, 20, 50]","['The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the novelty of the method, they express significant concerns about the insufficient experiments and lack of comparison to other methods. The positive aspects (clear description, novelty) are outweighed by the critical feedback. The politeness score is moderately positive (50) as the reviewer uses neutral language and balances criticism with praise. They objectively list pros and cons without using harsh or judgmental language, maintaining a professional tone throughout the review.', ""The sentiment score is -60 because the review is predominantly critical. While it acknowledges the paper addresses an important problem and may present interesting ideas, it states the paper is 'not in a good shape for publication' and lists numerous significant issues with writing clarity, missing details, insufficient experiments, and lack of comparisons to related work. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism. They avoid harsh personal attacks and use phrases like 'may present interesting ideas' and 'should be significantly improved' rather than outright dismissive language. However, the overall tone is more matter-of-fact than overtly polite, hence the modest positive score."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and finds the approach 'quite clever', they also point out significant shortcomings in the experimental results and presentation. The reviewer states that 'experimental results are lacking' and that the presentation is 'somewhat wanting', indicating a generally critical stance despite some positive aspects. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's merits and framing criticisms constructively. They use phrases like 'I think' and 'in my opinion' to soften their critiques, and there's no harsh or dismissive language. However, the review doesn't go out of its way to be overly polite or encouraging either, maintaining a professional tone.""]"
"['edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.\n\nI also agree with the introduction of the Chi vector, and with the use of the term of ""photometric BA"", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.\n\n---------------\n\n\nThis paper presents a method for dense Structure-from-Motion using Deep Learning:\nThe input is a set of images; the output is the camera poses and the depth maps for all the images.\nThe approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function. This Jacobian is used to update an estimate of the camera poses. As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step. In LM optimization, this lambda evolves with the improvement of the estimate. Here lambda is also predicted using a network based on the feature difference.\n\nIf I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.\n\nThe method is compared against DeMoN and other baselines with good results.\n\nI like the fact that the method is based on LM optimization, which is the standard method in \'geometric bundle adjustment\', while related works consider Gauss-Newton-like optimization steps. The key was to include a network to predict lambda as well.\n\nHowever, I have several concerns:\n\n* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct. The image features learned with the proposed method are re-used in an approach using a fixed lambda. If I understand correctly, there are 2 things wrong with that:\n- for GN optimization, lambda should be set to 0 - not a constant value. Several constant values should also have been tried.\n- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.\nThus, the advantage of using a LM optimization scheme is not very convincing.\n\nSince the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.\n\n* CodeSLAM (best paper at CVPR\'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.\n\nLess critical concerns that still should be taken into account if the paper is accepted:\n\n- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.\n\n- the name \'Bundle Adjustment\' is actually not adapted to the proposed method.  \'Bundle Adjustment\' in \'geometric computer vision\' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors. Here the objective function is based on image feature differences. I thus find the name misleading. The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error. The proposed method is more about dense alignment for multiple images.\n\n\nMore minor points:\n\n1st paragraph:  Marquet -> Marquardt\ntitle of Section 3: revisitED\n1st paragraph of Section 3: audience -> reader\ncaption of Fig 1: extractS\nEq (2) cannot have Delta Chi on the two sides. Typically, the left side should be \\hat{\\Delta \\Chi}\nbefore Eq (3): the \'photometric ..\' -> a \'photometric ..\'\n1st paragraph of Section 4.3: difficulties -> reason\ntypo in absolute in caption of Fig 4\nEq (6): Is B the same for all scenes?  It would be interesting to visualize it.\nSection 4.5: applies -> apply\n', 'I believe that the authors have a solid contribution that can be interesting for the ICLR community.\nTherefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow). \n\nSummary:\nThe authors propose a new method called BA-Net to solve the SfM problem by explicitly incorporating geometry priors into a machine learning task. The authors focus on the Bundle Adjustment process. \n\nGiven several successive frames of a video sequence (2 frames but can be extended up to 5), BA-Net jointly estimates the depth of the first frame and the relative camera motion (between the first frame and the next one).\nThe method is based on a convolutional neural network which extracts the features of the different pyramid levels of the two images and in parallel computes the depth map of the first frame. The proposed network is based on the DRN-54 (Yu et al., 2017) as a feature extractor. \n\nThis is complemented by the linear combination of depth bases obtained from the first image.\nThe features and the initial depth then passed to the optimization layer called BA-layer where the feature re-projection error is minimized by the modified LM algorithm. \n\nThe authors adapt the standard multi-view geometry constraints by a new concept of feature re-projection error in the BA framework (BA-layer) which they made differentiable. \nDifferentiable optimization of camera motion and image depth via LM algorithm is now possible and can be used in various other DL architectures (ex. MVS-Net can probably benefit from BA-layer).\n\nThe authors also propose a novel depth parametrization in the form of linear combination of depth bases which reduces the number of parameters for the learning task, \nenables integration into the same backbone net as used or feature pyramids and makes it possible to jointly train the depth generator and the BA-layer. \n\nOriginally the proposed approach depicts the network operating in the two-view settings. The extensibility to more views is also possible and, as shown by authors, proved to improve performance. It is, however, limited by the GPU capacity. \n\nOverall, the authors came up with an interesting approach to the standard BA problem. They have managed to inject the multi-view geometry priors and BA into the DL architecture. \n\nMajor comments regarding the paper:\n\nIt would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.\n\nMinor comments regarding the paper:\n\n-\tThe spacing between sections is not consistent. \n-\tFigures 1 is way too abstract given the complicated set-up of the proposed architecture. It would be nice to see more details on the subnet for depth estimator and output of the net. \nOverall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.\n-\tTalking about proposed formulation of BA use either of the following and be consistent across the paper:\nFeaturemetric BA / Feature-metric BA / Featuremetric BA / ‘Feature-metric BA’\n-\tTalking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.\n-\tAttention should be given to the notation in formulas (3) and (4). The projection function there is no longer accepts a 3D point parametrized by 3 variables. Instead only depth is provided. \nIn addition, the subindex ‘1’ of the point ‘q’ is not explained. \n-\tMore attention should be given to the evaluation section. Specifically to the tables (1 and 2) with quantitative results showing the comparison to other methods. \nIt is not clear how the depth error is measured and it would be nicer to have the other errors explained exactly as they referred in the tables (e.g. ATE?).\n-\tHow the first camera pose is initialized?\n-\tIn Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?\n-\tAttention should be given to the grammar, formatting in particular the bibliography. \n\n\n\n', 'This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.\nSpecifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of ""depth bases"".\nThese values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.\nThen, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).\nBy unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks\' parameters.\n\nThe paper is clear, well organized, well written and easy to follow.\nEven if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.\nIn particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.\nThe experiments are quite convincing and seem to clearly support the efficacy of the proposed method.\n\nI don\'t really have any major criticism, but I would like to hear the authors\' opinions on the following two points:\n\n1) In page 5, the authors write ""learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations"".\nI don\'t really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.\nThe word ""guarantee"" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.\n\n2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.\nGiven this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.']","[50, 60, 80]","[70, 70, 90]","[""The sentiment score is 50 (slightly positive) because the reviewer starts by acknowledging improvements made by the authors, stating the paper is 'much more convincing now' and increasing their rating to 'clear accept'. However, they still list several concerns and recommendations, balancing the positive opening. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, thanks the authors for making changes, and frames criticisms constructively. They use phrases like 'If I understand correctly' and 'I like the fact that' before offering critiques. The reviewer also provides detailed explanations for their concerns and offers suggestions for improvement, which is considerate and helpful."", ""The sentiment score is 60 (positive) because the reviewer recommends accepting the paper, stating it's a 'solid contribution' that's 'interesting for the ICLR community'. However, they also mention the need for revisions due to typos and lacking details, which slightly lowers the score from being extremely positive. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the authors' work positively ('interesting approach', 'managed to inject...'), and frames criticisms constructively ('It would be interesting to know...', 'It would be nice to see...'). The reviewer also provides detailed feedback to help improve the paper, which is a polite and helpful gesture. The language is professional and courteous, without being overly formal or effusive."", ""The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, describing it as 'clear, well organized, well written and easy to follow.' They also mention that the experiments are 'quite convincing' and 'clearly support the efficacy of the proposed method.' The reviewer states they don't have any 'major criticism,' which further indicates a positive sentiment. The score is not 100 as the reviewer does raise two points for the authors to address, showing some level of critique.\n\nThe politeness score is 90 (very polite) due to the reviewer's consistently respectful and constructive tone. They use phrases like 'I would like to hear the authors' opinions' and 'I wonder why,' which are polite ways of raising questions or potential issues. The reviewer also acknowledges the paper's strengths before presenting their questions, which is a courteous approach. The language throughout is professional and considerate, without any harsh or rude comments.""]"
"['This paper proposes Leap, a meta-learning procedure that finds better initialization for new tasks. Leap is based on past training/optimization trajectories and updates the initialization to minimize the total trajectory lengths. Experiments show that Leap outperforms popular alternatives like MAML and Reptile.\n\nPros\n- Novel idea\n- Relatively well-written\n- Sufficient experiment evidence\n\nCons\n- There exist several gaps between the theory and the algorithm\n\nI have several concerns.\n1. The idea is clearly delivered, but there are several practical treatments that are questionable. The first special treatment is that on page 5, when the objective is increased instead of decreased, the sign of the f part is flipped, which is not theoretically sound. It is basically saying that when we move from psi^i to psi^{i+1} with increased objective, we lie to the meta-learner that it is decreasing. The optimization trajectory is what it is. It would be beneficial to see the effect of removing this trick, at least in the experiments. Second, replacing the Jacobian with the identity matrix is also questionable. Suppose we use a very small but constant learning rate alpha for a convex problem. Then J^i=(I-G)^i goes to the zero matrix as i increases (G is small positive). However, instead, the paper uses J^i=I for all i. This means that the contributions for all i are the same, which is unsubstantiated.\n\n2. The proof of Thm1 in Appendix A is not complete. For example, ""By assumption, beta is sufficiently small to satisfy F"", which I do not understand the inequality. Is there a missing i superscript? Isn\'t this the exact inequality we are trying to prove for i=0? As another example, ""if the right-most term is positive in expectation, we are done"", how so? BTW, the right-most term is a vector so there must be something missing. It would be more understandable if the proof includes a high-level proof roadmap, and frequently reminds the reader where we are in the overall proof now.\n\n3. The set \\Theta is not very well-defined, and sometimes misleading. Above Eq.(6), \\Theta is mathematically defined as the intersection of points whose final solutions are within a tolerance of the *global* optimum, which is in fact unknown. As a result, finding a good initialization in \\Theta for all the tasks as in Eq.(5) is not well-defined.\n\n4. About the experiments. What is the ""Finetuning"" in Table 1? Presumably it is multi-headed but it should be made explicit. What is the standard deviation for Fig.4? The claim that ""Leap learns faster than a random initialization"" for Breakout is not convincing at all.\n\nMinors\n- In Eq.(4), f is a scalar so abs should suffice. This also applies to subsequent formulations.\n- \\mu is introduced above Eq.(8) but never used in the gradient formula.\n- On p6, there is a missing norm notation when introducing the Reptile algorithm.', '\\documentclass[10pt]{article}\n\\usepackage{geometry}[1in]\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsmath}\n\\usepackage{enumerate}\n\\usepackage{indentfirst}\n\n\\begin{document}\n\t\n\t\\section*{SUMMARY}\n\t\n\tThe article proposes Leap, a novel meta-learning objective aimed at outperforming state-of-the-art approaches when dealing with collections of tasks that exhibit substantial between-task diversity.\n\t\n\tSimilarly to prior work such as MAML [1] or Reptile [2], the goal of Leap is to learn an initialization $\\theta_{0}$ for the model parameters, shared across tasks, which leads to good and data-efficient generalization performance when fine-tuning the model on a set of held-out tasks. In a nutshell, what sets Leap apart from MAML or Reptile is its cost function, which explicitly accounts for the entire path traversed by the model parameters during task-specific fine-tuning -- i.e., ``inner loop\'\' optimization --, rather than mainly focusing on the final value attained by the model parameters after fine-tuning. More precisely, Leap looks for an initialization $\\theta_{0}$ of the model parameters such that the energy of the path traversed by $\\gamma_{\\tau}(\\theta) = (\\theta, f_{\\tau}(\\theta))$ while fine-tuning $\\theta$ to optimize the loss $f_{\\tau}(\\theta)$ of a task $\\tau$ is minimized, on average, across $\\tau \\sim p(\\tau)$. Thus, it could be argued that Leap extends Reptile, which can be informally understood as seeking an initialization $\\theta_{0}$ that minimizes the average squared Euclidean distance between $\\theta_{0}$ and the model parameters after fine-tuning on each task $\\tau \\sim p(\\tau)$ [2, Section 5.2], by using a distance function between initial and final model parameters that accounts for the geometry of the loss surface of each task during optimization.  \n\t\n\tThe final algorithm introduced in the paper considers however a variant of the aforementioned cost function, motivated by its authors on the basis of stabilising learning and eliminating the need for Hessian-vector products. The resulting approach is then evaluated on image recognition tasks (Omniglot plus a set of six additional computer vision datasets) as well as reinforcement learning tasks (Atari games).\n\t\n\t\\section*{HIGH-LEVEL ASSESSMENT}\n\t\n\tThe article proposes an interesting extension of existing work in meta-learning. In a slightly different context (meta-optimization), recent work [3] pointed out the existence of a ``short-horizon bias\'\' which could arise when using meta-learning objectives that apply only a small number of updates during ``inner-loop\'\' optimization. This observation is well-aligned with the motivation of this article, in which the authors attempt to complement successful methods like MAML or Reptile to perform well also in situations where a large number of gradient descent-based updates are applied during task-specific fine-tuning. Consequently, I believe the article is timely and relevant.\n\t\n\tUnfortunately, I have some concerns with the current version of the manuscript regarding (i) the proposed approach and the way it is motivated, (ii) the underlying theoretical results and, perhaps most importantly, (iii) the experimental evaluation. In my opinion, these should ideally be tackled prior to publication. Nonetheless, I believe that the proposed approach is promising and that these concerns can be either addressed or clarified. Thus I look forward to the rebuttal.\n\t\n\t\\section*{MAJOR POINTS}\n\t\n\t\\subsection*{1. Issues regarding proposed approach and its motivation/derivation}\n\t\n\t\\textbf{1.a} Section 2.1 argues in favour of studying the path traversed by $\\gamma_{\\tau}(\\theta) = (\\theta, f_{\\tau}(\\theta))$ rather than the path traversed by the model parameters $\\theta$ alone. However, this could in turn exacerbate the difficulty in dealing with collections of tasks for which the loss functions have highly diverse scales. For instance, taking the situation to the extreme, one could define an equivalence class of tasks $[\\tau] = \\left\\{\\tau \\mid f_{\\tau}(\\theta) = g(\\theta) + \\mathrm{constant} \\right\\}$ such that any two tasks $\\tau_{1}, \\tau_{2} \\in [\\tau]$ would essentially represent the same underlying task, but could lead to arbitrarily different values of the Leap cost function. \n\t\n\tGiven that Leap is a model-agnostic approach, like MAML or Reptile, and thus could be potentially applied in many different settings and domains, I believe the authors should study and discuss (theoretically or experimentally) the robustness of Leap with respect to between-task variation in the scale of the loss functions and, in case the method is indeed sensitive to those, propose an effective scheme to normalize them.\n\t\n\t\\textbf{1.b} The current version of the manuscript motivates defining the cost function in terms of $\\gamma_{\\tau}(\\theta) = (\\theta, f_{\\tau}(\\theta))$ rather than the model parameters $\\theta$ alone in order to ``avoid information loss\'\', making it seem that this modification is ``optional\'\' or, at least, not critical. Nevertheless, taking a closer look at the Leap objective and the meta-updates it induces, I believe it might actually be essential for the correctness of the approach. I elaborate this view in what follows. Let us write the Leap objective for a task $\\tau$ as\n\t\\[\n\tF_{\\tau}(\\theta_{0},\\widetilde{\\theta}_{0}) = \\underbrace{\\sum_{i=0}^{K_{\\tau} - 1}{\\left\\vert\\left\\vert u^{(i+1)}_{\\tau}(\\widetilde{\\theta}_{0}) - u^{(i)}_{\\tau}(\\theta_{0}) \\right\\vert\\right\\vert^{2}}}_{C_{\\tau, 1}(\\theta_{0},\\widetilde{\\theta}_{0})} + \\underbrace{\\sum_{i=0}^{K_{\\tau} - 1}{\\left( f_{\\tau}\\left(u^{(i+1)}_{\\tau}(\\widetilde{\\theta}_{0})\\right) - f_{\\tau}\\left(u^{(i)}_{\\tau}(\\theta_{0})\\right) \\right)^{2}}}_{C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0})},\n\t\\]\n\twhere $\\widetilde{\\theta}_{0}$ denotes a ``frozen\'\' or ``detached\'\' copy of $\\theta_{0}$ and $u^{(i)}_{\\tau}$ maps $\\theta_{0}$ to $\\theta_{i}$, the model parameters after applying $i$ gradient descent updates to $f_{\\tau}$ according to Equation (1) in the manuscript. Then, differentiating $C_{\\tau, 1}(\\theta_{0},\\widetilde{\\theta}_{0})$ and $C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0})$ with respect to $\\theta_{0}$ separately yields:\n\t\\begin{align*}\n\t\\nabla_{\\theta_{0}} C_{\\tau, 1}(\\theta_{0},\\widetilde{\\theta}_{0}) &= -2 \\sum_{i=0}^{K_{\\tau} - 1}{J_{i}^{T}\\left(\\theta_{i+1} - \\theta_{i} \\right)} = -2 \\alpha \\sum_{i=0}^{K_{\\tau} - 1}{J_{i}^{T} g_{i}} \\\\\n\t\\nabla_{\\theta_{0}} C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0}) &= -2 \\sum_{i=0}^{K_{\\tau} - 1}{\\left(f_{\\tau}(\\theta_{i+1}) -  f_{\\tau}(\\theta_{i})\\right) J_{i}^{T}g_{i}} = -2 \\sum_{i=0}^{K_{\\tau} - 1}{\\Delta f^{i}_{\\tau} J_{i}^{T}g_{i}}\n\t\\end{align*}\n\twhere $J_{i} = J_{\\theta_{0}}u^{(i)}_{\\tau}(\\theta_{0})$ denotes the Jacobian of $u^{(i)}_{\\tau}$ with respect to $\\theta_{0}$, $g_{i} = \\left. \\nabla_{\\theta} f_{\\tau}(\\theta)\\right\\rvert_{\\theta=\\theta_{i}}$ denotes the gradient of the loss function $f_{\\tau}$ evaluated at $\\theta_{i}$ and $\\Delta f^{i}_{\\tau} = f_{\\tau}(\\theta_{i+1}) -  f_{\\tau}(\\theta_{i})$ stands for the change in the loss function after the $i$-th update. To simplify the exposition, a constant ``inner-loop\'\' learning rate and no preconditioning were assumed, i.e., $\\alpha_{i} = \\alpha$ and $S_{i} = I$.\n\t\n\tFurthermore, the article claims that all Jacobian terms are approximated by identity matrices (i.e., $J_{i} = I$) as suggested in Section 5.2 of [1], leading to the following approximations:\n\t\\begin{align*}\n\t\t\\nabla_{\\theta_{0}} C_{\\tau, 1}(\\theta_{0},\\widetilde{\\theta}_{0}) \\approx -2 \\alpha \\sum_{i=0}^{K_{\\tau} - 1}{ g_{i}} \\\\\n\t\t\\nabla_{\\theta_{0}} C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0}) \\approx -2 \\sum_{i=0}^{K_{\\tau} - 1}{\\Delta f^{i}_{\\tau} g_{i}}\n\t\\end{align*}\n\t\n\tInterestingly, it can be seen that the contribution to the meta-update of the energy of the path traversed by the model parameters $\\theta$, $g_{\\mathrm{Leap},1} =\\nabla_{\\theta_{0}} C_{\\tau, 1}(\\theta_{0},\\widetilde{\\theta}_{0})$, actually points in exactly the opposite direction than the meta-update of Reptile, given by $g_{\\mathrm{Reptile}} = \\sum_{i=0}^{K_{\\tau} - 1}{g_{i}}$ (e.g. Equation (27) in [2]). In summary, if the Leap objective was defined in terms of $\\theta$ rather than $(\\theta, f_{\\tau}(\\theta))$, minimising the Leap cost function should maximise Reptile\'s cost function and viceversa. It is only the term $g_{\\mathrm{Leap},2} =\\nabla_{\\theta_{0}} C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0})$ that presumably ``re-aligns\'\' $g_{\\mathrm{Reptile}}$ and $g_{\\mathrm{Leap}} = g_{\\mathrm{Leap},1} + g_{\\mathrm{Leap},2}$. Indeed, \n\t\\[\n\tg_{\\mathrm{Leap}} = 2 \\sum_{i=0}^{K_{\\tau} - 1}{\\left(-\\Delta f^{i}_{\\tau} - \\alpha \\right) g_{i}}\n\t\\]\n\twill have positive inner product with $g_{\\mathrm{Reptile}}$ if each gradient update yields a sufficient decrease in the loss $f_{\\tau}$, that is, $\\Delta f^{i}_{\\tau} < -\\alpha$.\n\t\n\tMoreover, I also wonder if this is the reason why the authors introduce the ``regularization\'\' term $\\mu_{\\tau}^{i}$, which as it currently stands in the manuscript, does not seem to relate in a particularly intuitive manner to the original objective of minimising the energy of $\\gamma(t)$. By introducing $\\mu_{\\tau}^{i}$, the term $C_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0})$ becomes\n\t\\[\n\t\tC^{\\prime}_{\\tau, 2}(\\theta_{0},\\widetilde{\\theta}_{0}) = \\sum_{i=0}^{K_{\\tau} - 1}{-\\mathrm{sign}  \\left( f_{\\tau}\\left(u^{(i+1)}_{\\tau}(\\widetilde{\\theta}_{0})\\right) - f_{\\tau}\\left(u^{(i)}_{\\tau}(\\theta_{0})\\right) \\right) \\left( f_{\\tau}\\left(u^{(i+1)}_{\\tau}(\\widetilde{\\theta}_{0})\\right) - f_{\\tau}\\left(u^{(i)}_{\\tau}(\\theta_{0})\\right) \\right)^{2}},\n\t\\]\n\tleading to $g^{\\prime}_{\\mathrm{Leap},2} = 2 \\sum_{i=0}^{K_{\\tau} - 1}{\\vert \\Delta f^{i}_{\\tau} \\vert g_{i}}$ and \n\t\\[\n\tg^{\\prime}_{\\mathrm{Leap}} = 2 \\sum_{i=0}^{K_{\\tau} - 1}{\\left(\\vert \\Delta f^{i}_{\\tau} \\vert - \\alpha \\right) g_{i}}.\n\t\\]\n\tIn turn, this relaxes the sufficient condition under which Leap and Reptile lead to meta-updates with positive inner product, namely, it changes the condition $\\Delta f^{i}_{\\tau} < -\\alpha$ by a less restrictive counterpart $\\vert \\Delta f^{i}_{\\tau} \\vert \\ge \\alpha$.\n\t\n\tIf these derivations happen to be correct, then I believe the way Leap is currently motivated in the article could be argued to be slightly misleading. What seems to be its main inspiration, accounting for the path that the model parameters traverse during fine-tuning, does not seem to be what drives the meta-updates towards the ``correct\'\' direction. Instead, the component of the objective due to the path traversed by the loss function values appears to be more important or, at least, not optional. Furthermore, I believe the regularization term $\\mu_{\\tau}^{i}$ should be better motivated, as the current version of the manuscript does not seem to justify its need clearly enough.\n\t\n\tFinally, under the assumption that the above is not mistaken, I wonder whether further tweaks to the meta-update, such as $g^{\\prime\\prime}_{\\mathrm{Leap}} = 2 \\sum_{i=0}^{K_{\\tau} - 1}{\\mathrm{max}\\left(\\vert \\Delta f^{i}_{\\tau} \\vert - \\alpha, 0 \\right) g_{i}}$, could perhaps turn out to be helpful as well.\n\n\t\\subsection*{2. Theoretical results}\n\t\n\t\\textbf{2.a} Theorem 1 currently claims that the Pull-Forward algorithm converges to a local minimum of Equation (5). However, due to the non-convexity of the objective function, only convergence to a stationary point is established.\n\t\n\t\\textbf{2.b} Most importantly, I am not entirely certain that the proof of Theorem 1 is complete in its current form. As I understand it, using the notation introduced by the authors in Appendix A, the following identities hold:\n\t\\begin{align*}\n\t\tF(\\psi_{s};\\Psi_{s}) &= \\mathbb{E}_{\\tau,i} \\vert\\vert h_{\\tau}^{i} - z_{\\tau}^{i} \\vert\\vert^{2} \\\\\n\t\tF(\\psi_{s+1};\\Psi_{s}) &= \\mathbb{E}_{\\tau,i} \\vert\\vert h_{\\tau}^{i} - x_{\\tau}^{i} \\vert\\vert^{2} \\\\\n\t\tF(\\psi_{s};\\Psi_{s+1}) &= \\mathbb{E}_{\\tau,i} \\vert\\vert y_{\\tau}^{i} - z_{\\tau}^{i} \\vert\\vert^{2} \\\\\n\t\tF(\\psi_{s+1};\\Psi_{s+1}) &= \\mathbb{E}_{\\tau,i} \\vert\\vert y_{\\tau}^{i} - x_{\\tau}^{i} \\vert\\vert^{2}.\n\t\\end{align*}\n\t\n\tThe bulk of the proof is then devoted to show that $\\mathbb{E}_{\\tau,i} \\vert\\vert y_{\\tau}^{i} - z_{\\tau}^{i} \\vert\\vert^{2} = F(\\psi_{s};\\Psi_{s+1}) \\ge \\mathbb{E}_{\\tau,i} \\vert\\vert y_{\\tau}^{i} - x_{\\tau}^{i} \\vert\\vert^{2} = F(\\psi_{s+1};\\Psi_{s+1})$. However, I do not immediately see how to make the final ``leap\'\' from $F(\\psi_{s+1};\\Psi_{s+1}) \\le F(\\psi_{s};\\Psi_{s+1})$ to the actual claim of the Theorem, $F(\\psi_{s+1};\\Psi_{s+1}) \\le F(\\psi_{s};\\Psi_{s})$.\n\t\n\t\\subsection*{3. Experimental evaluation}\n\t\n\t\\textbf{3.a} The experimental setup of Section 4.1 closely resembles experiments described in articles that introduced continual learning approaches, such as [4]. However, rather than including [4] as a baseline, the current manuscript compares against meta-learning approaches typically used for few-shot learning, such as MAML and Reptile. Consequently, I would argue the combination of experimental setup and selection of baselines is not entirely fair or, at least, it is incomplete.\n\t\n\tTo this end, I would suggest to (i) include [4] (or a related continual learning approach) as an additional baseline in the experiments currently described in Section 4.1 as well as (ii) perform a new experiment to compare the performance of Leap to that of MAML and Reptile in few-shot classification tasks using OmniGlot and/or Mini-ImageNet as datasets.\n\t\n\t\\textbf{3.b} The Multi-CV experiment described in Section 4.2 currently does not have strong baselines other than Leap. If possible, I would suggest including [5] in the comparison, as it is the article which inspired this particular experiment.\n\t\n\t\\textbf{3.b} Likewise, the same holds for the experiment described in Section 4.3. In this case, I would suggest comparing to [4] for the same reason described above.\n\t\n\t\\section*{MINOR POINTS}\n\t\n\t\\begin{enumerate}\n\t\n\t\\item In Section 2.1, it is claimed that ""gradients that largely point in the same direction indicate a convex loss surface, whereas gradients with frequently opposing directions indicate an ill-conditioned loss landscape"". Nevertheless, convex loss surfaces can in principle be ill-conditioned as well.\n\t\n\t\\item Introducing a mathematical definition for the metric ""area under the training curve"" could make the experiment in Section 4.1 more self-contained.\n\t\n\t\\item Several references are outdated, as they cite preprints that have since been accepted at peer-reviewed venues.\n\t\n\t\\item The reinforcement learning experiments in Section 4.3 would benefit from additional runs with multiple seeds, and the subsequent inclusion of confidence intervals.\n\t\n\t\\item I believe certain additional experiments could be insightful. For example, (i) studying how sensitive the performance of Leap is to parameter of the ``inner-loop\'\' optimizer (e.g. choice of \n\toptimizer, learning rate, batch size) or (ii) describing how the introduction of $\\mu_{\\tau}^{i}$ affects the performance of Leap.\n\t\n\t\\end{enumerate}\n\t\n\t\\section*{TYPOS}\n\t\n\t\\begin{enumerate}\n\t\n\t\\item The first sentence entirely in page 6 appears to have a superfluous word.\n\t\n\t\\item The Taylor series expansion in the proof of Theorem 1 is missing the $O(\\bullet)$ terms (or a $\\approx$ sign).\n\t\n\t\\item Also in the proof of Theorem 1, if $c_{\\tau}^{i} = (\\delta_{\\tau}^{i})^{2} - \\alpha_{\\tau}^{i}\\xi_{\\tau}^{i}\\delta_{\\tau}^{i}$, wouldn\'t $\\omega = \\underset{\\tau, i}{\\mathrm{sup}} \\langle \\hat{x}^{i}_{\\tau} - \\hat{z}^{i}_{\\tau}, g(\\hat{x}^{i}_{\\tau}) - g(\\hat{z}^{i}_{\\tau})\\rangle + \\xi_{\\tau}^{i}\\delta_{\\tau}^{i}$ instead?\n\t\n\t\\end{enumerate}\n\n        \\section*{ANSWER TO REBUTTAL}\n        Please see comments in the thread.\n\n\t\n\t\\section*{REFERENCES}\n\t\n\t\\begin{enumerate}[ {[}1{]} ]\n\t\t\\item Finn et al. ``Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\'\' International Conference on Machine Learning. 2017.\n\t\t\\item Nichol et al. ``On First-Order Meta-Learning Algorithms.\'\' arXiv preprint. 2018\n\t\t\\item Wu et al. ``Understanding Short-Horizon Bias in Stochastic Meta-Optimization.\'\' International Conference on Learning Representations. 2018.\n\t\t\\item Schwarz et al. ``Progress \\& Compress: A scalable framework for continual learning.\'\'  International Conference on Machine Learning. 2018.\n\t\t\\item Serr{\\`a} et al. ``Overcoming Catastrophic Forgetting with Hard Attention to the Task.\'\'  International Conference on Machine Learning. 2018.\n\t\\end{enumerate}\t\n\\end{document}', ""In this paper, the authors study an important transfer learning problem, i.e., knowledge transfer between distinct tasks, which is usually called 'far transfer' (instead of 'near transfer'). Specifically, the authors propose a lightweight framework called Leap, which aims to achieve knowledge transfer 'across learning processes'. In particular, a method for meta-learning (see Algorithm 1) is developed, which focuses on minimizing 'the expected length of the path' (see the corresponding term in Eqs.(4-6)). Empirical studies on three public datasets show the effectiveness of the proposed method. Overall, the paper is well presented.\n\nSome comments/suggestions:\n(i) The details of the experiments such as parameter configurations are missing, which makes the results not easy to be reproduced.\n\n(ii) For the baseline methods used in the experiments, the authors are suggested to include more state-of-the-art transfer learning methods in order to make the results more convincing.\n\n(iii) Finally, if the authors can use some commonly used datasets in existing transfer learning works, the comparative results will be more interesting. \n""]","[-20, -20, 60]","[50, 60, 70]","[""The sentiment score is slightly negative (-20) because while the reviewer acknowledges some pros ('Novel idea', 'Relatively well-written', 'Sufficient experiment evidence'), they express several significant concerns and criticisms about the paper's methodology, proofs, and experiments. The overall tone suggests that major revisions are needed. The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns ('I have several concerns', 'It would be beneficial to see', 'This means that...') rather than using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper before diving into criticisms, which is a polite approach in academic reviews."", ""The sentiment score is slightly negative (-20) because while the reviewer finds the article 'interesting' and 'timely and relevant', they express several major concerns that 'should ideally be tackled prior to publication'. The overall tone suggests the reviewer sees potential in the work but has significant reservations. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the potential of the work, and expresses a willingness to consider the authors' rebuttal. They use phrases like 'I believe', 'I would suggest', and 'I look forward to the rebuttal' which maintain a collegial tone despite the criticisms. The reviewer also provides detailed, constructive feedback rather than dismissive criticism."", ""The sentiment score is 60 (positive) because the reviewer starts with a positive overview, stating that the paper studies an important problem and is well presented. They use phrases like 'important transfer learning problem' and 'Overall, the paper is well presented,' indicating a generally positive view. However, it's not extremely high as they also provide suggestions for improvement. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, framing their suggestions as 'comments/suggestions' rather than criticisms. They use phrases like 'the authors are suggested to' and 'if the authors can,' which are polite ways of offering advice. The tone is constructive and professional, without any harsh or rude language.""]"
"['(Score raised from 8 to 9 after rebuttal)\nThe paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such “winning-tickets” on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets.\n\nThe paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing “feature” of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version.\n\n1) The main “weakness” of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively “small” and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author’s decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds “in general”. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies.\n\n2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably “break” the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, …? On page 2, second paragraph, the paper states: ”When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch”. I don’t fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough “juice” and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers).\n\n3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow “shipping” each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer).\n\n4) Some things that would be interesting to try:\n4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all “behind” some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights’ “trajectories” during training to see if there is a correlation between the “distance weights traveled” and whether or not they end up in the winning ticket.\n4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning?\n\n5) Typo (should be through): “we find winning tickets though a principled search process”\n\n6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well)\n', '==== Summary ====\n\nIt is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated. This paper proposes a conjecture to explain this phenomenon that the authors call “The Lottery Ticket Hypothesis”:  large networks that can be trained successfully contain at initialization time small sub-networks — which are defined by both connectivity and the initial weights that the authors call “winning tickets” — that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely.\n\n==== Detailed Review ====\n\nI have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while.  For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the “right” direction for the beginning of the training, as mentioned in [1] that was cited in this submission. Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously — and that is why I strongly vote for the acceptance of this paper.\n\nThough I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues:\n\n1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject. For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results. The same should be done here. For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6].\n2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster “convergence”) and about generalization (better “generalization accuracy”). However, there is a slight issue with how these terms are treated in the paper. First, “convergence” is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum — it is better to write that early stopping regularization was used in this case. Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets). Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set. As a minor note, “generalization accuracy” as a term is not that common and might be a bit confusing, so it is better to write “test accuracy”.\n\nTo conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to ICLR.\n\n[1] Bengio et al. Convex neural networks. NIPS 2006.\n[2] Zhang et al. Understanding deep learning requires rethinking generalization. ICLR 2017.\n[3] Arora et al. Stronger generalization bounds for deep nets via a compression approach. ICML 2018.\n[4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning. Arxiv preprint 2018.\n[5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry. ICLR 2017.\n[6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. ICLR 2018. \n\n==== Updated Review Following Rebuttal ====\n\nThe authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. ', 'It was believed that sparse architectures generated by pruning are difficult to train from scratch. The authors show that there exist sparse subnetworks that can be trained from scratch with good generalization performance. To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary,  the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy.  They also present an algorithm to identify the winning tickets.\n\nThe conjecture is interesting and it is still a open question for whether a pruned network can reach the same accuracy when trained from scratch. It may helps to explain why bigger networks are easier to train due to “having more possible subnetworks from which training can recover a winning ticket”. It also shows the importance of both the pruned architecture and the initialization value. Actually another submission (https://openreview.net/forum?id=rJlnB3C5Ym) made the opposite conclusions.\n\nThe limitations of this paper are several folds:\n\n- The paper seems a bit preliminary and unfinished.  A lot of notations seems confusing, such as “when pruned to 21%”. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the “original initialization”. It is quite confusing as there is no definition anywhere about the “original initialization”. It would be clearer if the author can use some math notations.\n\n- As identified by the authors themself, lacking of supporting experiments on large-scale dataset and real-world models. Only MNIST/CIFAR-10 and toy networks like LeNet, Conv2/Conv4/Conv6 are used. The author has done experiments on resnet, I would be better to move it to the main paper.\n\n- There is no explanation about why the “lottery ticket” can perform well when trained with the “original initialization” but not with random initialization. Is it because the original initialization is not far from the pruned solution? Then this is a kind of overting to the obtained solution.\n\n- The other problem is that the implications are not clearly useful without showing any applications. The paper could be stronger if the authors can provide more results to support the applications of this conjecture.\n\n- The authors only explore the sparse networks. Model compression by sparsification has good compression rate, especially for networks with large FC layers. However, the acceleration relies on specific hardware/libraries. It would be more complete if the author can provide experiments on structurally pruned networks, especially for CNNs.\n\n- The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read.\n\nQuestions:\n- Does the winning tickets always exist?\n- What is the size of winning tickets for a very thin network? Would it also be less than 10%?\n\n\n------update----------\n\nI appreciate the author’s efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it.\n\nIt can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as “Improve our theoretical understanding of neural networks”, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.\n\nSpecifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even “resnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19”, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as “much thinner” networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., “winning tickets near or below 10-20%, depending on the level of overparameterization of the original network.”\n\nThe observation of “winning ticket weights tend to change by a larger amount then weights in the rest of the network” in Figure 19 seems natural and the conjecture of the reason “magnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude” sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization.\n\nThe figures could also be improved and simplified as the lines are hard to read and compare.\n\n']","[80, 80, -30]","[90, 90, 20]","[""The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, stating it 'adds a novel and interesting angle' to an important question, praises the experiments as 'clean' and 'well executed', and explicitly recommends accepting the paper for presentation. The reviewer also offers constructive criticism and suggestions for improvement, which slightly tempers the overall positivity. The politeness score is 90 (very polite) due to the reviewer's consistently respectful and constructive tone. They use phrases like 'I acknowledge and support the author's decision', 'The paper would be stronger, if...', and frame criticisms as suggestions for improvement rather than demands. The reviewer also takes care to explain that their critiques are meant to be constructive and do not detract from their overall positive assessment of the paper."", ""The sentiment score is 80 (positive) because the reviewer strongly recommends acceptance, describes the paper as 'thought-provoking' and 'highly deserving of being accepted', and states they 'very much enjoyed reading this submission'. They also mention the hypothesis is 'very appealing' and the experiments 'pretty convincing'. The score is not 100 as the reviewer does point out some issues that need addressing. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone. The reviewer uses phrases like 'I have found', 'I generally found', and 'I think that' to soften criticisms, and balances negative feedback with positive comments. They also use polite language like 'I urge the authors to address' rather than making demands. The score is not 100 as the review maintains a professional rather than overly deferential tone."", ""The sentiment score is -30 because while the reviewer acknowledges the interesting nature of the conjecture, they express several significant limitations and concerns about the paper. The review starts positively but quickly shifts to a more critical tone, highlighting multiple areas for improvement. The updated section further emphasizes that the reviewer's concerns were not fully addressed, indicating a generally negative sentiment. The politeness score is 20 because the reviewer uses respectful language throughout, acknowledging the authors' efforts and using phrases like 'I appreciate' and 'It would be better if'. However, the criticism is direct and substantive, preventing a higher politeness score. The reviewer maintains a professional tone while clearly expressing their reservations about the paper's readiness for acceptance.""]"
"['The paper proposes a convolutional alternative to self-attention. To achieve this, the number of parameters of a typical convolution operation is first reduced by using a depth-wise approach (i.e. convolving only within each channel), and then further reduced by tying parameters across layers in a round-robin fashion. A softmax is applied to the filter weights, so that the operation computes weighted sums of its (local) input (LightConv).\n\nBecause the number of parameters is dramatically reduced now, they can be replaced by the output of an input-dependent linear layer (DynamicConv), which gives the resulting operation a ""local attention"" flavour. The weights depend only on the current position, as opposed to the attention weights in self-attention which depend on all positions. This implies that the operation is linear in the number of positions as opposed to quadratic, which is a significant advantage in terms of scaling and computation time.\n\nIn the paper, several NLP benchmarks (machine translation, language modeling) that were previously used to demonstrate the efficacy of self-attention models are tackled with models using LightConv and DynamicConv instead, and they are shown to be competitive across the board (with the number of model parameters kept approximately the same).\n\nThis paper is well-written and easy to follow. The proposed approach is explained and motivated well. The experiments are thorough and the results are convincing. I especially appreciated the ablation experiment for which results are shown in Table 3, which provides some useful insights beyond the main point of the paper. The fact that a linear time approach can match the performance of self-attention based models is a very promising and somewhat surprising result.\n\nIn section 5.3, I did not understand what ""head band, next band, last band"" refers to. I assume this is described in the anonymous paper that is cited, so I suppose this is an artifact of blind review. Still, even with the reference unmasked it might be useful to add some context here.', 'Overall, this is a really good paper.\nThe authors propose an alternative to content based similarity for NL applications as compared to self-attention models by proposing the parameter and sequence length efficient Lightweight and Dynamic Convolutions.\nThe authors show, over various NL tasks like Translation, LM and Abstractive summarisation, the comparison of self attention models with Lightweight and Dynamic convolution layer.\nThe weight sharing was particularly interesting and can be seen as applying different heads for the same kernel. \n\nThe experimental results give strong evidence for these alternatives proposed by the authors.\nThe lightweight and dynamic convolution layers, both perform similar or better than the self-attention layer in all the tasks.\nThe WMT EnFr result is much better than all the other models, establishing a new state of the art.\n\nQuestion for the authors:\n1. Is the weight sharing within the kernel mostly for reducing computation?\nIf so, did you trying varying H size and measure how much that affects performance? What is surprising is that, in the ablation table the weight sharing increases the BLEU score by 0.1. \n2. Did you run any experiments where the kernel size covers the whole sentence?\n3. Since the number of parameters only change linearly wrt sequence length, did you try running this on datasets that have really long sequences to show the effectiveness of this approach further?\n4. How important was softmax normalization for training?', 'The authors present lightweight convolutions and dynamic convolutions, two significant advances over existing depthwise convolution sequence models, and demonstrate very strong results on machine translation, language modeling, and summarization. Their results go even further than those of the Transformer paper in countering the conventional wisdom that recurrence (or another way of directly modeling long-distance dependencies) is crucial for sequence-to-sequence tasks. Some things that I noticed:\n\n- While you do cite ""Depthwise Separable Convolutions for Neural Machine Translation"" from Kaiser et al. (ICLR 2018), there are some missed opportunities to compare more directly to that paper (e.g., by comparing to their super-separable convolutions). Kaiser et al. somewhat slipped under the community\'s radar after the same group released the Transformer on arXiv a week later, but it is in some ways a more direct inspiration for your work than the Transformer paper itself.\n\n- I\'d like to see more analysis of the local self-attention ablation. It\'s fantastic to see such a well-executed ablation study, especially one that includes this important comparison, but I\'d like to understand more about the advantages and drawbacks of local self-attention compared to dynamic convolutions. (For instance, dynamic convolutions are somewhat faster at inference time in your results, but I\'m unsure if this is contingent on implementation choices or if it\'s inherent to the architecture.)\n\n- From a systems and implementation perspective, it would be great to see some algorithm-level comparisons of parallelism and critical path length between dynamic convolutions and self-attention. My gut feeling is that dynamic convolutions significantly more amenable to parallelization on certain kinds of hardware, especially at train time, but that the caching that\'s possible in self-attention inference might make the approaches more comparable in terms of critical path latency at inference time; this doesn\'t necessarily line up with your results so far though.\n\n- You mostly focus on inference time, but you\'re not always as clear about that as you could be; I\'d also like to see train time numbers. Fairseq is incredibly fast on both sides (perhaps instead of just saying ""highly optimized"" you can point to a paper or blog post?)\n\n- The nomenclature in this space makes me sad (not your fault). Other papers (particularly a series of three papers from Tao Shen at University of Technology Sydney and Tianyi Zhou at UW) have proposed architectures that are similarly intermediate between self-attention and (in their case 1x1) convolution, but have decided to call them variants of self-attention. I could easily imagine a world where one of these groups proposed exactly your approach but called it ""Dynamic Local Self-Attention,"" or even a world where they\'ve already done so but we can\'t find it among the zillions of self-attention variants proposed in the past year. Not sure if there\'s anything anyone can do about that, but perhaps it would be helpful to briefly cite/compare to some of the Shen/Zhou work.\n\n- I think you should have tried a language modeling dataset with longer-term dependencies, like WikiText-103. Especially if the results were slightly weaker than Transformer, that would help place dynamic convolutions in the architecture trade-off space.\n\nThat last one is probably my most significant concern, and one that should be fairly easy to address. But it\'s already a great paper.']","[90, 90, 80]","[80, 80, 70]","[""The sentiment score is 90 (highly positive) because the reviewer expresses strong approval of the paper, describing it as 'well-written and easy to follow' with 'thorough' experiments and 'convincing' results. They also note that the findings are 'very promising and somewhat surprising'. The only minor criticism is about a lack of clarity in one section, which doesn't significantly impact the overall positive sentiment. The politeness score is 80 (quite polite) due to the reviewer's consistently respectful and professional tone. They use phrases like 'I especially appreciated' and provide constructive feedback without harsh criticism. The language is formal and courteous throughout, maintaining a positive and encouraging tone while still offering a balanced review."", ""The sentiment score is 90 because the review is overwhelmingly positive. The reviewer describes the paper as 'really good' and praises the authors' proposed alternative, experimental results, and establishment of a new state of the art. The only slight criticism is in the form of questions for further clarification. The politeness score is 80 because the language used is respectful and constructive throughout. The reviewer uses phrases like 'particularly interesting' and asks questions in a polite manner. The tone is professional and encouraging, without any harsh criticism or demanding language. The slightly lower score than the sentiment is due to the review being more neutral in tone rather than overtly polite, which is appropriate for a scientific peer review."", ""The sentiment score is 80 (positive) because the reviewer starts by praising the authors' work as 'significant advances' with 'very strong results', and describes the paper as 'already a great paper'. The overall tone is enthusiastic and supportive, with constructive suggestions for improvement. The politeness score is 70 (polite) due to the reviewer's respectful and considerate language throughout. They use phrases like 'I'd like to see', 'it would be great to see', and 'I think you should have tried', which offer suggestions without being demanding. The reviewer also acknowledges that some issues are 'not your fault' and praises aspects like the 'well-executed ablation study'. The slightly lower politeness score (compared to sentiment) is due to some direct critiques, though these are still phrased diplomatically.""]"
"[""There are several ingredients in this paper that I really liked. For example, (1) the notion that an agent should build a deterministic function of the past which implicitly captures the belief (the uncertainty or probability distribution about the state), by opposition for example to sampling trajectories to capture uncertainty, (2) modelling the world's dynamic in a learned encoded state-space (by opposition to the sensor space), (3) instead of modeling next-step probabilities p(z(t+1)|z(t)), model 'jumpy transitions' p(z(t+delta)|z(t)) to avoid unrolling at the finest time scale.\n\nNow for the weak points:\n(a) the justification for the training loss was not completely clear to me, although I can see that it has a variational flavor\n(b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to next-step probabilities via the chain rule of probabilities, so we don't have a clear way to compare the TD-VAE models with jumpy predictions against other more traditional models\n(c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).\n\nSo I believe that the authors are onto a great direction of investigation, but the execution of the paper could be improved."", 'The authors propose TD-VAE to solve an important problem in agent learning, simulating the future by doing jumpy-rollouts in abstract states with uncertainty. The authors first formulate the sequential TD-VAE and then generalize it for jumpy rollouts. The proposed method is well evaluated for four tasks including high dimensional complex task.\n\nPros.\n- Advancing a significant problem\n- Principled and quite original modeling based on variational inference\n- Rigorous experiments including complex high dimensional experiments\n- Clear and intuitive explanation (but can be improved further)\n\nCons. \n- Some details on the experiments are missing (due to page limit). It would be great to include these in the Appendix. \n- It is a complex model. For reproducibility, detail specification on the hyperparameters and architecture will be helpful.\n\nMinor comments\n- Why q(z_{t-1}|z_t, b_{t-1}, b_t) depends both  b_{t-1}, b_t, not only b_t?\n- The original model does not take the jump interval as input. Then, it is not clear how the jump interval is determined in p(z’|z)?\n', 'This paper proposes the temporal difference variational auto-encoder framework, a sequential general model following the intuition of temporal difference learning in reinforcement learning. The idea is nice and novel, and I vote for acceptance.\n1. The introduction of belief state in the sequential model is smart. How incorporate such technique in such an autoregressive model is not easy.\n2. Fig 1 clearly explained the VAE process.\n3. Four experiments demonstrated the main advantages of the proposed framework, including the effectiveness of proposed belief state construction and ability to jumpy rolling-out, \n\n\nOther Comments and Questions:\n1. Typo, p(s_{t_2}|s_{t_1}) in the caption of Fig 1.\n2. Can this framework partially solve the exposure bias?\n3. The author used uniform distribution for t_2 - t1, and from the ``NOISY HARMONIC OSCILLATOR`` we can indeed see larger interval will result in worse performance. However, the author also mentioned other distortion could be investigated, so I am wondering if the larger probability mass is put on larger dt, what the performance will become.\n4. The code should be released. I think that it is a fundamental framework deserving further development  by other researchers.']","[20, 80, 80]","[60, 70, 70]","[""The sentiment score is slightly positive (20) because the reviewer starts by highlighting several aspects they 'really liked' about the paper, showing appreciation for the authors' work. However, they also point out 'weak points', which balances out the positivity. The overall tone suggests the paper has potential but needs improvement. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the strengths before discussing weaknesses, and concludes with a constructive statement about the 'great direction of investigation'. The reviewer avoids harsh criticism and frames their comments as suggestions for improvement rather than outright criticism."", ""The sentiment score is 80 (positive) because the review starts with a clear statement of the paper's contribution and lists several pros, including 'advancing a significant problem' and 'rigorous experiments'. The cons mentioned are relatively minor and constructive. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. Phrases like 'It would be great to include' and 'will be helpful' indicate a polite and supportive tone. The reviewer also balances positive and negative feedback, which contributes to the overall politeness of the review."", ""The sentiment score is 80 (positive) because the reviewer explicitly states 'I vote for acceptance' and provides several positive comments about the paper, such as calling the idea 'nice and novel' and praising the 'smart' introduction of belief state. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, offers constructive feedback, and frames suggestions as questions or gentle recommendations. The reviewer also acknowledges the difficulty of the work ('not easy') and praises specific aspects ('clearly explained', 'demonstrated the main advantages'). The slightly lower politeness score compared to sentiment is due to the direct nature of some comments, such as pointing out a typo, which while not impolite, is less overtly courteous than other parts of the review.""]"
"['The paper is well written and flow well. The only thing I would like to see added is an elaboration of \n""run a semantic parsing module to translate a question into an executable program"". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. \n\nThis is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. \n\nIn concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words?   ', 'To achieve the state-of-the-art on the CLEVR and the variations of this, the authors propose a method to use object-based visual representations and a differentiable quasi-symbolic executor. Since the semantic parser for a question input is not differentiable, they use REINFORCE algorithm and a technique to reduce its variance. \n\nQuality: \nThe issue of invalid evaluation should be addressed. CLEVR dataset has train, validation, and test sets. Since the various hyper-parameters are determined with the validation set, the comparison of state-of-the-art should be done using test set. As the authors mentioned, REINFORCE algorithm may introduce high variance, this notion is critical to report valid results. However, the authors only report on the validation set in Table 2 including the main results, Table 4. For Table 5, they only specify train and test splits. Therefore, I firmly recommend the authors to report on the test set for the fair comparison with the other competitive models, and please describe how to determine the hyperparameters in all experimental settings. \n   \nClarity:\nAs mentioned above, please specify the experimental details regarding setting hyperparameters.\nIn Experiments section, the authors used less than 10% of CLEVR training images. How about to use 100% of the training examples? How about to use the same amount of training examples in the competitive models? The report is incomplete to see the differential evident from the efficient usage of training examples.\n\nOriginality and significance:\nThe authors argue that object-based visual representation and symbolic reasoning are the contributions of this work (excluding the recent work, NS-VQA < 1 month). However, bottom-up and top-down attention work [1] shows that attention networks using object-based visual representation significantly improve VQA and image captioning performances. If the object-based visual representation alone is the primary source of improvement, it severely weakens the argument of the neuro-symbolic concept learner. Since, considering the trend of gains, the contribution of the proposing method seems to be incremental, this concern is inevitable. To defend this critic, the additional experiment to see the improvement of the other attentional model (e.g, TbD, MAC) using object-based visual representations, without any other annotations, is needed.\n\nPros:\n- To confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the CLEVR dataset for visual reasoning diagnosis.\n\nCons:\n- Invalid evaluation to report only on the validation set, not test set.\n- The unclear significance of the proposed method combining object-based visual representations and symbolic reasoning\n- In the original CLEVR dataset paper, the authors said ""we stress that accuracy on CLEVR is not an end goal in itself"" and ""..CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems."" Based on this suggestion, can this work generalize to real-world settings? This paper lacks to discuss its limitation and future direction toward the general problem settings.\n\nMinor comments:\nIn 4.3, please fix the typos, ""born"" -> ""brown"" and ""convlutional"" -> ""convolutional"".\n\n\n[1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., & Zhang, L. (2018). Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. IEEE Computer Vision and Pattern Recognition (CVPR\'18).', '\nSummary:\n=========\nThe paper proposes a joint learning of visual representation and word and semantic parsing of the sentences given paired images and paired Q/A with a model called neuro-symbolic concept learner using curriculum learning. The paper reads well and is easy to follow. The idea of jointly learning visual concepts and language is an important task. Human reasoning involves learning and recall from multiple moralities. The authors use the CLEVR dataset for evaluation.\n\nStrength:\n========\n- Jointly learning the language parsing and visual representations indirectly from paired Q/A and paired images is interesting. Combining the visual learning with the visual questions answers by decomposing them into primitive symbolic operations and reasoning in symbolic space seems interesting.\n\n- End-to-end learning of the visual concepts, Q/A decomposition into primitives and program execution was shown to be competitive to baseline methods.\n\nWeakness:\n=========\n- Although, the joint learning and composition is interesting, the visual task is simplistic and it is not obvious how this would generalize into other complex VQA tasks.\n\n- Experiments are not as rigorous as the discussion of the methods suggests. Evaluation on more datasets would have made the comparisons and drawn conclusions more stronger. Although CLEVR is suited for learning relational concepts from referential expressions, it is a toy dataset. Applicability of the proposed method on other realistic datasets would have made the paper more stronger.']","[60, -30, 50]","[80, 50, 75]","[""The sentiment score is 60 (positive) because the reviewer starts by praising the paper as 'well written and flow well.' They then provide constructive suggestions for improvement, which indicates a generally positive view of the work. The politeness score is 80 (quite polite) due to the reviewer's use of respectful language throughout. They frame their suggestions as requests ('I would like to see') and opportunities rather than demands. The phrase 'This is not a requirement but an opportunity' is particularly polite. The reviewer also uses phrases like 'can you explain' and 'would be appreciated,' which maintain a courteous tone. The overall language is professional and constructive, without any harsh criticism or rude remarks."", ""The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('insightfully exploit'), they raise several significant concerns about the methodology, evaluation, and significance of the work. The overall tone is critical, with phrases like 'invalid evaluation', 'unclear significance', and 'lacks to discuss its limitation'. However, it's not entirely negative as they do mention some pros and offer constructive feedback. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use polite language such as 'please' when making requests and 'I firmly recommend' instead of more forceful language. They also balance criticism with positive remarks and provide detailed explanations for their concerns, which is a courteous approach in academic review."", ""The sentiment score is 50 (slightly positive) because the review begins with positive comments about the paper being well-written and addressing an important task. It lists several strengths before mentioning weaknesses, indicating a generally favorable view despite some criticisms. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits and framing criticisms constructively as suggestions for improvement rather than harsh judgments. The reviewer maintains a professional tone, using phrases like 'would have made the paper stronger' rather than directly criticizing.""]"
"[""Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.\n\n------\n\nThe paper proposes an end-to-end neural architecture for learning protein structures from sequences. The problem is highly important. The method proposes to use a Langevin simulator to fold the protein ‘in silico’ from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). The system works on internal coordinates, which are conditioned and integrated on the fly. The method seems to perform very well, improving upon their baseline model considerably.\n\nIn spite of the paper being an outstanding work, I have two criticisms about the accessibility and impact of the paper on the broader ICLR audience. In its current form and complexity, the paper feels accessible mostly to a narrow audience.\n\nFirst, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding (for instance, the appendix reveals that MSA is used to generate more data), and even with it the method is difficult to grasp as a whole. This paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. \n\nSecond, the introduction lists numerous competing methods both on the protein modelling side and on the MCMC vs optimisation side. The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general. No bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. Finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. The discussion on the exploding gradients and dampening is excellent however. The only baseline is one with the simulator replaced by an RNN. There does not seem to be any running time analyses. As such, it is hard to interpret the current system, and it feels like a black box."", 'This paper presents an end-to-end differentiable model (NEMO) for protein structure prediction. I found this paper very interesting and the idea of training the network through the sampling procedure promising. The authors present the challenges and techniques (damping, Lyapunov regularization etc) in detail.\n\nThe paper is clearly written, however the description of the method can be confusing. This stems in part from the many components of the network as well as the fact that the protein is represented using various coordinate systems and features, so that it is not easy to follow which applies at each stage. Fig. 6 in the appendix helps, however it would be better to have a (perhaps more concise) overview in the main text.\n\nIn the evaluation, the NEMO method is compared to a baseline approach using RNNs. While NEMO trained on profile features performs best, the baseline is trained on sequences only. However, it outperforms the NEMO model trained on sequence-only in every category. Therefore, it would be interesting to see whether NEMO outperforms a baseline trained on profile features. Otherwise, I am not certain whether I can follow the conclusion that ""NEMO generalizes more effectively"". Beyond that, it would be interesting to see some generated atomic substructures from the imputation network, in particular an analysis of how diverse the generated atom positions are and whether they depend on the local environment.\n\nOverall, I appreciate the general idea and find the proposed approach very interesting. The contribution could have been stronger with a more detailed evaluation and better presentation.', ""Overall this is an important piece of work that deserves publication at ICLR. I recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.\n\n# Quality\n\nThe hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by Langevin dynamics versus simply using a neural network to regress shape from sequence. They construct a flexible deep energy model where the sequence and structure dependent parts are separated in such a way that fast rollouts are possible. They also adapt the learning algorithm to  ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a GPU. \n\nThe only criticism in terms of quality of work is that it somewhat lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? I realise it will not be possible to give a completely fair like to like comparison, but it will help readers put the results in context if they understood, for example, what the average TM score for CASP12 results was, as summarized in this paper for example: https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423. Similarly, it would be useful to compare the baseline - at least qualitatively - with the results from AlQuraishi et. al. whose model seems very similar in spirit.\n\n# Clarity\n\nI think in terms of clarity, the paper could be improved a little to take into account the audience of ICLR. In particular:\n\n* It may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. Currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone. In fact a brief explanation of the importance of homology might help the reader understand the relevance of the hierarchical approach taken to splitting the training set.\n\n* Fig. 2 caption. Could add some information to explain what panel B is showing. I think this would go a long way to explain why both cartesian and internal coordinates are important.\n\n* Fig. 4 second panel. The x axis should be labeled fraction or be numbered 0-100.\n\n* Fig 4. caption. The figure does not have a caption explaining what the graphs are showing. This would be a good place to explain that the colors refer to test sets that overlap with the training set in the full CATH code (black), overlap only in the CAT code (orange) etc. I admit I had found the explanation of the test/train/validation split rather confusing. It is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc.\n\n* The nature of the loss. The appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the log-likelihood terms are mixed together. \n\n# Originality\n\nThe work is original and is references the relevant literature.\n"", ""The paper proposes a new end-to-end training framework for computational prediction of protein structure from sequence. \nThis is a very important problem and any progress due to new data and/or methods for utilizing may have high impact. \n\nThe paper presents several technical contributions in the modelling and training procedure - for example, automatic transformation between Cartesian and angular coordinates, using Langevin dynamics, and imputation method to get fine atomic coordinates. \n\nThe overall breadth and depth of the methods presented in the paper are impressive. The paper describes a quite complicated systems with multiple modules interacting between them. The paper doesn't describe the system built in enough details, although many of the details are given in the appendix.  \nFigure. 6 presents a scheme of the entire system, but it lacks details about the different modules, and it is not clear how they interact and how their training together is performed. \nThe pseudo-code boxes describing Algorithms 1-4, and Table 2 describing the representation are informative and helpful, and more descriptions of this type would help. \nFor example:    - In Algorithm 3, what do 'CartesianStep' and 'ClippedInternalStep' mean? where are they described? (should have their own boxes/description).  \n\t\t- I didn't see an Algorithm describing the atomic imputation part. \n\t\t- It would be good to add a high-level pseudo-code for the entire end-to-end training algorithm. In it there could be calls to Algorithms 1-4 when needed.  \n\nThere is also no single place where all the parameters used by the authors to achieve their empirical results are presented \n(e.g. learning rates, Gaussian kernel widths, how are random time steps for enforcing Lipschitz condition chosen etc.). \nIn addition, the empirical loss defined in eq. (8) is a sum of 6 different losses. It is not clear how are these very different losses scaled to the same 'units', which ones are more important, \nif and how are constants multiplying them chosen to give lower/higher weights to some of the losses etc. - I guess these choices will have a large effect on the training. \n\nThe authors present generalization results of their trained model in predicting 3D structures from CATH at different generalization level\n(i.e. different similarity levels to the training set proteins). It is not clear to me how good are these results, except that they are shown \nto be better than a baseline simple model. How well does the author's model compare to other recently suggested end-to-end models? \n(the authors mention AlQuraishi, Anand&Huang, papers). How do they compare to state-of-the art structure prediction programs? (e.g. CASP winners)? \nI realize giving an automatic end-to-end solution is interesting even if performance is below that of best programs, but still it would be good to know gaps.\nIf such comparisons are less meaningful/not practical to perform this should be argued convincingly. \nIt would also be useful to add some metrics of running time - it is not clear how computationally heavy and scalable is the author's model and training, compared to other methods. \n\n\nThere are many typos and inconsistent notations which makes it harder for the reader to understand the paper. \nFor example, 'Figure ??' in multiple locations, wrong Figure referenced, using s vs. S for sequence - S is defined as an L*20 matrix but in the appendix there are\n3 indices: s_{i,l,j} and it looks like different sequences in alignment should be denoted s_i. \nEquation for M_{l,j} isn't clear: j is used both as fixed index and index in summation. \nThe indexing in 'orientation vectors' v-hat_ij definition seems off (the formula of base vectors gives 0/0)\n\n\n\n""]","[50, 50, 70, 20]","[70, 75, 80, 50]","[""The sentiment score is 50 (slightly positive) because while the reviewer praises the paper as 'outstanding' and acknowledges its significant contributions, they also express major concerns about its accessibility and lack of comparisons to other methods. The initial paragraph indicates they raised their score after the authors addressed some concerns, showing overall positive sentiment despite remaining issues. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They phrase concerns as suggestions rather than harsh criticisms (e.g. 'I wonder if...', 'it is difficult to assess...'). The reviewer also balances critiques with positive comments, maintaining a constructive tone."", ""The sentiment score is 50 (moderately positive) because the reviewer finds the paper 'very interesting' and appreciates the general idea, but also points out areas for improvement such as the confusing method description and the need for a more detailed evaluation. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. Phrases like 'I found this paper very interesting', 'I appreciate the general idea', and 'The contribution could have been stronger' demonstrate a balanced and courteous approach to feedback."", ""The sentiment score is 70 (positive) because the reviewer starts by stating it's 'an important piece of work that deserves publication' and provides mostly constructive feedback. The only criticism is about lacking context with other results, but this is presented as a suggestion for improvement rather than a major flaw. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, offering suggestions rather than demands (e.g., 'I recommend', 'it may be useful', 'could be improved'). The reviewer also acknowledges the quality of the work and provides detailed, constructive feedback without using harsh or critical language."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the importance of the problem and the impressive breadth and depth of the methods presented. However, they also point out several areas where more detail or clarity is needed, and raise questions about comparisons to other methods. This mix of praise and constructive criticism results in a mildly positive overall sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions rather than direct attacks, and acknowledges the value of the work. They maintain a professional tone while providing detailed feedback. The score is not higher because the review doesn't include overtly polite phrases or compliments, focusing instead on objective analysis.""]"
"[""This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.\nThe contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. \nThe density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. \nAlthough the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.\nConcerning discussions and analysis:\n- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?\nIn algorithm 1, is \\epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator."", 'Summary:\nThis paper discusses an advance in the framework of normalizing flows for generative modeling, named FFJORD. The authors consider normalizing flows in the form of ordinary differential equations, as also discussed in [1]. Their contributions are two-fold: (1) they use an unbiased estimator of the likelihood of the model by approximating the trace of the jacobian with Hutchinson’s trace estimator, (2) they have implemented the required ODE solvers on GPUs. \n\nThe models are evaluated on a density estimation task on tabular data and two image datasets (MNIST and CIFAR10), as well as on variational inference for auto-encoders, where the datasets MNIST, Omniglot, Freyfaces and Caltech Silhouettes are considered. \n\nThe authors argue that the trace estimator, in combination with reverse-mode automatic differentiation to compute vector-Jacobian products, leads to a computational cost of O(D), instead of O(D^2) for the exact trace of the jacobian. \nThey compare this to the cost of computing a Jacobian determinant for finite flows, which is O(D^3) in general. They argue that in general all works on finite flows have adjusted their architectures for the flows to avoid the O(D^3) complexity, and that FFJORD has no such restriction.\nHowever, I would like the authors to comment on the following train of thought: autoregressive models, such as MAF, as well as IAF (inverse of an autoregressive model) do not require O(D^3) to compute jacobian determinants as the jacobian is of triangular form. Note however, they are still universal approximators if sufficient flows are applied, as any distribution can be factorized in an autoregressive manner. With this in mind, I find the red cross for MAF under free-form Jacobian slightly misleading. Perhaps I misunderstood something, so please clarify. \n\nAnother topic that I would like the authors to comment on is efficiency and practical use. One of the main points that the authors seem to emphasise, is that contrary to autoregressive models, which require D passes through the model to sample a datapoint of size D, FFJORD is a ‘single-pass’ model, requiring only one pass through the model. They therefore indicate that they can do efficient sampling. However, for FFJORD every forward pass requires a pass through an ODE solver, which as the authors also state, can be very slow. I could imagine that this is still faster than an autoregressive model, but I doubt this is actually of comparable speed to a forward pass of a finite flow such as glow or realNVP. \nOn the other hand, autoregressive models do not require D passes during training, whereas, if I understand correctly, FFJORD relies on two passes through ODE solvers, one for computing the loss, and a second to compute the gradient of the loss with respect to model parameters. So autoregressive models should train considerably faster. The authors do comment on the fact that FFJORD is slower than other models, but they do not give a hint as to how much slower it is. This would be of importance for practical use, and for other people to consider using FFJORD in future work. \n\nFor the density estimation task, FFJORD does not have the best performance compared other baselines, except for MNIST, for which the overall best model was not evaluated (MAF-DDSF). For variational inference FFJORD is stated to outperform all other flows, but the models are only evaluated on the negative evidence lower bound, and not on the negative log-likehood (NLL). I suspect the NLL to be absent from the paper as it requires more computation, and this takes a long time for FFJORD. Without an evaluation on NLL the improvement over other methods is questionable. Even if the improvement still holds for the NLL, the relative improvement might not weigh heavily enough against increased runtime. FFJORD does require less memory than its competitors.\n\nThe improved runtime by implementing the ODE solvers on GPU versus the runtime on a CPU would be useful, given that this is listed as one of the main contributions.\n\nBesides these questions/comments, I do think the idea of using Hutchinsons trace estimator is a valid contribution, and the experimental validation of continuous normalizing flows is of interest to the research community. Therefore, in my opinion, the community will benefit from the information in this paper, and it should be accepted. However I do wish for the authors to address the above questions as it would give a clearer view of the practical use of the proposed model. \n \nSee below for comments and questions:\n\nQuality\nThe paper has a good setup, and is well structured. The scope and limitations section is very much appreciated. \n\nClarity\nThe paper is clearly written overall. The only section I can comment on is the related work section, which is not the best part of the paper. The division in normalizing flows and partitioned transformations is a bit odd. Partitioned transformations surely are also normalizing flows. Furthermore IAF by Kingma et al. is put in the box of autoregressive models, whereas it is the inverse of an autoregressive model, such that it does not have the D-pass sample problem. For a reader who is not too familiar with normalizing flows literature, I think this section is a little confusing. Furthermore, there is no related work discussed on continuous time flows, such as (but not limited to) [2].\n\nOriginality\nThe originality of the paper is not stellar, but sufficient for acceptance. \n\nSignificance\nThe community can benefit from the experimental analysis of continuous time flows, and the GPU implementation of the ODE solver. Therefore I think this work is significant. \n\nDetailed questions/comments:\n\n1. In section 4.2, as an additional downside to MAF-DDSF, the authors argue that sampling cannot be performed analytically. Since FFJORD needs to numerically propagate the ODE, I do not think FFJORD can sample analytically either. Is this correct?\n2. The authors argue that they have no restriction on the architecture of the function f, even if they have O(D) estimation of the trace of the jacobian. However, they also say they make use of the bottle-neck trick to reduce the variance that arises due to Hutchinson’s estimate of the trace. This seems like a limitation on the architecture to me. Can the authors comment?\n3. In B.1 in the appendix, the street view house numbers dataset is mentioned, but no results appear in the main text, why not?\n4. In the results section, it is not clear to me which numbers of the baselines for different datasets are taken from other papers, and which numbers are obtained by the authors of this paper. Please clarify.\n5. In the conclusions, when discussing future work, the authors state that they are interested in reducing the number of function evaluations in the ODE solvers. In various disciplines many people have worked on this problem for a long time. Do the authors think major improvements are soon to be made?\n6. In section 5.2 the dependence of the number of function evaluations (NFE) on the data dimension D is discussed. As a thought experiment they use the fact that going from an isotropic gaussian distribution (in any D), to an isotropic gaussian distribution has a corresponding differential equation of zero. This should convince the reader that NFE is independent of D. However, this seems to me to be such a singular example, that I gain no insight from it, and it is not very convincing. Do the authors agree that this particular example does not add much? If not, please explain. \n\n[1] Chen et al. Neural ordinary differential equations. NIPS 2018\n[2] Chen et al. Continuous-time flows for deep generative models.\n\n**** EDIT *****\n\nI have read the response of the authors and appreciate their clarifications and the additional information on the runtimes. See my response below for the concern that remains about the absence of the estimate of the log likelihood for the VAE experiments. Besides this issue, the other comments/answers were satisfactory, and I think this paper is of interest to the research community, so I will stick with my score.\n\n', 'This paper discusses a technique for continuous normalization flow in which the transformations are not required to be volume preserving (the transformation with unity Jacobian), and architecture of neural network does not need to be designed to hold such property. Instead authors proposed no restriction on architecture of neural network to design their reversible mapping.\nThe Paper has good background and literature review, and as authors mentioned  this paper is base on the idea of  Chen, Tian Qi, et al. ""Neural Ordinary Differential Equations.""\xa0arXiv preprint arXiv:1806.07366\xa0(2018). Chapter two of this paper is summary of  ""Neural Ordinary Differential Equations.""\xa0and chapter Three is main contribution of this paper that can be summarized under two points:\n\n1- Authors borrowed the ""continuous normalizing flow "" in Chen et al. and they have designed unbiased log density estimator using Hutchinson trace estimator and evaluated the trace with complexity of O(D) (dimension of data) instead of O(D^2) that  is used in chen et al. Paper\n\n2- They proposed by reducing the hidden layer dimension of neural network, it is possible that variance of estimator to be reduced \n\nNovelty and Quality:\nthe main contribution of this paper is summarized above.\nThe paper do not contain any significant theorem or mathematical claims, it is more focused on design of linear algorithm that estimate continuous normalizing flow that they have borrowed from the Chen et al. paper.  This is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. Also no guarantees or bound has been given about the variance reduction of  estimator and it is more based on the authors intuition.\n\nClarity:\nThe paper is well written and previous relevant methods have been reviewed well. There are a few issues that are listed below:\n1-in section 3 the reason that dimensionality of estimator can reduce to D from D^2 can be explained more clearly \n\n2- Figure 1 is located on first page of the paper but it has never been referred in main paper, just it is mentioned once in appendix , it can be moved to appendix.\n\n3- in section 3.1.1 the “view view” can be changed to “view”\n\nsignificance and experiments:\nThe experiments are very detailed and extensive and authors have compared their algorithm with many other competing algorithms and showed improvement in many of the cases. \nAs mentioned in Quality and Novelty part of the review, just one comparison is missing and that is the comparison to method that the paper is inspired by. It would be interesting to see how much trace estimator approach that has been used in this paper, would sacrifice the negative log-likelihood or ELBO specially in real data like MNIST and CIFAR 10.  it seems original paper has not reported the performance on those data-sets as well, is this difficult as chen et. al. paper algorithm for trace calculation has complexity of O(D^2)? \n']","[50, -20, 50]","[75, 60, 70]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and its ability to be competitive, while also pointing out some limitations and areas for improvement. The review is not overwhelmingly positive, but it does recognize the value of the work. The politeness score is 75 (fairly polite) because the reviewer uses respectful language throughout, asks questions rather than making harsh criticisms, and provides constructive feedback. The reviewer acknowledges the paper's merits and frames suggestions as inquiries, which contributes to a polite tone. The language is professional and objective, without any rude or dismissive comments."", ""Sentiment score: The review expresses mixed sentiments. While the reviewer acknowledges some positive aspects ('the idea of using Hutchinsons trace estimator is a valid contribution', 'the community will benefit from the information in this paper'), there are several critical comments and concerns raised. The reviewer questions the efficiency, practical use, and performance comparisons of the proposed method. The overall tone suggests more concerns than praise, leading to a slightly negative sentiment score of -20.\n\nPoliteness score: The language used in the review is generally polite and professional. The reviewer uses phrases like 'I would like the authors to comment on', 'please clarify', and 'I do think' which maintain a respectful tone. They also acknowledge positive aspects of the paper and provide constructive feedback. However, some critiques are presented quite directly, which prevents the score from being extremely high. A politeness score of 60 reflects the overall courteous but frank nature of the review."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's good background, literature review, and extensive experiments. They also highlight the main contributions and improvements made by the authors. However, the reviewer points out some missing comparisons and lack of mathematical guarantees, which prevents a higher positive score. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive feedback. The suggestions for improvement are framed as questions or observations rather than harsh criticisms. The reviewer maintains a professional tone while offering both positive comments and areas for potential enhancement.""]"
"['This paper combines state of the art models for piano transcription, symbolic music synthesis, and waveform generation all using a shared piano-roll representation.  It also introduces a new dataset of 172 hours of aligned MIDI and audio from real performances recorded on Yamaha Disklavier pianos in the context of the piano-e-competition.  \n\nBy using this shared representation and this dataset, it is able to expand the amount of time that it can coherently model music from a few seconds to a minute, necessary for truly modeling entire musical pieces.\n\nTraining an existing state of the art transcription model on this data improves performance on a standard benchmark by several percentage points (depending on the specific metric used).\n\nListening test results show that people still prefer the real recordings a plurality of the time, but that the syntheses are selected over them a fair amount.  One thing that is clear from the audio examples is that the different systems produce output with different equalization levels, which may lead to some of the listening results.  If some sort of automatic mastering were done to the outputs this might be avoided.\n\nWhile the novelty of the individual algorithms is relatively meager, their combination is very synergistic and makes a significant contribution to the field.  Piano music modeling is a long-standing problem that the current paper has made significant progress towards solving.\n\nThe paper is very well written, but there are a few minor issues:\n* Eq (1) this is really the joint distribution between audio and notes, not the marginal of audio\n* Table 4: What do precision, recall, and f1 score mean for notes with velocity?  How close does the system have to be to the velocity to get it right?\n* Table 6: NLL presumably stands for Negative Log Likelihood, but this should be made explicity\n* Figure 2: Are the error bars the standard deviation of the mean or the standard error of the mean?\n', ""The paper addresses the challenge of using neural networks to generate original and expressive piano music.  The available techniques today for audio or music generation are not able to sufficient handle the many levels at which music needs to modeled.  The result is that while individual music sounds (or notes) can be generated at one level using tools like WaveNet, they don't come together to create a coherent work of music at the higher level.  The paper proposes to address this problem by imposing a MIDI representation (piano roll) in the neural modeling of music audio that serves as an intermediate (and interpretable) representation between the analysis (music audio -> MIDI) and synthesis (MIDI -> music audio) in the pipeline of piano music generation.  In order to develop and validate the proposed learning architecture, the authors have created a large data set of aligned piano music (raw audio along with MIDI representation).  Using this data set for training, validation and test, the paper reports on listening tests that showed slightly less favorable results for the generated music.  A few questions and comments are as follows.  MIDI itself is a rich language with ability to drive the generation of music using rich sets of customizable sound fonts.  Given this, it is not clear that it is necessary to reproduce this function using neural network generation of sounds.  The further limitation of the proposed approach seems to be the challenge of decoding raw music audio with chords, multiple overlayed notes or multiple tracks.  MIDI as a representation can support multiple tracks, so it is not necessarily the bottleneck.  How much does the data augmentation (audio augmentation) help?"", ""This paper describes a new large scale dataset of aligned MIDI and audio from real piano performances and presents experiments using several existing state-of-the-art models for transcription, synthesis, and generation. As a result of the new dataset being nearly an order of magnitude larger than existing resources, each component model (with some additional tuning to increase capacity) yields impressive results, outperforming the current state-of-the-art on each component task. \nOverall, while the modeling advances here are small if any, I think this paper represents a solid case study in collecting valuble supervised data to push a set of tasks forward. The engineering is carefully done, well-motivated, and clearly described. The results are impressive on all three tasks. Finally, if the modeling ideas here do not, the dataset itself will go on to influence and support this sub-field for years to come. \nComments / questions:\n-Is MAPS actually all produced via sequencer? Having worked with this data I can almost swear that at least a portion of it (in particular, the data used here for test) sounds like live piano performance captured on Disklavier. Possibly I'm mistaken, but this is worth a double check.\n-Refering to the triple of models as an auto-encoder makes me slightly uncomfortable given that they are all trained independently, directly from supervised data. \n-The MAESTRO-T results are less interesting than they might appear at first glance given that the transcriptions are from train. The authors do clearly acknowledge this, pointing out that val and test transcription accuracies were near train accuracy. But maybe that same argument could be used to support that the pure MAESTRO results are themselves generalizable, allowing the authors to simplify slightly by removing MAESTRO-T altogether. In short, I'm not sure MAESTRO-T results offer much over MAESTRO results, and could therefore could be omitted. \n""]","[80, 20, 80]","[70, 60, 70]","[""The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, highlighting its significant contributions and progress in the field. They use phrases like 'significant contribution,' 'significant progress,' and 'very synergistic.' The few criticisms are minor and constructive. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while offering constructive feedback. They use phrases like 'very well written' and frame their suggestions as 'minor issues.' The tone is professional and courteous, avoiding harsh criticism or dismissive language."", ""The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's attempt to address a challenging problem in music generation and notes the creation of a large dataset. However, they also express some skepticism about the necessity and limitations of the approach. The politeness score is moderately high (60) as the reviewer uses neutral language, asks questions rather than making harsh criticisms, and provides constructive feedback. The review begins with a summary of the paper's content before moving into specific comments and questions, which is a polite and professional approach. The language used is respectful throughout, without any rude or dismissive remarks."", ""The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, praising its 'impressive results,' 'solid case study,' and potential long-term influence on the field. The reviewer acknowledges the paper's strengths in data collection, engineering, and clear description. The few criticisms are minor and presented constructively. The politeness score is 70 (polite) due to the reviewer's respectful tone throughout. They use phrases like 'I think' and 'maybe' when offering suggestions, and acknowledge the authors' own transparency about potential limitations. The reviewer also offers specific, constructive feedback without harsh language. The score is not 100 as the review maintains a professional, slightly formal tone rather than being excessively polite.""]"
"[""This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.\n\nOverall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.\n\nI do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.\n\nIn equation 4.1 describes the GIN update, which is proposed as “the most powerful GNN”.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isn’t made very clear in the paper.\n\nThe training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.\n\nAs mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR.\n\n--------\n\nConsidering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.\n\nIn the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.\n\n--------\n\nUpdate: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again."", 'The author study the expressive power of neighborhood aggregation mechanisms used in Graph Neural Networks and relates them to the 1-dimensional Weisfeiler-Lehman heuristic (1-WL) for graph isomorphism testing. The authors show that GCNs with injections acting on the neighborhood features can distinguish the same graphs that can be distinguished by 1-WL. Moreover, they propose a simple GNN layer, namely GIN, that satisfies this property. Moreover, less powerful GNN layers are studied, such as GCN or GraphSage. Their advantages and disadvantages are discussed and it is shown which graph structures they can distinguish. Finally, the paper shows that the GIN layer beats SOTA GNN layers on well-known benchmark datasets from the graph kernel literature.\n\nStudying the expressive power of neighborhood aggregation mechanisms is an important contribution to the further development of GCNs. The paper is well-written and easy to follow. The experimental results are well explained and the evaluation is convincing.\n\nHowever, I have some concerns regarding the main result in Theorem 3. A consequence of the theorem is that it makes no differences (w.r.t. expressive power) whether one distinguishes the features of the node itself from those of its neighbors. This is remarkable and counterintuitive, but not discussed in the article. However, it is discussed in the proof of Theorem 3 (Appendix) which suggests that the number of iterations must be increased for some graphs in order to obtain the same expressive power. Unfortunately, at this point, the proof is a bit vague. I would like to see a discussion of this differences in the article. This should be clarified in a revised version. \n----\nEdit:\nThe counter example posted in a comment ( https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X&noteId=rkl2Q1Qi6X ) actually shows that my concerns regarding Theorem 3 and its proof were perfectly justified. I agree that the two graphs provide a counterexample to the main result of the paper. Therefore, I have adjusted my rating. I will increase my rating again when the problem can be resolved. However, this appears to be non-trivial.\n----\nMoreover, the novelty of the results compared to the related work, e.g., mentioned in the comments, should be pointed out.\n\n\nSome further questions and remarks:\n\n(Q1) Did you use a validation set for evaluation? If not, what kind of stopping criteria did was use?\n\n(Q2) You use the universal approximation theorem to prove Theorem 3. Could you please say something about the needed width of the networks?\n\n(R1) Could you please provide standard deviations for all experiments. I suspect that the accuracies on the these small datasets fluctuates quite a bit.\n\n(R2) In the comments it was already mentioned, that some important related work, e.g., [1], [2], are not mentioned. You should address how your work is different from theirs.\n\n\nMinor remarks:\n\n- The colors in Figure 1 are difficult to distinguish\n\n\n\n[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4703190\n[2] https://people.csail.mit.edu/taolei/papers/icml17.pdf\n\n-------------------\nUpdate:\nMost of the weak points were appropriately addressed by the authors and I have increased my rating accordingly.', 'This papers presents an interesting take on Weisfeiler-Lehman-type GNNs, where it shows that a WL-GNNs classification power is related to its ability to represent multisets. The authors show a few exemplar networks where the mean and the max aggregators are unable to distinguish different multisets, thus losing classification power. The paper also proposes averaging the node representation with its neighbors (foregoing the “concatenate” function) and using sum pooling rather than mean pooling as aggregator. All these observations are wrapped up in a GNN, called GIN. The experiments on Table 1 are inconclusive, unfortunately, as the average accuracies of the different methods are often close and there are no confidence intervals and statistical tests to help guide the reader to understand the significance of the results.\n\nMy chief concern is equating the Weisfeiler-Lehman test (WL-test) with Weisfeiler-Lehman-type GNNs (WL-GNNs). The WL-test relies on countable set inputs and injective hash functions. Here, the paper is oversimplifying the WL-GNN problem. After the first layer, a WL-GNN is operating on uncountable sets. On uncountable sets, saying that a function is injective does not tells us much about it; we need a measure of how closely packed we find the points in the function’s image (a measure in measure theory, a density in probability). On countable sets, saying a function is injective tells us much about the function. Moreover, the WL-test hash function does not even need to operate over sets with total or even partial orders. As a neural network, the WL-GNN “hash” ($f$ in the paper) must operate over a totally ordered set (\\mathbb{R}^n, n > 0). Porting the WL-test argument of “convergence to unique isomorphic fingerprints” to a WL-GNN requires a measure-theoretic analysis of the output of the WL-GNN layers, and careful analysis if the total order of the set does not create attractors when they are applied recursively. \n\nTo illustrate the above *attractor* point, let’s consider the construct of Theorem 1 of (Xu et al., 2018), where the WL-GNN “hash” ($f$) is (roughly) described as the transition probability matrix of a random walk on the input graph. Under well-known conditions, the successive application of this operator (""hash"" or transition probability matrix P in this case) can go towards an attractor (the steady state). Here, we need a measure-theoretic analysis of the “hash” even if it is bijective: random walk mixing. The random walk transition operator can be invertible (bijective), but we still say the random walker will mix, i.e., the walker forgets where it started, even if the transition operation can be perfectly undone by inversion (P^{-1}). In a WL-GNN that only uses the last layer for classification, this would manifest itself as poor performance in a WL-GNN with a large number of layers, and vanishing gradients. Of course, since (Xu et al., 2018) argued to revert back to the framework of (Duvenaud et al., 2015) of using the embeddings of all layers, one can argue that this mixing problem is just a problem of “wasted computation”.\n\nThe matrix analysis of the last paragraph also points to another potential problem with the sum aggregator. GIN needs to be shallow. With ReLU activations the reason is simple: for an adjacency matrix $A$, the value of $A^j$ grows very quickly with $j$ (diverges). With sigmoid activations, GIN would experience vanishing gradients in graphs with high variance in node degrees.\n\nThe paper should be careful with oversimplifications. Simplifications are useful for insight but can be dangerous if not prefaced by clear warnings and a good understanding of their limitations. I am not asking for a measure-theoretic analysis revision of the paper (it could be left to a follow-up paper). I am asking for a *relatively long* discussion of the limitations of the analysis.\n\nSuggestions to strengthen the paper:\n•\tPlease address the above concerns.\n•\tTable 1 should have confidence intervals (a statistical analysis of significance would be a welcome bonus).\n•\tPlease mention the classes of graphs where the WL-test cannot distinguish two non-isomorphic graphs. See (Douglas, 2011), (Cai et al., 1992) and (Evdokimov and Ponomarenko, 1999) for the examples. It is important for the WL-GNN literature to keep track of the more fundamental limitations of the method.\n•\t(Hamilton et al, 2017) also uses the LSTM aggregator, besides max aggregator and mean aggregator, which outperforms both max and mean in some tasks. Does the LSTM aggregator also outperforms the sum aggregator in the tasks of Table 1? It is important for the community to know if unusual aggregators (such as the asymmetric LSTM) have some yet-to-be-discovered class-distinguishing power.\n\n\n--------- Update -------\n\nThe counter-example in \nhttps://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X\nis indeed a problem for Theorem 3 if  \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is not a typo for a set of tuples \\{(h_v^{(k-1)}, h_u^{(k-1)}) : u \\in \\mathcal{N}_v\\}. Unfortunately, in their proof, the submission states ""difficulty in proving this form of aggregation mainly lies in the fact that it does not immediately distinguish the root or central node from its neighbors"", which means \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is actually \\{h_v^{(k-1)}\\} \\cup \\{ h_u^{(k-1)} : u \\in \\mathcal{N}_v\\}, which is not as powerful as WL. Concatenating is more powerful than the summing the node\'s own embedding, but it results in a  simpler model and could be easier to learn in practice. And I am still concerned about the countable x uncountable domain/image issue I raised in my review.\n\nStill, the reviewers seem to be doing all the discussion among themselves, with no input from the authors. I am now following Reviewer 2.\n\n----\n\nReverting my score to my original score. The authors have addressed most of my concerns, thank you. The restricted theorems and propositions better describe the contribution.\n\nI would like to note that while the proof of (Xu et al., 2018) is limited that does not mean it is not applicable to GIN or GraphSAGE or similar models. The paper uses 5 GNN layers, which in my experience is the maximum I could ever use with GNNs without seeing a degradation in performance. I don\'t think this should be a topic for this paper, though.\n\n\nXu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., & Jegelka, S. (2018). Representation Learning on Graphs with Jumping Knowledge Networks. In ICML.\n\nCai, J. Y., Fürer, M., & Immerman, N. (1992). An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4), 389-410.\n\nDouglas, B. L. (2011). The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211.\n\nEvdokimov, S., & Ponomarenko, I. (1999). Isomorphism of coloured graphs with slowly increasing multiplicity of Jordan blocks. Combinatorica, 19(3), 321-333.\n\n\n']","[70, 50, -20]","[80, 75, 50]","[""The sentiment score is 70 (positive) because the reviewer expresses strong approval overall, stating they 'quite liked this paper' and would vote to accept it. They praise the paper's approach as 'very interesting' and 'important'. However, they do raise some concerns and suggestions for improvement, which prevents a perfect score. The politeness score is 80 (very polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before offering constructive criticism. They use phrases like 'I feel the authors should...' rather than making demands. The reviewer also acknowledges when their own opinion has changed, showing humility. The tone remains professional and courteous throughout."", 'The sentiment score is 50 (moderately positive) because the reviewer acknowledges the importance of the work and praises the paper as well-written and convincing, but also raises significant concerns about the main theorem and its proof. The initial positive tone is tempered by the later discovery of a counterexample, though the reviewer remains open to increasing their rating if the issues are resolved. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames concerns as questions or suggestions rather than harsh criticisms. The reviewer also acknowledges when their concerns are addressed and adjusts their rating accordingly, showing fairness and openness to dialogue.', ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'interesting' and provides constructive feedback, they express several significant concerns and criticisms. The reviewer points out issues with the paper's core arguments, experimental results, and theoretical foundations. However, the tone is not entirely negative, as they offer suggestions for improvement. The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'My chief concern is...' and 'Suggestions to strengthen the paper:', which are polite ways to express criticism and offer advice. The reviewer also acknowledges positive aspects of the paper before delving into criticisms. The language is formal and constructive, avoiding harsh or personal criticisms.""]"
"['General:\nThe paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images. The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image. The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels. In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN. The obtained results are impressive, the generated images are large and contain realistic details.\n\nIn my opinion the paper would be interesting for the ICLR audience.\n\nPros:\n+ The paper is very technical but well-written.\n+ The obtained results constitute new state-of-the-art on HQ image datasets.\n+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling. The proposed approach is a very interesting step towards this direction.\n\nCons:\n- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.\n- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, “Taming VAEs”, 2018).\n\n--REVISION--\nI would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2).', 'Summary: \nThis paper addresses an important problem in density estimation which is to scale the generation to high fidelity images. Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA). This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples). The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos. Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits. The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations). Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation. \n\nDetails:\nMajor:\n-1. Can you point out the total number of parameters in the models? Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)? If not, I am curious to know how many GPUs were used.\n0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this.\n1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can\'t be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don\'t think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger.\n2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn\'t, given that subscale ordering didn\'t really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.\n3. I didn\'t quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn\'t it just normal padding -> space to depth in a structured block-wise fashion? \n4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. \n5. I also don\'t understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels\' salient bits now and not just meta-positions prior to this slice? \n6. It is really cool that you don\'t lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn\'t as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation.\n7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, ""parameter attention"", conv channels? \n\nMinor: \nTypo: unpredented --> unprecedented ', 'Authors propose a decoder arquitecture model named Subscale Pixel Network. It is meant to generate overall images as image slice sequences with memory and computation economy by using a Multidimensional Upscaling method.\nThe paper is fairly well written and structured, and it seems technically sound.\nExperiments are convincing.\nSome minor issues:\nFigure 2 is not referenced anywhere in the main text.\nFigure 5 is referenced in the main text after figure 6.\nEven if intuitively understandable, all parameters in equations should be explicitly described (e.g., h,w,H,W in eq.1)']","[80, 90, 70]","[90, 80, 50]","[""The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, highlighting its technical merits, impressive results, and relevance to the field. They explicitly state it would be interesting for the ICLR audience and list several pros. While they do mention some cons, these are presented as minor suggestions for improvement rather than major flaws. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone. The reviewer uses phrases like 'I would like to thank the authors' and 'I highly appreciate', showing courtesy. They also frame their criticisms as questions or suggestions rather than harsh critiques. The language throughout is professional and considerate, with no rudeness detected."", ""The sentiment score is 90 because the reviewer expresses very positive views about the paper, calling it a 'huge advance in density modeling' and stating it 'deserves an oral presentation'. They highlight several strengths and innovations of the work. The politeness score is 80 because the reviewer uses respectful and professional language throughout, offering constructive feedback and asking clarifying questions rather than making harsh criticisms. They use phrases like 'would be good to know' and 'I would really like to know' which are polite ways of requesting additional information. The reviewer also compliments the authors' work multiple times. The only slight reduction in politeness score is due to the directness of some requests, which is typical in academic reviews but could be seen as slightly less polite in other contexts."", ""The sentiment score is 70 (positive) because the reviewer states that the paper is 'fairly well written and structured', 'technically sound', and that the 'experiments are convincing'. These are all positive comments indicating approval of the work. However, it's not a perfect score due to the mention of 'minor issues'. The politeness score is 50 (slightly polite) because the reviewer uses neutral language throughout and frames their criticisms as 'minor issues', which is a polite way to present negative feedback. The reviewer also uses phrases like 'fairly well' which softens the praise in a polite manner. The tone is professional and constructive without being overly formal or deferential, hence the moderate positive score.""]"
"['This paper addresses questions about the representation of visual information in the retina. The authors create a deep neural network model of the visual system in which a single parameter (bandwidth between the “retina” and “visual cortex” parts) is sufficient to qualitatively reproduce retinal receptive fields observed across animals with different brain sizes, which have been hard to reconcile in the past. \n\nThis work is an innovative application of deep neural networks to a long-standing question in visual neuroscience. While I have some questions about the analyses and conclusions, I think that the paper is interesting and of high quality.\n\nMy main concern is that the authors only show single examples, without quantification, for some main results (RF structure). For example, for Fig. 2A and 2B, an orientation selectivity index should be shown for all neurons. A similar population analysis should be devised for Fig 2C, e.g. like Fig 3 in [1]\n\nMinor comments:\n1. Page 4: “These results suggest that the key constraint ... might be the dimensionality bottleneck..”: The analyses only show that the bottleneck is *sufficient* to explain the differences, but “the key constraint” also implies *necessity*. Either soften the claim or provide control experiments showing that alternative hypotheses (constraint on firing rate etc.) cannot explain this result in your model.\n\n2. I don’t understand most of the arguments about “cell types” (e.g. Fig. 2F and elsewhere). In neuroscience, “cell types” usually refers to cells with completely different connectivity constraints, e.g. excitatory vs. inhibitory cells or somatostatin vs. parvalbumin cells. But you refer to different CNN channels as different “types”. This seems very different than the neuroscience definition. CNN channels just represent different feature maps, i.e. different receptive field shapes, but not fundamentally different connectivity patterns. Therefore, I also don’t quite understand what you are trying to show with the weight-untying experiments (Fig. 2E/F).\n\n3. It is not clear to me what Fig. 3B and the associated paragraph are trying to show. What are the implications of the nonlinearity being due to the first or second stage? \n\n4. Comment on Fig 3F: The center-surround RFs probably implement a whitening transform (which is linear). Whitened inputs can probably be represented more efficiently in a network trained with L2-regularization and/or SGD. This might explain why the “quasi-linear” retina improves separability later-on.\n\n[1] Cossell, Lee, Maria Florencia Iacaruso, Dylan R. Muir, Rachael Houlton, Elie N. Sader, Ho Ko, Sonja B. Hofer, and Thomas D. Mrsic-Flogel. “Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex.” Nature 518, no. 7539 (February 19, 2015): 399–403. https://doi.org/10.1038/nature14182.', 'I enjoyed reading this paper which is a great example of solid computational neuroscience work.\n\nThe authors trained CNNs under various biologically-motivated constraints (e.g., varying the number of units in the layers corresponding to the retina output to account for the bottleneck happening at the level of the optic nerve or varying the number of ""cortical"" layers to account for differences across organisms). The paper is clear, the hypotheses clearly formulated and the results are sound. The implications of the study are quite interesting suggesting that the lack of orientation selectivity in the retina would arise because of the bottleneck at the level of the optic nerve. The continuum in terms of degree of linearity/non-linearity observed across organisms at the level of the retina would arise as a byproduct of the complexity/depth of subsequent processing stages. While these results are somewhat expected this is to my knowledge the first time that it is shown empirically in an integrated computational model.\n\nMinor point: The authors should consider citing the work by Eberhardt et al (2016) which has shown that the exists an optimal depth for CNNs to predicting human category decisions during rapid visual categorization.\n\nS. Eberhardt, J. Cader & T. Serre. How deep is the feature analysis underlying rapid visual categorization? Neural Information Processing Systems, 2016.\n\n', 'EDIT: On the basis of revisions made to the paper, which significantly augment the results, the authors note: ""the call for papers explicitly mentions applications in neuroscience as within the scope of the conference"" which clarifies my other concern. For both of these reasons, I have changed my prior rating.\n\nThis paper is focused on a model of early visual representation in recognition tasks drawing motivation from neuroscience. Overall the paper is an interesting read and reasonably well written (albeit with some typos). The following addresses the positives and negatives I see associated with this work:\n\nPositives:\n- There are relatively few efforts that focus heavily on more shallow models with an emphasis on representation learning, and for this reason this paper fills an important space\n- The connections to neuroscience are interesting albeit it\'s unclear the extent to which this is the mandate of the conference\n- The most interesting bit of the paper to me is the following: ""A bottleneck at the output of the retina yielded center-surround retinal RFs"" - it is somewhat a foregone conclusion that most networks immediately converge on orientation selective and color opponent representations. That this model produces isotropic filters is a very interesting point.\n\nNegatives:\n- The work feels a little bit shallow. It would have been nice to see a bit more density in terms of results and ablation studies. This also relates to my second point.\n- Given the focus on early visual processing, there seems to be a missed opportunity in examining the role of normalization mechanisms or the distinction between simple and complex cells. If the focus resides in the realm of neuroscience and early visual representation, there is an important role to these mechanisms. e.g. consider the degree of connectivity running from V1 to LGN vs. LGN to V1.\n\n']","[60, 90, 50]","[80, 80, 75]","[""The sentiment score is 60 (positive) because the reviewer states that the work is 'innovative' and 'interesting and of high quality', despite having some concerns. They also mention that the paper addresses long-standing questions in the field. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames their concerns as questions or suggestions rather than harsh criticisms. They use phrases like 'I think that' and 'My main concern is' which maintain a collegial tone. The reviewer also provides constructive feedback and specific recommendations for improvement, which is a polite and helpful approach in peer review."", ""The sentiment score is 90 because the reviewer expresses clear enjoyment and appreciation for the paper, calling it a 'great example of solid computational neuroscience work.' They describe the paper as clear, with well-formulated hypotheses and sound results. The implications are noted as 'quite interesting,' and the work is praised for being the first to show certain results empirically. The politeness score is 80 because the reviewer uses respectful and positive language throughout, such as 'I enjoyed reading this paper' and 'The paper is clear.' They offer constructive feedback in a polite manner, using phrases like 'The authors should consider' for their minor suggestion. The overall tone is professional and courteous, without any harsh criticism or rude language."", ""The sentiment score is 50 (slightly positive) because the reviewer begins by noting that the paper is 'an interesting read and reasonably well written,' and lists several positives. However, they also mention some negatives, creating a balanced view. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms constructively. They use phrases like 'it would have been nice to see' rather than direct criticism. The reviewer also mentions changing their prior rating based on the authors' revisions, showing flexibility and fairness.""]"
"['Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence ""Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied.""  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase ""in the settings studied"" replaced by ""for an alexnet architecture"".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.\n\nRevision 1: This is a revision of my earlier review.  My overly-excited earlier rating was based on tables 1 and 2 and the claim to have unsupervised features that are competitive with fully-supervised features. (I also am subject to an a-priori bias in favor of mutual information methods.)  I took the authors word for their claim and submitted the review without investigating existing results on CIFAR10.  It seems that tables 1 and 2 are presenting extremely weak fully supervised baselines.  If DIM(L) can indeed produce features that are competitive with state of the art fully supervised features, the result is extremely important.  But this claim seems misrepresented in the paper.\n\nOriginal review:\n\nThere is a lot of material in this paper and I respect this groups\nhigh research-to-publication ratio. However, it might be nice to have\nthe paper more focused on the subset of ideas that seem to matter.\n\nMy biggest comment is that the top level spin seems wrong.\nSpecifically, the paper focuses on the two bullets on page 3 ---\nmutual information and statistical constraints.  Here mutual\ninformation is interpreted as the information between the input and\noutput of a feature encoder.  Clearly this has a trivial solution\nwhere the input equals the output so the second bullet --- statistical\nconstraints --- are required.  But the empirical content of the paper\nstrongly undermines these top level bullets.  Setting the training\nobjective to be the a balance of MI between input and output under a\nstatistical consrtraint leads to DIM(G) which, according the results in\nthe paper, is an empirical disaster.  DIM(L) is the main result and\nsomething else seems to be going on there (more later).  Furthermore,\nthe empirical results suggest that the second bullet --- statistical\nconstraints --- is of very little value for DIM(L). The key ablation\nstudy here seems to be missing from the paper.  Appendix A.4 states\nthat ""a small amount of the [statistical constraint] helps improve\nclassification results when used with the [local information\nobjective].  No quantitative ablation number is given.  Other measures\nof the statistical constraint seem to simply measure to what extent\nthe constraint has been successfully enforced.  But the results\nsuggest that even successfully enforcing the constraint is of little,\nif any, value for the ability of the features to be effective in\nprediction.  So, it seems to me, the paper to really just about the\nlocal information objective.\n\nThe real power house of the paper --- the local information objective\n--- seems related to mutual information predictive coding as\nformalized in the recent paper from deep mind by van den Oord et al\nand also an earlier arxiv paper by McAllester on information-theoretic\nco-training.  In these other papers one assumes a signal x_1, ... x_T\nand tries to extract low dimensional features F(x_t) such that F(x_1),\n..., F(x_t) carries large mutual information with F(x_{t+1}).  The\nlocal objective of this paper takes a signal x1, ..., x_k (nXn\nsubimages) and extracts local features F(x_1), ... F(x_k) and a global\nfeature Y(F(x_1), ..., F(x_k)) such that Y carries large mutual\ninformation with each of the features F(x_i).  These seem different\nbut related.  The first seems more ""on line"" while the second seems\nmore ""batch"" but both seem to be getting at the same thing, especially\nwhen Y is low dimensional.\n\nAnother comment about top level spin involves the Donsker-Varadhan\nrepresentation of KL divergence (equation (2) in the paper).  The\npaper states that this is not used in the experiments.  This suggests\nthat it was tried and failed.  If so, it would be good to report this.\nAnother contribution of the paper seems to be that the mutual\ninformation estimators (4) and (5) dominate (2) in practice.  This\nseems important.\n\n', 'This paper presents a representation learning approach based on the mutual information maximization. \nThe authors propose the use of local structures and distribution matching for better acquisition of representations (especially) for images.\n\nStrong points of the paper are: \n* This gives a principled design of the objective function based on the mutual information between the input data point and output representation. \n* The performance is gained by incorporating local structures and matching of representation distribution to a certain target (called a prior).\n\nA weak point I found was: \nThe local structure and evaluation are specialized for classification task of images. \n\nQuestions and comments.\n* Local mutual information in (6) may trivially be maximized if the summarizer f (E(x) = f \\circ C(x) with \\psi omitted for brevity) concatenates all local features into the global one.\nHow was f implemented? Did you compare this concatenation approach?\n* Can we add DIM like a regularizer to an objective of downstream task? \nIt would be very useful if combining an objective of classification/regression or reinforcement learning with the proposed (8) is able to improve the performance of the given task.\n* C^(i)_\\psi(X) in (6), but X^(i) in (8): are they the same thing?', 'This paper proposes Deep InfoMax (DIM), for learning representations by maximizing the mutual information between the input and a deep representation. By structuring the network and objectives to encode input locality or priors on the representation, DIM learns features that are useful for downstream tasks without relying on reconstruction or a generative model. DIM is evaluated on a number of standard image datasets and shown to learn features that outperform prior approaches based on autoencoders at classification.\n\nRepresentation learning without generative models is an interesting research direction, and this paper represents a nice contribution toward this goal. The experiments demonstrate wins over some autoencoder baselines, but the reported numbers are far worse than old unsupervised feature learning results on e.g. CIFAR-10. There are also a few technical inaccuracies and an insufficient discussion of prior work (CPC). I don\'t think this paper should be accepted in its current state, but could be persuaded if the authors address my concerns.\n\nStrengths:\n+ Interesting new objectives for representation learning based on increasing the JS divergence between joint and product distributions\n+ Good set of ablation experiments looking at local vs global approach and layer-dependence of classification accuracy\n+ Large set of experiments on image datasets with different evaluation metrics for comparing representations\n\nWeaknesses:\n- No comparison to autoencoding approaches that explicitly maximize information in the latent variable, e.g. InfoVAE, beta-VAE with small beta, an autoencoeder with no regularization, invertible models like real NVP that throws out no information. Additionally, the results on CIFAR-10 are worse than a carefully tuned single-layer feature extractor (k-means is 75%+, see Coates et al., 2011). \n- Based off Table 9, it looks like DIM is very sensitive to hyperparameters like gamma for classification. Please discuss how you selected hyperparameters and whether you performed a similar scale sweep for your baselines.\n- The comparison with and discussion of CPC is lacking. CPC outperforms JSD in almost all settings, and CPC also proposed a ""local"" approach to information maximization. I do not agree with renaming CPC to NCE and calling it DIM(L) (NCE) as the CPC and NCE loss are not the same. Please elaborate on the similarties and differences!\n- The clarity of the text could be improved, with more space in the main text devoted to analyzing the results. Right now the paper has an overwhelming number of experiments that don\'t fit concisely together (e.g. an entirely new generative model experimentsin the appendix).\n\nMinor comments:\n- As noted by a commenter, it is known that MI maximization without constraints is insufficient for learning good representations. Please cite and discuss.\n- Define local/global earlier in the paper (intro?). I found it hard to follow the first time.\n- Why can\'t SOMs represent complex relationships?\n- ""models with reconstruction-type objectives provide some guarantees on the amount of information encoded"": what do you mean by this? VAEs have issues with posterior collapse where the latents are ignored, but they have a reconstruction term in the objective.\n- ""JS should behave similarly as the DV-based objective"" - do you have any evidence (empirical or theoretical) to back up this statement? As you\'re maximizing JSD and not KL, it\'s not clear that DIM can be thought of as maximizing MI.\n- Have you tried stochastic encoders? This would make matching to a prior much easier and prevent the introduciton of another discriminator.\n- I\'m surprised NDM is much smaller than MINE given that your encoder is deterministic and thus shouldn\'t throw out any information. Do you have an explanation for this gap?\n- there\'s a trivial solution to local DIM where the global feature can directly memorize everything about the local features as the global feature depends on *all* local features, including the one you\'re trying to maximize information with. Have you considered masking each individual local feature before computing the global feature to avoid this trivial solution? \n\n-----------------------\n\nUpdate: Apologies for the slow response. The new version with more baselines, comparisons to CPC, discussion of NCE, and comparisons between JS and MI greatly improve the paper! I\'ve increased my score (5 -> 7) to reflect the improved clarity and experiments. ']","[50, 60, -20]","[70, 80, 60]","[""The sentiment score is 50 (slightly positive) because while the reviewer acknowledges improvements and raises the score, they still express some concerns and suggest changes. The overall tone is constructive but not overwhelmingly positive. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges their own delay in responding, and frames criticisms as suggestions rather than demands. Phrases like 'I apologize' and 'The authors should simply consider' contribute to the polite tone. The reviewer also balances critique with praise, recognizing the value of new comparisons and the group's 'high research-to-publication ratio'."", ""The sentiment score is 60 (positive) because the reviewer begins by highlighting the strong points of the paper, praising its principled design and performance gains. They do mention a weak point, but it's presented as a minor issue. The overall tone is supportive and interested. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, frames criticisms constructively, and poses questions in a curious rather than confrontational manner. The use of phrases like 'Strong points of the paper are:' and the way questions are framed as suggestions for improvement or clarification contribute to the polite tone."", ""The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as an 'interesting contribution', they state it 'shouldn't be accepted in its current state' and list several weaknesses and concerns. However, they are open to changing their mind if issues are addressed. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges strengths, and frames criticisms constructively as suggestions for improvement rather than harsh judgments. They also apologize for a slow response and acknowledge improvements in an update. The balanced tone and constructive approach, even when critical, contribute to the politeness.""]"
"['The paper proposed a novel differentiable neural GLCM network which captures the high reference textural information and discard the lower-frequency semantic information so as to solve the domain generalisation challenge. The author also proposed an approach “HEX” to discard the superficial representations. Two synthetic datasets are created for demonstrating the methods advantages on scenarios where the domain-specific information is correlated with the semantic information. The proposal is well structured and written. The quality of the paper is excellent in terms of novelty and originality. The proposed methods are evaluated thoroughly through experiments with different types of dataset and has shown to achieve good performance. ', 'Summary:\nThe paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks. The goal is to address the domain adaptation (DA)/domain generalization (DG) issue. The paper introduces a new learning task where the domain identity is unavailable during training, called unguided domain generalization (UDG). The proposed approach is based on an old method of using gray level co-occurence matrix, updated to allow for differentiable training. This new approach is used in two different ways to reduce the effect of background texture in a classification task. The paper introduces a new dataset, and shows extensive and carefully designed experiments using the new data as well as existing domain generalization datasets.\n\nThis paper revisits an old idea from image processing in a new way, and provides an interesting unsupervised method for identifying so called superficial features. The proposed block seems to be very modular in design, and can be plugged into other architectures. The main weakness is that it is a bit unclear exactly what is being assumed as ""background texture"" by the authors.\n\n\nOverall comments:\n- Some more clarity on what you mean by superficial statistics would be good. E.g. by drawing samples. Are you assuming the object is centered? Somehow filling the image?  Different patch statistics? How about a texture classification task?\n- please derive why NGLCM reduces to GLCM in the appendix. Also show the effect of dropping the uniqueness constraint.\n- Section 3.2: I assume you are referring to an autoencoder style architecture here. Please rewrite the first paragraph. The current setup seems to indicate that you doing supervised training, since you have labels y, but then you talk about decoder and encoder.\n- Section 3.2: Please expand upon why you use F_L for training but F_P during testing\n\n\nMinor typos/issues:\n- Last bullet in Section 1: DG not yet defined, only defined in Section 2.\n- page 2, Section 2, para 1: data collection conduct. Please reword.\n- page 2, Section 2, para 2: Sentence: For a machine learning ... There is no object in this sentence. Not sure what you are trying to define.\n- page 2, Section 2, para 2: Is $\\mathcal{S}$ and $\\mathcal{T}$ not intersecting?\n- page 2, Section 2.1: Heckman (1977), use \\citep\n- page 2, Section 2.1: Manski, citep and missing year\n- page 3, Section 2.1: Kumagai, use citet\n- page 3, Section 3.1: We first expand ... --> We first flatten A into a row vector\n- page 4, Section 3.1: b is undefined. I assume you mean d?\n- page 4, Section 3.1: twice: contrain --> constraint\n- page 4, Section 3.2: <X,y> --> {X,y} as used in Section 3.1.\n- page 4, Section 3.2, just below equation: as is introduced in the previous section. New sentence about MLP please. And MLP not defined.\n- page 4, Section 3.2, next paragraph: missing left bracket (\n- page 4, Section 3.2: inferred from its context.\n- page 5, Section 4: popular DG method (DANN)\n- page 7: the rest one into --> the remaining one into\n- page 8: rewrite: when the empirical performance interestingly preserves.\n- page 8, last sentence: GD --> DG\n- A2.2: can bare with. --> can deal with.\n- A2.2: linear algebra and Kailath Variant. Unsure what you are trying to say.\n- A2.2: sensitive to noises --> sensitive to noise.\n', 'The paper is clear regarding motivation, related work, and mathematical foundations. The introduced cross-local intrinsic dimensionality- (CLID) seems to be naive but practical for GAN assessment. Notably, the experimental results seem to be convincing and illustrative. \n\nThe domain generalization idea from CNN-based discriminative feature extraction and gray level co-occurrence matrix-based high-frequency coding (superficial information), is an elegant strategy to favor domain generalization. Indeed, the linear projection learned from CNN, and GLCM features could be extended to different real-world applications regarding domain generalization and transferring learning. So, the paper is clear to follow and provides significant insights into a current topic.\n\nPros: \n- Clear mathematical foundations.\n- The approach can be applied to different up-to-date problems.\n-Though the obtained results are fair, the introduced approach would lead to significant breakthroughs regarding domain generalization techniques.\n\nCons:\n-Some experimental results can be difficult to reproduce. Indeed, authors claim that the training heuristic must be enhanced.\n-Table 2 results are not convincing.\n']","[90, 60, 70]","[70, 70, 50]","[""The sentiment score is 90 (highly positive) because the reviewer uses phrases like 'novel', 'excellent in terms of novelty and originality', and 'good performance', indicating strong approval of the paper. They also mention that the paper is 'well structured and written' and that the methods are 'evaluated thoroughly'. There are no negative comments. The politeness score is 70 (polite) because the reviewer uses respectful and professional language throughout, acknowledging the authors' work positively without being overly effusive. The tone is formal and constructive, focusing on the paper's merits without personal comments or harsh criticism."", ""The sentiment score is 60 (positive) because the reviewer starts with a neutral summary but then expresses that the paper 'provides an interesting unsupervised method' and praises its 'extensive and carefully designed experiments'. The main criticism is mild, calling it 'a bit unclear'. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offers constructive feedback, and phrases criticisms gently (e.g., 'Some more clarity... would be good'). The reviewer also provides helpful suggestions and points out minor typos, which is considerate. The tone remains professional and supportive throughout, without any harsh or rude comments."", ""The sentiment score is 70 (positive) because the review starts with praise for the paper's clarity, motivation, and mathematical foundations. It describes the introduced approach as 'practical' and the results as 'convincing and illustrative'. The reviewer also highlights the potential for significant breakthroughs. While there are some cons mentioned, they are relatively minor compared to the positive aspects. The politeness score is 50 (slightly polite) because the language used is professional and respectful. The reviewer provides a balanced view, mentioning both pros and cons, and uses phrases like 'elegant strategy' and 'significant insights'. However, the tone is more neutral than overtly polite, hence the moderate score.""]"
"[""The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. \n\nQuality \nThe proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. \nThe results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).\n\nClarity\nThe paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. \n\nOriginality\nThere is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. \n\nSignificance\n- pros clear and seemingly state-of-the-art results, intuitive approach, \n-cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).\n\nOne comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. "", 'This work brings a novel meta-learning approach that learns unsupervised learning rules for learning representations across different modalities, datasets, input permutation, and neural network architectures. The meta-objectives consist of few shot learning scores from several supervised tasks. The idea of using meta-objectives to learn unsupervised representation learning is a very interesting idea.\n\nAuthors mentioned that the creation of an unsupervised update rule is treated as a transfer learning problem, and this work is focused on learning a learning algorithm as opposed to structures of feature extractors. Can you elaborate on what aspect of learning rules and why they can be transferable among different modalities and datasets? For this type of meta-learning to be successful, can you discuss the requirements on the type of meta-objectives? Besides saving computational cost, does using smaller input dimensions favor your method over reconstruction type of semi-supervised learning, e.g. VAE?\n\nIn the section ""generalizing over datasets and domains"", the accuracy of supervised methods and VAE method are very close. This indicates those datasets may not be ideal to evaluate semi-supervised training.\n\nIn the section ""generalizing over network architectures"", what is the corresponding supervised/VAE learning accuracy?\n\nIn the experimentation section, can you describe in more details how input permutations are conducted? Are they re-sampled for each training session for tasks? If the input permutations are not conducted, will the comparison between this method, supervised and VAE be different?\n\nAfter reviewing the author response, I adjusted the rating up to focus more on novelty and less on polished results.', ""This paper introduces a novel meta-learning approach to unsupervised representation learning where an update rule for a base model (i.e., an MLP) is meta-learned using a supervised meta-objective (i.e., a few-shot linear regression from the learned representation to classification GTs). Unlike previous approaches, it meta-learns an update rule by directly optimizing the utility of the unsupervised representation using the meta-objective. In the phase of unsupervised representation learning, the learned update rule is used for optimizing a base model without using any other base model objective. Experimental evaluations on few-shot classification demonstrate its generalization performance over different base architectures, datasets, and even domains.  \n\n+  Novel and interesting formulation of meta-learning by learning an unsupervised update rule for representation learning. \n+  Technically sound, and well organized overall with details documented in appendixes. \n+  Clearly written overall with helpful schematic illustrations and, in particular, a good survey of related work. \n+ Good generalization performance over different (larger and deeper) base models, activation functions, datasets, and even a different modality (text classification).\n\n-  Motivations are not very clear in some parts. E.g., the reason for learning backward weights (V), and the choice of meta-objective.  \n- Experimental evaluation is limited to few-shot classification, which is very close to the meta-learning objective used in this paper. \n- The result of text classification is interesting, but not so informative given no further analysis. E.g., why domain mismatch does not occur in this case?\n\nI enjoyed reading this paper, and happy to recommend it as a clear accept paper. The idea of meta-learning update networks looks a promising direction worth exploring, indeed. \nI hope the authors to clarify the things I mentioned above. Experimental results are enough considering the space limit, but not great. Since the current evaluation task is quite similar to the meta-objective, evaluations on more diverse tasks would strengthen this paper. \n\nFinally, this paper aims at unsupervised representation learning, but it’s not clear from the current title, which is somewhat misleading. I think that's quite an important feature of this paper, so I highly recommend the authors to consider a more informative title, e.g., `Learning Rules for Unsupervised Representation Learning’ or else. ""]","[60, 60, 70]","[50, 70, 80]","[""The sentiment score is 60 (positive) because the reviewer generally expresses a favorable view of the paper. They describe the algorithm as 'well defined' and 'competitive', noting that it 'slightly outperforms' other approaches. The reviewer also praises the paper as 'well written and clearly structured'. However, it's not extremely positive as they mention some cons and areas for improvement, such as the results being only 'very modestly better than other methods'. The politeness score is 50 (somewhat polite) because the reviewer maintains a professional and respectful tone throughout. They offer balanced feedback, highlighting both strengths and weaknesses without using harsh language. The reviewer also provides constructive suggestions, such as including the local gradient in the update rule. While not overly formal or excessively polite, the language is appropriate and considerate for a peer review."", ""The sentiment score is 60 (moderately positive) because the reviewer describes the work as 'novel' and the idea as 'very interesting', indicating appreciation for the research. However, they also raise several questions and points for improvement, suggesting room for enhancement. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, framing their comments as questions and suggestions rather than criticisms. They acknowledge the authors' response positively at the end. The reviewer maintains a professional and constructive tone, avoiding harsh language or direct negativity, which contributes to the polite impression."", ""The sentiment score is 70 (positive) because the reviewer expresses overall positive sentiment, recommending the paper as a 'clear accept' and describing it as 'novel and interesting'. They mention enjoying reading the paper and see the approach as promising. However, they also point out some limitations, which prevents the score from being higher. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames suggestions positively (e.g., 'I hope the authors to clarify...'). The reviewer balances praise with areas for improvement and uses phrases like 'I enjoyed reading this paper', which contributes to a polite tone. The score is not 100 as the criticism, while constructive, is still direct.""]"
"['Language is hierarchically structured: smaller units (e.g., noun phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the different units of an LSTM can learn to track information at different time scales, the standard architecture does not impose this sort of strict hierarchy. This paper proposes to add this constraint to the system by ordering the units; a vector of ""master"" input and forget gates ensures that when a given unit is reset all of the units that follow it in the ordering are also reset.\n\nStrengths:\n* The paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly.\n* The evaluation tasks are very sensible. It\'s good that the model is shown to obtain good perplexity and slightly improve over an LSTM baseline; it\'s not the state of the art, but that\'s not the point of the paper (in fact, I would emphasize that even more than the authors do). The unsupervised parse evaluation (Table 2) is the heart of the paper, in my opinion (and should probably be emphasized more) -- the results from the second layer are quite impressive.\n* The (mildly) better performance than LSTMs on long-distance dependencies, and (mildly) worse performance on local dependencies, in the Marvin & Linzen dataset, is interesting (and merits additional analysis).\n\nWeaknesses:\n* The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused. I am not sure that neural networks with latent syntactic structures can really address the seemingly very fundamental question mentioned in the first paragraph (whether syntax is related to ""an underlying mechanism of human cognition"") - I would suggest eliminating this part. At the same time, the authors might want to add another motivation for studying architectures that discover latent structure (as opposed to being given that structure) - this setting corresponds more closely to human language acquisition, where children aren\'t given annotated parse trees.\n* The authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the LSTM is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences.\n* There is limited analysis of the model. Why does the second layer show better unsupervised parsing performance than the third layer? (Could this be related to syntactic vs. semantic/discourse units I mention in the previous bullet?) Why is the model better at ADJP boundaries than NP boundaries? It would have been more useful to report less experiments but analyze the results of each experiment in greater depth.\n* In this vein, I am not sure it\'s useful to include WSJ10 in Table 2, which is busy as it is. These sentences are clearly too easy, as the right branching baseline shows, and require additional POS tagging.\n* I found it difficult to read Figure A.2: could you help us understand what we should take away from it? \n* It is not entirely clear why the model needs both unit-specific forget/input gates and the ""master"" forget/input gates, and there is no discussion of this issue. Have you tried using only the ""master"" gates?\n\nMinor notes:\n* RNNGs are described as having an explicit bias to model syntactic structure; this is an arguably confusing use of the word ""bias"", in that the architecture has a hard constraint enforcing syntactic structures (bias implies a soft constraint).\n* There are some language issues: agreement errors (e.g. ""have"" in the sentence that starts with ""Developing"" in the introduction), typos (""A order should exist"", ""co-occurance""), determiner issues (""values in [the] master forget gate"", ""when the overlap exists"") - I would suggest going through and copy editing the paper.\n* ""cummax"" seems like a better choice of name for cumulative maximum than ""cumax"".\n* It may be helpful to remind the reader of the update equation for c_t in a standard LSTM.\n* Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?\n* It\'s not clear if the results in Table 2 reflect the best seed out of five (as the title of the column ""max"" indicates) or the average (as the caption says).\n', 'The paper proposes a new RNN unit: ON-LSTM. The idea is to explicitly integrates the latent tree structure into recurrent models. Experiments are conducted to evaluate performances on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. Good results on unsupervised parsing show that the model learns something close to human judgments of the sentence parses.\n\nThe paper is clearly written, and the experiments seem planned well.\nThe language modeling results are not state-of-the-art, but the unsupervised parsing results of layer 2 are quite impressive. The analyses are reasonable.\n\nOverall, the paper seems worthy of being accepted.', 'Quality\n - Pro:\n   o This paper was in general a quality effort. It had a thorough bibliography of both older and recent relevant research contributions\n   o Providing useful, well done experimental results on four tasks was also a sign of this good thoroughness\n - Con: none observed\n\nClarity\n - Pro:\n   o The paper was generally well-written and clear. Results were clearly presented.\n - Con:\n   o Notwithstanding the half page of explanation of the intuition behind the new ON-LSTM update rules (top of p.5), it wasn\'t really enough for my old brain to get a good sense of what was going on – though I\'m sure younger, smarter people will have made more sense of it. :) It would really help to try to provide more intuition and understanding here. Things that would probably really help include a worked example and diagrams.\n   o There were minor English/copyediting problems, but nothing that interfered with understanding. E.g., ""monotonously"" on p.4 should be ""monotonically"" (twice).\n\nOriginality\n - Pro\n   o This was REALLY NEAT! This paper had a real, clear, different idea that appeared interesting and promising. That puts it into the top half of accepted papers right there.\n   o The basic idea of the different update time scales, done flexibly, controlled by the master forget/input gates seemed original, flexible, and good.\n - Con: Nothing really observed; there are clearly a bunch of slightly related ideas, well referenced in this paper.\n\nSignificance\n - Pro\n   o If this idea pans out well, it would be a really interesting new structural prior to add to the somewhat impoverished vocabulary of successful techniques for building deep learning systems.\n   o Has an original, promising approach. That has the opportunity for impact and significance.\n - Con:\n   o The results so far are interesting, and in places promising, but not so clearly good that this idea doesn\'t need further evaluation of its usefulness.\n   o All the results presented are on small datasets (Penn Treebank WSJ (1 million words) size or smaller). What are the prospects on bigger datasets?  It looks like in principle this shouldn\'t be a big obstacle – except for not having a highly tuned CuDNN implementation, it looks like this should basically be fairly efficient like an LSTM and not hard to scale like, e.g., an RNNG.\n\nOther comments:\n - Some of the wording on page 1 seemed strange to me. Natural language has a linear overt form as spoken and (hence) written. It\'s really not that the sequential form is just how people conventionally ""present"" it. That is, it\'s not akin to a chemical compound which is really 3 dimensional but commonly ""presented"" by chemists in a convenient sequential notation.\n - p.2 2nd paragraph: Don\'t RNNs ""explicitly impose a chain structure"" not ""implicitly""?!?\n - I wasn\'t sure I was sold on the name ""Ordered Neurons"". I\'m not sure I have the perfect answer here, but it feels more like ""multi-timescale units"" is what is going on.\n - The LM results look good.\n - Because of all the different datasets, etc. it was a little hard to call the grammar induction results, but they at least look competently strong.\n - The stronger results on long dependencies in targeted syntactic evaluation look promising, but maybe you need a bigger hidden size so you can also do as well on short dependencies?\n - The logical inference results were promising – they seem to suggest that you capture some but not all of the value of explicit tree structure (a TreeLSTM) on a task like this.\n - The tree structures in Appendix A look promisingly good.\n']","[50, 80, 80]","[80, 60, 90]","[""The sentiment score is 50 (mildly positive) because the reviewer begins by highlighting the strengths of the paper, praising its 'elegant' approach and 'sensible' evaluation tasks. They note that the results are 'quite impressive' in some areas. However, they also point out several weaknesses, maintaining a balanced view. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions ('I would suggest...') and asking questions rather than making blunt statements. They also use positive language when possible ('it's good that...', 'interesting'). The reviewer maintains a professional and constructive tone, even when pointing out areas for improvement."", ""The sentiment score is 80 because the review is generally positive. The reviewer states that the paper is 'clearly written', the experiments are 'planned well', and the unsupervised parsing results are 'quite impressive'. The reviewer concludes that the paper 'seems worthy of being accepted', which is a strong positive sentiment. However, it's not a perfect 100 as there are some neutral comments, such as noting that the language modeling results are 'not state-of-the-art'. The politeness score is 60 because the language used is professional and respectful, without any harsh criticisms. The reviewer offers balanced feedback, acknowledging both strengths and limitations of the work. The tone is constructive and supportive, which contributes to its politeness. However, it doesn't go out of its way to be overly polite or complimentary, maintaining a neutral professional tone, which is why it's not scored higher."", ""The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, using phrases like 'quality effort', 'REALLY NEAT!', and 'original, promising approach'. They highlight multiple pros and few cons, indicating a very positive overall sentiment. The score isn't 100 as there are some minor criticisms and suggestions for improvement. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone. The reviewer uses polite language throughout, even when offering criticisms. For example, they humorously refer to their 'old brain' when struggling to understand a concept, rather than blaming the authors. The reviewer also balances critiques with praise and offers specific, helpful suggestions for improvement, demonstrating respect for the authors' work.""]"
"['Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. \n\nIn this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.\n\nIf the authors address my major concerns, I would increasing my rating 1-2 points.\n\nMajor Comments:\n\nThe results of this paper are quite compelling and address some underlying challenges in the literature on how CNN\'s function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.\n\n1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are \'conclusive\' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:\n\n  --> ""These experiments provide conclusive behavioural evidence in favour of the texture hypothesis""\n  --> ""we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.""\n\nI would prefer to see language such as ""We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs."" or ""We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements"". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.\n\n2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?\n\nAlthough the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?\n\n3. Semantics of \'object shape\'. I suspect that others in the field of computer vision may take issue with your definition of \'object shape\'. Please provide a crisp definition of what you test for as \'object shape\' in each of your experiments (i.e. ""the convex outline of object segmentation"", etc.).\n\nMinor Comments:\n\n- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.\n\n- Figure 2. Are people forced to select a choice or could they select \'I don\'t know\'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.\n\n- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.\n\n- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?\n\n- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?\n\n- Please use names of Shape-ResNet, etc. in Table 2.\n\n- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.\n\n- A.2. ""not not used in the experiment"" --> ""not used in the experiment""\n', 'This paper talks about the behavior bias between human and advanced CNN classifier when classifying objects. A clear conclusion is that DNN classifiers lean on texture cues more than human, which is in contrast to empirical evidence. The experimental results are delighting and convincing to some extent. This paper is also inspiring and potentially useful to interpret how CNN works in object classification task. \n\nNevertheless, I have several small issues: \n-\tI like the writing of this paper, fluent description and clear topic. Besides, it provides sufficient information about experiment details, thus I think the experiments are fully reproducible. But I want to remind the authors to downplay their claims. Some sentences are not with academic rigor. i.e. “Textures, not object shapes, are the most important cues for CNN object recognition.”I don’t think it a good idea to claim textures as the “most important”cue.\n-\tAlthough adequate experiments are conducted on ResNet-50 on ImageNet, I miss experiments on a different object classification dataset i.e. PASCAL VOC, and a different network backbone such as very deep ResNet-152 or wider DenseNet. This lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results. The adopted network ResNet-50, AlexNet, VGG-16 and GoogLeNet are not deep enough or either wide as DenseNet. Although transfer learning experiment is carried out upon PASCAL VOC, it’s not straightforward and not so truly telling. We’re curious about universal conclusions rather than that based on one dataset or network architecture of the same category. As a matter of fact, I’m nearly convinced by the provided results. But I think the demanding experiments will make the conclusions more solid. \n\nBesides, I think the constructed dataset is beneficial to further research or fair comparison of future works, and I wonder the authors’ intention to publish such a dataset in the future. \n\nI would raise my scores if the aforementioned problems are convincingly checked and solved.', 'The paper is well written and easy to follow. It was a nice read for me.\n\nThe paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style.\n\nI believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains.\n\nThe paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes ""novel Stylized-ImageNet dataset"" and shows that models can learn shape/texture features both but there is not much detail/explanation on why ""Stylized"" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it ""novel"". Additionally, I would like answers to the following questions:\n\n1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results?\n2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold?\n3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results?\n4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn\'t show if these biases are removed by training on SIN.\n5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how\'s the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset?\n6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?']","[50, 50, 50]","[80, 75, 75]","[""The sentiment score is 50 (moderately positive) because the reviewer expresses enthusiasm for the results, calling them 'compelling' and 'very cool', while also providing constructive criticism and suggestions for improvement. The reviewer states they would increase their rating if concerns are addressed, indicating a generally positive but not overwhelmingly enthusiastic sentiment. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers praise where due, and frames criticisms constructively. They use phrases like 'I would prefer to see' and 'Please provide' when making suggestions, maintaining a collegial tone. The reviewer also acknowledges the authors' work positively, demonstrating professional courtesy."", ""The sentiment score is 50 (slightly positive) because the reviewer begins with positive comments about the paper's clear conclusion, delighting and convincing results, and inspiring potential. However, they also mention 'several small issues' and suggest improvements, indicating a balanced view. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I like the writing', 'I want to remind the authors', and 'I would raise my scores if...', which maintain a courteous tone while providing feedback."", ""The sentiment score is 50 (moderately positive) because the reviewer begins by praising the paper as 'well written and easy to follow' and calls it a 'good empirical study'. However, they also point out that the paper has 'no novel contributions' and raise several questions and concerns, balancing out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as questions or suggestions rather than direct criticisms. Phrases like 'I believe' and 'Can authors show' maintain a polite tone. The reviewer also acknowledges the validity of the authors' choices even while questioning them, which is a polite approach to criticism.""]"
"['This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes. \nThe key components of the approach are :\n- increasing the batch size by a factor 8\n- augmenting the width of the networks by 50% \nThese first two elements result in an Inception score (IS) boost from 52 to 93.  \n- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.\nThe core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.\nVariations of this threshold lead to variations in FD and IS, as shown in insightful experiments. The comments that more data helps (internal dataset experiments) is also informative. \nVery nice to have included negative results and detailed parameter sweeps.\n\nThis is a very nice work with impressive results, a great progress achievement in the field of image generation. \nVery well written.\n\nSuggestions/questions: \n- it would be nice to also propose unconditioned experiments. \nIt would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve. \n- I understand that no data augmentation was used during training?    \n- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?\n- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.\n- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.\n- It would be nice to display more Nearest neighbors for the dog image.\n- It would be nice to add a figure of random generations.\n- make the bib uniform: remove unnecessary doi - url - cvpr page numbers\n', 'Summary:\nThe authors present a empirical investigation of methods for scaling GANs to complex datasets, such as ImageNet, for class-conditioned image generation. They first build and describe a strong baseline based on recently proposed techniques for GANs and push the performance on large datasets with several modifications presented sequentially, to obtain strong state-of-the-art IS/FID scores, as well as impressive visual results. The authors propose a simple truncation trick to control the fidelity/variance which is interesting on its own but cannot always scale with the architecture. The authors further propose a orthogonalization-based regularization to mitigate this problem. An investigation of training collapse at large scale is also performed; the authors investigate some regularization schemes based on gathered empirical evidence. As a result, they explore and discard Spectral Normalization of the generator as a way to prevent collapse and show that a severe tradeoff between stability and quality can be controlled when using zero-centered gradient penalties in the Discriminator. In the end, no solution that can ensure quality and stability is found, except having prohibitively large amounts of data (~300M images). Models are evaluated on the ImageNet and on this internal, bigger dataset.\n\nPros:\n- This investigation gives a significant amount of insights on GAN stability and performance at large scales, which should be useful for anyone working with GANs on complex datasets (and that have access to great computational resources).\n\n- Even though commonly used evaluations metrics for GANs are still not fully adequate, the authors obtain quantitative performance significantly beyond previous work, which seems indeed correlated with remarkable visual results.\n\n- The baseline and added modifications are well presented and clearly explained. The Appendices also have great value in that regard.\n\n\nCons:\n- Discussions sometimes lack depth or are absent.\nFor example, it is unclear to me why some larger models are not amenable to truncation. Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts? Were samples from those networks better without using truncation? Why would this be?\n\nAuthors report how wider networks perform best, and how deeper networks degrade performance. Again, discussions are lacking, and it doesn’t seem the authors tried to understand why such behaviors were shown.\n\nEven though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.\n\n- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear. Is this something the reader should understand from Table 1? \n\n- I question the choice of sections chosen to be in the main paper/appendices. I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value. However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems. In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance. However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information. With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.\n\n\nSuggestions/Comments:\n\n- Regarding the diversity/fidelity tradeoff using different truncation thresholds, I think constraining the norm of the sampled noise vectors to the exact threshold value (by projecting the samples on the 0-centered hyper-sphere of radius = threshold) could yield even more interesting or more informative Figures, as obtained scores or samples on the edge of that hyper-sphere might provide information on the ‘guaranteed’ (not proven) quality/fidelity of samples mapped from inside that hyper-sphere. \n\n- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values. Similar curves could also be produced with the hyper-sphere projection proposed above to have a slightly clearer idea of the behavior on the limit of that hyper-sphere.\n\n- In Section 4.2, in the second paragraph, you refer to Appendix F and describe “sharp upward jump at collapse” in D’s loss. However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.\n\n- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.\n\n\nThis investigation of GAN scalability is successful results-wise even though the inability to stabilize training without sacrificing great performance on ImageNet is disappointing. The improvement over previous SOTA is definitely significant. This work thus shows a modern GAN architecture for complex datasets that could be a strong basis for future work. However, I think the paper could and should be improved with some more detailed analysis and discussions of exhibited behaviors in order to further guide and encourage future work. It could also be clarified on some aspects, and potentially re-structured a bit to be better align with its probable impact directions.  I would also be curious to see the proposed techniques applied on simpler datasets. Can this be useful for someone having less compute power and working on something similar to CelebA? \n', 'Summary:\nThis paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images. The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:\n-\tIncreasing batch size (8x) and model size (2x)\n-\tSplitting noise z in multiple chunks, and injecting it in multiple layers of the generator\n-\tSampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled. This seems to be used only at test-time and is used to control variety-fidelity tradeoff. The generator is encouraged to be smooth using an orthogonal regularization term.\nIn addition, the paper proposes practical recipes for characterizing collapse in GANs. In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse. In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs. Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.\n\nStrengths:\n-\tProposed techniques are intuitive and very well motivated\n-\tOne of the big pluses of this work is that authors try to ""quantify"" each proposed technique with training speed and/or performance improvement. This is really a good practice.\n-\tDetailed analysis for detecting collapse and improving stability in large-scale GAN\n-\tProbably no need to mention that, but results are quite impressive\n\nWeaknesses:\n-\tComputational budget required is massive. The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.\n\nComments/Questions:\n-\tCan you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance? \n-\tIt is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs. Providing such analysis would be also helpful for the community.\n-\tHow do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?\n\nOverall recommendation:\nThe paper is well written, ideas are well motivated/justified and results are very compelling. This is a good paper and I higly recommend acceptance.\n']","[90, 60, 90]","[80, 80, 80]","[""The sentiment score is 90 (highly positive) because the reviewer describes the paper as 'a very nice work with impressive results' and 'a great progress achievement in the field of image generation'. They also praise it as 'Very well written' and mention 'impressive images generations'. The politeness score is 80 (quite polite) due to the use of positive and respectful language throughout, such as 'Very nice to have included...', 'it would be nice to...', and phrasing suggestions as questions or polite requests. The reviewer provides constructive feedback and suggestions without any harsh criticism, maintaining a professional and courteous tone throughout the review."", ""The sentiment score is 60 (positive) because the reviewer acknowledges significant improvements over previous work, impressive visual results, and valuable insights provided by the paper. They use phrases like 'significant amount of insights,' 'remarkable visual results,' and 'strong basis for future work.' However, the score is not higher due to some criticisms about lack of depth in discussions and suggestions for improvements. The politeness score is 80 (very polite) because the reviewer maintains a respectful and constructive tone throughout. They balance praise with criticism, use phrases like 'I think,' 'I would be likely to,' and offer suggestions for improvement rather than harsh criticisms. The reviewer also acknowledges the value of reporting negative results, showing respect for the authors' work even when discussing potential shortcomings."", ""The sentiment score is 90 because the review is overwhelmingly positive. The reviewer highlights multiple strengths, describes the results as 'impressive,' and 'highly recommends acceptance.' The only weakness mentioned is the computational budget required, which doesn't significantly detract from the overall positive sentiment. The politeness score is 80 because the reviewer uses respectful and professional language throughout. They offer constructive feedback and questions without being critical or harsh. The use of phrases like 'Can you elaborate more' and 'Providing such analysis would be also helpful' demonstrates a polite and collaborative tone. The overall recommendation is phrased very positively and courteously.""]"
"['This paper introduces a novel feature selection method by utilizing GAN to learn the distributions. The novelty of this paper is to incorporate two recent works, i.e. knockoff for feature selection and W-GAN for generative models. Compared to the latest knockoff work which requires a known multivariate Gaussian distribution for the feature distribution, the proposed work is able to generate knockoffs for any distribution and without any prior knowledge of it.\n\nPros: This paper is very well written. I enjoyed reading this paper. It is novel and addresses an important problem. The numerical study clearly shows the advantage of the proposed work. \n\nCons:\n\nQ1: In the discriminator, instead of training with respect to the full loss, the authors consider to mask some information by using a multivariate Bernoulli random variable $B$ with success probability 0.9. Then the discriminator needs to predict only when $B_i = 0$. Can the authors provide some justification of such choice of the parameters? This choice is a little bit mysterious to me.\n\nQ2: How sensitive are the hyper-parameters $\\eta$ (set to 10 in the experiments), $\\lambda$, and $\\mu$ (set to 1 in the experiments)?\n\nQ3: In the real data example, the feature selection performance is less justified as there is no truth. One suggestion is to evaluate the prediction errors using the selected features and compare with the benchmarks.\n\n\n\n', 'This manuscript describes an extension of the knockoff framework, which is designed to carry out feature selection while controlling the FDR among selected features, to settings in which the generative distribution of the features is not Gaussian. Specifically, the authors employ a GAN (with several modifications and additions) in which the generator produces knockoffs and the discriminator attempts to identify which features have been swapped between the original and the knockoff.\n\nThe method works as follows: (1) A conditional generator takes random noise and the real features as input, and outputs knockoff features. (2) A modified discriminator is used in such a way that the generator learns to generate knockoffs satisfying the necessary swap condition, so as to control the FDR of the knockoff procedure. (3) A power network uses Mutual Information Neural Estimation (MINE) to estimate the mutual information between each feature and its knockoff counterpart, so as to maximize the power of the knockoff procedure.\n\nResults are provided on synthetic and real data. As for the synthetic data, when the underlying feature distribution is Gaussian, the proposed method, KnockoffGAN, performs almost as well as the original knockoff and outperforms the BHq method; when the underlying feature distribution is non-Gaussian, KnockoffGAN dominates both the original knockoff and BHq methods. As for the real data, the authors claim to identify nine relevant features for cardiovascular disease and eight relevant features for diabetes, whereas the original knockoff procedure identifies zero features from the same data.\n\nGeneral comments: \n\nThis is an extremely impressive piece of work. The manuscript itself is a pleasure to read, and the results clearly demonstrate that the proposed KnockoffGAN both controls FDR and achieves power comparable to the original knockoff procedure in the Gaussian setting and much better than the original knockoff when the underlying distribution is not Gaussian.\n\nStrengths: \n\nThe combination of GANs and knockoff filter is a very promising and intriguing idea.\n\nThe use of the modified discriminator to ensure that the generated knockoffs satisfy the necessary swap condition is novel and intuitively sound.\n\nThe use of MINE to maximize power by maximizing the mutual information between each feature and its knockoff counterpart is also interesting.\n\nThe paper is well written, reads smoothly and the ideas are well exposed.\n\nThe illustrative figure is straightforward.\n\nWeaknesses:\n\nIntuitively, the modified discriminator and the power network should conflict with each other. I expect it was tricky to achieve a good tradeoff between two, but the authors failed to elaborate on these details.\n\nThe authors do not provide the design details of the neural networks. How dependent on the specific parametrization of the network architecture is the performance? How does the training order of four networks matter to the performance? \n\nThe manuscript should cite [[ Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. ""Knockoffs   for the mass: new feature importance statistics with false discovery   guarantees."" arXiv:1807.06214, 2018. ]] which proposes a way to generate knockoffs for a Gaussian mixture model, and this method should be included in the relevant supplementary figure.\n\nIn Section 5.1.4, I would like to know, for a fixed data set, how the regularization affects the final values of the other loss terms.\n\nThe analysis of real data in Section 5.2 is unsatisfying in several respects.\n\nFirst, there is an unfortunate oversight in Table 1: the text refers to three features that are ""trivial,"" but only one of these is marked with an asterisk.  This leaves open the question of whether there are other trivial features beyond the three mentioned in the text. In addition, it is not clear exactly what it means for a feature to be ""trivial"" in this context.\n\nThis point gets to a deeper problem with the evaluation, which is that we are told, with no evidence, that these features are supported by literature in PubMed.  I would like to see two things here.  First, it seems obvious to me that if you are going to say that there is support in PubMed, you are obliged to actually report the citations that supposedly give this support. This could be done in the appendix. Equally importantly, there is a potential here for ascertainment bias which should be combatted in some fashion.  Presumably, some human expert had to do the PubMed searches to make this assessment.  I would like to know how ""permissive"" this assessor is.  To assess this, one could give the assessor a collection of terms, some of which were selected by KnockoffGAN and some at random, and then report the results. Obviously, some features that are significant may not be in the list of selected terms (because KnockoffGAN does not achieve 100% power) and so may appear as false negatives. But without some assessment like this, I have trouble believing this assessment.\n\nA related point is that it seems quite unfortunate that the authors chose a data set that cannot be described at all due to the anonymity constraint.  At the very least, it seems that we should be told the dimensionality of the data set. The Knockoff literature contains real data sets that could have been used here.\n\nMinor comments:\n\nOn the first page, the sentence beginning ""On the other hand,"" should clarify that this is only in expectation.\n\np. 3: Missing right paren after [7].\n\np. 5: Write out “Gaussian process.”\n\np. 5: ""as little"" -> ""as little as possible""\n\np. 6: ""to show that in"" -> ""to show, in""\n\nIn Figures 2-5, add a horizontal line at 10% FDR for reference.\n\np. 10: ""features ones"" -> ""features""\n\nNote to program committee:\n\nI did not review the technical details of the proof in the appendix.\n\n\n', 'The paper presents a deep-learning-based version of the knockoff method by Candes et al. for FDR control in feature selection problems to avoid assumptions posed on the distribution of features by the original method. In a supervised feature selection setting, the goal of the knockoff framework is to select a set of input features that are statistically associated to an output variable Y, while controling the FDR. The basic idea behind knockoff is to generate artificial input feature vectors, (i.e. knockoffs) that are independent of Y, when conditioned on the real feature vector X, but after swapping arbitrary elements with X, are distributed as X. Sets of associated features and FDR estimates are obtained by contrasting suited feature selction criteria that measure associations of knockoffs and real features with the target Y.\nLasso coefficients and random forests are used in the paper.\nThe main contribution of the current paper is the use of a GAN to generate knockoffs, and, according to the paper in particular, the use of a discriminator that tries to identify the positions of knockoff features that have been swapped into real feature vectors X to control equlaity in distribution between knockoffs and feature vectors. Additionally, a Wasserstein discriminator and a MINE loss are used to control the knockoff distribution. Otherwise, the paper follows the standard knockoff procedure.\n\nThe approach is evaluated in simulations, varying two degrees of freedom: i) Gaussian distribute features vs. features that follow mixtures of Gaussians. ii) Gaussian and logit distributions of Y conditioned on linear functions of a subset of X features.\nUsing a Lasso-based feature selection criterion, the GAN knockoff method achieves the highest TP rates among a number of methods that empirically are shown to roughly control a target FDR of 10%. However, the figures are too small to judge FDR control more fine-graoned than 10% +-5%. Here, I would have wished i) a higher resolution to demonstrate FDR control, as well as an evaluation of different FDR cutoffs, especially including smaller cutoffs.\n\nAdditionally, an appliction to real data is performed. However, this evaluation is not very informative for several reasons. i) The dataset is not specified, making the experiment intransparent and non-reproducible. ii) A different feature selection criterion based on random forests is used, compared to the Lasso-based criterion in the synthetic experiments. iii) A different FDR cutoff of 5% has been used compared to the simulations. It is not clear, if the method shows FDR control in synthetic settings at 5%. For these reasons, the real-world experiment is hardly comparable to the synthetic settings.\n\n\nThe paper is relatively well-written and clear. Discussion of related work is appropriate.\n\nIn sum, the paper has some limitations in the empirical evaluation, but nonetheless the use of a GAN promises significant gains in statistical power.\n']","[80, 80, 50]","[90, 90, 70]","[""The sentiment score is 80 (positive) because the reviewer expresses clear appreciation for the paper, describing it as 'very well written', 'novel', and addressing 'an important problem'. They also mention enjoying reading it and highlight its advantages. The few questions raised are presented as constructive feedback rather than criticisms. The politeness score is 90 (very polite) due to the reviewer's use of respectful and encouraging language throughout. They begin with positive comments, phrase their questions politely (e.g., 'Can the authors provide...'), and offer suggestions rather than demands. The tone is consistently professional and courteous, showing respect for the authors' work while providing thoughtful feedback."", ""The sentiment score is 80 (positive) because the reviewer describes the work as 'extremely impressive' and 'a pleasure to read'. They highlight several strengths and express that the results clearly demonstrate the effectiveness of the proposed method. The few weaknesses mentioned are presented as areas for improvement rather than major flaws. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone. The reviewer uses phrases like 'I would like to know' and 'I expect' when suggesting improvements, and balances criticism with praise. They also acknowledge the quality of the work multiple times. The review maintains a professional and courteous tone throughout, even when pointing out areas that need clarification or improvement."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and potential benefits, such as 'significant gains in statistical power,' while also pointing out some limitations in the empirical evaluation. The overall tone is constructive and balanced. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, offering constructive criticism without harsh words. Phrases like 'I would have wished' and 'The paper is relatively well-written and clear' demonstrate a polite approach to feedback. The reviewer also acknowledges the paper's strengths alongside its weaknesses, maintaining a courteous tone.""]"
"['This paper deals with Memory Augmnted Neural Networks (MANN) and introduces an algorithm which allows full writes to the dense memory to be only exectued every L timesteps. The controller produces a hidden output at most timestps, whih is appended to a cache. Every L steps, soft attention is used to combine this cache of N hidden states to a single one, and then this is used as the input hidden state for the controller, with the outputs performing a write in the full memory M, along with clearing the cache.\n\nThe authors first derive ""Uniform Writing"" (UW) which updates the memory at regular intervals instead of every timestep. The derivation is based on the ""contribution"" which is norm of the gradient of some input timestep to some hidden state (potentially at a different timestep). I am not clear on whether this terminology for the quantity is novel, if this is the case maybe the authors should state this more clearly. UW says that if all timesteps are equally important, and only D writes can be made in a sequence of length T, then writes should be done every T/(D+1) steps. I have not checked the proof in detail but this seems reasonable that it would maximise the contribution quantity introduced. I am less clear on whether this is obviously the right thing to do - sometimes this value is referred to in relation to information, but that term does not strictly seem to be being used in the information theory sense (no mention of bits or nats anywhere). Regardless, as the authors point out, in real problems there are obviously timesteps which have less or no useful information, and clearly UW is mostly defined in order to build towards CUW.\n\nCUW expands on UW by adding the cache of different hidden states, and using soft attention over them. This feels like a reasonable step, although I would presume there are times when the L hidden states were collected over timesteps with no information, and so the resulting write is not that useful, and times when all of hte L timesteps contain different useful information. In these circumstances it seems like the problem of getting the *useful* information into the memory is still present, as the single write done with the averaged hidden state will need to contain lots of information, which may be more ideal written with several timesteps.\n\nThe experiments are well described and overall the paper seems reproducable. The standard toy datasets of copy / reverse / sinusoid are used. The results are interesting - regular DNC with memory size 50 performs surprisingly badly on clean Sinusoid, my guess would be that with hyperparameter tuning this could be improved upon. I\'m not sure that using exactly the same hyperparameters for a wide variety of models is appropriate - even with optimizers like Adam and RMSProp, I would want to see at least some sweeping for the best hyperparams, and then graphs like figure 3 should show error bars averaged across multiple runs with the best per-model hyperparameters. However, The DNC with CUW seems to perform well across all synthetic tasks.\n\nThere is no mention of Adaptive Computation Time/ACT (Graves, https://arxiv.org/abs/1603.08983) throughout the paper, which is surprising considering Alex Graves\' models form two of the baselines used throughout the paper. ACT aims to execute an RNN a variable number of times, usually to do >1 timestep of processing for a single timestep of input. In the context of this paper, I believe it could be adapted to do either zero or one steps of computation per timestep, and that would yield a very comparable network where the LSTM controller always executes, and writes to the memory only happen sometimes. Given that it allows a learned process to decide whether to write, as opposed to having a fixed L which separates full writes, this should have the potential to outperform CUW, as it could learn that at certain times, writes must happen at every step. In my view ACT is attempting to solve essentially the same problem as this paper, so it should either be included as a baseline, or the manuscript should be updated to explain why this is not an appropriate comparison.\n\n\nI think this is an interesting paper, trying to make progress on an important problem. The results look good, but I can only give a borderline score due to missing ACT numbers, and a few other unclear points. The addition of ACT experiments, and error bars on certain results, would change my mind here.\n\n\nNotes:\n\n""No solution has been proposed to help MANNs handle ultra long sequence"" - (Rae et al 2016) is an attempt to do this, by improving the complexity of reads / writes. This allows bigger memory and longer sequences to be processed.\n\n""Current MANNS only support dense writing"" - presumably this means dense as in \'every timestep\', but this terminology is overloaded - you could consider NTM / DNC as doing dense writing, and then work of Rae et al 2016 doing sparse writing.\n\nIn my experience training these kind of RNNs can have reasonably high variance across seeds - figures 2 & 3 should have error bars, and especially Table 4 as that contains the most important results. Getting 99 percent accuracy when previous SOTA is only 0.1% lower is only really meaningful if the standard deviation across seeds is very small.\n\nAppendix A: the \'by induction\' result - I believe there is an error, it should be:\n\nh_t = \\sigma_{i=1}^t U_{t-i}W x_i + C\n\nAs W is applied to inputs, before the repeated applications of U? I believe the rest of the derivation still holds the same, after the correction.\n\n', 'This paper investigates the average contribution of a sequence input to the contents of memory and derives a simple scheme to maximize the information content in memory, which is essentially to write at uniformly spaced intervals. Furthermore they present an attention-based version, where the network caches all hidden states in an interval and selects the hidden state to store via attention. \n\nThe paper is very well written and has a nice balance of relevant theoretic motivation and experiments. Furthermore the question that the authors are tackling --- how should we compress information into external memories --- feels important and under-explored. The fact that the resulting scheme is simple is nice, because it\'s easy for people to try, and it now has some motivation beyond a heuristic decision.\n\nI think this paper will have impact in opening up more comprehensive research into the reduction of redundancy in the external memories of neural networks, and also could be instantly impactful for people using DNCs and NTMs --- especially since we see the incorporation of UW / CUW can help bridge the gap (or even surpass) LSTMs for the modeling of natural data. As such I think it is a clear accept. \n\n---\n\nComments to the authors:\n\nThe results in Figure 2 (c) I think are misleading. The NTM with an RNN controller can solve this task, the limit of 10,000 steps implies that the model may converge to some 50% value with 14 slots but I am absolutely certain that the NTM + RNN controller would converge in 10,000 steps with a careful tuning of gradient clipping and learning rate. I think this is basically a false result. Furthermore I would like to really know what the best final performance of the models are on this task once converged, it\'s not clear if 10,000 steps was enough.\n\nFor equation (9), was it necessary to construct the attention weights in this way? How much better was it to a direct softmax query: softmax(h_{t-1}^T d_j)? If you are backpropagating through the attention then the network can shape the hidden states to facilitate the relevant attention, as well as contain the information.\n\nIn the second paragraph of S2.2.2 you have ""a_{t, j} is the attention score"" but you should have ""\\alpha_{t, j} is the attention score"".\n\nTable 3: just include the Transformer results in the table!? The reasoning to exclude it is not really coherent.\n\nIt would have been nice (and would raise my score) to see the UW scheme operating with a large(ish) number of memory slots.\n\n\n\n\n\n\n', 'This paper looks at ways to improve memory-writing in memory augmented neural networks. Authors proposed two methods to compare against ""regular writing"" method as well as compare against each other, namely ""uniform writing"" and ""cached uniform writing"". Latter one attempts to utilize a small size memory efficiently by introducing memory overwriting in other words ""forgetting"".\n\nAuthors started with a very interesting section (namely section 2.1.1) and presented a theoretical formulation of ""remembering"" capability of RNNs, which is fundamental to this work and I really liked it that they did not jump to the proposed methods right away and instead focused on something very fundamental. Authors presented details of the proposed methods very well, and evaluated them on simple tasks such as ""double task"", ""synthetic reasoning"", etc. as well as on more challenging/real tasks such as ""document classification"" or ""image recognition task from MNIST"". I really liked the fact that the paper looked at different tasks instead of going with one. Results are convincing overall, especially for CUW. One thing that will improve the paper is the analysis part.\n\nDue to having 5+ tasks in the results section, I got the feeling that it is hard to follow the analysis presented by authors within each task as well as across tasks. Also, in some tasks analysis is quite limited. It would be great for authors to zoom into the memory write operations in each task (e.g., taking a diff between RW and URW for example and see how memory changes and more importantly how ""remember"" capability changes) and provide more stats on these, and do this across tasks in one section rather than in different sections allocated for each task. Also, analysis in more realistic tasks (e.g., document classification) can be extended as well, rather than only comparing against state-of-the-art methods in terms of final metric.\n\nWhile reviewing the paper, I couldn\'t help asking why larger memories were not tried. I can see the motivation of trying to use smaller augmented memory, however experimentation around slightly larger augmented memories will be useful for the audience to draw some conclusions. Especially I\'m curious about the effect of memory size on accuracy in tasks like image recognition or document classification.\n\n']","[20, 90, 70]","[60, 80, 80]","[""The sentiment score is slightly positive (20) because while the reviewer finds the paper interesting and acknowledges its contributions, they also point out several limitations and areas for improvement. The reviewer states 'I think this is an interesting paper, trying to make progress on an important problem' but also mentions 'I can only give a borderline score due to missing ACT numbers, and a few other unclear points.' The politeness score is moderately high (60) as the reviewer maintains a professional and constructive tone throughout. They offer specific suggestions for improvement and acknowledge the paper's strengths, using phrases like 'The experiments are well described' and 'The results are interesting.' The reviewer also uses polite language when pointing out issues, such as 'I am not clear on' and 'I would presume,' rather than making blunt criticisms."", ""The sentiment score is 90 because the reviewer expresses very positive views about the paper, calling it 'very well written' with a 'nice balance', and stating it will have impact and is a 'clear accept'. The only slight criticism is about Figure 2(c), which prevents a perfect 100 score. The politeness score is 80 because the reviewer uses respectful and constructive language throughout, offering praise and suggestions in a professional manner. The critique is presented politely, and the reviewer uses phrases like 'I think' to soften criticisms. However, it's not overly deferential, maintaining a professional tone, which is why it's not 100."", ""The sentiment score is 70 (positive) because the reviewer expresses appreciation for several aspects of the paper, such as the interesting theoretical section, well-presented methods, and convincing results. They use phrases like 'I really liked' multiple times. However, it's not 100 as they also suggest improvements. The politeness score is 80 (polite) due to the constructive and respectful tone throughout. The reviewer acknowledges the authors' efforts and uses phrases like 'It would be great for authors to...' when suggesting improvements, rather than using demanding language. They also express curiosity about certain aspects, showing engagement with the work. The review maintains a professional and courteous tone while providing both positive feedback and constructive criticism.""]"
"['Summary:\n\nThis work is tackling two difficulties in current VB applied to DNNs (""Bayes by backprop""). First, MC approximations of intractable expectations are replaced by deterministic approximations. While this has been done before, the solution here is new and very interesting. Second, a Gaussian prior with length scales is learned by VB empirical Bayes alongside the normal training, which is also very useful.\n\nThe term ""fixing VB"" and some of the intro is not really supported by the rather weak experiments, done on small datasets and networks, where much older work like Barber&Bishop would apply without any problems. While interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.\n\n- Quality: Extremely well written paper, I learned a lot from it. Approximations are\n   tested, great figures to explain things. And the major technical novelty, the\n   expression for <h_j h_l>, is really interesting and useful.\n- Clarity: Excellent writing until it comes to the experiments. Here, important\n   details are just missing, for example what q(w) is (fully factorized Gaussian?).\n   Very nice literature review, also historical.\n- Originality: The idea of matching Gaussian moments along the network graph is\n   previously done in PBP (Lobato, Adams), as acknowledged here. Porting this from\n   ADF to VB gives dDVI. PBP also has the property that a DL system gives you the\n   gradients. Having said that, I think dDVI may be more useful than PBP.\n   While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in\n   there. Now, what is done here, is more elegant, does not need 1D quadrature.\n- Significance: Judging from the existing experiments, the significance may be\n   rather small, *if one only looks at test log likelihood*. I\'d still give this the\n   benefit of the doubt, as in particular dDVI could be really interesting at large\n   scale as well. But the authors may tone down their language a bit.\n   To increase significance, I recommend to comment beyond just test log\n   likelihood scores. For example:\n   - Does the optimization become simpler, less tuning required, more automatic?\n      Would one not expect so, given you make a big point out of reducing variance?\n      Does it converge faster?\n   - Can you do something with your posterior that normal DNN methods cannot\n      do? Better decisions (bandits, active learning, HPO)? Continual learning?\n      In the end, who really cares about test log likelihood?\n\nExperiments:\n- What is the q(w) family being used here? Fully factorized Gaussian? I\n   suppose so for dDVI. But for DVI? Not said anywhere, in main paper or\n   Appendix\n- A bit disappointing. Why not evaluate at least dDVI with diagonal q(w) on\n   some much larger models and datasets? Why not quote numbers on speed\n   and robustness of learning, etc? Show what you really gain by reducing the\n   variance.\n- Experiments are OK, but on pretty small datasets, and for single hidden\n   layer NNs. On such data and models, the Barber&Bishop 98 method could\n   be run as well\n- Was MCVI run with re-parameterization? This is really important. If not,\n   this would be an important missing comparison. Please be clear in the main\n   text\n- Advantages over MCVI are not very large. At least, dDVI should be faster to\n   converge than MCVI.\n   Can you say something about robustness of training? Is it easier to train\n   dDVI than MCVI?\n- Why not show the PBP-1 results, comparing to dDVI, in the main text? Are they\n   obtained with the same model? dDVI is doing better.\n\nOther points:\n- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98. Yours is\n   more elegant and faster (does not need 1D quadrature)\n- Relation to PBP: Note that dDVI has an advantage in practice. With PBP, I need\n   to compute gradients for every datapoint. In dDVI, I can do mini-batch\n   updates.\n- I just *love* the header ""Wild approximations"". I tend to refer to this kind of work\n   as ""weak analogies"". Why do you not also compare against this, and show it really\n   does not work?\n', ""This paper considers a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.  Variational lower bound gradients are obtained by approximating the lower bound using Gaussian approximations and moment propagation for network activations, and using a closed form expression for the variational expectation of the log-likelihood, the latter being available for the models considered in the paper.  \n\nThis is an interesting paper.  The Gaussian approximations and moment propagation approximations are clever and highly original although the derivation is rather heuristic.  There is some empirical support that the approximations work well.  The paper is generally well written and clearly motivated in the context of the existing literature.\n\nThe approximations work well for the examples presented in the paper.  The experiments are for rather small datasets and for the DVI method if I understand correctly only models with a single hidden layer are considered.  I wonder if the Gaussian and moment propagation approximations cause difficulty when applied repeatedly in deeper networks.  Are the problems with MCVI and high gradient variance most serious for large datasets and more complex models?  If so a comparison of DVI with MCVI in a more complex example is of interest.  The empirical Bayes approximations are interesting - I would have thought similar approximations been used in the literature before, in addition to the work you mention in Section 5?  I don't feel there is much to compare the proposed EB approximations to, although a comparison with manual tuning is given in Section 6.  \n\n"", 'The authors propose a new approach to perform deterministic variational inference for feed-forward BNN with specific nonlinear activation functions by approximating layerwise moments. Under certain conditions, the authors show that the proposed method achieves better performance than existing Monte Carlo variational inference. This paper is interesting since most of the existing works focus on Monte Carlo variational inference. The main contribution of this paper is to perform Gaussian approximation. The authors show that for specific activation functions, the Gaussian approximation is reasonable. The main concern is the cumulative error due to the Gaussian approximation. Since the authors argue that the proposed method fixes the issues of stochastic VI for BNN, the authors should also investigate/clarify the following cases. \n(1)  A deep BNN to show that the cumulative error is negligible as the number of the hidden layers increases \n(2)  Small latent dimension since CLT may not hold\n(3)  A heavy-tailed variational distribution since the second moment may not be finite \n(4)  Other nonlinear activations since the Gaussian approximation may not be accurate due to (generalized) Berry-Esseen theorem\n(5) A BNN with skip connections  since a Bayesian multiplayer perceptron with skip connections is also a feed-forward BNN\n \nAmong these cases, I am eager to see some results on a deep thin BNN. For example, a BNN with 5 hidden layers, where the latent dimension at each layer is less than 32. \nFurthermore, I would like to see some empirical comparison on real-world datasets between DVI and MCVI under a *fixed* prior since such comparison demonstrates the approximation accuracy of DVI and rule out the confounding factor introduced by the empirical Bayes approach.\n\n']","[50, 60, 50]","[80, 80, 75]","[""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths, such as being 'extremely well written' and presenting 'interesting and potentially very useful novelties'. However, they also point out weaknesses, particularly in the experiments and motivation. The overall tone is constructive rather than overtly negative or positive. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers specific praise (e.g., 'Excellent writing'), and frames criticisms constructively (e.g., 'both experiments and motivation can be improved'). The reviewer also uses phrases like 'I learned a lot from it' and 'I just *love* the header', which contribute to a polite and collegial tone. The suggestions for improvement are presented as recommendations rather than demands, further contributing to the polite tone."", ""The sentiment score is 60 (positive) because the reviewer describes the paper as 'interesting' and 'clever and highly original'. They also mention that the paper is 'generally well written and clearly motivated'. The positive tone is consistent throughout, although there are some suggestions for improvement, which prevents it from being extremely positive. The politeness score is 80 (very polite) because the reviewer uses respectful language throughout, such as 'I wonder if...' and 'I would have thought...'. They offer constructive feedback and suggestions in a non-confrontational manner, maintaining a professional and courteous tone. The reviewer also acknowledges the paper's strengths before presenting areas for potential improvement, which is a polite approach to peer review."", ""The sentiment score is 50 (slightly positive) because the reviewer describes the paper as 'interesting' and acknowledges its contribution, but also expresses concerns and suggests several areas for improvement. The review is not overwhelmingly positive or negative, but leans towards positive with constructive criticism. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing their suggestions as recommendations rather than demands. Phrases like 'I am eager to see' and 'I would like to see' indicate a polite and collaborative tone. The reviewer also acknowledges the authors' work and contributions before providing suggestions for improvement, which is a polite approach in academic reviews.""]"
"['\nGiven the growing interest in building trust worthy and privacy protecting AI systems, this paper demonstrates a novel approach to achieve these important goals by allowing a trusted, but slow, computation engine to leverage a fast but untrusted computation engine. For the sake of protecting privacy, this is done by establishing an additive secret share such that evaluation on one part of the share is performed offline and the computation on the other part of the share is performed on the untrusted engine. To verify the correctness of the computation on the untrusted server, a randomized algorithm is used to sample the correctness of the results. Using these techniques, the authors demonstrate an order of magnitude speedup compared to running only on the trusted engine and 3-4 orders of magnitude speedup compared to software-based solutions.\n\nOverall this is a strong paper which presents good ideas that have influence in ML and beyond. I appreciate the fact that the authors are planning to make their code publicly available which makes it more reproducible. Below are a few comments/questions/suggestions \n\n1.\tThis papers, and other papers too, propose mechanisms to protect the privacy of the data while outsourcing the computation on a prediction task. However, an alternative approach would be to bring the computation to the data, which means performing the prediction on the client side. In what sense is it better to outsource the computation? Note that outsourcing the computation requires both complexity on the server side and additional computation on the client side (encryption & decryption).\n2.\tYou present the limitations of the trust model of SGX only in the appendix while in the paper you compare to other techniques such as Gazzelle which have a different trust model and assumption. It makes sense to, at least, hint the reader on these differences. \n3.\tIn section 2.2: “has to be processed with high throughput when available” is it high throughput that is required or low latency?\n4.\tIn Section 4.3: in one of the VGG experiment you computed only the convolution layers which, as you say, are commonly used to generate features. In this case, however, doesn’t it make more sense that the feature generation will take place on the client side while only the upper layers (dense layers) will be outsourced?\n5.\tIn section 4.3 “Private Inference” : do you include in the time reported also the offline preprocessing time? As far as I understand this should take the same amount of time as computing on the TEE.\n', ""The authors propose a new method of securely evaluating neural networks. The approach builds upon existing Trusted Execution Environments (TEE), a combination of hardware and software that isolates sensitive computations from the untrusted software stack. The downside of TEE is that it is expensive and slow to run. This paper proposes outsourcing the linear evaluation portions of the DNN to an untrusted stack that's co-located with the TEE. To achieve privacy (i.e., the input isn't revealed to the untrusted evaluator), the approach adds a random number r to the input vector x, evaluates f(x+r) on the untrusted stack, then subtracts off f(r) from the output. This limits the approach to be applicable to only linear functions. To achieve integrity (verify the correctness of the output), the paper proposes testing with random input vectors (an application of Freivalds theorem, which bounds the error probability). The techniques for integrity and privacy works only on integer evaluations, hence the network weights and inputs need to be quantized. The paper tries to minimize degradation in accuracy by quantizing as finely as numerically allowable, achieving <0.5% drop in accuracy on two example DNNs. Overall, compared to full evaluation in a TEE, this approach is 10x faster on one DNN, and 40x to 64x faster on another network (depending on how the network is formulated).\n\nDisclaimer: I am a complete outsider to the field of HW security and privacy. The paper is very readable, so I think I understand its overall gist. I found the approach to be novel and the results convincing, though I may be missing important context since I'm not familiar with the subject.\n\nTo me, the biggest missing piece is a discussion of the limitations of the approach. How big of a network can be evaluated this way? Is it sufficient for most common applications? What are the bottlenecks to scaling this approach?\n\nIt's also not clear why integrity checks are required. Is there a chance that the outsourcing could result in incorrect values? (It's not obvious why it would.)\n\nLastly, a question about quantization. You try to quantize as finely as possible (to minimize quantization errors) by multiplying by the largest power of 2 possible without causing overflow. Since quantization need to be applied to both input and network weights, does this mean that you must also bound the scale of the input? Or do you assume that the inputs are pre-processed to be within a known scale? Is this possible for intermediate outputs (i.e., after the input has been multiplied through a few layers of the DNN)?\n\nPros:\n- Simple yet effective approach to achieve the goals laid out in the problem statement\n- Clearly written\n- Thorough experiments and benchmarks\n- Strong results\n\nCons:\n- No discussion of limitations\n- Minor questions regarding quantization and size limits\n\nDisclaimer: reviewer is generally knowledgeable but not familiar with the subject area."", 'In this paper, the authors consider solving three ML security related challenges that would primarily arise\nin the cloud based ML model. Namely, they consider the setting where a client wishes to obtain predictions\nfrom an ML model hosted on a server, while being sure that the server is running the model they believe is being run\nand without the server learning nothing about their input. Additionally, the server wishes for the user to learn \nnothing about the model other than its output on the user\'s input. To solve this problem, the authors introduce a\nnew scheme for running ML algorithms in a trusted execution environment. The key idea is to oursource expensive\ncomputation involved with forwarding images through a model to an untrusted GPU in a way that still allows for\nthe TEE to verify the integrity of the GPU\'s output. Because the authors\' method is able to utilize GPU computing,\nthey achieve substantial speed-ups compared to methods that run the full neural network in trusted hardware.\n\nOverall, I found the paper to be very well written and easy to digest, and the basic idea to be simple. The \nauthors strike a nice balance between details left to the appendix and the high level overview explained in\nthe paper. At the same time, the authors\' proposed solution seems to achieve reasonably practicable performance\nand provides a simple high-throughput solution to some interesting ML security problems that seems readily\napplicable in the ML-as-a-cloud-service use case. I only have a few comments and feedback.\n\nI would recommend the authors use the full 10 pages available by moving key results from the appendix to the main\ntext. At present, much of the experimental evaluation performed is done in the appendix (e.g., Figures 3 through \n5). \n\nThe notation PR_{s \\overset{s}{\\gets}\\mathbb{S}^{n}}[...] is not defined anywhere as far as I can tell\nbefore its first usage in Lemma 2.1. Does this just denote the probability over a uniform random draw of\ns from \\mathbb{S}? If so, I might recommend just dropping the subscript: A, B, and C being deterministic\nmakes the sample space unambiguous. ""negl(\\lambda)"" is also undefined. \n\nIn section three you claim that Slalom could be extended to other architectures like residual networks.\nCan you give some intuition on how straightforward it would be to implement operations like concatenation\n(required for DenseNets)? I would expect these operations could be implemented in the TEE rather than \non the coprocessor and then verified. However, the basic picture on the left of Figure 1 may then change,\nas the output of each layer may need to be verified before concatenation? I think augmenting the right\nof Figure 1 to account for these operations may be straightforward. It would be interesting to see\nthroughput results on these networks, particularly because they are known to substantially outperform\nVGG in terms of classification performance.']","[80, 70, 80]","[90, 80, 90]","[""The sentiment score is 80 (positive) because the reviewer starts by highlighting the paper's importance and novelty, praising its 'strong' nature and 'good ideas'. They appreciate the authors' plan to make their code publicly available. The overall tone is very supportive, with constructive feedback provided. The politeness score is 90 (very polite) due to the respectful language used throughout. The reviewer uses phrases like 'I appreciate' and frames their comments as 'suggestions' rather than criticisms. They ask questions to prompt further thought rather than making direct criticisms. The review maintains a professional and courteous tone while providing detailed feedback, demonstrating high politeness in academic discourse."", ""The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, noting its readability, novelty, and convincing results. They use phrases like 'very readable', 'novel', and 'strong results'. However, it's not 100 as they do point out some missing elements and have questions. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, acknowledges their own limitations ('I am a complete outsider'), and frames criticisms as questions or suggestions rather than direct criticisms. They also explicitly list pros and cons, which is a balanced approach. The reviewer's tone is professional and constructive throughout, without any harsh or rude language."", ""The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, describing it as 'very well written and easy to digest' and praising the authors for striking 'a nice balance' in their presentation. The reviewer also commends the practicality and applicability of the proposed solution. The score is not 100 because the reviewer does have some comments and feedback, indicating room for improvement. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone throughout the review. The reviewer uses phrases like 'I found the paper to be...', 'I would recommend...', and 'Can you give some intuition...', which are polite ways of offering feedback and asking questions. The reviewer also balances criticism with praise, showing respect for the authors' work. The score is not 100 as there's always room for even more politeness, but this review is notably courteous.""]"
"['This paper studies auto-encoders under several assumptions: (a) the auto-encoder\'s layers are fully connected, with random weights, (b) the auto-encoder is weight-tied, (c) the dimensions of the layers go to infinity with fixed ratios. The main contribution of the paper is to point out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations. The idea is original and will probably lead to new directions of research. Already the first applications that the paper suggests are exciting.\n\nThe paper does a good job in justifying assumptions (a), (b) and (c) in the introduction. It is convincing in the fact that this point of view may bring practical insights on training initialization for real-world autoencoders. Thus my opinion is that this paper brings original and significant ideas in the field.\n\nOne flaw of this paper is that the writing might be clearer. For instance when presenting the technical theorem (Theorem 1), it would be useful to have an intuitive explanation for the theorem and the state-evolution-like equations. However, I believe that there are some easy fixes that would greatly improve the clarity of the exposition. Here is a list of suggestions: \n\n- In Section 2.1, a large number of notations are introduced. It would help a lot if the authors made a graphical representation of these. For instance, a diagram where every linearity / non-linearity is a box, and the different variables $x_l$, $\\hat{x}_l$ appear would help a lot. \n\n- Section 2.2 is rather technical. The authors could try to give some more intuition of what\'s happening. For instance, they could spend more time after the theorem explaining what $\\tau_l, \\gamma_l$ and $\\rho_l$ mean. They could also introduce the notation S_sig and S_var early and this section (and not in Section 3), because it helps interpreting the parameters. It would also help if they could write a heuristic derivation of the state-evolution-like equations. From the paper, the only way the reader can understand the intuition behind those complicated equations is to look at the proof of Theorem 1 (which is rather technical). \n\n- In Section 3.1, I did not understand the difference between interpretations 1 and 2. Could the authors clarify? \n\n- In Section 3.4, I did not understand the sentence: ""In particular, near the phase transition of \\gamma, S_sig/S_var = \\Omega(\\beta^{1.5}"". If one uses the \\Omega notation, it means that some parameter is converging to something. What is the parameter? As a consequence, I did not understand this paragraph. \n\n- In Section 3.5, the authors should make clear from the beginning why they are running those specific simulations. What hypothesis are they trying to check? I finally concluded that they are running simulations to check if the hypothesis they make in the first paragraph are true. They also want to compare with some other criteria in the literature, named EOC, that also gives insights about the trainability of the network. However, they could explicitly say in the beginning of the second paragraph that this is the goal.\n\n- In a similar spirit, the authors should end Section 3.5 with a clear conclusion on whether or not the framework enables us to predict the trainability of the autoencoder. \n\n\n\nMinor edits / remarks: \n\n- Typo: last but one paragraph of the introduction: ""whose analysis is typically more straighforwards"" -> ""straightforward"".\n\n- At the end of Section 3.2: what can be proved about the behavior of \\gamma / \\sqrt{\\rho}? It is obviously a central quantity and the authors do not say what happens in the phases where \\gamma and \\rho go to infinity for instance. Is it because it is hard to analyse?\n\n', 'This work applies infinite width limit random network framework (a.k.a. Mean field analysis) to study deep autoencoders when weights are tied between encoder and decoder. Random network analysis allows to have exact analysis of asymptotic behaviour where the network is infinitely deep (but width taken to infinite first). This exact analysis allows to answer some theoretical questions from previous works to varying degrees of success. \n\nBuilding on the techniques from Poole et al (2016) [1], Schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties. The fact that the network component are split into encoder/decoder architecture choice along with weight tying shows various interesting phase of network configuration. \n\nMain concern with this work is applicability of the theoretical analysis to real networks. The autoencoding samples on MNIST provided in the Appendix at least visually do not seem to be a competitive autoencoder (e.g. blurry and irrelevant pixels showing up). \n\nAlso the empirical study with various schemes is little hard to parse and digest. It would be better to restructure this section so that the messages from theoretical analysis in the earlier section can be clearly seen in the experiments.\n\nThe experiments done on fixed learning rate should not be compare to other architectures in terms of training speed as learning rates are sensitive to the architecture choice and speed may be not directly comparable. \n\nQuestions/Comments\n- Without weight tying the whole study is not much different from just the feedforward networks. However, as noted by the authors Vincent et al (2010) showed that empirically autoencoders with or without weight tying performs comparably. What is the benefit of analyzing more complicated case where we do not get a clear benefit from? \n\n- Many auto encoding networks benefit from either bottleneck or varying the widths. The author’s regime is when all of the hidden layers grows to infinity at the same order. Would this limit capture interesting properties of autoencoders?\n\n- When analysis is for weight tied networks, why is encoder and decoder assume to have different non-linearity? It does show interesting analysis but is it a practical choice? From this work, would you recommend using different non-linearities?\n\n- It would be interesting to see how this analysis is applied to Denoising Autoencoders [3], which should be straightforward to apply similar to dropout analysis appeared in Schoenholz et al [2].\n\n[1] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential\nexpressivity in deep neural networks through transient chaos. In Advances in neural information\nprocessing systems, pp. 3360–3368, 2016.\n[2] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[3] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371–3408, 2010.\n', 'Building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder.  Technically, the paper is using the state evolution formalism. In particular the main theorem uses the analysis of the multi-layer version of these algorithm, the so-called state evolution technics, in order to analyse the behaviour of optimal decoding in weight-tied decoder. It is based on a clever trick that the behaviour of the decoding is similar to the one of the reconstruction on a multilayer estimation problem. This is a very orginal use of these technics.\n\nThe results are 3-folds: (i) a deep analysis of the limitation of weight-tied DAE, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that  to train a DAE.\n\nPro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and \nCon: I do not see much against the paper. A minor comment: the fact that the DAE is ""weight-tied"" is fundamental in this analysis. It actually should be mentioned in the title!\n\n']","[70, 50, 80]","[80, 75, 70]","[""The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, calling it 'original' and 'significant', and stating that it will 'probably lead to new directions of research'. The reviewer also mentions that the paper's applications are 'exciting'. However, it's not a perfect score as the reviewer does point out some flaws, particularly in the clarity of writing. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'Could the authors clarify?' and 'It would help a lot if the authors...', which are polite ways of pointing out areas for improvement. The reviewer also balances criticism with praise, acknowledging the paper's strengths before discussing its weaknesses."", ""The sentiment score is 50 (slightly positive) because the reviewer acknowledges the interesting properties and theoretical contributions of the work, but also expresses concerns about its practical applicability and some experimental aspects. The review begins with positive comments about the theoretical analysis and interesting findings, but later raises several questions and concerns. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, frames criticisms as questions or suggestions, and acknowledges the value of the work even while pointing out areas for improvement. The reviewer uses phrases like 'It would be better to' and 'It would be interesting to see' rather than making blunt criticisms, and poses thoughtful questions that could help improve the paper."", ""The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, highlighting its originality, rigor, and clever use of techniques. They mention 'very original use,' 'rigorous work,' and 'very deep answer to interesting questions.' The only criticism is a minor comment about the title. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths without being overly effusive. They present their minor criticism constructively, framing it as a suggestion rather than a demand. The use of phrases like 'clever method' and 'very original use' shows appreciation for the authors' work, contributing to the polite tone.""]"
"['Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.\n\n------\n\nThe paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The paper presents the overall idea beautifully and is very easy to follow. The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical. The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements. Similarly in Flickr and MovieLens the method performs well. This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper. \n\nAn illustrating figure would still be nice to include, also for the convolutions of eq 2. The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.\n\nThe paper should clarify that the \\prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the “a” in the p(a), should it be ""p(x)” ? \n\nI have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. The definition of the m(x) is too clever, please clarify the function in more conventional notation.  ', 'This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives. The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction.\n\nThe paper is well-written, with clear explanations of the desired properties of the model and a concise set of experiments that are easy to follow. The strongest result is that on unbalanced WordNet, while the Flickr and MovieLens results are a little less clear but do show that this technique does not cause any loss in performance.\n\nA few points of feedback:\n\n- Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings. They also report very high numbers on WordNet, though I\'m not sure they are directly comparable.\n\n- The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma. It\'s not clear if this is also implicit in the softplus derivation (by analogy with Eq. (4), should we assume that it approximates the \\sigma = 1 case?). What effect does this have on the embedding space? Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data.\n\n- The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare?\n\n- Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? (Why would we expect the smoothed box model to handle unseen captions better?)\n\n- There\'s a strong emphasis on how smoothing makes training easier. Do you have any metrics to directly support this, such as variance under random restarts?\n\n- In the abstract and introduction, it\'s easy to gloss over ""inspired by"" and assume that the actual model is a Gaussian convolution. Could be more direct here that it\'s a softplus approximation.', 'The paper proposes a method for learning embedding of hierarchies. Specifically, the paper builds on a a geometrically inspired embedding method using box representations. The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA). The observation is that when two boxes are disjoint in the model but have overlap in the ground truth, no gradient can flow to the model to correct the problem (which is happens in case of sparse-data.\n\nTo alleviate the above problem, the paper proposes smoothing the model. That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape. The diffusion process corresponds to convincing the objective function with the Gaussian kernel.\n\nI find the idea of converting such combinatorial problems to differentiable, specially when gradient methods can succeed in optimizing them afterward, very fascinating. I believe this paper is taking a theoretically sound path to construct the differentiable form of the originally non-differentiable problem. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data.\n\nOne downside of the current submission is that the details of optimization are now provided at all. What algorithm do you use to optimize the objective function? What are the hyper parameters? What value of sigma (for diffusion) do you use [or maybe you use the continuation method to gradually anneal sigma from large toward zero?). These are important details that I ask the authors to include.\n\nAlso, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of ""Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures"". I hope such illustration is added to the submission.']","[90, 70, 80]","[80, 80, 70]","[""The sentiment score is 90 because the review is overwhelmingly positive. The reviewer states that 'All my concerns were addressed' and calls it a 'great paper' that 'should be accepted'. They describe the paper as 'well-founded, elegant and practical' and predict it will be 'high-impact'. The few criticisms are minor and constructive. The politeness score is 80 because the language is consistently respectful and encouraging. The reviewer uses phrases like 'beautifully presented' and 'excellent results', showing appreciation for the authors' work. They offer suggestions in a non-demanding way, using phrases like 'would still be nice to include' and 'should clarify', rather than giving direct orders. The tone is professional and courteous throughout."", ""The sentiment score is 70 (positive) because the reviewer expresses strong interest in the paper, praising it as 'well-written' with 'clear explanations' and 'concise' experiments. They highlight the 'strongest result' and mention that the technique doesn't cause performance loss. The politeness score is 80 (polite) due to the constructive and respectful tone throughout. The reviewer offers 'feedback' rather than criticism, uses phrases like 'Could you include...' for suggestions, and acknowledges the paper's strengths before providing specific points for improvement. The language is professional and courteous, avoiding any harsh or dismissive comments."", ""The sentiment score is 80 (positive) because the reviewer expresses strong interest in the paper's approach, calling it 'very fascinating' and 'theoretically sound'. They also highlight the improved performance against SOTA on relevant benchmarks. The slight deduction from a perfect score is due to the mention of a 'downside' regarding missing optimization details. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and expressing their beliefs positively. The suggestions for improvement are framed as requests ('I ask the authors to include') rather than demands. The score isn't higher as the language, while polite, doesn't go out of its way to be exceptionally courteous.""]"

review,reasoning,sentiment_score,politeness_score
"This manuscript joins a crowded space of methods for low bit quantization to enable inference on more efficient hardware. In the past, these methods often were limited to 8-bit quantization, or smaller networks, or result in accuracy degradation. This paper is part of a recent crop of methods that achieve full accuracy on ResNet50 with 4-bit weights and activations. 

The method in this paper is based around a simple, yet powerful observation: Fine-tuning at low precision introduces noise in the gradient. Using the relationship between noise, batch-size and learning rate, that has recently been receiving a lot of attention in the context of large batch training, they compensate for this added noise by increasing the batch size. 

I like the simplicity and effectiveness, and believe that this method will be a useful addition to the toolbox for low-precision inference. 

Overall, the paper is well written, and the claims are well supported experimentally. Results are demonstrated on a wide range of networks, including various configurations of ResNet, DenseNet, Inception. It's not clear whether these experiments are from a single run. If they are, with sub 1% differences between methods we are getting close to the run-to-run variability, and it would be preferable to see results averaged across multiple runs. 

Ultimately, I am on the fence if this is a sufficient contribution for acceptance. In particular, this paper claims ""first evidence ... matching the accuracy of full precision"". While this may in a narrow technical sense be the case, PACT https://arxiv.org/abs/1805.06085 also works on ResNet50 without an accuracy drop. While this is not published work, it was rejected at ICLR last year, making it hard to recommend acceptance here. There is also work concurrently submitted to this forum (which I obviously don't expect the authors to cite or take into account, but want to mention for the sake of completeness) such as https://openreview.net/forum?id=HyfyN30qt7 which achieves the same or better results, and does not require 8-bit BN scale factors and 32-bit bias. 

This manuscript could be made stronger in multiple ways, e.g. by combining with the recently proposed clipping techniques like Choi et al. (2018), and pushing towards 2 or 3 bit training, or eliminating all larger bit-width parameters to make for easier hardware design. 
","The sentiment score is 50 (slightly positive) because the reviewer expresses appreciation for the simplicity and effectiveness of the method, stating it will be a 'useful addition to the toolbox'. However, they are 'on the fence' about acceptance, suggesting room for improvement. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths ('well written', 'claims are well supported') while offering constructive criticism. They use phrases like 'I like' and 'I believe' to soften their opinions, and provide specific suggestions for improvement without being harsh or dismissive.",50.0,75.0
"This paper suggests a new metric for assessing the quality of hierarchical clustering. Dasgupta recently suggested the first such metric with interesting properties. This has encouraged a number of recent works about designing algorithms that work well for the metric and other similar metrics. This paper suggests a new metric for evaluating hierarchical clustering of given graph data. 

Here are the main comments about the paper:
- I am not convinced about the advantages of the new metric over the previously suggested metrics by Dasgupta and Cohen-Addad et al.
    - Theoretical analysis shows properties of the new metric that are similar to that of Dasgupta (since the metric itself has similarities). However, the advantage of the new metric is not very clear. 
    - Experimental analysis just shows that the new metric is different from Dasgupta’s but again there is no evidence to suggest why the new metric may be better. 

- In the abstract it is mentioned that “The best representation of the graph for this metric in turn yields a novel hierarchical clustering algorithm.” I do not understand this. Which novel algorithm is being referred to? 

- Again, it is mentioned in the abstract that “Experiments on both real and synthetic data illustrate the efficiency of the approach”. What efficiency is being referred to here and what is the approach? What I see is that known clustering algorithms are used to compare the new metric with the previous one by Dasgupta.

Overall, I think more work is needed in this paper. There are some non-trivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.

Other comments:
- Section 5: NP-hardness has not been shown and it is just mentioned that the authors believe that the problem is NP-hard just as the problem associated with the cost function of Dasgupta et al.","The sentiment score is -50 because the review is generally critical of the paper, pointing out several shortcomings and expressing skepticism about the advantages of the proposed metric. The reviewer states that 'more work is needed' and that they are 'not convinced about the advantages of the new metric.' However, it's not entirely negative as the reviewer acknowledges some 'non-trivial observations.' The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'I think' and 'I am not convinced' rather than making harsh declarative statements. The reviewer also provides specific points for improvement, which is helpful and courteous. The language is generally neutral, neither overly polite nor rude, but leans slightly towards politeness in its constructive approach.",-50.0,20.0
"In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior.  In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient.  On the other hand,  in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods.  The main contributions of this work are (1)  introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. 

However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) 
Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b  this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017)

Since the authors mainly follow Ollivier, 2017 and make unnatural approximations,  the work has a limited impact.  To get a higher rating, the authors should clearly give justifications and insights of these approximations.

Detailed comments:
(1) On Page 1,  ""The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates.   ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. ""
The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). 

Minor: You should use \approx at Eq (8) since a rank-1 approximation is used.  

(2) On page 2, ""It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients.  However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e.  arguably an online data, rather than a true Bayesian filtering setting)."" 
The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that ""this is equivalent ... or to the addition of an artificial process noise ... in the model"".  I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission.  Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see  Eq 2.25 at Lemma 9  and Lemma 10 of Ollivier, 2017).  The authors should cite Ollivier, 2017.

Minor: Eq (6) should be E_p [ - \nabla_z^2 \log p(d|z) ] = E_p  [ e e^T ], where ""-"", the negative sign is missing. Please see the definition of the Fisher information matrix.
 
(3) On page 2, ""While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer."" 
Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of  Khan et al., 2018 and Khan, 2017b )
Unfortunately, the ""root-mean-square form"" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form  (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission.
To justify these assumptions, the authors should explain when ""the steady state posterior variance"" (see sec 2.21) and  ""a self-consistent solution"" (see sec 7.1) achieve.  As far as I know, \sigma^2_t = \sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \inifity.  Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point.

(4) Section 7.1 is also confusing.
In sec 7.1, the authors assume that A \in O(\eta). However, A=\eta^2/(2\sigma^2) in sec 2.2 and A_{1,1} =  ( \eta_w^2+\eta^2 )/ (2\sigma^2) at Eq (14). In both cases, A can be \in O(\eta^2). This is very *critical* since the authors argue that O(\eta^3) can be neglected in sec 7.1.  The authors use this point to show that Adam is a special case. 
If A \in O(\eta^2), we know that ""A \Sigma_{post}"" \in O(\eta^3) should be neglected. At the last equation on page 10,  the authors do not neglect ""A \Sigma_{post}"". Why?  The authors should clarify this point to avoid doing *selective* neglection.  Again, the impact of this paper should be inspiring new adaptive methods.
The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? 


References
[1] Ollivier, Yann. ""Online Natural Gradient as a Kalman Filter."" arXiv preprint arXiv:1703.00209 (2017).
[2] Khan, Mohammad Emtiyaz, and Wu Lin. ""Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models."" arXiv preprint arXiv:1703.04265 (2017).
[3] Khan, Mohammad Emtiyaz, et al. ""Vprop: Variational Inference using RMSprop."" arXiv preprint arXiv:1712.01038 (2017b).
[4] Khan, Mohammad Emtiyaz, et al. ""Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam"" (2018)

","The sentiment score is -50 because the review is generally critical of the paper, pointing out several limitations and areas for improvement. The reviewer states that the work has 'limited impact' and uses phrases like 'unnatural approximations' and 'should clearly give justifications'. However, it's not entirely negative as it acknowledges the authors' attempts and contributions. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use polite phrases like 'the authors should comment' and 'please see' when making suggestions. The reviewer also provides detailed explanations and references to support their critiques, which is a respectful approach. However, some direct criticisms like 'limited impact' and 'unnatural approximations' prevent a higher politeness score.",-50.0,20.0
"This paper proposes a generalization of variational auto-encoders to account for meta-data (attributes), learning new ones, in a way that these can be controlled to generate new samples. The model learns how to decouple the attributes in an adversarial way by means of a discriminator. The problem is interesting, but I found two main issues with this paper:
1.- Lack of clarity: I found the paper difficult to follow, even after reading Sec. 2 and 3 several times.
2.- Almost absence of experiments: The paper only has one experiment, which is in the appendix, and is about sampling using the MNIST dataset. Given that this paper proposes a model, whose properties can be assessed by means of experiments, the fact that there is nothing of the kind provides no support to any benefits the model may have.

Other points:
What in the model prevents the solution of z_* being just random (independently of x)?

This paper seems relevant Esser, Patrick, Ekaterina Sutter, and Björn Ommer. ""A Variational U-Net for Conditional Appearance and Shape Generation."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.","The sentiment score is -60 because the reviewer expresses significant concerns about the paper, particularly regarding its lack of clarity and insufficient experimental support. The reviewer states two 'main issues' and uses phrases like 'difficult to follow' and 'almost absence of experiments', indicating a largely negative view. However, the score is not at the extreme negative end as the reviewer does acknowledge that the problem is 'interesting'. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use neutral language like 'I found' and 'The paper' rather than making personal attacks. The reviewer also offers constructive feedback by suggesting a relevant paper, which adds to the politeness. However, the overall tone is more matter-of-fact than overtly polite, hence the relatively low positive score.",-60.0,20.0
"Post-rebuttal update: The review process has identified several issues such as missing citations and lack of clarity with respect to aims of the paper. Although the authors have failed to update the paper within the rebuttal period, their responses show an understanding of the issues that need to be addressed as well as a broad appreciation of work in EC that would be included in a final version, making it a useful resource for the wider ML community. On top of this they will include an even larger amount of empiricial data from the experiments they have already run, which is a valuable resource considering the amount of compute needed to obtain this data.

---

The current landscape of reinforcement learning - particularly in domains with high-dimensional structured input spaces such as images or text - relies heavily on backpropagation-based reinforcement learning algorithms.  An alternative that has re-emerged is ES, due to its simplicity and scalability. However, ES can also be considered a gradient-based method. In this paper, the authors apply a similar treatment to GAs, another simple method at its most basic. The authors claim to have 3 main contributions: extending the scale to which GAs can operate, suggesting that gradient-based methods may not achieve the best performance, and making available a vast array of techniques available from the EC literature; they demonstrate the latter by utilising novelty search (NS).

In my opinion the authors do indeed have a valuable contribution in a) demonstrating that a simple GA can successfully be applied to larger networks than was previously thought to be possible and b) introducing a novel software implementation that allows GAs to be efficiently scaled/distributed (similar in nature to the work of Salimans et al.). This is by itself valuable, as, along with recent work on ES, it potentially extends the range of problems that are perhaps best tackled using black-box optimisation techniques. Going against the prevailing trends in order to investigate alternative methods is an underappreciated service to the community, and I believe the evaluation of the methods and the choice of comparative methods to be just about satisfactory. As exemplified by NS, there is a wealth of techniques from the EC literature that could be applied to many topical problems, and the authors' main contributions opens up the road for this.

A lot of care has been put into evaluation on Atari games. The details in the main paper and supplementary material, with, e.g., clear definitions of ""frames"", make me believe that fair comparisons have taken place. All methods in the table, including GAs, perform best at some games (except for RS, which is a necessary baseline for GAs). It would be better to provide more data points that relate to prior works - such as scores at 200M frames to evaluate sample complexity (indeed, the authors note that good solutions can be found by GAs within a few generations, so it would be best to tabulate this) and at ~ 4d to evaluate wall-clock time (is it possible to push performance even further?). Since the GA presented is very rudimentary, I consider the baselines in the main paper to be reasonable, but it would be misleading to not present newer work. The authors do so in the supplementary material, and it is promising to note that the GA still achieves state-of-the-art performance in a few games even when compared to the most sophisticated/distributed state-of-the-art DRL algorithms developed in a concentrated effort over the last few years. Despite having an NS variant, it is a shame that the authors did not show that this could potentially improve performance on Atari, when BCs such as the game RAM or preferably random CNN features are easily available.

The authors also evaluate on a maze that is a staple task in the EC literature to demonstrate the power of NS. While the results are unsurprising, it is a reasonable sanity check. The final evaluation is on a difficult continuous control task, in which GAs solve the task, but have much poorer sample complexity than ES. Given the range of continuous control tasks used to benchmark RL algorithms nowadays, the authors would do well to present results across more of these tasks. Again, NS was not evaluated on this task.

A major weakness of this paper is the presentation. The authors discuss some interesting findings, but would be better served by being more concise and focused. In particular, the emphasis should be more on showcasing quantitative results. Doing so, with more continuous control tasks, would make the claims of this paper more substantiated.","The sentiment score is 50 (moderately positive) because the reviewer acknowledges valuable contributions of the paper, such as demonstrating the applicability of GAs to larger networks and introducing a novel software implementation. They also appreciate the authors' effort in going against prevailing trends. However, the reviewer also points out some weaknesses, like the need for more data points and evaluation on more tasks, which prevents the score from being higher. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the authors' efforts and contributions. They provide constructive criticism and suggestions for improvement without using harsh or dismissive language. Phrases like 'In my opinion' and 'I believe' are used to soften critiques, and the reviewer balances positive feedback with areas for improvement.",50.0,75.0
"The authors focus solely on universal adversarial perturbations, considering both epsilon ball attacks and universal adversarial patches. They propose a modified form of adversarial training inspired by game theory, whereby the training protocol includes adversarial examples from previous updates alongside up to date attacks.

Originality: I am not familiar with all the literature in this area, but I believe this approach is novel. It seems logical and well motivated.

Quality and significance: The work was of good quality. However I felt the baselines provided in the experiments were insufficient, and I would recommend the authors improve these and resubmit to a future conference.

Clarity: The work was mostly clear.

Specific comments:
1) At the top of page 5, the authors propose an approximation to fictitious play. I did not follow why this approximation was necessary or how it differed from an stochastic estimate of the full objective. Could the authors clarify?

2) The method proposed by the authors is specifically designed to defend against universal adversarial perturbations, yet all of the baselines provided defend against conventional adversarial perturbations. Thus, I cannot tell whether the gains reported result from the inclusion of ""stale"" attacks in adversarial training, or simply from the restriction to universal perturbations. This is the main weakness of the paper.

3) Note that as a simple baseline, the authors could employ standard adversarial training, for which the pseudo universal pertubations are found across the current SGD minibatch.


","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the originality and quality of the work, they express significant concerns about insufficient baselines and recommend resubmission to a future conference. This suggests the paper is not yet ready for acceptance in its current form. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the strengths of the paper, and frames criticisms constructively as recommendations or questions rather than harsh judgments. The reviewer also uses phrases like 'I believe' and 'I felt' to soften their critiques. However, the score is not extremely high as the language, while polite, is still direct and doesn't include overtly complimentary phrases.",-20.0,60.0
"The paper gives convergence guarantees to the true neural network classifier for the networks that are explicitly regularizing the (exact) Lipschitz constant of the network. The bound stated decays as ( log(n)/n )^{1/m}, where n is the number of training points and m is the dimension of the data manifold. Thus the decay rate is pretty slow when the data lies on a high-dimensional manifold. I believe it should also depend on the volume of the data manifold.

Computing the exact Lipschitz constant of the network is intractable. All the theorems apply to minimization problem defined in (1) with the exact Lip(u,X) being regularized, and not to the minimization problem of the lower bound (2). I believe there are also no convergence guarantees how quickly this lower bound on the Lipschitz constant approaches Lip(u,X), unless some assumptions are made on the smoothness of the data manifold (comments and insights would be appreciated). Thus in my opinion the current results have little practical importance. Nevertheless, it’s still interesting to see some ideal-setting guarantees being established.


Theorem 2.7: Is m == m_0 (the dimension of the data manifold)? Shouldn’t the bound be 2*C*L_0….? (assuming lemma 2.9 is correct)

Cor 2.8: Where does the volume Vol(M) of the manifold disappear? Is C in equation (6) the same C as in Theorem 2.7? Also, it looks like the bound should bound should have C^2 (assuming theorem 2.7 is correct, and the C’s are the same).


Introduction: I assume u(x,w) is not the last layer map, but a map from the input space to the labels (i.e., the whole neural network function and not just the map from the last hidden layer to the labels). If I am correct, it’s misleading to refer to u(x,w) as the last layer map. And if it is the last layer map, please justify why it is enough to consider the Lipschitz constant of the last layer.

The term “clean data” is never defined. My guess is that “clean data” refers to the realizable  setting, and “noisy” to agnostic, where the hypothesis space consist of neural networks of arbitrary size.


“Our analysis can avoid the dependence on the weights because we make the assumption that there are enough parameters so that u can perfectly fit the training data. The assumption is justified by Zhang et al. (2016).”
Note, that the network used in practice achieve zero classification error, as demonstrated by Zhang et al, but I doubt the cross entropy loss (that is usually being minimized) is exactly zero.

Remark 1.4 “will result in an expected loss..”  (there is also a typo here) should specify that you are talking about empirical error (0-1 loss), since I don’t think the loss function is fixed anywhere earlier in the text.

Remark 2.2 : Just wanted to note, that it is more common to call L(u,\rho) risk. The gap between L(u,\rho) and the empirical risk L(u,\rho_n) is usually called the generalization error (and only in the case of zero empirical risk, L(u,\rho) is equal to the generalization error). I did check the reference in Goodfellow et al. book, and I see that it is consistent with your definition.

Just below Remark 2.2:
“We would also expect the sequence of generalization losses L[u_n ; \rho] to converge to zero in the case of perfect generalization.”
Once again, this is true only in the realizable setting.

Could the authors comment on the connection to Cranko et al. 2018 work?

Typos:
In the abstract, no which: “..corrupted labels which we prove convergence to...”
“...a candidate measure for the Rademacher complexity, which a measure of generalization...”.
“1-Lipschitz networks with are also important for Wasserstein-GANs”
Section 2.1 “is it clear that u0, is a solution of “, should be “it is”


---------
[UPDATE]

Regarding the comment ""Our paper resolves the question posed in ICLR Best paper 2017 ""Understanding deep learning requires rethinking generalization"""", I don't think that analyzing networks with explicit regularization resolves the questions stated in Zhang et al paper. As other reviewers mentioned, there are a number of other papers that formally define quantities that correlate with the generalization error, and are larger for random vs true labels. There are also other papers showing that one can tune some parameters of the optimization algorithm to avoid overfitting on random labels (while it is a modification to the algorithm, it is still similar to explicitly regularizing the Lipschitz constant of the network) (see e.g., Dziugaite et al work on SGLD).

Therefore, the claim in the abstract ""A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?"" needs to be toned down a bit.

In my opinion, the work presented in this paper is a valuable contribution to learning theory. The new version of the paper is easy to read. Therefore, I recommend acceptance if the authors change the claim about resolving  the questions posed by Zhang et al.

Another typo:
 - for convergence, we require that the network also grow(s), ","The sentiment score is slightly positive (20) because while the reviewer points out some limitations and issues, they also acknowledge the work as 'interesting' and a 'valuable contribution to learning theory'. They recommend acceptance with minor changes. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, asks for clarifications politely ('Could the authors comment on...'), and offers constructive feedback. They also acknowledge the paper's strengths alongside critiques. The reviewer maintains a professional tone, avoiding harsh criticism while still providing thorough feedback.",20.0,60.0
"This paper looks to predict ""unstructured"" set output data. It extends Rezatofighi et al 2018 by modeling a latent permutation.

Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers. 
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me. The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others. The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense. Conditioned on the permutation of the set, the points are exchangeable. Let's just consider a 2 element ""set"" at the moment Y = (y_1, y_2). Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word. we have:
p_\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))
*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way. Essentially the paper has just written a mixture model for the output points where there are as many components as permutations. I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues. It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3. This needs to be justified.

There are some stylistic shortcomings as well. For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs). Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1. But these and other points are minor.

The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above.","The sentiment score is -80 because the review is highly critical of the paper, pointing out several major methodological issues and stating that the paper should not be published until these are resolved. The reviewer uses phrases like 'identity crisis', 'very odd choices', and 'does not make sense', indicating strong disapproval. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. Phrases like 'Unfortunately, there is a bit of an identity crisis happening in this paper' and 'I don't think this makes much sense' are quite direct and could be perceived as somewhat impolite in academic discourse. However, the reviewer does acknowledge some minor positive aspects and uses some hedging language ('a bit', 'feels like'), which prevents the score from being even lower.",-80.0,-20.0
"The authors are tackling sample efficiency in the reinforcement learning setting by designing a reward function that encourages exploration. To achieve this they propose you use the successor function which basically counts how often a state has been visited. At first the show this for discrete settings and extend their approach to the continuous state spaces in the Atari 2600 environments. 

The paper is well written and the motivation and methods are clear from the beginning. 

My biggest concerning is regarding the experimental results of this work. In Table 1 the authors show the results for the tabular games River Swim and Six Arms and copare their approach which they dub ESSR to three methods (E3, R-MAX, MBIE). The numbers in the table indicate that their method ESSR is outperforming E3 and R-MAX on both environments but is itself outperformed by MBIE. The authors don't mention this at al in the respective paragraph nor do the provide a reason as to why this could be case. Also, they neither introduce any of these methods nor do the explain the meaning of the acronyms. Only in the section 6 (of 7) they talk about related works are R-MAX and E3 introduced briefly. But yet again, MBIE is not mentioned. 

I have similar concerns about the results presented for the Atari benchmarks. In table 2 the authors compare their method to the classic DQN approach and two more approaches. While their approach outperforms DQN in almost all tasks, this does not hold for the remaining algorithms. Their method is being outperformed in all but one (Venture) task, where they report a higher variance and a small performance boost compared to DQN_e^MMC. Also it is not clear to me where the numbers for the DNQ_e^MMC come from. The authors just say ""[...] denotes another baseline used in the comparison"". Is this the proposed method of this work but without the successor representation?

In my opinion this work is lacking some clear and convincing results.  Is the main benefit of this method that it does not rely on domain-specific knowledge? If so, then it is not communicated clearly. The authors mention this briefly in the conclusion but provide no further analysis","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('The paper is well written and the motivation and methods are clear'), they express significant concerns about the experimental results and lack of clear, convincing outcomes. The reviewer points out several issues with the presentation and interpretation of results, which contributes to the overall negative sentiment. However, it's not extremely negative as the reviewer does recognize some merits of the paper.

The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'My biggest concern is...' and 'In my opinion...', which are polite ways to express criticism. The reviewer also acknowledges positive aspects of the paper before diving into their concerns. While the criticism is direct, it's presented in a constructive manner without using harsh or rude language.",-30.0,50.0
"This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.

Overall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.

My main complaint is around Figure 5, Table 3, and the corresponding analysis.
1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies ""representative power of UG-WGAN grows as we increase the number of languages"". It could be that the language-universal impose more constraints to model all languages, so the two variation (\lambda=0.0 or 0.1) come closer to each other.

2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.

Minor:
1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \circ and the second one use parenthesis?","The sentiment score is 70 (positive) because the reviewer expresses an overall positive view of the paper, describing the method as 'novel and nice' and the results as 'good'. They also mention that the paper is clearly written and could be useful for future research. However, it's not a perfect score due to some complaints and suggestions for improvement. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They frame their concerns as questions or suggestions rather than harsh criticisms. The use of phrases like 'My main complaint' and 'Minor:' to introduce critiques is a polite way to structure feedback. The reviewer maintains a professional and constructive tone throughout the review.",70.0,80.0
"The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs.

However, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)?

Moreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018).

Minor:
- It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions).
- What kind of activation quantization is used?
- In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step?
- What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some advantages of the proposed method, they also express significant concerns and criticisms. The review starts positively but then raises several important questions and points out limitations, including a lack of novelty. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They raise criticisms and questions in a constructive manner, without using harsh or dismissive language. The reviewer also provides specific, helpful feedback in the 'Minor' section, which contributes to the politeness score.",-20.0,50.0
"The paper attempts to examine the reasons behind the strong generalisation performance of DNNs trained via SGD. The authors propose an analysis which offers a fresh view to this problem. This view has been articulated very well, and is based on sound mathematical arguments. 

On the other hand, since there is no formal theorem to support the introduced assumptions, the authors have attempted to provide empirical evidence through experiments with standard DNN architectures and benchmark datasets. However, this is where the weakness of this paper lies: The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups. 

Although I accept the claim of the authors concerning the lack of space, they could also trim the Introduction so as to free up some space, as well as provide an indefinite number of extra supporting evidence in the form of Supplementary Material/Appendices. 


","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's fresh perspective and sound mathematical arguments, but also points out significant weaknesses in the empirical evidence. The overall tone is constructive rather than dismissive. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and constraints, and offering suggestions for improvement rather than harsh criticism. The reviewer balances positive feedback with constructive criticism, maintaining a professional and courteous tone throughout the review.",20.0,60.0
"The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier ""most"" (""most of the dots are red""). It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too. This is consistent with human behavior.

Strengths:
* The introduction lays out an ambitious program of comparing humans to deep neural networks.
* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.

Weaknesses:
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely. While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model. I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).
* I found it difficult to follow the theoretical motivation for performing the work. The goal seems to be to test whether the network is performing the task in way that ""if not human-like, at least is cognitively plausible"". I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue. Later in the same paragraph, the authors argue that ""in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable"". This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there). In general, I don't understand why we would want a visual question answering system that returns approximate answers -- isn't it better to have it count exactly how many red dots there are compared to non-red dots?
* The authors assume that explicit counting is not ""likely to be learned by the 'one-glance' feed-forward-style neural network"" evaluated in the paper. What is this statement based on? Why would a ""one-glance"" network have trouble counting objects? (What is a “one-glance network”?)
* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"". What is ""pattern matching"" and how does it differ from ""higher-level concepts""?
* Why would the pairing strategy in a neural network be affected by the clustering of the objects? I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.

Minor comments:
* Is the definition of ""most"" really a central piece of evidence for ""the apparent importance of a cardinality concept to human cognition""? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.
* Please use the terms ""interpretation"" and ""verification"" consistently.
* ""One over the other strategy"" -> ""one strategy over the other"".
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.
","The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper, they list more weaknesses and express significant concerns about the methodology, theoretical motivation, and clarity of concepts. The overall tone suggests the paper needs substantial improvements. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I would like to see' and 'I don't understand' rather than making harsh criticisms. They also begin by noting the paper's strengths before moving on to weaknesses, which is a polite approach. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a neutral, objective tone overall.",-30.0,50.0
"This work presents a method to translate non-systematic names of chemical compounds into their systematic equivalents. Beyond that, a corpus of systematic and non-systematic chemical names is introduced that were extracted from chemistry and manually labelled.

The paper is well-structured and the authors introduce the problem setting very nicely to a machine learning audience, explain the challenges and motivate the architecture of their model. The model is a combination of tested approaches such as a spelling error correction, a byte pair encoding tokenizer and a sequence-to-sequence model consisting of (Bi)LSTMs and attention mechanisms. 

The evaluation appears solid. The model achieves significantly improved results on the proposed corpus, even though compared to a underwhelming baseline. The analysis could be improved by showing and explaining some examples of failed translation attempts. It would also be interesting to see how the failed attempts are distributed over the error types (spelling, order, common name, synonym). The authors suggest a scenario where non-systematic names are converted and checked against a database of systematic names. For this, it would be interesting to know whether there are cases where a non-systematic name was translated into the wrong (but valid) systematic name.

Concluding, the paper presents an interesting application for machine translation, a new dataset and a method that successfully labels 50% of the given corpus.

Minor issue: The color scale of Fig. 5 is hard to recognize due to low contrast.","The sentiment score is 70 (positive) because the reviewer generally praises the paper, describing it as 'well-structured' with a 'nicely' introduced problem setting. They commend the model's 'significantly improved results' and call it an 'interesting application'. However, it's not 100 as they suggest some improvements and mention a 'minor issue'. The politeness score is 80 (polite) as the reviewer uses respectful language throughout, offering constructive feedback and suggestions rather than harsh criticism. They use phrases like 'could be improved' and 'it would be interesting' when suggesting changes, which maintains a polite tone. The overall language is professional and courteous, without any rude or dismissive comments.",70.0,80.0
"This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.

The paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it’s unclear what’s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision.

The paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That’s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there’s no way to really verify this assumption. Thus, it’s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don’t really fit the exact assumption in this paper.

Moreover, I’d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm.

Choice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1].

[1] Sakai, Tomoya, et al. ""Semi-supervised classification based on classification from positive and unlabeled data."" arXiv preprint arXiv:1605.06955 (2016).","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('clarity', 'easy to follow'), they raise several concerns and suggest improvements. The overall tone indicates that the paper needs significant work. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offering constructive criticism without harsh words. They use phrases like 'Maybe the authors can elaborate' and 'I'd like to see' which are polite ways of suggesting improvements. The reviewer also balances critique with positive comments, which contributes to the polite tone.",-20.0,60.0
"In this paper, the authors try to interpret the prediction mechanism of Layered Neural Networks (LNNs). The authors proposed to first define a feature vector that represents the roles of each hidden layer unit, via computing Pearson correlation coefficient. Then a hierarchical clustering method is applied to the generated feature vectors, such that tree-structured relationships among hidden layer units are revealed.

The purpose of the paper is to understand the prediction mechanism of Layered Neural Networks (LNNs). But based on the results in the experiments, I do not think the model achieves this purpose. Given the tree structure of LNN for the MNIST data set, I am still not able to understand how this LNN distinguishes the digit 0 from other digits. I am also not able to understand why a particular sample is classified as 0 rather than 6.

In Section 1, the authors mension that there are existing clustering-based methods that interpret LNN. The authors do not compare the proposed methods with these existing methods, either quantitatively or qualitatively. So I am also not sure the contribution of this paper, provided the existing methods.

In Section 3.1, the authors state that ""there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit"". I do not agree with this statement, because Ross et.al (2017) has proposed to measure it via gradient, although they are trying to solve a slightly different problem. Since the output of a hidden unit is a non-linear function of the input, I am not convinced that the proposed method that computes Pearson correlation coefficient is better choise than computing the gradient.

The proposed method provides a tree structure to describe the relationships between the hidden layer units. The authors also do not illustrate why learning the tree structure is particularly important. We can also run k-means with cosine similarity on the generated vector $v$, and learn the number of clusters via Bayesian information criterion (BIC). The authors do not explain why the tree-structured clustering results are more superior than the k-means clustering results.

In summary, I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.


References
Ross, Andrew Slavin, Michael C. Hughes, and Finale Doshi-Velez. ""Right for the right reasons: training differentiable models by constraining their explanations."" Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI). 2017.","The sentiment score is -80 because the reviewer recommends rejection and provides several critical points about the paper's shortcomings. The review highlights that the proposed method doesn't achieve its purpose, lacks comparison with existing methods, and has questionable design choices. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I do not think' and 'I am not convinced' rather than harsh language. The reviewer also provides specific reasons for their recommendation and cites relevant literature, which adds to the constructive nature of the feedback despite its overall negative sentiment.",-80.0,20.0
"General comment
==============
The authors describe an attention mechanism for training with images of different sizes. The paper is hard to understand due to major grammatical errors and unclear descriptions. Methods for training with images of different sizes have been proposed before, e.g. spatial pyramid networks. I also have concerns about their evaluation. Overall, I believe that the paper is not ready to be submitted to a conference or journal.

Major comments
=============
1. Methods for training with images already exists, e.g. spatial pyramid pooling (http://arxiv.org/abs/1406.4729) or fully-convolutional networks (https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf). These are not cited in the paper and not included as baselines in their evaluation.

2. The attention mechanisms looks similar to classificat soft-attention (https://arxiv.org/abs/1502.), which is not cited in the paper.

3. The paper contains major spelling and grammatical errors, making it hard to understand important aspects.

4. I can not see a clear improvement of their method over ResNet and DenseNet when the same number of model parameters is about the same. Without making sure that the number of model parameters is about the same, it is unclear if the performance gain is due the increased number of model parameters or the methodology.","The sentiment score is -80 because the review is predominantly negative. The reviewer states that the paper is 'hard to understand', has 'major grammatical errors', and is 'not ready to be submitted'. They express concerns about the evaluation and lack of citation of existing methods. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without much attempt to soften the criticism. The reviewer does not use polite phrases or acknowledge any positive aspects of the work. However, they do provide specific recommendations for improvement, which prevents the score from being lower.",-80.0,-20.0
"The focus of this paper is to show that finite-width deep neural networks with fully connected layers and ReLU activations are rate-distortion optimal approximators of certain classes of functions, meaning the approximation error decays exponentially in the number of neurons in the network. The function classes explored in this paper are: 1-d polynomials (on bounded intervals), 1-d sinusoidal functions (on bounded intervals), and other 1-d functions built from compositions or linear combinations of these, such as the so-called class of “oscillatory textures” and a class of continuous but nowhere differentiable functions known as Weierstrass functions. Finally, the paper also shows that as the desired approximation accuracy goes to zero finite-width deep ReLU networks require asymptotically fewer neurons than finite-depth wide ReLU networks in approximating a broad class of smooth functions.


The paper is well-written and the technical results are presented in a way that is easy to understand. The results are somewhat novel, although they do build off other recent works, namely Yarotsky (2016) and Telgarsky (2015). However, the authors were careful to cite when they reuse proof techniques from these and other works. The results in the main text appear to be technically sound. I did not check carefully all the proofs in the supplemental materials.


My major criticism is that the focus on certain specific function classes (oscillatory textures, Weierstrauss functions) seems arbitrary, and leaves open many questions. For example, there is existing work on the approximation ability of deep ReLU networks for functions in more general Holder and Sobolev spaces:

Hadrien Montanelli and Qiang Du. Deep ReLU networks lessen the curse of dimensionality. arXiv preprint arXiv:1712.08688, 2017.

J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv e-prints, August 2017.

I was left wondering how the present results relate to these works, and what insight we get from understanding these particular function classes that we don't get from understanding Holder or Sobolev spaces.


Major comments


In Section 3, I found the progression of the results from approximation of x^2, to multiplication xy, and to general smooth functions to be very natural and well-motivated. However, sections 4 and 5 seem lack somewhat in motivation, since here the authors focus on very specific function classes (sinusoidal functions, oscillatory textures, and Weierstrass functions). While these results are still interesting, focusing on such specific functions is less satisfactory, since it raises questions about the true scope of the results (e.g., will similar approximation rates extend to other fractal functions, or just Weierstrauss functions?). Could the authors give further justification for why these function classes are interesting to focus on, or why they limit themselves in this way? Can the authors also put these results more into context with existing results on the approximation with ReLU networks?


The authors state multiple times that “all our results apply to the multivariate case” but that they restrict themselves to the univariate case for simplicity of presentation. While this is fine, some indication of how the results are altered in the multivariate case would be useful. For example, does the fixed-width M in multivariate generalizations of Prop 3.1--3.3 need to be bigger, smaller, or the same? What other constants are dimensionally dependent? Do the multivariate generalization of their results bear the ""curse of dimensionality"", i.e., does the number of neurons needed to reach epsilon accuracy depend geometrically on the dimension?

Minor comments


A conclusion or discussion section summarizing the overall technical contribution would be useful for the reader. Also, it would be useful to include some discussion on remaining open problems or future work.

On pg. 2, the authors state “the approximation results throughout the paper guarantee that the magnitude of the weights in the network does not grow faster than polynomially in the cardinality of of the domain over which the approximation takes place”. What does “cardinality of the domain” here mean? I think the authors mean the size D of the interval [-D,D] over which the approximation is valid.

On pg. 7, the authors say “We note that this result allows to show that local cosine bases (cite) can be approximated by deep ReLU networks with exponential error decay…”. I think the authors mean to say “...this result allows us to show…” or “this result allows one to show…”. Although it’s not clear to me whether this means it has been shown (it’s a direct corollary), or could possible be shown (it’s a corollary, but needs some non-trivial work). Also, one line to specify what a “local cosine basis” is would be helpful.","The sentiment score is 50 (moderately positive) because the reviewer begins by praising the paper as 'well-written' and the results as 'somewhat novel' and 'technically sound'. However, they also express a 'major criticism' about the focus on specific function classes, which tempers the overall positivity. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and phrases their concerns as questions or suggestions rather than direct criticisms. They use phrases like 'Could the authors give further justification' and 'some indication... would be useful', which are polite ways of requesting improvements. The reviewer also acknowledges the authors' work and cites relevant literature, showing respect for the authors' efforts and the broader scientific context.",50.0,80.0
"This paper presents the following main insight (quoting the authors): Resnets classify input patterns based on the sum of the transient dynamics of the residuals in different layers.
The authors formulate this insight mathematically, and experimentally demonstrate its validity on a toy binary classification problem. They also show that the behaviour of a Residual network trained on MNIST is in line with the claims. Finally, a method to adapt the depth of a Residual network during training is proposed and applied to the toy classification problem.

The paper is generally of good quality and easy to understand. My only complaint: the introduction (including related work) is too long and I think it will be unclear to a general reader before reading section 2, where the terms used in the paper are explained and clarified. I think it will be an improvement to leave out detailed discussion of related work for a separate section, and focus on clarifying what the paper is about.

Overall, while the paper is related to an interesting and potentially fruitful perspective of neural networks (as dynamical systems), in my view the contributions are not significant at this stage. That the sum of transients determines network outputs is almost by design, and can be shown without a dynamical systems perspective. Using the paper’s notation, one can sum over the equations for all the layers to obtain this.

x(1) = x(0) + y(1)
…
x(T) = x(T-1) + y(T)
————————————————————————
x(T) = x(0) + sum(y(t))

Since the classification is performed using x(T), it is clear that the sum of transients is what causes the change in representation and that y(T) can be the same or not for different class inputs.

Based on my understanding, I don’t find the findings to be significantly novel or surprising. In particular, I don’t see any concrete benefits of employing a dynamical systems perspective here. Nevertheless, the experimental analysis is interesting, and I’m hopeful that this direction will lead to more insights in the future.

The final contribution is a new method to learn the depth of Residual networks during training, but this is insufficiently explored (only tested on the toy dataset) so its practical significance can not be evaluated at this stage.


Minor notes:
- Note that many citations are textual when they should be parenthetical.
- The reference “No & Liao (2016)” has incorrect author name.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper is 'generally of good quality and easy to understand', they also state that 'the contributions are not significant at this stage' and that the findings are not 'significantly novel or surprising'. The reviewer expresses hope for future insights but overall seems underwhelmed by the current work. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges positive aspects of the paper, and frames criticisms constructively. They use phrases like 'I think it will be an improvement' and 'I'm hopeful that this direction will lead to more insights in the future', which maintain a polite and encouraging tone even while expressing concerns.",-20.0,60.0
"The authors describe a framework of how to learn a ""fair"" (demographic parity) representation that can be used to train certain classifiers, in their case facial expression and activity recognition. The method describes an adversarial framework with a constraint that bounds the distortion of the learned representation compared to the original input.

Clarity:
The paper is well written and easy to follow. The appendix is rather extensive though and contains some important parts of the paper, though the paper can be understood w/o it.

I didn't quite follow Sec 3. It is a bit sparse on the details and the final conclusion isn't entirely clear. It also isn't clear to me how general the conclusions drawn from the Gaussian mixture model are for more complex cases.

Novelty:
Adversarial fairness methods are not new, but in my opinion the authors do a good job of summarizing the literature and formalizing the problem. I am not fully familiar with the space to judge if this is enough novelty.

Using the distortion constraint is interesting and seems to work according to the experiments. Generally though, I think that distortion can be a very restrictive constraint. One could imagine representations with a very high distortion (e.g. by completely removing the sensitive attribute) and predictive qualities equivalent to the original representation. Some further discussion of this would be good.

Experiments:
The experiments are somewhat limited, but show the expected correlations (e.g. distortion vs predictiveness). 

Overall, I do believe that this work is in the right direction in this more and more popular area of great importance. I also think that contributions compared to other works could be made more clear, as well as additional experiments and discussions of the shortcomings of this approach may be added.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths (well-written, good literature summary, interesting approach) while also pointing out areas for improvement (limited experiments, need for more clarity on contributions). The overall tone suggests the work is promising but could benefit from some enhancements. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offering constructive criticism without harsh words. They use phrases like 'I believe', 'I think', and 'could be made more clear', which maintain a courteous tone while providing feedback. The review balances positive comments with suggestions for improvement in a professional manner.",50.0,75.0
"This paper proposes an objective function for auto-encoding they
call information maximizing auto encoding (IMAE).  To set the stage
for my review I will start with the following ""classical"" formulation
of auto-encoding as the minimization of the following where we are
training models for P(z|x) and P(x|z).

beta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1)

Here H(z) is defined by drawing x from the population and then drawing
z from P(z|x).  This is equivalent to classical rate-distortion coding
when P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just
the L2 distortion between x and its reconstruction.  The parameter
beta controls the trade-off between the compression rate and the L2
distortion.

This paper replaces minimizing (1) with maximizing

beta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2)

This is equivalent to replacing H(z) in (1) by -I(x,z).  But (2)
admits a trivial solution of z=x.  To prevent the trivial solution this
paper proposes to regularize P(z) toward a
desired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z))
by minimizing

beta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3)

The paper contains an argument that this replacement is reasonable
when Q(z) and P(z|x) are both Gaussian with diagonal covariances.  I
did not verify that argument but in any case it seems (3) is better than (2). 
For beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value
H(Q).  The regularization probably has other benefits.

But these suggestions are fairly simple and any real assessment of their
value must be done empirically.  The papers experiments with MNIST
seem insufficient for this.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions and provides a detailed analysis, they ultimately express skepticism about the sufficiency of the experiments ('The papers experiments with MNIST seem insufficient for this'). This suggests that the reviewer is not fully convinced by the paper's findings. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They provide a thorough explanation of the paper's approach and offer constructive criticism without using harsh language. The reviewer also acknowledges potential benefits of the proposed method ('The regularization probably has other benefits'), showing a balanced perspective. However, the review lacks overtly polite phrases or compliments, keeping it from scoring higher on the politeness scale.",-20.0,50.0
"This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.
The method is tested on several scenarios and datasets, showing promising results in prediction accuracy.

The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. 

Related to this latter point, the use of the term “Bayesian nonparametric” is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn’t present any element related to parameters inference, uncertainty estimation, … Even the fact that the method uses an algorithm illustrated in [1] doesn’t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. 

Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.  





[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012

[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. ""Unsupervised deep embedding for clustering analysis."" International conference on machine learning. 2016.
[3] Ji, Pan, et al. ""Deep subspace clustering networks."" Advances in Neural Information Processing Systems. 2017.
[4] Jiang, Zhuxi, et al. ""Variational deep embedding: An unsupervised and generative approach to clustering."" IJCAI 2017
[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. ""Semantic autoencoder for zero-shot learning. CVPR 2017.","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('promising results', 'reasonable and rather intuitive idea'), the overall tone is critical. The reviewer expresses significant concerns about the paper's clarity, methodology, and novelty. The politeness score is 0 (neutral) because the reviewer maintains a professional tone without being overly polite or rude. They express criticisms directly but without harsh language, using phrases like 'in my opinion' and 'the paper would largely benefit from' to soften the critique. The review balances some positive comments with substantial criticisms, maintaining a neutral, academic tone throughout.",-50.0,0.0
"This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions.

Pros:
* This paper proposes to jointly learn the local mask to make the translation focus on the foreground instead of the whole image.
* The local mask-based highway adaptive instance normalization apply the style information to the local region correctly.

Cons:
* There seems a conflict in the introduction (page 1): the authors clarify that “previous methods [1,2,3] have a drawback of ....” and then clarify that “[1,2,3] have taken a user-selected exemplar image as additional input ...”. 
* As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. 
* It is mentioned in the introduction (page 2) that “This approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation”. However, the differences between the proposed method with these prior works are not compared or mentioned. Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. I wonder the advantages of the proposed method compared to these works.
* The experiment setting is not clear enough. If I understand correctly, the face images are divided into two groups based on their attributes (e.g. smile vs no smile). If so, what role does the exemplar image play here? Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? 
* The github link for code should not provide any author information.

[1] Multimodal Unsupervised Image-to-Image Translation
[2] Diverse Image-to-Image Translation via Disentangled Representations
[3] Exemplar Guided Unsupervised Image-to-Image Translation
[4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation

Overall, I think the proposed method is well-designed but the comparison and experiment setting are not explained well. My initial rating is weakly reject.","The sentiment score is -40 because the overall tone is somewhat negative, with the reviewer pointing out several cons and concluding with a 'weakly reject' recommendation. However, they do mention some pros and describe the method as 'well-designed', which prevents the score from being more negative. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism without being harsh or rude. They use phrases like 'I strongly recommend' and 'I wonder' which maintain a polite tone while expressing concerns. The reviewer also balances criticism with positive comments, which contributes to the politeness of the review.",-40.0,50.0
"The paper presents an inference method (implicit distribution particle smoothing) for neural Hawkes processes that accounts for latent sequences of events that influence the observed trajectories.

Quality
+ The paper combines ideas from multiple areas of machine learning to tackle a challenging task of inference in multivariate continuous-time settings.
- The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.

Clarity
- notation is complex replete with symbols ""@"" and text in math formulas
- It's not clear what p (""the data model"") and p_miss (""the missingness mechanism"") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol.  In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as: p(r|x_u, x_o) = p(r|x_o), r is a missingness vector, x_u is x unobserved, and x_o is x observed, then r corresponds to observation or not, whereas in the manuscript p_miss is on the values themselves, i.e. on the space where z={k_{i,j}@t_{i,j}} resides.  We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown.  If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.

Originality
+ the work is rich, complex, original, and uses leading methods from multiple areas of ML.

Significance
+ the significance of this work could be high, as it may provide a way to conduct difficult inference in an effective way to produce increasingly flexible modeling of trajectories amidst partial observation.
- however the exposition (particularly the experiments) does not fully demonstrate this.","The sentiment score is 50 (slightly positive) because the review acknowledges both strengths and weaknesses of the paper. It praises the originality, complexity, and potential significance of the work, but also points out areas for improvement in clarity and experimental reporting. The positive aspects slightly outweigh the criticisms, hence the moderately positive score. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'would strengthen the paper' instead of direct criticisms, and acknowledge the paper's strengths before discussing its weaknesses. The review maintains a professional and courteous tone throughout, without any harsh or rude language.",50.0,75.0
"Overview: 
This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at ICLR. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time.

Pros:
The paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.

Issues:

* (p.5) ""R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients.""

This statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian).

Control variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook ""Simulation.""

The fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s ""Mirage of Action-Dependent Baselines"", https://arxiv.org/abs/1802.10031.

Since the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale.

* The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: ""We choose the baseline ... to be a function of state ... it must be independent of the action ...."" and ""it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased."" In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited.

Cons
* Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. 
* I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.
* The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper.

*EDIT: 
I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:

(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that ""everyone knows"" what is meant when the actual claim is misleading.

(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.

(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.

The variance of the policy gradient estimator, subject to a baseline ""phi,"" is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from ""phi(a,s)"", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:
""We expect this to be the case when single actions have a large effect on the overall discounted
return (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward).""
Please see Sec. 3, ""Policy Gradient Variance Decomposition"" of the Mirage paper for further details.
The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. 

It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in ""A Better Second Order Baseline"" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.

(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.

(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation.","The sentiment score is -70 because the reviewer does not recommend publication and identifies several significant issues with the paper. They state the novelty and scale of contribution are too low for ICLR, and point out incorrect assertions that reduce the value of the work. However, it's not maximally negative as they do note some positives like the paper being well-written. The politeness score is 20 because the reviewer uses generally professional language and offers constructive feedback, even praising aspects of the paper. However, they are also quite direct in their criticisms, which prevents a higher politeness score. The reviewer balances critique with some positive comments and suggestions for improvement, maintaining a respectful but frank tone throughout.",-70.0,20.0
"This paper proposes a new policy gradient method for reinforcement learning.
The method essentially combines SARAH and trust region method using Fisher information matrix.
The effectiveness of the proposed method is verified in experiments.

SARAH is a variance reduction method developed in stochastic optimization literature, which significantly accelerates convergence speed of stochastic gradient descent.
Since the policy gradient often suffers from high variance during the training, a combination with variance reduction methods is quite reasonable.
However, this work seems to be rather incremental compared to a previous method adopting another variance reduction method (SVRG) [Xu+2017, Papini+2018].
Moreover, the advantage of the proposed method over SVRPG (SVRG + policy gradient) is unclear both theoretically and experimentally.
[Papini+2018] provided a convergence guarantee with its convergence rate, while this paper does not give such a result.
It would be nice if the authors could clarify theoretical advantages over SVRPG.

Minor comment:
- The description of SVRG updates in page 2 is wrong.
- The notation of H in Section 3.1 (""ODE analysis"") is not defined at this time.
",,,
"The paper considers the problem of (Generalized) Zero-Shot Learning. Most zero-shot learning methods embed images and text/attribute representations into a common space. The main difference here seems to be that Variational AutoEncoder (VAEs) are used to learn the mappings that take different sources as input (images and attributes). 
As in JMVAE (Suzuki et al., 2016) (which was not proposed for zero-shot learning), decoders are then used to reconstruct objects from the latent space to the input sources.

My main concerns are about novelty. The contribution of the paper is limited or not clear at all, even when reading Section C in the appendix. The proposed approach is a straightforward extension of JMVAE (Suzuki et al., 2016) where a loss function is added (Eq. (3)) to minimize the KL divergence between the outputs of the encoders (which corresponds to optimizing the same problem as most zero-shot learning approaches).
The theoretical aspect of the method is then limited since the proposed loss function actually corresponds to optimize the same problem as most zero-shot learning approaches but with VAEs.

Concerning experiments, Generalized Zero shot learning (GZSL) experiments seem to significantly outperform other methods, whereas results on the standard zero-shot learning task perform as well as state-of-the-art methods. 
Do the authors have an explanation of why the approach performs significantly better only on the GZSL task?

In conclusion, the contributions of the paper are mostly experimental. Most arguments in the model section are actually simply intuitions.


after the rebuttal:
After reading the different reviews, the replies of the authors and the updated version, my opinion that the ""explanations"" are simply intuitions (which is related to AnonReviewer3's concern ""Regarding advantages of learning a joint model as opposed to unidirectional mappings"") has not been completely addressed by the authors. Fig. 4 does address this concern by illustrating their point experimentally. However, I agree with AnonReviewer3 that the justification remains unclear.","The sentiment score is -50 because the reviewer expresses significant concerns about the novelty and theoretical contributions of the paper, stating that 'the contribution of the paper is limited or not clear at all' and that 'the theoretical aspect of the method is limited'. However, they do acknowledge some positive experimental results, which prevents the score from being more negative. The politeness score is 20 because the reviewer maintains a professional and objective tone throughout, avoiding harsh language. They use phrases like 'My main concerns are' and 'Do the authors have an explanation', which are polite ways of expressing criticism and asking for clarification. The reviewer also acknowledges positive aspects of the work, showing a balanced approach. However, the overall critical nature of the review prevents the politeness score from being higher.",-50.0,20.0
"The proposed approach aims to mitigate catastrophic forgetting in continual learning (CL) problems by structure learning: determining whether to reuse or adapt existing parameters, or initialise new ones, when faced with a new task. This is framed as an architecture search problem, applying ideas from Differentiable Architecture Search (DARTS). The approach is verified on the Permuted MNIST dataset and evaluated on the Visual Decathlon, showing an improvement.

I think this is an interesting idea with potential, and is worth exploring, and the paper is well-structured and easy to follow.

Unfortunately, I feel the paper fails to consider recent work on CL, both in terms of discussion and benchmarking. The only previous work that is compared is EWC, on permuted MNIST, and the Visual Decathlon performance is only compared to simple baselines (such as adding an adapter or fine tuning) which makes it difficult to gauge the contribution.
There are recent works, some with better results on more difficult problems, such as Variational Continual Learning [1], Progress and Compress [2], or (Variational) Generative Experience Replay [3][4].
Given the approach is based on dynamically adding parameters or modules, Progressive Networks and Dynamically Expandable Networks (both cited) are especially relevant and should be compared (I believe the former may be related to the “adapter” baseline, but this should be made explicit).

I have some questions / discussion points:
- What's the intuition behind implementing the “adapt” operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?
- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the ""adapt"" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1×1 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)
- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.
- For the ""reuse - tuned"" case, isn’t the model effectively maintaining a new network for each task?

I also have a number of other comments:
- Reference to figure in page 6 should be figure 4, not 5.
- I think the readability of the paper would benefit from another few proofreads; there are a number of grammatical issues throughout, and several sentence fragments, eg. in the top para of page 2: “..., it has the potential to encourage information sharing. Since now the irrelevant part can be handled…”.

I would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.

[1] Nguyen, Cuong V., et al. ""Variational Continual Learning."" ICLR, 2018.
[2] Schwarz, Jonathan, et al. ""Progress & Compress: A scalable framework for continual learning."" ICML, 2018.
[3] Shin, Hanul, et al. ""Continual learning with deep generative replay."" NIPS, 2017.
[4] Farquhar, Sebastian, and Yarin Gal. ""Towards Robust Evaluations of Continual Learning."" arXiv, 2018.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting idea and potential, they express significant concerns about the lack of consideration for recent work and comparisons to stronger baselines. The reviewer also points out several issues and questions about the methodology. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. They use phrases like 'I think,' 'I feel,' and 'I would encourage' to soften their critiques. The reviewer also provides specific suggestions for improvement and additional references, which is helpful and courteous.",-20.0,60.0
"This paper proposed a LBPNet for character recognition, which introduces the LBP feature extraction into deep learning. Personally I think that this idea is interesting for improving the efficiency of CNNs, as traditionally LBP has been demonstrated its good performance and efficiency in some vision tasks such as face recognition or pedestrian detection. However, I do have the following concerns about the paper:

1. Calculation/Implementation of Eq. 4: I do not quite understand how it derived, and how to use Eq. 3 in calculation. I suggest the authors to explain more details, as this is the key for implementation of LBP layers.

2. Effects of several factors on performance in the experiments are missing: (1) random projection map in Fig. 5, (2) $k$ in Eq. 2, and (3) the order of images for computing RHS of Eq. 3. In order to better demonstrate LBPNet, I suggest to add such experiments, plus training/testing behavior comparison of different networks. 

3. Does this network work with more much deeper?

4. Data: The datasets used in the experiments are all well-aligned. This makes me feel that the RHS of Eq. 3 does make sense, because it will capture the spatial difference among data, like temporal difference in videos. How will the network behave on the dataset that is not aligned well, like affnist dataset?

5. How will this network behave for the applications such as face recognition or pedestrian detection where traditionally LBP is applied?
","The sentiment score is slightly positive (20) because the reviewer finds the idea 'interesting' and acknowledges the potential for improving CNN efficiency. However, they express several concerns, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms constructively (e.g. 'I suggest'), and acknowledges potential merits. They maintain a professional tone without harsh language. The review balances positive aspects with areas for improvement, using phrases like 'I do have the following concerns' rather than outright criticism.",20.0,60.0
"The main context for this paper is two recent publications: Giusti et al.’s ""A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots” (2016) and Smolyanskiy et al.’s ""Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness” (2017). 

Giusti introduced a dataset of trail images (later called the “IDSIA dataset”) acquired by having a hiker wear three head-mounted cameras. The forward facing image is associated with a label “go straight”, whereas the two side images are associated with labels for “go left” and “go right”. Giusti then trained a convolutional neural network to predict these labels and used the network to guide a ""quadrotor micro aerial vehicle”. 

Smolyanskiy improves on Giusti’s work by (1) gathering additional trail image data using three cameras mounted to face forward but with lateral offsets and (2) using this additional data to train a 6 output neural network (“Trailnet”) which predicts both view orientation and lateral offset. In addition, they also combined predicted pose relative to the trail with predictions of localized objects and a depth map for potential obstacles. They compared several neural network architectures for predicting the view angle on the IDSIA data as well as the closed-loop performance of each network in avoiding collisions while operating within a UAV on a previously unseen trail. Though Trailnet did not achieve the highest accuracy (84% vs. the max 92% achieved by ResNet-18), it was the only network that achieved 100% collision avoidance on their UAV test course. 

This paper, ""A CASE STUDY ON OPTIMAL DEEP LEARNING MODEL FOR UAVS”, attempts to evaluate two potentially better convolutional neural networks for UAV trail guidance. They fine tune pertained Inception-Resnet and MobileNet models to predict the IDSIA dataset. These then both achieve better accuracy on the IDSIA test set and were analyzed for inference time and power consumption. These two models are then run through a single simulated path, where both seem to perform adequately across 2 turns in the path. 

This paper has a variety of essential flaws.

1. A large portion of the text is devoted to their hardware and UAV control but they were not able to actually run models on a physical UAV ""due to a hardware bug we were facing with the FCU”. 
2. The paper claims to ""introduce to the best of our knowledge, a very first comparative study of three algorithms in order to find a better motion control of a drone for detecting a trail”. This is a confusing claim since a comparison of neural network architectures is a central part of the evaluation in the Smolyanskiy paper. 
3. The higher accuracy on view orientation does not seem relevant since it was also achieved by Smolyanskiy et al. with networks that they then showed performed worse when combined with object detection, obstacle depth inference and combined controller.
4. The sentence ""An important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying an autonomous drone” appears to have been plagiarized from “Learning to Fly by Crashing” (2017) which contains ""A important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying in an indoor environment”. ","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper has 'a variety of essential flaws' and proceeds to list four major issues, including inability to run models on a physical UAV, questionable claims of novelty, irrelevance of higher accuracy results, and potential plagiarism. These criticisms significantly outweigh any positive aspects mentioned. The politeness score is -20 because while the reviewer maintains a professional tone overall, the language becomes increasingly critical and direct, especially when pointing out flaws. The use of phrases like 'essential flaws' and the direct accusation of plagiarism contribute to a somewhat impolite tone. However, the review isn't overtly rude, maintaining some level of objectivity in its critique.",-70.0,-20.0
"=== Post-rebuttal update ===

The authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.

Hence, I've increased my score for this paper.

=== Pre-rebuttal review ===
This paper presents a meta-learning approach to zero-shot learning. The idea is to train a correction module which is trained to produce a correction to the output of a previously trained task module. The hypothesis is that the correction should depend on the nature of the training data of the task module, and so the correction module receives as input a representation of the training data of the task module. An episodic approach is then used for training the correction module, whereby many different task modules are trained on various subsets of the total training data, the rest being used as unseen data for the correction module.

The proposed idea is original and the results are strong. Generally, I'd be inclined to see this paper published.

However right now, the paper lacks A LOT of details on how the experiments were run. I would like to see these answered in the rebuttal, before I consider raising my rating for this paper:
- What are the architectures used for M_T and M_C?
- What distance functions was used for training?
- What optimizer was used for training?
- How was convergence established in the inner and outer while loops of algorithm 1?
- Text mentions that before evaluation, M_T is trained on all data in D_S. How is this done exactly (e.g. how is convergence assessed)?
- How is the T_S computed exactly?
- How expensive is it to run Algorithm 1 (i.e. to train the correction module)? Since a new task module M_T needs to be trained for each subset S^s, it seems like it might be expensive to run... if not, why?

I would also strongly suggest the authors release their code if this paper ends up being published.

In summary:

Pros
- Claims SOTA results on two good benchmarks for zero-shot learning
- Approach is original

Cons
- Paper lacks a lot of methodological and experimental details

Some minor details:

- ""We found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset."" => I don't understand what this means. Isn't the output of the task module already trained to classify samples from its training dataset? So why is this additional single hidden layer needed?
- Typos:
  - on few shot learn => on few shot learning
  - but needs not => but need not
  - image image classification => image classification
  - the the compatibility => the compatibility
  - psuedo => pseudo
  - ""The task module is trained to minimize"" => that reads like an unfinished sentence
  - \hat{\mu}_U \hat{\mu}_U => \hat{\mu}_U 
  - inputted => input
  - FOr => For
  - it's inputs => its inputs
  - otherhand => other hand","Sentiment score (20): The review starts with a positive tone, acknowledging the originality of the idea and strong results. The reviewer is 'inclined to see this paper published'. However, they express concerns about lacking details, which tempers the overall positivity. The post-rebuttal update indicates an increase in score, suggesting an improvement in sentiment. This mix of positive and negative elements, with a slight lean towards positive, justifies a mildly positive score of 20.

Politeness score (60): The reviewer maintains a professional and respectful tone throughout. They use polite phrases like 'I'd be inclined to see this paper published' and 'I would like to see these answered'. The critique is presented constructively, with clear explanations of what needs improvement. The reviewer also acknowledges the authors' efforts in the rebuttal. However, the directness of some statements (e.g., 'the paper lacks A LOT of details') prevents a higher politeness score. Overall, the review is more polite than neutral, hence a score of 60.",20.0,60.0
"AFTER REBUTTAL:
I think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. 

------------
The method tackles the problem of interpretability that is a very important issue for usually black-box deep networks. Unfortunately it is not very clear how is the achieved. I have read several times the part explaining the influence maps and the clustering based on them and it still doesn't make a lot of sense to me. I think that part has to be better justified and exposed. Moreover, results do not support the claim which makes me doubt even more about how effective the method proposed actually is. In conclusion, I think that better exposition and more solid experimental analysis is needed.  

Also please check some writing problems: 

> Introduction: 
""to acquire a generative function mapping a latent space (such as Rn)"" >  difficult to read, rephrase. 
""making it difficult to add human input"" > confusing. What do you mean by human input? I assume you refer to having control to make decisions about design. 

> Section 3.1
""the internal variable may leave the manifold it is implicitly embedded in as a result of the model’s training"" : not clear, rephrase. 

","The sentiment score is -70 because the reviewer clearly states that the paper is not ready for publication and points out several significant issues with the paper, including problems with structure, exposition, and experimental analysis. The reviewer also expresses doubt about the effectiveness of the proposed method and states that the results do not support the claims made. However, it's not entirely negative as the reviewer acknowledges the importance of the topic and offers suggestions for improvement. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They offer hope for future improvement and frame their criticisms as 'guidelines for improving the paper'. The reviewer also uses polite phrases like 'I think' and 'please check' when pointing out issues, which softens the criticism. However, the overall tone is still quite direct and critical, preventing a higher politeness score.",-70.0,20.0
"This paper presents an empirical analysis of the convergence of deep NN training (in particular in language models and speech).

Studying the effect of various hyperparameters on the convergence is certainly of great interest. However, the issue with this paper is that its analyses are mostly *descriptive*, rather than conclusive or even suggestive. For example, in Figure 2, it is shown that the convergence slope of Adam is steeper than that of SGD, when the x-axis is the model size. Very naturally I would be interested in a hypothesis like “Adam converges quicker than SGD as we increase the model size”, but there is no discussion like that. Throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice. I don’t have a good take-away after reading it.

The writing of this paper also needs to be improved significantly. In particular, lots of statements are made casually without justification. For example,

“If hidden dimension is wide enough to absorb all the information within the input data, increasing width obviously would not affect convergence” -- Not so obvious to me, any reference? 

“Figure 4 shows a sketch of a model’s convergence curve ...” -- it’s not a fact but only a hypothesis. For example, what if for super large models the convergence gets slow and the curve gets back up again?

In general, I think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results.","The sentiment score is -30 because while the reviewer acknowledges the topic is interesting and important, they express significant concerns about the paper's analysis being mostly descriptive rather than conclusive, lack of clear takeaways, and need for improved writing. The slightly negative score reflects these criticisms while recognizing the potential value of the research. The politeness score is 20 because the reviewer uses generally respectful language and offers constructive criticism. They acknowledge the paper's merits and potential while clearly stating areas for improvement. The tone is professional and not personally critical, though direct in pointing out flaws.",-30.0,20.0
"In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. 
The final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).


The authors mainly combine a few previous approaches ""Self-Imitation Learning,"" ""Automatic Goal Generation for Reinforcement Learning Agents,"" and ""Curiosity-driven exploration by self-supervised prediction"" to design this algorithm which makes this approach less novel.

General comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many others

The main idea of the algorithm is clear, but the description of the pieces is missing.

It is not clear in stochastic setting how well this approach will perform. 

The authors state that
""Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments.""
I took a look at this paper and could not find neither proof or quantification of ""effective""-ness. Please clarify what the meaning this statement is.

Why s=s' is ambiguous to the inverse dynamics?

What is the definition of acc in fig2?

why (consin+1)^3/8 is chosen?
","The sentiment score is -30 because the review is generally critical, pointing out several issues with the paper such as lack of novelty, missing definitions, and unclear aspects. However, it's not entirely negative as it acknowledges that the main idea is clear. The politeness score is 20 because the reviewer uses neutral language and phrases criticisms as questions or suggestions rather than direct attacks. They use phrases like 'Please clarify' and 'It is not clear' which maintain a professional tone. The reviewer also provides specific, constructive feedback, which is helpful and respectful to the authors.",-30.0,20.0
"The paper presents a new version of CIFAR10 that is labelled by multiple people (the test part of the data). They use it to improve the calibration of several image classifiers through “fine-tuning” and other techniques
The title is too general, taking into account that this setting has appeared in classification in many domains, with different names (learning from class distributions, crowd labellers, learning from class scores, etc.). See for instance,
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994863/
http://www.cs.utexas.edu/~atn/nguyen-hcomp15.pdf 
Also, at the end of section 2 we simply reach logloss, which is a traditional way of evaluating the calibration of a classifier, but other options exist, such as the Brier score. At times, the authors mention the trade-off between classification accuracy and cross-entropy. This sounds very much the trade-off between refinement and calibration, as one of the possible decompositions of the Brier score.
The authors highlight the limitations of this work, and they usually mention that the problem must be difficult (e.g., low resolution). Otherwise, humans are too good to be useful. I suggest the authors to compare with psychophysics and possible distortions of the images, or time limits for doing the classifications. 
Nevertheless, the paper is not well motivated, and the key procedures, such as “fine-tuning” lack detail, and comparison with other options.
In section 2, which is generally good and straightforward, we find that p(x|c) being non-overlapping as a situation where uncertainty would be not justified. Overlap would simply say that it is a categorisation (multilabel classification) problem rather than a classification problem, but this is different from the situation where labels are soft or given by several users. 
In the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.
Pros:
-	More information about borderline cases may be useful for learning. This new dataset seems to capture this information.
Cons:
-	The extra labelling is very costly, as the authors recognise.
-	The task is known in the classification literature, and a proper comparison with other approaches is required.
-	Not compared with calibration approaches or other ways where boundaries can be softened with less information from human experts. For instance, a cost matrix about how critical a misclassification is considered by humans (cat <-> dog, versus cat <-> car) could also be very useful, and much easier to obtain.
","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('Pros'), they also highlight several significant limitations and criticisms ('Cons'). The reviewer states that the paper is 'not well motivated', lacks detail in key procedures, and requires more comparison with existing approaches. These criticisms outweigh the positive aspects mentioned.

The politeness score is slightly positive (20) because the reviewer maintains a professional and objective tone throughout. They use neutral language to express their criticisms, such as 'the paper is not well motivated' rather than using harsh or dismissive language. The reviewer also acknowledges some positive aspects of the work ('Pros') and uses phrases like 'I suggest' when offering recommendations, which is polite. However, the score is not higher because the review is primarily focused on critiquing the paper rather than offering extensive praise or encouragement.",-30.0,20.0
"Quality: The paper proposed a new method to learn some physics prior in an environment along with a new SpatialNetwork Architecture. Instead of learning a specific dynamics model, they propose to learn a dynamics model that is action-free, purely learning the extrinsic dynamics.  They formulate this problem as a video prediction problem. A series of experiments are conducted on PhysWorld (a new physics based simulator) and a subset of Atari games.
Clarity: The writing is good.
Originality: This work is original as most of the model-based RL works are focusing on learning one environment instead of common rules of physics.
significance of this work: This work propose an interesting direction to pursue.

cons:
1. In Figure 4, the authors show that a pretrained model can learn faster than random initialization. However, it is hard to ablate the factor that causes this effect.  Does the dynamics predictor learn the physics priors or is it just because it learn the visual prior of the shape of the objects, etc? 
2. The baseline for atari games is quite limited. First of all, 3 out of 5  atari games  in the original PPO paper show that ACER performs better than PPO. (asteroid, breakout, DemonAttack). I think it is better to make some improvement upon state-of-the-art methods.
3. All the experiments are shown with only 3 random seeds, without error bar in the main paper. Although the reward plots are shown in Figure 11. 
4. 5 out of 10 atari games are similar to PPO (according to Figure 11). It's hard to be conclusive when half of the experiments are positive and the rest are not. 
5. Lack of discussion about ego-dynamics. There are physics priors for both the environment and the controller. Usually the controller/agent  requires an action to predict its dynamics. Then why should we omit the ego-dynamics and only model the outer world. 
6. Physics prior usually happen in physical environment. The proposed method works well in the physworld environments. But is there some task that are more realistic than atari games that can leverage the power of physics priors more? It's good that this method works in some atari games. But isn't learning the dynamics of atari games a bit off the topic? 
7. The transfer learning experiments should contain a baseline -- maml/reptile. Since you are learning physics prior, it is fair to add meta-learning baselines for comparison.

I think the direction is interesting and the effort is made well. But the experiments are less convincing than the abstract/introduction.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the originality and interesting direction of the work, they express several significant concerns about the experiments and results. The review starts positively but ends with 'the experiments are less convincing than the abstract/introduction,' indicating overall disappointment. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the effort made, and frames criticisms as suggestions or questions rather than harsh statements. They use phrases like 'I think it is better to' and 'It's good that' which maintain a constructive tone. The reviewer also balances criticism with positive remarks about the work's originality and potential significance.",-20.0,60.0
"The paper proposes an alignment of two manifolds that is performed in a low-dimensional parameter space corresponding to a low-pass ""filtering"" of the Graph Fourier transform obtained from the underlying data graphs for the two manifolds. The numerical results show the quality of the alignment for some toy image datasets and a biological dataset.

The derivation of the technical details of the approach is not clear - see the comments below on Pages 5,6 and 9 in particular. The paper is not clear enough for acceptance at this point.

Detailed comments:

Page 2: Grammar error ""that is invariant batch effects"". When denoising is discussed, can you explain whether this is denoising or simply regularization? When is the selected subspace a good approximation for the ""signal subspace""?
Page 3: Should X^(S) be X^(s)? When W and W(s) are defined, do they also rely on a neighborhood graph? It appears that in the definition of psi_j the eigenvectors phi_j should be obtained from W, not P (which is how they are defined earlier in the page).
Page 4: There is an abuse of notation on f, used both as a linear function on X(s) and an element of X(s).
Page 5: Typos ""exlpained"", ""along the along the"". It is not clear what applying a window to eigenvalues means, or what the notation g_xi(lambda) means. The construction of the filters described here needs to be more explicit. h_xi is undefined. How is H in (1) defined when i = 1?
Page 6: M should be M(s1,s2). Typesetting error in Lambdabar(s). Which matrix is referred to in ""the laplacian eigenvalues of each view""? What is the source and target of the embedding E? How is the embedding applied to data x(s1), x(s2)?
Page 7: Figure 1a appears to have an error in the orientation of one of the blue ""3""s. The text on the arrow between the manifold embeddings does not agree with the notation in the paper. In Figure 1b, it is not clear which image is the original point and which images are the neighbors, or why some images are smaller than others. Results for the other algorithms are missing (why no comparison?). Typo ""Wang&Mahadevan"". Can you be more specific as to why that algorithm was ""unable to recover k-neighborhoods"" in certain cases?
Page 8: Why no comparison with Wang & Mahadevan in Figure 2?
Page 9: There is little description as to how manifold learning is applied in the biological data example. What is the ambient dimensionality and the dimension of the manifolds? How are the ""abundances"" extracted from the data? 
""Which we explore in 4"" -> ""Which we explore in Fig. 4""","The sentiment score is -50 because the review starts with a neutral description of the paper but then states that 'The paper is not clear enough for acceptance at this point,' indicating a negative overall sentiment. The reviewer points out several issues and areas needing clarification, which further supports the negative sentiment. However, it's not entirely negative as the reviewer acknowledges some positive aspects like the numerical results showing quality alignment.

The politeness score is 20 because the reviewer maintains a professional and respectful tone throughout. They use neutral language to point out issues, such as 'The derivation of the technical details of the approach is not clear' instead of using harsh criticism. The reviewer also asks questions for clarification rather than making accusatory statements. However, the score is not higher because the review doesn't include explicitly polite phrases or compliments, maintaining a mostly neutral, matter-of-fact tone.",-50.0,20.0
"The paper produces a heat-map of ride-share requests in four cities in the USA. For each city 'block' they produce a time-sequence of 2016 images representing a week-long run from combining each 5-minute interval. This is used with a GAN to produce new data. The techniques applied, although not commonly used in the context of ride sharing / hailing, have been used extensively in other literature.

Some major points on the paper:
1) A GAN approach is normally used to generate more data when enough real data is not obtainable. However, here you only use one week of data from a much larger set. Surely, it would be better to make use of all the weeks available?

2) It is not clear how the heat-maps once produced could be used in the future. There is a hint in the results section about how they can be converted back to ride requests, but this is not clearly defined.

3) There are a number of cases where you state that some approach has been found to be better. However, no evidence is presented for how you determined this to be true.

4) The conversion of data to heat-maps has been used extensively in prior research. Although I'm not directly aware of the use in machine learning I am aware of the use in transport - ""Interactive, graphical processing unit- based evaluation of evacuation scenarios at the state scale"". The novelty here seems to be the application to this specific problem.

More specific points:
- ""Our real ride request data sets consist of all the ride requests for an entire week for the four cities."" - it's not clear - are all four cities used to train one model?

- ""Hence the week-long data should be quite representative."" - This fails to take into account such things as national holidays or other major events such as sports. Did your chosen week contain one of these?

- ""Hence we believe the ride request data sets also reflect the overall urban mobility patterns for these cities."" - This is a huge assumption, which would seem to need evidence to back it up.

- ""and lump together all the ride requests within each interval."" - Presumably you mean that all time values are to the granularity of 5 minutes? 

- ""We arbitrarily sized each block to represent an image of 2424 pixels"" - this seems particularly small.

- ""Each image of that block is labeled with a time interval (for our experiments, the hour in a day)."" - Can the variability within an hour not make this more difficult? 

- ""We find that small networks are appropriate for the training data"" - evidence to support this.

- ""This network is pre-trained on the training data"" - which training data are you referring to?

- ""This is found to increase the efficiency of the training process"" - evidence?

- ""In this work we set the block size for each of the four cities to be
1200  1200 meters"" - how was this value arrived at?

- You state that GPUs were no more efficient, it would be good to see more analysis of this.

- ""To help enhancing the scalability"" -> ""To help enhance the scalability""

- ""and other useful functions"" - such as?

- Figure 4 would probably work better as a speedup graph.

- ""Running times for sampling ride requests from the trained models and stitching the images of all the blocks together are significantly less than the training times, and are not included in these results."" - at least some figures to give an idea of scale should be provided.","The sentiment score is -30 because the review is generally critical, pointing out several major issues with the paper. The reviewer questions the necessity of using a GAN approach, the clarity of the heat-map usage, the lack of evidence for certain claims, and the novelty of the approach. However, it's not entirely negative as the reviewer acknowledges some positive aspects, such as the application to a specific problem. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'Some major points' and 'More specific points' to organize their critique, and often phrase their criticisms as questions or suggestions rather than direct attacks. The language is constructive and aimed at improving the paper, rather than being dismissive or harsh.",-30.0,50.0
"The presented analysis well characterizes the behavior of the spatially transformed adversarial inputs and the proposed defense is empirically confirmed to achieve more accurate and robust classification under attacks.

One concern is that the defender cannot learn whether the adversary employs spatially transformed AEs or pixel-based AEs (or some others). What happens if the classifier trained with the proposed defense accept pixel-based AEs? I recommend the authors to associate spatially transformed AEs with pixel-based AEs to learn whether the proposed defense performs more robustly compared to existing defenses. If the proposed defense method performs well for spatially transformed AEs but is vulnerable to pixel-based AEs, it is useless.

It should be better to discuss more on computational efficiency of the proposed defense since it contains SDP solving. Is the proposed deense works with larger datasets such as CIFAR100 or ImageNet?

 
","The sentiment score is 50 (slightly positive) because the reviewer starts with a positive comment about the analysis and empirical confirmation of the defense's effectiveness. However, they also express concerns and recommendations for improvement, balancing out the overall sentiment. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, framing their concerns as recommendations and suggestions rather than harsh criticisms. They use phrases like 'I recommend' and 'It should be better to discuss' which maintain a constructive and polite tone. The reviewer also acknowledges the strengths of the work before presenting their concerns, which is a polite approach to peer review.",50.0,70.0
"This paper proposes to use a  stochastically quantized network combined with adversarial training to improve the robustness of models against adversarial examples. The main finding is that, compared to a full precision network, the quantized network can generalize to unseen adversarial attacks better while training only on FGSM-perturbed input. This provides a modest speedup over traditional adversarial training.

While the findings are certainly interesting, the method lacks experimental validation in certain aspects. The comparison with other adversarial training methods is not standardized across networks, making the efficiency claims questionable. Furthermore, I am uncertain whether the authors implemented expectation over transformations (EoT) for the C&W attack.  Since the network produces randomized output, vanilla gradient descent against an adversarial loss is likely to fail. It is conceivable that by taking an average over gradients from different quantizations, the C&W adversary would be able to circumvent the defense better. I would be willing to reconsider my review if the authors can address the above weaknesses.

Pros:
- Surprising result showing that quantization leads to improved generalization to unseen attack methods.

Cons:
- Invalid comparison to other adversarial training techniques since the evaluated models are very different.
- Lack of evaluation against EoT adversary.
- Algorithm 1 is poorly presented. I'm sure there are better ways of expressing such a simple quantization scheme.
- Figures 2 and 3 are uninteresting. The fact that the model is robust against adversaries implies that the activations remain unchanged when presented with perturbed input.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the findings as 'interesting' and notes a 'surprising result', they express significant concerns about the paper's experimental validation and methodology. The reviewer states they would be 'willing to reconsider' if the authors address the weaknesses, indicating the review is not entirely negative. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging both pros and cons, and offers constructive criticism. They use phrases like 'I would be willing to reconsider' and 'I am uncertain whether', which maintain a polite tone while expressing concerns. The reviewer also provides specific suggestions for improvement, which is a courteous approach in academic peer review.",-20.0,50.0
"The paper makes its intent plainly clear, it wants to remove the assumption that demonstrations are optimal.  Thus it should show that in a case that some demonstrations are bad, it outperforms other methods which assume they are all good. The method proposed, while interesting, well-conceived and potentially novel, is not convincingly tested to this end. 

The paper should also show that the method can detect the bad demonstrations, and select the good demonstrations. 

The experiments are on toy tasks and not existing tasks in the literature. Why not use an existing dataset/domain and simply noise up the demonstrations?

Furthermore, many crucial details are omitted, such as the nature of the heuristic function K, and how precisely the weighting $c_i$ is adapted (section 4.4). Is it done by gradient descent? We would have to know what K is, and if it is differentiable to know this.

Also the writing itself needs a thorough revision.

I think there may well be promise in the method, but it does not appear ready for publication.
","The sentiment score is -50 because the review is generally critical, pointing out several shortcomings of the paper, but also acknowledges some positive aspects ('interesting, well-conceived and potentially novel'). The reviewer concludes that the paper is not ready for publication, which is a negative outcome. The politeness score is 0 (neutral) because the language used is professional and objective, without being particularly polite or rude. The reviewer provides constructive criticism without using overly harsh language, but also doesn't use explicitly polite phrases. The tone is matter-of-fact and focused on the content of the paper rather than personal opinions or emotions.",-50.0,0.0
"In this paper, the novel MAOP model is described for self-supervised learning on past video game frames to predict future frames. The presented results indicate that the method is capable of discovering semantically important visual components, and their relation and dynamics, in frames from arcade-style video games. Key to the approach is a multi-level self-learning approach:  more abstract stages focus on simpler problems that are easier to learn, which in turn guide the learning process at the more complex stages.
A downside is that it the method is complex, consisting of many specific sub-components and algorithms, which in turn have again other sub-components. This makes the paper a long read with a lot of repetition, and various times the paper refers to the names of sub-components that are only explained later. Other methodological details that are relevant to understand how the method operates are described in the Appendices. I expect that if the paper would be better structured, it would be easier to understanding how all the parts fit together. Another downside of this complexity is that the method seems designed for particular types of video game frames, with static backgrounds, a fixed set of objects or agents. It is unclear how the method would perform on other types of games, or on real-world videos. While the method therefore avoids the need for manual annotation, it instead encodes a lot of domain knowledge in its design and components.
I also didn't fully understand how the self-supervised model is used for Reinforcement Learning in the experiments. Is the MAOP first trained, and the fixed to perform RL with the learned agent models, or is the MOAP learned end-to-end during RL?

Pros:
+ MAOP seems successful on the tested games in the experiments
+ Demonstrates that, with a sufficiently engineered method, self-supervised learning can be used to discover different types of objects, and their dynamics.

Cons:
- writing could be improved, as the methodology currently reads as a summation of facts, and some parts are written out of order, resulting in various forward references to components that only become clear later. Several times, the paper states that some novel algorithm is used, but then provides no further explanation in the text as all description of this novelty is deferred to an appendix. 
- method does not seem generic, hence it is unclear how relevant this architecture it is to other use cases
- many hyperparameters for the individual components, algorithms. Unclear how these parameter setting affect the results

Below are more detailed comments and questions:

General comments:
* The proposed MOAP method consists of many subalgorithms, resulting in various (hyper)parameters which may impact the results (e.g. see Appendix A, B). Appendix D lists several used hyperparameter settings, though various parameters for the algorithms are still missing (e.g. thresholds alpha, beta in Algo.2). Were the used parameters optimized? How are these hyperparameters set in practice? How does changing them impact your results?
* Methods seems particularly designed for 'video games', where the object and background structures have well defined sizes, appearance, etc. How will the MOAP fair in more realistic situations with noisy observations, occluded objects, changing appearances and lighting conditions, etc.?
* How about changing appearance of an agent during an action, e.g. a 'walking animation' ? Can your method learn the sequence of sprites to accurately predict the next image? Is that even part of the objective?
* Appendix D has important implementation details, but is never mentioned in the text I believe! Didn't realize it existed on first read through.

* Introduction:
	* What prediction horizon are you targeting? 1 step, T steps into the future, 1 to T steps in the future simultaneously?
	What are you trying to predict? Object motion? Future observations?
	* ""... which includes a CNN-based Relation Net to ... "", the names Relation Net, Inertia Net, etc.. are used as if the reader is expected to know what these are already. If these networks were introduced in related work already, please add citations. Otherwise please rephrase to clarify that these are networks themselves are part of your novel design.

* Section 3.1
	* ""It takes multiple-frame video images ... and produce the predictions of raw visual observations."". As I understand from this, the self-supervised approach basically performs supervised learning to predict a future frame (target output) given past frames (input). I do not understand how this relates to Reinforcement Learning (RL) as mentioned in the introduction and Related Work. Is there still some reward function in play when learning the MAOP parameters? Or is the idea to first self-supervised learn the MAOP, and afterwards fix its parameters and use it in separate a RL framework? I believe RL is not mentioned anymore until Section 4.2. This connection between self-supervised and reinforcement learning should be clarified, or otherwise the related work should be adjusted to include other (self-supervised) work on predicting future image frames.
	* ""An object mask describes the spatial distribution of an object ..."" Does the distribution capture uncertainty on the object's location, or does it capture the spread of the object's extent ('mass distribution') ?
	* ""Note that Object Detector uses the same CNN architecture with OODP"". What does OODP stand for? Add citation here. (first mention of OODP is in Experiments section)
	* ""(similar with Section 3.2)"" → ""similar to"". Also, I find it a confusing to say something is similar to what will be done in a future section, which has not yet been introduced. Can you not explain the procedure here, and in Section 3.2 say that the procedure is ""similar to Section 3.1"" instead?
	* ""to show the detailed structure of the Effect Net module."" First time I see the name 'Effect Net', what is it? This whole paragraph different nets are named, with a rough indication of their relation, such as ""Dynamic Net"", ""Relation Net"" and ""Inertia Net"". Is ""Effect Net"" a different name for any of the three previous nets? The paper requires the reader to puzzle from Fig.2 that Relation Net and Inertia Net are parts of Effect Net, which in turn is part of Dynamics Net. This wasn't clear from the text at all.

* Section 3.2:
	* p7.: ""Since DISN leans"" → ""Since DISN learns"" ?
	* There are many losses throughout the paper, but I only see at the end of Section 3.1 some mentioning that multiple losses are combined. How is this done for the other components, .e.g is the total loss for DISN a weighted sum of L_foreground and L_instance ? Are the losses for all three three MAOP levels weighted for full end-to-end learning?
	* This section states various times ""we propose a novel [method]"", for which then no explanation is given, and all details are explained in the Appendix. While the Appendix can hold important implementation details, I would still expect that novelties of the paper are clearly explained in the paper itself. As it stands, the appendix is used as an extension of the methodological section of an already lengthy paper.
	* ""Conversely, the inverse function is ... "" M has a mask for each of the n_o ""object classes"", hence the ""Instance Localization Module"" earlier to split out instances from the class masks. So how can there be a single motion vector STN^-1(M,M') if there are multiple instances for an object mask? How will STN^-1 deal with different amount of instances in M and M' ?

* Section 3.3:
	* What is the output of this level? I expect some mathematical formulation as in the previous sections, resulting in some symbol, that is then used in Section 3.2. E.g. is the output ""foreground masks F"" (found in Appendix A) ?  This paper is a bit of a puzzle through the pages for the reader.

* Section 4: 
	* ""We compare MAOP with state-of-the-art action-conditioned dynamics learning baselines, ..."" Please re-iterate how these methods differ in assumptions, what they model, with respect to your novel method? For instance, is the main difference your ""novel region proposal method"" and such? Is the overall architecture different? E.g. explain here already the AC Model uses ""pixel-level inference"", and that OODP has ""lacks knowledge on object-to-object relations"" to underline their difference to your approach, and provide context for your conclusions in Section 4.1.

* Appendix A:
	* Algorithm 1, line 7: ""sample a pixel coordinate"" → is this non-deterministically sampling?
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper ('MAOP seems successful', 'demonstrates that...self-supervised learning can be used'), they also highlight significant weaknesses. These include the complexity of the method, lack of clarity in writing, and concerns about generalizability. The cons outweigh the pros in the review.

The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They use polite phrases like 'I expect that' and 'I didn't fully understand' rather than making blunt criticisms. The reviewer also balances negative points with positive ones and provides detailed, helpful feedback for improvement.

The language is not overly formal or deferential, which keeps it from scoring higher on politeness, but it consistently shows respect for the authors' work while providing honest critique.",-20.0,50.0
"The paper proposes a greedy-like algorithm for sparse recovery that uses nearest neighbors algorithms to efficiently identify candidates for the support estimates obtained at each iteration of a greedy algorithm. It assumes that the norms of the columns of the matrix A are one to be able to change the project-and-sort step into a nearest neighbors search.

It is not clear what the value of Fact 1 is, given that none of the sparse recovery algorithms discussed here actually performs ell0 norm minimization. Additionally, it is common in theoretical analysis of sparse recovery to assume that the columns of the matrix A have unit norm. In fact, the RIP implies that the columns of the matrix must have norm within delta of 1. Nonetheless, it would be useful to have a discussion of the effect that having non-unit column norms would have on the proposed approach.

Similarly, Fact 2 is almost self-evident; I suggest to discard the proof.

The equivalence of Definition 1 and the statement involving ps and qs needs to be shown more clearly. The statement in Definition 1 is given in terms of distances (ball radiuses), not counts of neighbors.

I suggest swapping the use of CoSaMP and AIHT - the theoretical results of the paper refer to AIHT, so it is not clear why the algorithm itself is relegated to the supplementary material.

It is not clear how d0 is to be computed to implement Accelerated AIHT.

For Theorem 1, the authors should comment on when the assumption ""xtilde(t) converges linearly to a k-sparse signal with rate c"".

In Figures 1 and 2, does ""residual"" refer to the difference between x and xtilde, or b and Axtilde? 

Minor comments:
Typo in page 5 ""¿""
Grammar error in page 6 ""characterizing of the difficulty"".","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's proposal, they raise several concerns and suggest multiple changes. The reviewer questions the value of certain facts, asks for clarifications, and suggests restructuring parts of the paper. However, it's not entirely negative as the reviewer also provides constructive feedback. The politeness score is moderately positive (50) because the reviewer uses neutral language and phrases suggestions politely (e.g., 'I suggest', 'it would be useful'). They avoid harsh criticism and frame their points as recommendations rather than demands. The reviewer maintains a professional tone throughout, even when pointing out errors or areas needing improvement.",-20.0,50.0
"- The authors study the problem of negative transfer in representation learning, and propose to use the formulation proposed by Ganin & Lempitsky '15 for domain adaptation to reduce negative transfer. Instead of defining the domain classification as the adversarial task to learn a domain-independent representation, they collect a set of classification problems irrelevant to the main task as the adversarial tasks, and aim to learn a representation that focuses only on the primary task. There are very little changes compared to the proposal by Ganin & Lempitsky '15, but the application to solve the problem of negative transfer is interesting.  

- My main concern on the whole argument of the paper is whether the benefits we see in the experiments come from the elimination of negative transfer, or just come from having more training labels from different tasks available. In the main formulation of the approach (equation 7), the authors try to learn a feature representation that works well for the primary task but works poorly for the auxiliary(irrelevant) tasks. If we switch the sign for lambda, then it becomes very similar to traditional multi-task learning. I wonder how the multi-task formulation would compare against the adversarial formulation proposed by the authors. There are reasons to suspect the multi-task formulation will also work better than the logistic regression baseline, since more labels from different tasks are available to learn a better joint representation. It is not clear whether the improvements come from modeling the auxiliary tasks using negative transfer (where the adversarial approach should beat the baseline and multi-task approach), or just come from having more information (where both the adversarial approach and the multi-task approach beat the baseline, but have similar performance). 

- From a practical point of view, it is not easy to decide what prediction tasks are irrelevant. For example, in the birds dataset, I would expect the color and patterns in the body parts to have some correlations (primary_color, upperparts_color, underparts_color, wing_color, etc). In the case of occlusion of the relevant body parts, I could make a guess on the color based on the colors on other parts of the bird. In the ideal case for the current method, I would expect the adversarial approach proposed to learn a representation that mask out all the irrelevant parts of the animal or irrelevant contextual information. Apart from showing improved prediction performance, have the authors perform analysis on the image activation patterns similar to the motivation example in Figure 1 to see if the new approach actually focus on the relevant body parts of the animals? 

- The definition of auxiliary tasks are described in the second last paragraph of 3.3, but it would be clearer if it is also mentioned how they are defined in the experiments section. I went through the whole experiments section having trouble interpreting the results because I could not find the definition of adversarial tasks.  

- Overall I like this paper since it attempts to solve an interesting problem in computer vision, but I would like to see the above question on comparison with multi-task learning answered, or some image activation pattern analysis to provide a more solid argument that the improvements come from elimination of negative transfer. 

","The sentiment score is slightly positive (20) because while the reviewer expresses interest in the paper's approach and finds it 'interesting', they also raise several concerns and questions. The overall tone is constructive but with reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's merits while politely expressing their concerns. They use phrases like 'I like this paper' and 'I would like to see', which are courteous ways of providing feedback. The reviewer also offers suggestions for improvement in a constructive manner, rather than being dismissive or harsh.",20.0,60.0
"This paper proposes a simple improvement to methods for unit pruning. After identifying a unit to remove (selected by the experimenter’s pruning heuristic of choice), the activation of that unit is approximately incorporated into the subsequent unit by “mean replacement”. The mean unit activation (computed on a small subset of the training set) is multiplied by each outgoing weight (or convolutional filter) and added to each corresponding bias instead. Experiments show this method is generally better than the typical method of zero-replacement before fine-tuning, though the advantage is smaller after several epochs of fine-tuning.

While I find this paper intriguing and applaud the extensive experimentation and documentation, I have some concerns as well:
	1. There are unanswered questions about how this method relates to existing work. It is not clear from the paper how the “mean replacement” method differs from the two most related works (Ye, 2018) and (Morcos, 2018), which propose variations on replacing units with constant values or mean activations, respectively. Also, why does the method in this paper seem to yield good results, while the related method (Morcos, 2018) yields “inferior performance”?
	2. The results are stated to only apply to networks “without batch normalization”. The reason seems intuitive: any change that can be merely rolled into the bias will be lost after normalization (depending perhaps on the ordering of normalization and the non-linearities). This leaves an annually decreasing fraction of networks to which this method is applicable, given the widespread use of batch norm.
	3. Critically, it’s difficult to compare this work against other pruning works given the lack of results reported in terms of final test error and the lack of the ubiquitous “error vs. %-pruned” plot.
	
Overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues). I’m rating this paper below the threshold given the limitations, but I’m willing to consider an upgrade to the score if these questions are addressed.

Other notes:
	4. What is your definition of a convolutional “pruning unit”? (From context, I’d presume it corresponds to an output activation map.)
	5. In Section 3.1:  replace “in practice, people …” with  something like “in practice, it is common to”.
	6. In Equation 3, is the absolute value of the pruning penalty used in the evaluation?
	7. In the footnote in Section 3.2, how many training samples are needed for a good approximation? How many are used in the experiments?
	8. There are a couple typos in Section 3.2: “replacing -the- these units with zeroes” and “each of these output*s*”.
	9. Presumably the “\Delta Loss after pruning” in Figures 2-6 is validation or test loss, not training loss? Is this the cross-entropy loss? Also, it would be much easier to compare to other papers if test accuracy were reported instead or in addition.
	10. In Figure 4, the cost to recover using fine-tuning seems to be only roughly 2% of the original training time. How much time is lost to the process of computing the average unit activation?

UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions.","The sentiment score is -30 because while the reviewer finds the paper 'intriguing' and 'applauds the extensive experimentation', they express several concerns and ultimately rate the paper 'below the threshold'. The overall tone is more negative than positive, but not extremely negative. The politeness score is 60 because the reviewer uses polite language throughout, such as 'I find this paper intriguing' and 'I applaud the extensive experimentation'. They also phrase criticisms constructively, using phrases like 'I have some concerns' rather than direct attacks. The reviewer maintains a professional and respectful tone, even when pointing out limitations. However, it's not extremely polite, as it maintains a direct and critical stance where necessary.",-30.0,60.0
"Prons: 
This paper provides a simple and economic technique to accelerate adaptive stochastic algorithms. The idea is novel and preliminary experiments are encouraging.

Cons: 
1.	The theoretical analysis for AAMSGrad is standard and inherits from AMSGrad directly. Meanwhile, the convergence rate of AAMSGrad merely holds for strongly convex online optimization, which does not match the presented experiments. Hence, the theoretical contribution is limited. 
2.	The current experiments are too weak to validate the efficacy of the proposed accelerated technique. We recommend the authors to conduct more experiments on various deep neural networks. ","The sentiment score is slightly positive (20) because the review starts with positive aspects ('novel idea', 'encouraging experiments') but also includes significant criticisms. The overall tone is more constructive than negative. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers recommendations rather than harsh criticisms. They acknowledge the paper's strengths before presenting areas for improvement, which is a polite approach. The use of phrases like 'we recommend' instead of more directive language also contributes to the politeness.",20.0,50.0
"Summary: This paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The basic idea is to define accelerated gradient flows on the space of probability distribution. Because the defined flows include a term depending on the current distribution of the system, which is difficult to compute in general, the authors introduce an interacting particle approximation as a practical numerical approximation. The experiments are a proof-of-concept on simple illustrative toy examples.

Quality: The ideas are generally of high quality, but I think there might some typos (or at least some notation I did not understand). In particular
- tilde{F} is not defined for Table 1
- the lyapunov function for the vector column of table one includes a term referring to the functional over rho. I think this is a typo and should be f(x) - f(xmin) instead.

Clarity: The paper is generally clear throughout.

Originality & Significance: The paper is original to my knowledge, and a valuable extension to the interesting literature on the Bregman Lagrangian. The problem of simulating from probability distributions is an important one and this is an interesting connection between that problem and optimization.

Pros:
- An interesting extension that may fuel future study.

Cons:
- This algorithm appears naively to have an O(n^2) complexity per iteration, which is very expensive in terms of the number of particles. Most MCMC algorithms would have only O(n) complexity in the number of particles. This limits its applicability.
","The sentiment score is 60 (positive) because the reviewer expresses that the ideas are of high quality, the paper is clear, original, and valuable. They mention it as an 'interesting extension' that may fuel future study. However, they also point out some potential typos and a significant con regarding computational complexity, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths while constructively pointing out areas for improvement. They balance positive feedback with specific, helpful critiques without using harsh or dismissive language.",60.0,80.0
"Summary:

This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic.

Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.

- Quality: Paper is technically complex, but based on simple ideas. In the case of
   infinite mixtures, it is not clear what is done in the end in the experiments.
   Experimental results are rather poor, given state-of-the-art.
- Clarity: The paper is not hard to understand. What is done, is done cleanly.
- Originality: The idea of putting a mixture model on the global parameters is not
   surprising. Important questions, such as how to make this faster, are not
   addressed.
- Significance: The only comparative results on miniImageNet are worse than the
   state-of-the-art by quite a margin (admittedly, the field moves fast here, but it
   is also likely these benchmarks are not all that hard). This is even though better
   performing methods, like Versa, are much cheaper to run

While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea.

State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53:
- Versa: https://arxiv.org/abs/1805.09921.
   Importantly, this method uses a simpler model (logistic regression head models)
   and is quite a bit faster than MAML, so much faster than what is proposed here
- BMAML: https://arxiv.org/abs/1806.03836.
   This is also quite complex and expensive, compared to Versa, but provides good
   results.

Other points:
- You use a set of size N+M per task update. In your 5-way, 1-shot experiments,
   what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5,
   then I wonder why results are branded as 5-way, 1-shot, which to mean means
   that each update can use exactly 5 labeled points.
   Please just be exact in the main paper about what you do, and what main
   competitors do, in particular about the number of points to use in each task
   update.
- Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and
   uses further approximations (ICM, instead of Gibbs sampling).
   Can be seen as a heuristic to evolve the number of components.
   What is given in Algorithm 2, is not compatible with Section 4. How do you merge
   your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid
   that there is always one more (L -> L+1) components? Some threshold must be
   applied somewhere.
   An alternative would be to use split&merge heuristics for EM.
- Results reported in Section 5 are potentially interesting, but entirely lack a
   reference point. The first is artificial, and surely does not need an algorithm of this
   complexity. The setup in Section 5.2 is potentially interesting, but needs more
   work, in particular a proper comparison to related work.
   This type of effort is needed to motivate an extension of MAML which makes
   everything quite a bit more expensive, and lacks behind the state-of-art, which
   uses amortized inference networks (Versa, neural processes) rather than
   gradient-based.
","The sentiment score is -50 because the reviewer expresses several criticisms about the paper, including that the results are not near the state-of-the-art, the method is too expensive to justify mild gains, and the experiments do not show benefits of the idea. However, the reviewer does acknowledge some positive aspects, such as the paper being technically complex and potentially useful, which prevents the score from being more negative. The politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language to express criticisms. They offer constructive feedback and suggestions for improvement, which adds to the politeness. However, the overall critical nature of the review prevents the score from being higher on the politeness scale.",-50.0,20.0
"This paper proposes simple metrics for measuring the ""information density"" in learned representations. Overall, this is an interesting direction. However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain. And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared. 

+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important. 

+ The proposed metrics and simple and intuitive.

+ It is interesting that a few units seem to capture most task specific information. 

- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here. As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task. Yet the metrics proposed depend on supervision in the target domain. If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set. It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks. I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue. 

- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed. The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.

- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.

- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'. 
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting direction and some positive aspects, they also point out several key weaknesses and areas for improvement. The overall tone suggests more concerns than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, balancing criticism with positive comments, and phrases concerns as suggestions rather than harsh criticisms. They use phrases like 'it would be nice to see' and 'I am unconvinced' rather than more confrontational language. The reviewer also starts with positive points before moving to criticisms, which is a polite approach in academic reviews.",-20.0,60.0
"This study explores the class hierarchy to solve many-class few-short learning problem in both traditional supervised learning and meta-learning. The model integrates both the coarse-class and fine-class label as the supervision information to train the DNN, which aims to leverage coarse-class label to assist fine-class prediction. The core part in the DNN is memory-augmented attention model that includes at KNN classifier and Memory Update mechanism. The re-writable memory slots in KNN classifier aim to maintain multiple prototypes used to describe the data sub-distribution within a class, which is insured by designing the memory utility rate, cache and clustering component in Memory Update mechanism. This study presents a relatively complex system that combines the idea of matching networks and prototypical networks.

One of the contributions is that the study puts forward a concept of the many-class few-short learning problem in both supervised learning and meta-learning scenarios, and uses a dataset to describe this problem.

Using the memory-augmented mechanism to maintain multiple prototypes is a good idea. It may be more interesting if its effectiveness can be proved or justified theoretically. Furthermore, it is better to offer some discussion about the learned memory slots in the view of “diverse and representative feature”.

The experiment results in Table 4 and Table 5 compare the MahiNet with Prototypical Net on the mcfsImageNet and mcfsOmniglot dataset. It is better to compare MahiNet with other state-of-the-art works, such as the Relation Network whose performance is higher than Prototypical Net. In addition, if more  challenging datasets  can be further evaluated in the experiments,  the paper  might be more convincing.

In my opinion, the hierarchy information provides the guidance to fine-gained classification, which not only can be added to MahiNet but also the other models. Therefore, to prove its effectiveness, it is better to add hierarchy information to other models for comparison. In addition, regarding the results on the column of 50-5 and 50-10 in Table 4, when the number of class increase to 50, the results are just slightly higher than prototypical network. Considering that the memory update mechanism is of the high resource consumption and complexity, it is better to provide more details about clustering, and training and testing time.
","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the study's contributions and interesting ideas, while also providing constructive criticism and suggestions for improvement. The overall tone is balanced, recognizing both strengths and areas for enhancement. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offering suggestions in a considerate manner (e.g., 'It may be more interesting if...', 'It is better to...') and acknowledging the study's merits. The reviewer maintains a professional and courteous tone, even when pointing out limitations or areas for improvement.",50.0,75.0
"The idea of extending  Riemannian Langevin dynamics to functional spaces is elegant, however it is extremely hard to follow the proposed method as details are kept to a minimum. The finite approximation of the posterior distribution is a function of the parameters theta, however it displays parameters lambda. The couple of sentences: ""Then by sampling λ, we sample a functional f equivalently. The Riemannian Langevin dynamics on the functional space can thus be written as: (6)"" come without a single explanation.

Minor comments
* Max and Whye is the casual version for reference Welling and Teh.
* proper nouns in References should be capitalized",,,
"This paper describes multi-grained entity recognition. Experimental results show that the proposed Multi-Grained Proposal Network achieve better performance on NER tasks.

Major comments:

- A major weakness of this paper is lack of citations to recent related studies. There are studies on nested NER published this June:

A. Katiyar and C. Cardie, Nested Named Entity Recognition Revisited, NAACL/HLT 2018, June, 2018.
M. Ju, et al., A neural layered model for nested named entity recognition, NAACL/HLT 2018, June 2018.

You need to compare these conventional methods to your proposed method.

","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contribution and improved performance in the first paragraph. However, the score is not higher due to the 'major weakness' mentioned. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive criticism without harsh words. The reviewer provides specific suggestions for improvement, which is helpful and courteous. The use of 'you need to' is direct but not impolite in this context of academic review.",20.0,50.0
"This paper proposes a network architecture inspired by the primate visual cortex. The architecture includes feedforward, feedback, and local recurrent connections, which together implement a predictive coding scheme. Some versions of the network are shown to outperform the similar PredNet and PredRNN architectures on two video prediction tasks: moving MNIST and KTH human actions. Finally, the authors provide neural data from monkeys and argue that their network shows similarities to the biological data.

The paper contains intriguing ideas about the benefits of sparse and predictive coding, and the direct comparison to biological data potentially broadens the impact of the work. However, major claims are unsubstantiated, and accuracy and clarity need to be improved to make the manuscript acceptable.

Major concerns:
1. The authors claim that their architecture is more efficient because it uses sparse coding of residuals. Implementation details and some quantitative arguments, ideally benchmarks, need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet.

2. It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network. Does the PredRNN++ have access to as many current and future frames as the C-C net? Is this a fair comparison? Please provide a clearer description of the different versions of your network and how they relate to the baseline models. That section in particular has many confusing typos (frame-by-chunk, chunk-by-frame abbreviations mixed up).

3. In Figure 6, the authors claim that more layers lead to “better” representations. What does “better” mean? It is implied that the networks with more layers actually make the different motions more discriminable. Please quantify this. For example, a linear classifier could be trained on the neural activations. Also, how is this related to the rest of the paper? Do the authors claim that this result is unique to the proposed architecture? In that case, please provide a quantitative comparison to the PredNet or PredRNN++.

4. In Figure 9, the presentation is highly confusing. Plots (c) to (h) are clearly made to look like the monkey data in (b) (nonlinear x-axes?), but show totally different timescales (training epochs vs. milliseconds). Please explain why it makes sense to compare these timescales. Also, what does it mean for a training epoch to have a negative value? 

Minor comments:
1. I don’t understand the “tension” between hierarchical feature representations and residual representations brought up in Section 2. Do the PredNet and PredRNN++ not contain a hierarchy of representations?

2. Figure 1 is not fully annotated and could be clearer. What does the asterisk mean? Why are there multiple arrows between the P’s? What do the small arrows next to the big arrows mean? Please expand the legend. Consider using colors to differentiate between components.

3. I don’t understand Figure 4c. According to the text, this plot shows “effectiveness as a function of time”, but the x-axis is labeled “Layer Number”. What does “effectiveness over time” mean? What does the y-label mean (SSIM per day?)? What is “trunk prediction” (not mentioned anywhere in the text)?

4. For Figure 9, it is pointed out that activity is expected to be lower for E neurons, but is also lower for R and P. This is interesting and also applies to Figure 8, so it would be good to see Figure 8 split up by E/R/P, too. 

5. The word “Figure” is missing before figure references.

6. Please proof-read for typography, punctuation and grammar.","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('intriguing ideas', 'potentially broadens the impact'), they express major concerns and state that 'major claims are unsubstantiated'. The overall tone suggests significant improvements are needed. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and specific suggestions for improvement. They avoid harsh or dismissive language, instead using phrases like 'please explain' and 'consider using'. However, the review is not overly effusive or deferential, maintaining a professional tone.",-30.0,50.0
"This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results.

I understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described.
P1) What is the shape of the posterior distribution?
P2) How does the reparametrization trick work in your case?
P3) How can one choose the layout of the subspaces, or this is also learned?

Moreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. 

When discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect.

In addition, there are other (secondary) questions that require an answer.
S1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces?
S2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right?
S3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript?
S4) Which parameters of this family are learned, and which of them are set in advance?
S5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this?
S6) How is the Lp layout chosen?
S7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE?
S8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed.

Finally, there are a number of minor corrections to be made.
Abstract: latenT
Equation (3) missig a sum over j
Figure 1 has no caption
In (8), should be f(z) and not x.
Before (10), I understand you mean Lp-nested
I did not find any reference to Figure 3
In 4.1, the standard prior and the proposed prior should be referred to with different notations.

For all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.","The sentiment score is -70 because the reviewer recommends rejecting the paper, stating it's 'not mature enough for publication'. They list numerous issues and unanswered questions, indicating a largely negative view. However, they do acknowledge some positive aspects ('interesting results', agreeing with the motivation), preventing the score from being lower. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I understand' and 'Could you please', showing some courtesy. However, the overall critical nature and recommendation for rejection prevent a higher politeness score.",-70.0,20.0
"The paper tries to describe SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction. Assuming TV metric of probabilities, a trajectory is defined which fits to general learning behaviour of distributions.

The issue is that the paper abstracts the actual algorithm, model and data away and the only thing that remains are marginal distributions p(y) and conditional p(y'|y). At this point one can already argue that the result is either not describing real behavior, or is trivial. The proposed trajectory starts with a model that only predicts one-class (low entropy H(y') and high conditional entropy) and ends with the optimal model. the trajectory is linear in distribution space, therefore one obtains initially a stage where H(y') and H(y'|y) increase a lot followed by a stage where H(y'|y) decrease.

This is known to happen, because almost all models include a bias on the output, thus the easiest way to initially decrease the error is to obtain the correct marginal distribution by tuning the bias. Learning the actual class-label, depending on the observed image is much harder and thus takes longer. Therefore no matter what algorithm is used, one would expect this kind of trajectory with a model that has a bias.

It also means that the interesting part of an analysis only begins after the marginal distribution is learned sufficiently well. and here the experimental results deviate a lot from the theoretical prediction. while showing some parabola like shape, there are big differences in how the shapes are looking like.

I don't see how this paper is improving the state of the art, most of the theoretical contributions are well known or easy to derive. There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory)

 Therefore, I vote for a strong reject.","The sentiment score is -80 because the review is highly critical and ends with a 'strong reject' recommendation. The reviewer finds multiple issues with the paper, stating that it doesn't improve the state of the art, its contributions are well known or easy to derive, and there's no actual connection to SGD left. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite dismissive and blunt. Phrases like 'The issue is...', 'I don't see how this paper is improving...', and 'Therefore, I vote for a strong reject' contribute to a somewhat impolite tone. The reviewer doesn't attempt to soften their criticism or acknowledge any positive aspects of the paper, which would be expected in a more polite review.",-80.0,-20.0
"This paper proposed an 


1. For me, the argument of the paper is ambitious. Data augmentation for DNN includes different perspective, including nonlinearity, adversarial etc. Generalization of  spatial and appearance models is not enough. The model formulate from a simple classification setting but does not involve too many for DNN models.  I put more references below. 

2. The experimental results are not strong. Not all strong baselines are included (I put some in the references). The improvements are marginal. Besides, I need more experimental setting information.

3. The writing is not clear. For the related work part, it included many paragraph which are not related to the work, (e.g. GANs). In the introduction part, it did not mention the generalization of both spatial and appearance models, which is the main contribution.  

References:
a. Good Semi-supervised Learning that Requires a Bad GAN
b. Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference 
c. Temporal ensembling for semi-supervised learning","The sentiment score is -50 because the review is generally critical, pointing out several weaknesses in the paper, including an 'ambitious' but insufficient argument, 'not strong' experimental results, and unclear writing. However, it's not entirely negative as it acknowledges the paper's ambition and provides constructive feedback. The politeness score is 0 (neutral) because the reviewer uses direct language without being overtly polite or rude. They state their criticisms plainly (e.g., 'The writing is not clear') without using harsh language, but also without softening their critique with polite phrases.",-50.0,0.0
"The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for ICLR.

The paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years.

The contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this.

1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*.

2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1).

3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight.

4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode.

As the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following:
1. Provide pseudocode for the algorithm.
2. Provide pseudocode for exploration mechanism (unless subsumed by (1)).
3. Add uncertainty estimates to evaluation or at least report all runs.

I am willing to re-consider my decision once these things have been done.","The sentiment score is -50 because the reviewer expresses several criticisms and concerns about the paper, describing it as 'very incremental' and 'sparse on details'. They also state that it's 'difficult for me to accept' in its current state. However, the score isn't lower because the reviewer does acknowledge some positive aspects (e.g., 'The scope of the paper is a good fit for ICLR') and offers constructive feedback for improvement. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'I encourage the authors to' and 'I am willing to re-consider my decision', which show respect and openness. The language isn't overly polite, but it avoids rudeness and maintains a constructive approach.",-50.0,20.0
"The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). 

Though it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. 
  
On page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?

The URL in References looks out of bound. 


","The sentiment score is -30 because while the reviewer acknowledges some interesting aspects of the paper, they express significant criticism about its lack of innovation and depth. The overall tone is more negative than positive, but not extremely negative. The politeness score is 20 because the reviewer uses polite language and constructive criticism, such as 'it will be better to explain' and 'interesting paper', but doesn't go out of their way to be overly polite. They maintain a professional tone throughout, balancing positive comments with critiques.",-30.0,20.0
"This work proposes a novel tree structure positional embedding by uniquely representing each path in a tree using a series of transformation, i.e., matmul for going up or down the edges. The tree encoding is used for transformer and shows gains over other strong baselines, e.g., RNNs, in synthetic data and a program translation task.

Pros:

- An interesting approach for representing tree structure encoding using a series of transformation. The idea of transformation without learnable parameters is novel.

- Better accuracy both on synthetic tasks and code translation tasks when compared with other strong baselines.

Cons:

- Computation seems to be larger given that the encoding has to be recomputed in every decoding step. I'd like to know the latencies incurred by the proposed method.

Other comment:

- I'd like to see experimental results on natural language tasks, e.g., syntax parsing.

- Section 2:  ""we see that is is not at all necessary"" -> that is

- Section 3: Notation is a little bit hard to follow, "":"" for D and U, and "";"" in stacking.
","The sentiment score is 60 (positive) because the reviewer starts by highlighting the novelty of the approach and its better accuracy compared to strong baselines. They use phrases like 'interesting approach' and 'novel', indicating a positive view. However, they also mention some cons and requests for additional information, which prevents the score from being higher. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, framing criticisms as requests for information ('I'd like to know...', 'I'd like to see...') rather than direct criticisms. They also balance positive and negative feedback, and point out minor errors in a neutral way. The tone is professional and constructive throughout, without any harsh or rude language.",60.0,70.0
"PRO’s:
+well-written
+nice overall system: GAN framework for super-sampling audio incorporating features from an autoencoder
+some good-sounding examples

CON’s:
-some confusing/weakly-presented parts (admittedly covering lots of material in short space)
-I am confused about the evaluation; would like additional qualitative/observational understanding of what works, including more on how the results differ from baseline

SUMMARY: The task addressed in this work is: given a low-resolution audio signal, generate corresponding high-quality audio. The approach is a generative neural network that operates on raw audio and train within a GAN framework. 
Working in raw sample-space (e.g. pixels) is known to be challenging, so a stabilizing solution is to incorporate a feature loss. Feature loss, however, usually requires a network trained on a related task, and if such a net one does not already exist, then building one can have its own (possibly significant) challenges. In this work, the authors avoid this auxiliary challenge by using unsupervised feature losses, taking advantage of the fact that any audio signal can be downsampled, and therefore one has the corresponding upsampled signal as well.

The training framework is basically that of a GAN, but where, rather than providing the generator with a low-dimensional noise signal input, they provide the generator with the subsampled audio signal. The architecture includes a generator ( G(lo-fidelity)=high-fidelity ), a discriminator ( D(high-fidelity) = real or by super-sampled ? ), and an autoencoder ( \phi( signal x) = features of signal x at AE’s bottleneck). 

COMMENTS:

The generator network appears to be nearly identical to that of Kuleshov et al (2017)-- which becomes the baseline-- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term. This is overall a nice problem and a nice approach! In that light, I believe that there is a new focus in this work on the perceptual quality of the outputs, as compared to (Kuleshov et al 2017). I would therefore ideally like to see (a) some attempts at perceptually evaluating the resulting output (beyond PESQ, e.g. with human subjects and with the understanding that, e.g. not all AMT workers have the same aural discriminative abilities themselves), and/or (b) more detailed associated qualitative descriptions/visualization of the super-sampled signal, perhaps with a few more samples if that would help. That said, I understand that there are page/space limitations. (more on this next)

Given the similarity of the U-net architectures to (Kuleshov et al 2017), why not move some of those descriptions to the appendix? 

For example, I found the description and figure illustrating the “superpixel layers” to be fairly uninformative: I see that the figure shows interleaving and de-interleaving, resulting in trading-off dimensionalities/ranks/etc, and we are told that this helps with well-known checkerboard artifacts, but I was confused about what the white elements represent, and the caption just reiterated that resolution was being increased and decreased. Overall, I didn’t really understand exactly the role that this plays in the system; I wondered if it either needed a lot more clarification (in an appendix?), or just less space spent on it, but keeping the pointers to the relevant references.  It seems that the subpixel layer was already implemented in Kuleshov 2017, with some explanation, yet in the present work a large table (Table 1(b)) is presented showing that there is no difference in quality metrics, and the text also mentions that there is no significant perceptual difference in audio. If the subpixel layer were explained in detail, and with justification, then I would potentially be OK with the negative results, but in this case it’s not clear why spend this time on it here. It’s possible that there is something simple about it that I am not understanding. I’m open to being convinced. Otherwise, why not just write: “Following (Kuleshov et al 2017), we use subpixel layers (Shi et al) [instead of ...] to speed up training, although we found that they make no significant perceptual effects.” or something along those lines, and leave it at that? 

I did appreciate the descriptions of models’ sensitivity to size/structure of the conv filters, importance of the res connections, etc.

My biggest confusion was with the evaluation & results:

Since the most directly related work was (Kuleshov 2017), I compared the super resolution (U-net) samples on that website (https://kuleshov.github.io/audio-super-res/ ) to the samples provided for the present work ( https://sites.google.com/view/unsupervised-audiosr/home ) and I was a bit confused, because the quality of the U-net samples in (Kuleshov 2017) seemed to be perceptually significantly better than the quality of the Deep CNN (U-net) baseline in the present work. Perhaps I am in error about this, but as far as I can tell, the superresolution in (Kuleshov et al 2017) is significantly better than the Deep CNN examples here. Is this a result of careful selection of examples? I do believe what I hear, e.g. that the MU-GAN8 is clearly better on some examples than the U-net8. But then for non-identical samples, how come U-net4 actually generally sounds better than U-net8? That doesn’t make immediate sense either (assuming no overfitting etc). Is the benefit in moving from U-net4 to U-net8 within a GAN context but then stabilizing  it with the feature-based loss? If so, then how does MU-GAN8 compare to U-net4? Would there be any info for the reader by doing an ablation removing the feature loss from the GAN framework? etc. I guess I would like to get a better understanding of what is actually going on, even if qualitative. Is there any qualitative or anecdotal observation about which “types” of samples one system works better on than another? For example, in the provided examples for the present paper, it seemed to be the case that perhaps the MU-GAN8 was more helpful for supersampling female voices, which might have more high-frequency components that seem to get lost when downsampling, but maybe I’m overgeneralizing from the few examples I heard. 

Some spectrograms might be helpful, since they do after all convey some useful information despite not telling much of the perceptual story. For example, are there visible but inaudible artifacts? Are such artifacts systematic?

Were individual audio samples represented as a one-hot encoding, or as floats? (I assume floats since there was no mention of sampling from a distribution to select the value).

A couple of typos:

descriminator → discriminator 

pg 6 “Impact of superpixel layers” -- last sentence of 2nd par is actually not a sentence. “the reduction in convolutional kernels prior to the superpixel operation.”

Overall, interesting work, and I enjoyed reading it. If some of my questions around evaluation could be addressed-- either in a revision, or in a rebuttal (e.g. if I completely misunderstood something)-- I would gladly consider revising my rating (which is currently somewhere between 6 and 7).
","The sentiment score is slightly positive (20) because the reviewer acknowledges several strengths of the paper ('well-written', 'nice overall system', 'good-sounding examples') while also pointing out areas for improvement. The overall tone suggests the reviewer sees potential in the work but has some reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and expresses openness to revising their opinion ('I would gladly consider revising my rating'). The reviewer also acknowledges space limitations and their own potential misunderstandings. However, the score is not maximally polite as the review does directly point out weaknesses and areas of confusion without excessive softening language.",20.0,60.0
"The submission describes a sort of hybrid between reinforcement learning and imitation learning, where an auxiliary imitation learning objective helps to guide the RL policy given expert demonstrations.  The method consists of concurrently maximizing an RL objective--augmented with the GAIL discriminator as a reward—and minimizing the GAIL objective, which optimizes the discriminator between expert and policy-generated states.  Only expert states (not actions) are required, which allows the method to work given only videos of the expert demonstrations.  Experiments show that adding the visual imitation learning component allows RL to work with sparse rewards for complex tasks, in situations where RL without the imitation learning component fails.

Pros:
+ It is an interesting result that adding a weak visual imitation loss dramatically improves RL with sparse rewards 
+ The idea of a visual imitation signal is well-motivated and could be used to solve practical problems
+ The method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up RL in practice

Cons:
+ It seems possible that imitation only helps RL where imitation alone works pretty well already
+ Some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal GAIL
+ The presentation borders on hand-wavy at parts and may benefit from a clean, formal description

The submission tackles a real, well-motivated problem that would appeal to many in the ICLR community.  The setting is attractive because expert demonstrations are available for many problems, so it seems obvious that they should be leveraged to solve RL problems—especially the hardest problems, which feature very sparse reward signals.  It is an interesting observation that an imitation loss can be used as  a dense reward signal to supplement the sparse RL reward.  The experimental results also seem very promising, as the imitation loss seems to mean the difference between sparse-reward RL completely failing and succeeding.  Some architectural / feature selection details developed here seem to also be a meaningful contribution, as these factors also seem to determine the success or failure of the method.

My biggest doubt about the method is whether it really only works where imitation learning works pretty well already.  If we don’t have enough expert examples for imitation learning to work, or if the expert is not optimizing the given reward function, then it is possible that adding the imitation loss is detrimental, because it induces an undesirable bias.  If, on the other hand, we do have enough training examples for imitation learning to succeed and the expert is optimizing the given reward function, then perhaps we should just do imitation learning instead of RL.  So, it is possible that there is some sweet spot where this method makes sense, but the extent of that sweet spot is unclear to me.

The experiments are unclear on this issue for a few reasons.  First, figure 4 is confusing, as it is titled ‘comparison to standard GAIL', which makes it sound like a comparison to standard imitation learning.  However, I believe this figure is actually showing the performance of different variants of GAIL used as a subroutine in the hybrid RL-IL method.  I would like to know how much reward vanilla GAIL (without sparse rewards) achieves in this setting.  Second, figure 8 seems to confirm that some variant of vanilla imitation learning (without sparse rewards) actually does work most of the time, achieving results that are as good as some variants of the hybrid RL-IL method.  I think it would be useful to know, essentially, how much gain the hybrid method achieves over vanilla IL in different situations.

Another disappointing aspect of the paper is the ‘learning with no task reward’ section, which is a bit confusing.  The concept seems reasonable at a first glance, except that once we replace the sparse task reward with another discriminator, aren’t we firmly back in the imitation learning setting again?  So, the motivation for this section just seems a bit unclear to me.  This seems to be describing a variant of GAIL with D4PG for the outer optimization instead of TRPO, which seems like a tangent from the main idea of the paper.  I don’t think it is necessarily a bad idea to have another discriminator for the goal, but this part seems somewhat out of place.

On presentation: I think the presentation is a bit overly hand-wavy in parts.  I think the manuscript could benefit from having a concise, formal description.  Currently, the paper feels like a series of disjoint equations with unclear connections among them.  The paper is still intelligible, but not without knowing a lot of context relating to RL/IL methods that are trendy right now.  I feel that this is an unfortunate trend recently that should be corrected.  Also, I’m not sure it is really necessary to invoke “GAIL” to describe the IL component, since the discriminator is in fact linear, and the entropy component is dropped.  I think “apprenticeship learning” may be a more apt analogy.

On originality: as far as I can tell, the main idea of the work is novel.  The work consists mainly of combining existing methods (D4PG, GAIL) in a novel way.  However, some minor novel variations of GAIL are also proposed, as well as novel architectural considerations.

Overall, this is a nice idea applied to a well-motivated problem with promising results, although the exact regime in which the method succeeds could be better characterized.","The sentiment score is 60 (positive) because the reviewer expresses several positive aspects of the paper, including 'interesting result', 'well-motivated', 'promising results', and 'nice idea'. However, they also mention some concerns and areas for improvement, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. They use phrases like 'It seems possible that...' and 'I think it would be useful to know...' when suggesting improvements, which maintains a polite tone. The reviewer also balances positive and negative feedback, further contributing to the polite nature of the review.",60.0,80.0
"This paper proposes a definition for interpretability which is indeed the same as model simplicity using the MDL principle. It has several issues:

1) Interpretability is not the same as simplicity or number of model parameters. For example, an MLP is thought to be more interpretable than an RNN with the same number of parameters.

2) The definition of explainability in Eq. (5) is flawed. It should not have the second term L(M(X)|M^o, X) which is the goodness of M^o's fit. You should estimate M^o using that equation and then report L(M^o|M^p) as the complexity of the best estimate of the model (subject to e.g. linear class). Mixing accuracy of estimation of a model and its simplicity does not give you a valid explainability score. 

3) In Section 2.4.2, the softmax operator will shrink the large negative coefficients to almost zero (reduces the degrees of freedom of a vector by 1). Thus, using softmax will result in loss of information. In the linear observer case, I am not sure why the authors cannot come up with a simple solution without any transformation.

4) Several references in the text are missing which hinders understanding of the paper.","The sentiment score is -70 because the review is predominantly critical, pointing out several significant issues with the paper. The reviewer states that the paper has 'several issues' and then lists four major problems, including flawed definitions and methodological concerns. There are no positive comments to balance these criticisms. The politeness score is 0 (neutral) because the language is direct and professional without being overtly polite or rude. The reviewer states criticisms plainly without using harsh language, but also without softening the critique with polite phrases or positive feedback. The tone is matter-of-fact and focused on the technical aspects of the paper.",-70.0,0.0
"The paper introduces a new twist to the activation of a particular neuron. They use a modulator which looks at the input and performs a matrix multiplication to produce a vector. That vector is then used to scale the original input before passing it through an activation function. Since this modulating scalar can look across neurons to apply a per-neuron scalar, it overcomes the problem that otherwise neurons cannot incorporate their relative activation within a layer. They apply this new addition to several different kinds of neural network architectures and several different applications and show that it can achieve better performance than some models with more parameters.


Strengths:
- This is a simple, easy-to-implement idea that could easily be incorporated into existing models and frameworks.
- As the authors state, adding more width to a vanilla layer stops increasing performance at a certain point. Adding more complex connections to a given layer, like this, is a good way forward to increase capacity of layers.
- They achieve better performance than existing baselines in a wide variety of applications.
- The reasons this should perform better are intuitive and the introduction is well written.

Weaknesses:
- After identifying the problem with just summing inputs to a neuron, they evaluate the modulator value by just summing inputs in a layer. So while doing it twice computes a more complicated function, it is still a fundamentally simple computation.
- It is not clear from reading this whether the modulator weights are tied to the normal layer weights or not. The modulator nets have more parameters than their counterparts, so they would have to be separate, I imagine.
- The authors repeatedly emphasize that this is incorporating ""run-time"" information into the activation. This is true only in the sense that feedforward nets compute their output from their input, by definition at run-time. This information is no different from the tradition input to a network in any other regard, though.
- The p-values in the experiment section add no value to the conclusions drawn there and are not convincing.

Suggested Revisions:
- In the abstract: ""A biological neuron change[s]""
- The conclusion is too long and adds little to the paper

","The sentiment score is 60 (positive) because the review begins by highlighting the paper's novel approach and its potential benefits. The reviewer lists several strengths, including the simplicity and wide applicability of the idea, as well as improved performance across various applications. While there are some weaknesses mentioned, they are presented as constructive criticism rather than major flaws. The politeness score is 70 (polite) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to describe both strengths and weaknesses, and their suggestions for revision are presented in a helpful manner without being overly critical. The reviewer also acknowledges the authors' efforts and the potential value of their work, which contributes to the overall polite tone of the review.",60.0,70.0
"This paper proposes a model-based value-centric (MVC) framework for transfer learning in continuous RL problems, and an algorithm within that framework. The paper attempts to answer two questions: (1) ""why are current RL algorithms so inefficient in transfer learning"" and (2) ""what kind of RL algorithms could be friendly to transfer learning by nature""? I think these are very interesting questions to investigate, and researchers that work on transfer learning could benefit from insights on them. However, I am not yet convinced that this paper answers these questions satisfyingly. It would be great to hear the author's thoughts on my questions below. 

The main insight I take away from the paper is that policy gradient methods are not suitable for transfer learning compared to model-based and value-centric methods for some assumptions (the reward function not changing and the transition dynamics being deterministic). This insight and the experiments in the paper are interesting, but I am unsure if the paper as it is presented now passes the bar for ICLR.

In general the paper has two contributions:
A) analysis of value-centric vs policy-centric methods
B) an algorithm that is more useful for transfer learning.

Regarding A)
The authors argue that policy-centric algorithms are less useful for transfer learning than value-centric methods. 

They first illustrate this with an example in Section 3. Since this is just one example, as a reader I wonder if it would not be possible to construct an example that shows the exact opposite, where value iteration fails but policy gradient doesn't. It feels like there are many assumptions that play into the given example (the reward function not changing; the transition dynamics being deterministic; the choice of using policy gradients and value iteration). 

In addition, the authors provide a theoretical justification in the Appendix (which I have briefly scanned) and the intuition behind it in Section 5. From what I understand, the main problem arises from the policy's output space being a Gaussian distribution, which causes the policy being able to get stuck in a local optimum. Further, the authors show (in the Appendix) that under some assumtions the value function always converges. Are there any guarantees on this when we don't have access to the true reward and transition functions (which themselves could get stuck in a local optimum)?

Would the authors say that the phenomenon is more a problem with the algorithm (policy gradient vs value iteration) than policy-centric and value-centric methods in general? Are there other methods that would be able to transfer policies better than policy gradient methods?

Regarding B)
The author's proposed method (MVC) has three components: the value function, the dynamics model and the reward model, all of which are learned by neural networks. It seems like the main advantage comes from using a model (since that's the aspect which changes when having to transfer to an altered MDP). Does the advantage of this method over DDPG and TRPO come from the fact that the dynamics model changes smoothly, and we have an approximation to it? Then it is not surprising that this outperforms a policy gradient method. 

Other comments:

- Could you explain what is meant by ""precise"" and ""imprecise"" when speaking about policies or value functions?
- Could you explain what is meant by the algorithm being ""accessible"" (e.g., Definition 1)?

- Section 2.1: In Property 1, what is f? Could you make explicit why we are interested in the two properties listed? By ""not rigorously"", do you mean that those properties are based on intuition? These properties are used later in the paper and the appendix, so I wonder how strong of an assumption this is.
- Section 2.2: Could you explain what is meant by ""task""? You say that within the MDP, the transition dynamics and reward functions change, but the task stays the same. However, earlier (in the introduction) you state that only the environment dynamics change. I find it confusing that ""the task"" is something hand-wavy and not part of the formal definition of the MDP. In what exact ways can the reward function be influenced by the change in the transition dynamics? 
- Section 3: Replace ""obviously"" with ""hence""; remove ""it is not hard to find that"". This might not be so trivial for some readers.
- Appendix B: Refer to Table 1 in the text.

Clarity: The paper is written well, but I think some assumptions and their affects should be stated more clearly and put into context. The paper misses a discussion / conclusion section. It would be great to see a discussion on some of the assumptions; e.g., what if the low dimensional assumtion breaks down? What if we assume that also the reward function can change? The authors are in a unique position to give insight into these things (even if the results from the paper do not hold after dropping some assumptions) and it would be very helpful to share these with the reader in a discussion section.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting questions and insights of the paper, they express significant doubts about whether the paper satisfactorily answers these questions or meets the standards for ICLR. The reviewer raises several concerns and questions about the paper's assumptions, methodology, and conclusions. However, the tone is not entirely negative, as the reviewer shows interest in the topic and invites the authors to respond to their questions.

The politeness score is quite high (80) because the reviewer maintains a respectful and constructive tone throughout. They use phrases like 'It would be great to hear the author's thoughts,' 'Could you explain,' and 'I think these are very interesting questions to investigate.' The reviewer also offers specific suggestions for improvement and asks thoughtful questions rather than making harsh criticisms. The language is professional and courteous, even when expressing doubts or concerns about the paper's content.",-20.0,80.0
"This is an interesting paper, trying to find the adversarial cases in reinforcement learning agents. The paper discusses several different settings to investigate how generalizable the worst-case environment is across different models and conjectured that it comes from the bias in training the agents. Overall the paper is well-written and the experiments seem convincing. I have two questions regarding the presented result.

1. The search algorithm depicted in section 2 is only able to find a local optimum in the environment space. How robust is the result given different initializations?

2. It is briefly discussed in the paper that the failure in certain mazes might come from the structural bias in the training and the “complex” mazes are under-represented in the training dataset. It is hence natural to ask, if the procedure described in this paper can be incorporated to enhance the performance by some simple heuristics like re-weighting the training samples. I think some discussion on this would be beneficial for verifying the conjecture made here.

3. The authors compared the “hardness” of the mazes based on the number of walls in the maze. But it is arguably a good metric as the authors also mentioned visibility and other factors in measuring the complexity of the task. I would like to see more exploration in different factors that accounts for the complexity and maybe compare different agents to see if they are sensitive in the same set of factors. 

To summarize, I like the idea of the paper and I think the result can be illuminating and worth some more follow-up work to understand the RL training in general.
","The sentiment score is 70 (positive) because the reviewer describes the paper as 'interesting' and 'well-written' with 'convincing' experiments. They also mention liking the idea and finding the results illuminating. The overall tone is supportive, though not overwhelmingly enthusiastic. The politeness score is 80 (polite) due to the reviewer's constructive and respectful language. They frame their points as questions or suggestions rather than criticisms, and use phrases like 'I would like to see' and 'I think' to soften their recommendations. The reviewer also concludes with a positive summary, reinforcing the polite tone throughout the review.",70.0,80.0
"The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance. Specific comments follow.

1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). Experimental results of optimizing (5) seem missing. While the reviewer understands that (5) and (8)  are closely related, and the theoretical insights for (5) can potentially translate to the scenario in (8), the reviewer is not sure whether the theory for (5)  is rigorously justified by the experiments.

2. It is also unclear how tight the bound provided by Theorem 1 is.  Is the bound vacuous? Relevant statistics in the experiments might need to be reported to elucidate this point.

3. It is also unclear how the trade-off in point (b) of the abstract is justified in the experiments.

Minor Points:
page 2, the definition of $X^{(j)}$, the index of $l$ and $j$ seem to be typos.
page 2, definition 1, the definition of the bracket need to be specified. 
page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability.
page 4, Corollary 1, should it be $nnz(\hat{W})\le JN k_{\mathcal{S}}$?
page 7, Table 2, FLOPS should be FLOP? 
page 8, is FLOP related to the time/speed needed for compression? If so, it should be specified. If not, compression runtime should also be reported.




","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's theoretical analysis and experiments, they express several concerns and uncertainties about the methodology and results. The reviewer points out mismatches between theory and experiments, questions the tightness of bounds, and notes unclear justifications. These critiques suggest a somewhat skeptical view of the paper's contributions. However, the tone is not entirely negative, as the reviewer also recognizes the potential value of the work.

The politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'the reviewer understands' and 'it is unclear' rather than making accusatory statements. The review is structured and specific, offering constructive feedback and even including minor points for improvement. The language is neutral and objective, avoiding harsh criticism while still clearly communicating areas of concern. The use of 'specific comments follow' and the organized format of the review also contribute to its politeness.",-20.0,60.0
"The paper proposes an imitation learning model able to generate trajectories based on some expert trajectories. The assumption is that observed trajectories contain multi-modal (i.e. style) information that is not naturally captured by existing methods. The authors proposed a VAE based architecture that uses a prior distribution P(z) to simultaneously generate (state-action) pairs based on a LSTM decoder (actually, one LSTM for the states and one interleaved LSTM for the actions). This decoder is learned using a classical VAE auto-encoding loss, observed trajectories being encoder through a bi-LSTM. Experiments are made on three toy examples: a simple 2d Navigation case exhibiting 3 different 'styles', a 2D circle example with also 3 different styles, and a zombie attack scenario with two different styles. The results show that the model is able to capture different clusters of trajectories. 

First of all, the paper does not propose a new model, but an instantiation of an existing model to a particular case. The main difference with SoTA is that the authors propose to both decode states and actions without using a simulator. The contribution of the paper is thus quite light. Moreover, it is unclear how the model can be used to get a policy corresponding to a particular mode. Can we use the learned decoders to generate actions on-the-fly in a real/simulated environment? Right now (section 3.3), actions are generated on generated states, but not on observed ones.  The paper has to clarify this point since just generating trajectories seems to be a little bit useless. In general Section 3.3 lacks of details (e.g the rolling window is also unclear). Also, the model could be described a little bit more in term of architecture, particularly on the critical point about how the two decoding LSTMs are interacting. 

From the experimental point of view, the paper attacks very simple cases, without any comparison with state-of-the-art, and without almost any quantitative results. If Section 4.1 and 4.2 are useful to explore the ability of the model on simple cases, I would recommend the authors to merge these two sections in one smaller one, and then to focus on more realistic experiments. For example, it seems to me that the experimental setting proposed for example in [Li et al.] on driving styles could be interesting, and would allow a comparison with existing methods. Also the model proposed in [Co-Reyes et al.] could be an interesting comparison (at least, keeping the principle of this paper, without the hierarchical structure), particularly because this model is based on the use of a simulator while the proposed one is not. If a performance close to this baseline can be obtained with your model, it would be interesting for the community.

Right now, the experimental part and the too small contribution of the paper are not enough for acceptance. I would suggest the authors to:
* better describe their contribution i.e model architecture and how the model can be used to obtain a real policy
* use 'stronger' use cases for the experiments, and particularly existing use cases
* provide a deep quantitative and qualitative comparison with SoTA

Pro:
* simple method, no need of a simulator

Cons:
* not clear how to move from trajectory generation to a real policy
* small contribution
* too light experimental study without comparison with baselines and state of the art
","The sentiment score is -50 because the review is generally critical of the paper, pointing out several shortcomings such as a 'light' contribution, unclear aspects of the model, and insufficient experimental results. However, it's not entirely negative as it acknowledges some positive aspects ('simple method, no need of a simulator') and provides constructive suggestions for improvement. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and offer specific recommendations to improve the paper. Phrases like 'I would suggest' and 'it would be interesting' indicate a polite tone, even while delivering negative feedback.",-50.0,20.0
"The authors study in this paper the approximation capabilities of neural networks for real valued functions on probability measure spaces (and on tree structured domains). 

The first step of the paper consists in extending standard NN results to probability measure spaces, that is rather than having finite dimensional vectors as inputs, the NN considered here have probability measures as inputs. The extension to this case is straightforward and closely related to older extension on infinite dimensional spaces (see for instance the seminal paper of Stinchcombe https://doi.org/10.1016/S0893-6080(98)00108-7 and e.g. http://dx.doi.org/10.1016/j.neunet.2004.07.001 for an application to NN with functional inputs). Nothing quite new here.

In addition, and exactly as in the case of functional inputs, the real world neural networks do not implement what is covered by the theorem but only an approximation of it. This is acknowledged by the authors at the end of Section 2 but in a way that is close to hand waving. Indeed while the probability distribution point is valuable and gives interesting tools in the MIL context, the truth is that we have no reason to assume the bag sizes will grow to infinite or even will be under our control. In fact there are many situations were the bag sizes are part of the data (for instance when a text is embedded in a vector space word by word and then represented as a bag of vectors). Thus proving some form of universal approximation in the multiple instance learning context would need to take this fact into account, something that is not done at all here. 

Therefore I believe the contribution of this paper to be somewhat limited. ","The sentiment score is -50 because the reviewer expresses a generally negative view of the paper's contribution, stating it is 'somewhat limited' and that there is 'Nothing quite new here.' However, it's not entirely negative as the reviewer acknowledges some valuable points. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout, using phrases like 'I believe' to soften criticism. They also acknowledge the authors' work and provide constructive feedback, which is more polite than purely negative criticism. The language is not overtly polite, but it avoids rudeness, hence a slightly positive score.",-50.0,20.0
"Pros:
+ Improving joint training of non-differentiable pipelines is a meaningful and relevant problem
+ Using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea

Cons:
+ The main result of the paper concerning sufficient conditions for optimality of the method seems dubious
+ It is not obvious why this method would outperform simple baselines, and baselines for joint training were tried
+ The notation seems unnecessarily bloated and overly formal
+ The exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing

The submission describes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector), so that gradient-based methods may be applied to jointly train all the parameters of the pipeline.  In particular, the proposal involves recasting the pipeline as a stochastic computation graph (SCG), adding stochastic nodes to this graph, and then using REINFORCE-style policy gradients to perform parameter learning on the SCG.  It is claimed that under certain conditions, the optimal parameters of the resulting SCG are also optimal for the original pipeline.  The method is applied to optimizing the parameters of Faster-RCNN.

I think making non-differentiable pipelines differentiable is an intuitively appealing concept.  A lot of important, practical machine learning systems fall into this category, so devising a nice way to do global parameter optimization for such systems could potentially have significant impact.  In general, we can’t hope to make much meaningful progress on the problem of optimizing general nonlinear, differentiable functions, but it is plausible that a method that targets key non-differentiable components for smoothing—such as this paper—could outperform a generic black-box optimizer.  So, I think the basic idea here is plausible and addresses an important problem.

Unfortunately, I think this work loses sight of that high-level goal: to me, the key question is whether the proposed approach outperforms any other simple method for global parameter optimization in the presence of nonlinearities and nondifferentiability.  The paper fails to answer this question because no baselines for global parameter optimization were tried.  We can just treat the pipeline as a black box mapping parameters to training set performance, and so any black-box optimization method can be applied to this problem.  It is not clear that the proposed method would outperform an arbitrary black box optimization method such as simulated annealing, Nelder-Mead, cross-entropy method, etc.

I think there are also much simpler methods in a similar vein to the proposed method that might also perform just as well as the proposal.  One key conceptual issue here is that reducing the problem to a reinforcement learning problem, as the submission does, is not much of a reduction at all.  First, if the goal is to do global parameter optimization, then we don’t really have to smooth the pipeline itself: we can just smooth the black box mapping parameters to performance, and then optimize that with SGD.  There are many ways to do this--if we want to use policy gradient, we can just express the problem as something in this form:

min_\phi E_{\theta ~ q_\phi} C(\theta)

where C is the black-box mapping parameters \theta to a performance index (such as mean AP), q_\phi is a distribution over parameters (e.g., Gaussian), and \phi are the distribution parameters (e.g., mean, covariance of the Gaussian).  We can then optimize this using REINFORCE policy gradients.

If we want to really smooth the pipeline itself, then it is also easy to do this by devising a suitable MDP and then applying REINFORCE with the usual MDP structure.  We simply identify the state s_t at time t with the output of the t’th pipeline stage, introduce a new ‘action’ variable a_t representing a ’stochastified output’, and trivial dynamics (P(s_{t+1} | s_t, a_t) = \delta(s_{t+1} - a_t)).  If the policy is a Gaussian (P(a_t | s_t) = N(a_t; s_t, \Sigma)), then this is similar to relaxing the constraint that one stage’s output is equal to the input of the next stage, and somehow quadratically penalizing their difference.  In fact, there is a neural network training method based explicitly on this penalization view [A], and it would make yet another great baseline to try.

In fact, the proposed method is essentially similar to what I have just described, but it is unfortunately described in an overcomplicated way that obscures the true nature of the method.  I think the whole SCG framework is overkill here.  Too much of the paper is spent just rehashing the SCG framework, and the very heavy notation again just obscures the essential character of the method.

If there were, as the paper claims, some interesting condition under which the method produces solutions that are optimal under the original pipeline, that would be remarkable and interesting.  However, I have serious doubts about this part of the paper.  The key problem is the statement that “It follows that c(k_c, DEPS_c - k_c) = c(…) + z_c”.  The paper seems to be claiming that if E z = 0, then c(k + z) = c(k) + z, which can’t possibly be true in general.  

The heavy and opaque notation makes it very difficult to understand this section.  Perhaps it would help to consider a very simple example.  Suppose we want to minimize E_{x ~ q} c(y(x)) (where x ~ q means x is distributed as q).  We can introduce only one new stochastic node (k = y + z), between y and c.  Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0.

In summary, I think the submission needs a lot of work on multiple axes before it can make a significant impact.  The most important issues are a complete lack of relevant baselines and the dubious claims about sufficient conditions for optimality.  The idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [A]) as well as the simple baselines I have mentioned.  The presentation also needs to be revised to find the simplest expression of the method and to focus on the interesting parts.

[A] Taylor, Gavin, et al. ""Training neural networks without gradients: A scalable admm approach."" International Conference on Machine Learning. 2016.","The sentiment score is -60 because the review is predominantly negative. While it acknowledges the importance of the problem and the plausibility of the basic idea, it criticizes the paper's main results, lack of baselines, overcomplicated notation, and dubious claims. The reviewer expresses significant doubts about the paper's contributions and suggests substantial revisions. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I think' and 'Unfortunately' to soften criticisms. The reviewer also acknowledges some positive aspects of the work before delving into criticisms. However, the overall critical nature of the review prevents a higher politeness score.",-60.0,20.0
"Overall, this is a thorough attempt at a system for evaluating various generative models on synthetic problems vaguely representative of the kinds of problems claimed to be covered by GANs. I think the approach and the conclusions drawn are mostly reasonable, with one major caveat discussed shortly.

I also think it would help in a revision to add evaluations of more recent successors to RealNVP, such as MAF (NIPS 2017, https://arxiv.org/abs/1705.07057 ), Glow ( https://arxiv.org/abs/1807.03039 ), and (although of course this paper came out concurrently with your submission) the promising FFJORD ( https://openreview.net/forum?id=rJxgknCcK7 ). The scale of comparison of GAN variants is also much smaller than that of Lucic et al. or their followup, Kurach et al. ( https://arxiv.org/abs/1807.04720 ), which is not cited here (and should be).

But primarily, I think there are some serious concerns with your choice of metrics that make the results as they are difficult to interpret.


""Note that OT is not a distance in the standard mathematical sense, as for instance the 'distance' between two sets of points sampled from the same distribution is not zero."" -- You've confused some notions here. The Wasserstein-1 distance, which is a scalar times the variant of OT you use here, absolutely is a proper distance metric between distributions: W(P, Q) is a metric. But when you compute the OT distance between *samples*, OT(S, T) with S ~ P and T ~ Q, you're equivalently computing the distance W(\hat{P}, \hat{Q}) between the empirical distributions of the samples, \hat{P} = 1/N \sum_i \delta_{S_i} and the similar \hat{Q}, which of course are not the same thing as the source distributions themselves. You can, though, view OT(S, T) as an *estimator* of W(P, Q); the distance between *distributions* is what we actually care about.

It is well-known that these empirical distributions of samples \hat{P} converge to the true distribution P (in the Wasserstein sense, W(P, \hat{P})) exponentially slowly in the dimension, which is what your example about high-dimensional distributions demonstrates. Incidentally, this is exactly the example used in Arora et al. (ICML 2017, https://arxiv.org/abs/1703.00573 ). This means that, viewed as an estimator of the true distance between distributions, the empirical-distribution OT estimator is strongly biased. Thus it becomes very difficult to tell what the true OT value is at any sample size, and moreover this amount of bias might differ for different distribution pairs even at the same sample size, so *comparing* OT estimates at a fixed sample size is a tricky business. For example, in your Figure 2, when the ""oracle"" score is significantly more than zero, you know that all of your estimates are very strongly biased. There is not, as far as I know, any strong reason to suspect that this amount of bias should be comparable for different distribution pairs, making any conclusions drawn from these numbers suspect.


Your scheme you call ""Two-Sample Test,"" first, should have a more specific name. Two-sample testing is an extremely broad field, with instances including the classical Kolmogorov-Smirnov test and t tests, the popular-in-ML kernel MMD-based tests, and even Wasserstein-based tests (e.g. https://arxiv.org/abs/1509.02237 ). Previous applications of these tests in GANs and generative models include Bounliphone et al. (ICLR 2016, https://arxiv.org/abs/1511.04581 ), Lopez-Paz and Oquab (2016 - which you cite without a venue but which was at ICLR 2017), Sutherland et al. (ICLR 2017, https://arxiv.org/abs/1611.04488 ), Huang et al. (2018), and more, using a variety of schemes. Your name for this should include ""nearest neighbor"" or something along those lines to avoid confusion.

Also, you call this an ""extension of the original formulation,"" but in the common case where n(x) is more often right than wrong, your v is exactly \hat t - 1 of Lopez-Paz and Oquab; see their (2). If it's usually wrong, then v = 1 - \hat t; only when the signs differ per class does it significantly differ from theirs, and in any case I don't see a real motivation to put the absolute values for each class separately rather than just taking |\hat t - 1/2|.

Moreover, it's kind of crazy to term your v statistic a two-sample *test* -- you have nothing in there about its sampling distribution, which is key to hypothesis testing to obtain e.g. a p-value. (Maybe the variance of v is very different between different distributions; this is likely the case. In any case the variance will probably become extremely large as the dimension increases.) Comparing this score is thus difficult, but in any case calling it a ""test"" is potentially very misleading. You could, though, estimate the variance as described by Lopez-Paz and Oquab to construct a test.

Also: you can imagine the statistic v(S, T) as an estimator of the distance between distributions given as
  D(P, Q) = |1/2 - \int ( 1 if p(x) > q(x), 0 o.w.) p(x) dx|
          + |1/2 - \int (-1 if p(x) > q(x), 0 o.w.) q(x) dx|.
But v(S, T) is, like for the OT distance, a biased estimator of this distance, whose bias will get worse with the dimension. Thus, like with the OT, it's hard to meaningfully compare v(S, T) as an attempt to compare *distributions* based on D, which is what we actually care about. Here the oracle score does not show strong bias: assuming a reasonable number of samples, when P = Q the v estimator is always going be approximately 0. But this doesn't mean that other estimators aren't strongly biased, and indeed this is exactly what your Appendix C shows. The strong change in performance for KDE is somewhat hard to interpret, but maybe has something to do with the connection between KDE and NN-based methods?


Your log-likelihood score is an unbiased and asymptotically normal estimate of the true distribution score (the cross-entropy), so it's easy to compare. But it accounts only for a very small portion of comparing distributions.


There is at least one score in common use for this kind of evaluation with easy-to-compare estimators: the squared MMD. It has an easy-to-compute unbiased and asymptotically normal estimator, so it's easy to get confidence intervals for the true value between distributions at any sample size, making comparing the numbers based on a reasonable number of samples easy. There's also a well-devolped theory for how to construct p-values for a test if you want those; Bounliphone et al. above even developed a relative test to compare the MMDs of two models accounting for the correlations due to using the same ""target"" set, though if you use separate target sets (because you can easily sample more points from your synthetic distribution) then it's simpler. The choice of kernel does matter, but I think the median-heuristic Gaussian kernel would be a very reasonable score to add to your repertoire, and for particular distributions you also might be able to pick a better kernel (e.g. based on the causal factors when those exist). See also Binkowski et al. (ICLR 2018, https://arxiv.org/abs/1801.01401 ) for a detailed discussion of these issues in comparison to the FID score.

Using a metric whose estimation can be understood, and whose estimators can be reliably compared, is I think vital to any evaluation process. This also prevents issues like when RealNVP outperforms the oracle, which should be impossible with any proper evaluation metric.



Minor points:

- Why is Pedregosa et al. (2011) cited for fitting multivariate Gaussians by maximum likelihood? This is something that doesn't need a citation, especially not to scikit-learn, which doesn't even (I don't think) contain an implementation of fitting Gaussians beyond (np.mean(X, axis=0), np.cov(X, rowvar=False)).

- Mode coverage and related scores: this is based on assigning sample points to their single most likely clusters? I'd imagine that sometimes a model will output points far from any cluster, in which case the cluster that happens to be closest might happen to be the most likely, but it's strange to really count that point as part of that cluster for these scores. Or similarly, a point might be relatively evenly spaced between two clusters, in which case the assignment could be fairly arbitrary, again making these scores a little strange.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as a 'thorough attempt' with 'mostly reasonable' conclusions, they express 'serious concerns' about the choice of metrics and difficulty in interpreting results. They also point out several issues and suggest significant additions and changes. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and provides detailed explanations and suggestions for improvement. They use phrases like 'I think' to soften critiques and offer positive comments alongside criticisms. However, they don't use overtly polite language or praise, keeping the tone professional rather than overly courteous.",-20.0,50.0
"Summary
The authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms.
The method consists of a) mining documents that refer to the same topic, b) extracting from these documents parallel sentences, c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level.

Novelty: the approach is novel.

Clarity: the paper is clearly written.

Empirical validation: The empirical validation is solid but limited. The authors could further strengthen it by testing on low-resource language pairs (En-Ro, En-Ur).
It would also be useful to report more stats about the retrieved sentences in tab. 1 (average length compared to ground truth, BLEU using as reference the translation of a SoA supervised MT method, etc.)

Questions
1) Sec. 3.2 is the least clear of the paper. The notation of eq. 7 is quite unclear because of the overloading (e.g., P refers to both the model and the empirical distribution).
I am also unclear about this constraint about matching the topic distribution: as far as I understood, the model gets only one gradient signal for the whole document. I find then surprising that the authors managed to get any significant improvement by adding this term.
Related to this term, how is it computed? Are documents translated on the fly as training proceeds? Could the authors provide more details?

2) Have the authors considered matching sentences to any other sentence in the monolingual corpus as opposed to sentences in the comparable document?
 ","The sentiment score is 60 (positive) because the reviewer expresses a generally positive view of the paper, noting its novelty, clarity, and solid empirical validation. They describe the approach as 'novel' and the paper as 'clearly written'. However, they also point out some limitations and areas for improvement, which prevents the score from being higher. The politeness score is 70 (polite) because the reviewer uses respectful and constructive language throughout. They offer suggestions for improvement in a helpful manner, using phrases like 'The authors could further strengthen it by...' and 'It would also be useful to...'. The reviewer also poses questions in a polite way, showing engagement with the work. The language is professional and courteous, without any harsh criticism or rudeness.",60.0,70.0
"This paper proposes a sequence to sequence model augmented with a multinomial latent variable. This variable can be used to generate multiple candidate translations during decoding. This approach is simpler than previous work using continuous latent variables or modifying beam search to encourage diversity, obtaining more diverse translations with a smaller drop in translation accuracy.   

Strengths:
- Simple model that succeeds in achieving its goal of generating diverse translations.
- Provides insights into training models with categorical latent variables. 
Weaknesses:
- More insight into what the latent variable is learning to represent would strengthen the paper. 

While the model is simple, its simplicity has significant strengths: In contrast to more complex latent space, the latent variable assignments can be enumerated explicitly, which enables it to be used to control the generation and compare outputs. The simplicity of the model will force the latent variable towards capturing diversity - modelling uncertainty in how to express the output rather than uncertainty in the content. 

One question about the model architecture is just whether it is sufficient to feed the latent variable embedding only once, as it effect might be diluted across long output sequences (as opposed to, say, feeding the latent variable at each time step). 

The paper provides some interesting insights, such as the need to do hard EM-style training and turning off dropout when inferring the best latent variable assignment during training, to avoid mode collapse. 

What is the effect of initialization? This often has a large impact in EM-style training, and could also lead to mode collapse, though in this case the restricted parameterization might prevent that. 

What is the training time and computational resource requirements? Are multiple DGX-1s running in parallel required to train the model?

What is not clear enough from the paper is what kind of structure the latent variables learn to capture. In particular this model is not biassed towards any explicit notion of the kind of diversity one would like to learn. While there is some qualitative analysis, further analysis would strengthen the paper. 

Overall this is a very interesting contributions that offer useful insights into designing controllable sequence generation models.","The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, highlighting its strengths and interesting contributions. They use phrases like 'very interesting contributions' and 'useful insights', indicating approval. However, they also point out some weaknesses and areas for improvement, which prevents the score from being higher. The politeness score is 80 (polite) because the reviewer maintains a respectful and constructive tone throughout. They balance praise with constructive criticism, use phrases like 'would strengthen the paper' instead of more direct criticism, and ask questions rather than making demands. The language is professional and courteous without being overly deferential.",70.0,80.0
"

Review Summary
--------------
The paper presents a combination of rule lists, prototypes, and deep representation learning to fit classifiers that are said to be simultaneously ""accurate"" and ""interpretable"". While the topic is interesting and the direction seems novel, I don't think the work is quite polished or competitive enough to be accepted without significant revision. The major issues include non-competitive evaluation of what ""interpretability"" means, ROC AUC numbers that are indistinguishable from standard deep learning (RCNN) pipelines that use many fewer parameters, and many unjustified choices inside the method itself. The paper itself could also benefit from revision to improve flow and introduce technical ideas to be more accessible to readers.


Paper Summary
-------------
The paper presents a new method called ""PEARL"" (Prototype Learning via Rule Lists), which produces a rule list, a set of prototypes, and a deep feed-forward neural network that can embed any input data into a low-dimensional feature space. The primary intended application is classifying subjects into a finite set of possible disorders given longitudinal electronic health records with categorical features observed at T irregular time intervals. 

The paper suggests learning a representation for each subject's data by feeding the EHR time series into a recurrent convolutional NN. The input data is a 2 x T array, with one row representing observed data and second row giving time delay between successive observations. The vector output of an initial convolutional RNN is then fed into a highway network to produce a final vector denoted ""h"". 

Given an encoder to produce feature vectors, and a fixed rule list learned from data itself, the paper suggests obtaining a prototype for each rule by computing the average vector of all data that matches the given rule. The quality of these prototypes and related neural networks (for computing features and predicting labels from features) is then assessed via their loss function in Eq. 1: a weighted combination of how well the prototypes match the learned embeddings (distance to closest prototype) and how well the classifier predicts labels.  The core idea is that the embedding is learned to classify well while creating a latent space that looks like the prototypes of the rule list.

After training an embedding and NN classifier on a fixed rule list, it seems the data is reweighted according to some heuristic procedure to obtain better properties, then a new rule list is trained and the process repeats again. (I admit the reweight procedure's purpose was never clear to me).

Experiments are done on a proprietary heart failure EHR dataset and on a subset of MIMIC data. 

Strengths
---------
* Seems original: I'm unaware of any other method connecting rule lists AND prototypes AND NNs
* Neat applications to healthcare

Limitations
-----------
* Interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rule-lists shown is an apples-to-apples comparison
* Prototypes themselves never evaluated 
* Many design choices inside method not justified with experiments -- why highway networks + RCNNs?

Major Issues with Method
------------------------

## M1: Not clear that AUC difference between PEARL and baselines is significant

The major issue is that the presented approach does not seem significantly different in predictive performance than the baseline Recurrent CNN. Comparing ROC AUC, we have PEARL's 0.688 to RCNN'S 0.682 with stddev of 0.009 on the proprietary heart failure dataset, and PEARL's 0.769 to RCNN's 0.766 with stddev of 0.009. When AUCs match this closely, I struggle to believe one model is definitively better, especially given that the RCNN has 2x *fewer* parameters (8.4k to 18.4k). 

If the counterargument is that the resulting ""deep model"" is not ""interpretable"", one should at least compare to a post-processing step where the decision boundary of the RCNN is the reference to which a rule list or decision tree is trained.

## M2: Interpretability evaluation not clear.

Isn't the maximum number of rules set in advance? 

Additionally, prototypes are a key part of this work, but the learned prototypes are not evaluated at all in any figure (except to track avg. distance from prototype while training). If prototypes are so central to this work, I would like to see a formal evaluation of whether the learned prototypes are indeed better (in terms of distance, or inspection of values by an expert, or something else) than alternatives like Li et al.

## M3: Missing a good synthetic/small dataset experiment

Neither of the presented data tasks is particularly easy to understand for non-experts. I'd suggest creating an additional experiment where the audience of ML readers is likely to easily grasp whether a set of rule lists is ""good"" for the problem at hand... maybe create your own synthetic task or a UCI dataset or something, or even use the stop-and-frisk crime dataset from the Angelino et al. 2018 paper. Then you can compare against just a few relevant baselines (rule lists only or prototypes only). I think a better illustrative experiment will help readers grasp differences between methods. 

## M4: How crucial is feature selection?

In each iteration, Algo. 1 performs feature selection before learning rules. Are any other baselines (trees, rule lists) allowed feature selection before the classifier is learned? What would happen to PEARL without feature selection? What method is used for selection? (A search of the document only has 'feature selection' occur once, in the Alg. itself, so it seems explanation is missing).

## M5: Why are multiple algorithm iterations needed?

Won't steps 3 and 4 of Alg. 1 result in the same rules every time? It's not clear then why on subsequent iterations the algorithm would improve. Perhaps it's just the reweighting of data that causes these steps to change?

Minor issues
------------

## Loss function notation confusing

Doesn't the rule list classifier s_R take the data itself X? Not the learned embedding h(X)? Please fix or clarify Eq. 1. I think you might clarify notation by just writing yhat(h(X)) if you mean the predicted label of some example as done by your NNs. Using ""R"" makes folks think the rule list is involved.

## Not clear why per-example reweighting is required

None of the experiments assess why per-example reweighting (lines 6-9 of Algo. 1) is required. Readers would like to see a comparison of performance with and without this step.

## Not clear or justified when ""averaged"" prototypes are acceptable

Are your ""averaged"" prototypes guaranteed to satisfy the rule they represent? Is taking the average of vectors that match a rule always guaranteed to also match the rule? I don't think this is necessarily true. Consider a rule that says ""if x[0] == 0 or x[1] == 0, then ___"".  Suppose the only matching vectors are x_A = [0 1] and x_B = [1 0]. The average vector is [0.5 0.5] which doesn't work.

## Several different measures of distance used without careful justification 

Why use two different distances -- Euclidean distance to assess distance to prototypes for prototype assignment, and then cosine similarity when deciding which examples to upweight or downweight? Why not just use Euclidean distance for both (appropriately transformed to a similarity)?

Comments on Presentation
------------------------
Overall I think every section of the paper needs significant revision to improve a reader's ability to understand main ideas. Notation could be introduced slowly (explain purpose and dimension of every variable), assumptions could be clearly stated (e.g. each individual rule can have ANDs but not ORs), and design choices justified. You might try the test of giving the paper to a colleague and having them explain back the ideas of each section to you... currently I do not believe this version passes this test.

The introduction claims that ""clinicians are often unwilling to accept algorithm recommendations without clarity as to the underlying reasoning"", but I would be careful in blindly asserting this without evidence. For a nice argument about avoiding blind assumptions about what doctor's will and won't accept, see Lipton's 2017 paper ""The Doctor Just Won't Accept That"" (https://arxiv.org/abs/1711.08037)

Additionally, the authors should clarify more precisely what definition of interpretability is needed for their applications. Is it simplicity? Is it conceptual alignment with known medical facts? Is it the ability to transparently list the rules in plain English?

Line-by-line details
--------------------

## Sec. 2

When introducing p_j, should clarify this this is one prototype vector of many.

When defining p_j = f_j(X), can you clarify what dimensionality p_j has? Is it always the same size as each example's data vector x_i?


## Sec. 3

Fig. 2: I don't find this figure very easy-to-understand. It's clear that after embedding raw features to a new space, the learned rules are *different*, but it's not clear they are *better*.  None of the illustrated rules perfectly segments the different colors, for example. I guess the point is all the red dots are within one rule? But they aren't alone (there are blue and orange dots too), so it's still not clear this would be a better classifier.

For EHR datasets, are you assuming that events are always categorical? And that outcomes ""y"" are always discrete (one-of-L) variables? Or could y be real-valued?

Eq. 1: You should make notation clearly indicate which terms depend on \theta. Currently it seems that nothing is a function of \theta.

Eq. 1: Do you also find the prototype set P that minimizes this objective? Or is there another way to obtain P given parameters \theta? This is confusing just from reading the eqn.

What size is the learned representation h(X)? Is it a vector?

Eq. 6: Do you really need a ""network"" to compute the distance to each of the K prototypes? Can't you just compute these distances directly?

## Sec 4

""Mac OS 1.4"" : Do you mean Mac OS version 10.4? Not clear this is relevant.

4.3 Case Study: How do I read these rules? Is this rule applied only if ALL conditions are true? or if any individual one is true (""or"")? This is unclear.","The sentiment score is -50 because the reviewer expresses significant concerns about the paper, stating it is not 'polished or competitive enough to be accepted without significant revision'. They list several major issues and limitations. However, the score is not lower because the reviewer does acknowledge some strengths and the potential of the work. The politeness score is 50 because the reviewer uses professional and constructive language throughout, offering specific suggestions for improvement rather than harsh criticism. They use phrases like 'I don't think' and 'I would suggest' which maintain a respectful tone. The reviewer also highlights positives like 'neat applications' and 'seems original' before diving into critiques, which is a polite approach.",-50.0,50.0
"This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. 

Pros:
The author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss. As a result, this paper achieved impressive outcome for topic modeling tasks.

Comments:
1. High-level language is used to describe how to train two parts of the model, which is not technically clear. It would be better describe the algorithms in more details by listing steps for your algorithm in the section 3.3.

2. For text generation experiments, why didn’t you compare your model with any other related model such as SeqGAN or TextGAN? It is not so convincing to just use VAE+Wgan-gp as a baseline model.

3. For qualitative analysis part, you just listed some of your generated sentences for proving the fluency and relevance. Why didn’t you use some standard metrics for evaluating the quality of the text? I cannot judge the quality of your model through these randomly selected sentences.

4. As you mentioned in this paper “your model can be easily combined with any current text generation models”, have you done any experiments for demonstrating the original text generation model will get better performance after applying your framework? 

Minor comments:
1. On page 2 and page 4, you mentioned “the third term in (2)”. According to my understanding, this should be equation 1 instead. 
","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's 'impressive outcome' and 'creative object function', but also provides several critical comments and suggestions for improvement. The overall tone is constructive rather than overtly negative or enthusiastic. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, framing criticisms as suggestions ('It would be better...') and questions ('Why didn't you...?') rather than direct accusations. The reviewer also begins with positive points before moving to critiques, which is a polite approach. The language is professional and objective, avoiding personal attacks or overly harsh criticism.",20.0,60.0
"This paper proposes GASI to disambiguate different sense identities and learn sense representations given contextual information. 
The main idea is to use scaled Gumbel softmax as the sense selection method instead of soft or hard attention, which is the novelty and contribution of this paper.
In addition, the authors proposed a new evaluation task, contextual word sense selection, which can be used to quantitatively evaluate the semantic meaningfulness of sense embeddings.
The proposed model achieves comparable performance on traditional word/sense intrinsic evaluation and word intrusion test as previous models, while it outperforms baselines on the proposed contextual word sense selection task.

While the scaled Gumbel softmax is the claimed novelty, it is more like an extension of the original MUSE model (Lee and Chen, 2017), which proposed the sense selection and representation learning modules for learning sense-level embeddings.
The only difference between the proposed one and Lee and Chen (2017) is Gumbel softmax instead of reinforcement learning between sense selection and representation learning modules.
Therefore, the idea from the proposed model is similar to Li and Jurafsky (2015), because the sense selection is not one-hot but a distribution.
The novelty of this paper is limited because the model is relatively incremental.

From my perspective, the more influential contribution is that this paper points out the importance of evaluating sense selection capability, which is ignored by most prior work.
Therefore, I expect to see more detailed evaluation on the selection module of the model. 
Also, because the task of this paper is multi-sense embeddings, the traditional word similarity (without contexts) task seems unnecessary. 
Moreover, there is no error analysis about the result on the proposed contextual word sense selection task, which may shed more light on the strength and weakness of the model. 
Finally, I suggest the authors remove the word-level similarity task and try the recently released Word in Context (WiC) dataset, which is a binary classification task that determines whether the meaning of a word is different given two contexts.
It would be better to see that GASI performs well on this task given its better sense selection module.

Overall, the contribution is somewhat incremental and the evaluation/discussion should focus more on the sense selection module. 
Considering the issues mentioned above, I will expect better quality for an ICLR paper.","The sentiment score is -30 because while the reviewer acknowledges some positive aspects of the paper (e.g., proposing a new evaluation task and achieving comparable performance on traditional evaluations), they express significant concerns about the novelty and contribution of the work. The reviewer states that the contribution is 'somewhat incremental' and that they 'expect better quality for an ICLR paper.' These criticisms outweigh the positive elements, resulting in a slightly negative overall sentiment. The politeness score is 20 because the reviewer maintains a professional and respectful tone throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'I suggest' and 'It would be better to see,' which are polite ways of providing feedback. However, the score is not higher because the review is quite direct in its criticism and doesn't use many overtly polite phrases or softening language.",-30.0,20.0
"This work proposes to train an RL-based agent to simultaneously learn Embodied Question Answering and Semantic Goal Navigation on the ViZDoom dataset. The proposed model incorporates visual attention over the input frames, and also further supervises the attention mechanism by incorporating an auxiliary task for detecting objects and attributes.

Pros:
-Paper was easy to follow and well motivated
-Design choices were extensively tested via ablation
-Results demonstrate successful transfer between SGN, EQA, and the auxiliary detection task

Cons:
-With the exception of the 2nd round of feature gating in equation (3), I fail to see how the proposed gating -> spatial attention scheme is any different from the common inner-product based spatial attention used in a large number of prior works, including  [1], [2], and [3] and many more.
-The use of attribute and object recognition as an auxiliary task for zero-shot transfer has been previously explored in [3]


Overall, while I like the results demonstrating successful inductive transfer across tasks, I did not find the ideas presented in this work to be sufficiently novel or new.

[1] Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, Huijuan Xu, Kate Saenko
[2] Drew A. Hudson, Christopher D. Manning, Compositional Attention Networks for Machine Reasoning
[3] Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks, Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they ultimately conclude that the work lacks sufficient novelty. The final statement 'I did not find the ideas presented in this work to be sufficiently novel or new' indicates an overall negative assessment. The politeness score is moderately positive (60) as the reviewer uses professional and respectful language throughout, acknowledging both pros and cons, and providing specific feedback and references. They avoid harsh criticism and use phrases like 'I like the results' even while expressing concerns, maintaining a polite tone.",-20.0,60.0
"This paper formulates a new deep method called deep abstaining classifer. Their main idea is to introduce a new modified loss function that utilizes an absention output allowing the DNN to learn when abstention is a better option. The core idea resemble KWIK framework [1], which has been theoretical justified.

Pros:

1. The authors find a new direction for learning with noisy labels. Based on Eq. (1) (the modified loss), the propose \alpha auto-tuning algorithm, which is relatively novel. 

2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.
For example, they conduct experiments on CIFAR-10 and CIFAR-100. Besides, they conduct experiments on open-world detection dataset.

Cons:

We have three questions in the following.

1. Clarity: in Section 3, the author claim real-world data is corrupted in some non-arbitrary manner. However, in practice, it is really hard to reason the corrpution procedure for agnostic noisy dataset like Clothing1M [2]. The authors are encouraged to explain this point more.

2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [3], estimating noise transition matrix [4,5], and explicit and implicit regularization [6]. I would appreciate if the authors can survey and compare more baselines in their paper.

3. Experiment: 
3.1 Baselines: For noisy labels, the author should compare with [7] directly, which is highly related to your work. Namely, designing new loss function can overcome the issue of noisy labels. Without this comparison, the reported result has less impact. Moreover, the authors should add MentorNet [2] as a baseline https://github.com/google/mentornet

3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.

References:

[1] L. Li, M. Littman, and T. Walsh. Knows what it knows: a framework for self-aware learning. In ICML, 2008.

[2] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.

[3] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.

[7] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's novelty and experimental support for its claims, but also raises several questions and suggests improvements. This indicates a balanced view with a slight positive lean. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, frames criticisms as questions or suggestions (e.g., 'The authors are encouraged to...'), and acknowledges the paper's strengths before discussing areas for improvement. The reviewer also provides helpful references, which is a courteous gesture in academic discourse.",50.0,75.0
"This paper analyzes the limitation of probability density distillation with reverse KL divergence, and proposes two practical methods for probability distillation.

Detailed comments:

1) Typo: should be WaveNet, not Wavenet.

2) In Proposition 1. $c_i$ should be $\rho_i$.

3) One may explain “path derivative” with more details. Also, I am really confused by Proposition 1 and its underlying implication. Given p_s and p_t are centered at the origin, isn’t p_s(x) already the optimal if it’s just a unit Gaussian. Why do we need a derivative pointing away from the origin? At least, one need parameterize p_s as N(0, \phi)?

4) In section 3.2, “set $\mu = [2, 2]^T$”? Isn’t $\mu$ a T dimensional vector?

5) A lot of important details are missing in neural vocoder experiment. For x-reconstruction, do you use L1 or L2 loss?  For student model, do you use Gaussian IAF with WaveNet architecture as in ClariNet, or Logistic IAF as in Parallel WaveNet? Following this question, do you compute KLD in closed-form? Do you use the regularization term introduced in ClariNet? Student with KL loss and power loss outperforms x-reconstruction. Did you try x-reconstruction along with power loss?

Pros:
Certainly, there are some interesting ideas in this paper. 

Cons:
The experiment results are not good enough. The paper is poorly written. A lot of important details are missing.  

However, I would like to raise my rating to 6, if these comments can be properly addressed.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges 'some interesting ideas' in the paper, they also point out several issues such as missing details, poor writing, and unsatisfactory experimental results. The overall tone suggests more criticism than praise, but it's not entirely negative as the reviewer offers to raise their rating if concerns are addressed. The politeness score is moderately positive (40) as the reviewer uses professional language throughout, offers constructive criticism, and even suggests a way for the authors to improve their rating. The reviewer points out issues directly but without harsh language, maintaining a respectful tone while providing detailed feedback.",-20.0,40.0
"The paper proposes a feature smoothing technique, which generates virtual data points by interpolating the input space of two randomly sampled examples. The aim is to generate virtual training data points that are close to adversarial examples. Experimental results on both MNIST and Cifar10 datasets show that the proposed method augmented with other regularization techniques are robust to adversarial attacks and obtain higher accuracy when comparing with some testing baselines. Also, the paper presents some theoretical analyses showing that label smoothing, logit squeezing, weight decay, Mixup and feature smoothing all produce small estimated variance of the decision boundary when regularizing the networks. 

The paper is generally well written, and the experiments show promising results. Nevertheless, the proposed method is not very novel, and the method is not comprehensively evaluated with experiments.

Major remarks:

1.	The experiments show that feature smoothing has to combine with other regularizers in order to outperform other testing methods. In this sense the contribution of the feature smoothing along is not clear. For example, without integrating other regularizers, Mixup and feature smoothing obtain very close results for BlackBox-PGD, BlackBoxcw and Clean, as shown in Table 1. In addition, in the paper, the feature smoothing along is only validated on the MNIST (not even tested on Cifar10 in Table2). Consequently, it is difficult to evaluate the contribution of the proposed smoothing technique. 
2.	Experiments are conducted on datasets MNIST and Cifar10 with small number of target classes. Empirically, it would be useful to see how it performs on more complex data set such as Cifar100 or ImageNet.
3.	The argument for why the proposed feature smoothing method works is presented in Theorem4.3 in Section 4.2, but the theorem seems to rely on the assumption that one can add data around the true decision boundary. However, how we can generate samples near the true decision boundary and how we should chose the mixing ratio to attain this goal is not clear to me in the paper. In addition, how we can sure that the adding synthetic data from one class does not collide with manifolds of other classes as suggested in AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization)? This is particular relevant if the proposed feature smoothing strategy prefers to create virtual samples close to the true decision boundary.
4.	At the end of page4, the authors claim that both feature smoothing and Mixup generate new data points that are closer to the true boundary. I wonder if the authors could further justify or show that either theoretically or experimentally. 
5.	The proposed method is similar to SMOTE (Chawla et al., SMOTE: Synthetic Minority Over-sampling Technique). In this sense, comparison with SMOTE would be very beneficial.

Minor remarks:

1.	In the paper Mixup, value 1 was carefully chosen as the mixing policy Alpha for Cifar10 (otherwise, underfitting can easily occur as shown in AdaMixUp), and it seems in the paper the authors used a very large value of 8 for Mixup’s Beta distribution, and I did not see the justification for that number in the paper.
2.	Typo in the second paragraph of page2: SHNV should be SVHN
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'promising results'), they express several major concerns about the novelty, comprehensiveness of evaluation, and clarity of the method's contribution. The overall tone suggests that significant improvements are needed. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positives before critiques, and phrases concerns as suggestions or questions rather than direct criticisms. They use polite phrases like 'I wonder if the authors could' and 'it would be useful to see', maintaining a constructive tone even when pointing out issues.",-20.0,60.0
"The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. 

This paper covers the related work nicely, with details on both closed loop and open loop methods. The rest of the paper are also clearly written. However, I have some concerns about the proposed method.
- It is not clear how to define the kernel, the feature function and the quality function for the proposed method. The choices of those seem to have a huge impact on the performance. How was those functions decided and how sensitive is the result to hyperparameters of those functions?
- If the search space is continuous, what is the mixing rate of Alg. 2? In practice, how is ""mixed"" decided? What exactly is the space and time complexity? I'm not sure where k log(N) comes from in page 7.  
- Alg. 2 is a straight forward extension of Alg. 1, just with L not explicitly computed. I think it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is. 

Other small things:
- citation format problems in, for example, Sec. 4.1. It should be \citep instead of \cite. 
- it would be good to mention Figure 2 in the text first before showing it. 

[Post rebuttal]
I would like to thank the authors for their clarifications. However, I am still concerned with the novelty. The absence of provable mixing rate is also a potential weakness. I think a clearer emphasis on the novelty, e.g. current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('covers the related work nicely', 'clearly written'), they express several concerns about the proposed method and its novelty. The overall tone suggests the paper needs significant improvements. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive criticism, and thanks the authors for their clarifications. They provide specific suggestions for improvement without using harsh or dismissive language. The reviewer maintains a professional tone while clearly communicating their concerns and recommendations for strengthening the paper.",-20.0,60.0
"Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided.

Evaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up.

- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?
- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? 
- training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. 
- the intuition of layer-wise optimality: on page 4, the paper states that ""the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training"" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this?
- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?
- vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this?
- some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation","The sentiment score is slightly negative (-20) because the reviewer expresses 'mixed feelings' about the paper, pointing out several concerns and areas needing clarification. While they acknowledge some interesting theoretical contributions, they also highlight issues with interpretability, practicality, and lack of large-scale experiments. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, framing criticisms as questions or requests for clarification rather than direct criticisms. They also acknowledge their own potential misunderstandings and the possibility of missing critical information. The use of phrases like 'please enlighten me' and 'could you please explain' further contribute to the polite tone.",-20.0,60.0
"This paper proposes a model-based object-oriented algorithm, SOORL. 
It assumes access to an object detector which returns a list of objects with their attributes, an interaction function which detects interactions between objects, and a set of high-level macro actions. Using a simplified state representation obtained through the object detector, it performs optimistic MCTS while simultaneously learning transition and reward models. The method is evaluated on two toy domains, PongPrime and miniPitfall, as well as the Atari game Pitfall. It achieves positive rewards on Pitfall, which previous methods have not been able to do. 

Despite good experimental results on a notoriously hard Atari game, I believe this work has limited significance due to the high amount of prior knowledge/engineering it requires (the authors note that this is why they only evaluate on one Atari game). I think this would make a good workshop paper, but it's not clear that the contributions are fundamental or generally applicable to other domains. Also, the paper is difficult to follow (see below). 

Pros:
- good performance on a difficult Atari game requiring exploration
- sample efficient method

Cons:
- paper is hard to follow
- approach is evaluated on few environments
- heavily engineered approach
- unclear whether gains are due to algorithm or prior knowledge


Specific Comments:

- Section 3 is hard to follow. The authors say that they are proposing a new optimistic MCTS algorithm to support deep exploration guided by models, but this algorithm is not described or written down explicitly anywhere. Is this the same as Algorithm 3 from Section 5? They say that at each step and optimistic reward bonus is given, but it's unclear which bonus this is (they mention several possibilities) or how it relates to standard MCTS.
In Section 3.1, it is unclear what the representation of the environment is. I'm guessing it is not pixels, but it is discrete states? A set of features? 
The authors say ""we provided the right model class for both experiments"" - what is this model class? 

- Concerning the general organization of the paper, it would be clearer to first present the algorithm (i.e. Section 5), go over the different components (model learning, learning macro actions, and planning), and then group all the experiments together in the same section. 
The first set of experiments in Sections 3.1 and 3.2 can be presented within the experiments section as ablations.  

- Although the performance on Pitfall is good, it's unclear how much gains are due to the algorithm and how much are due to the extra prior knowledge. It would be helpful to include comparisons with other methods which have access to the same prior knowledge, for example with DQN/A3C and  pseudo-count exploration bonuses using the same feature set and macro actions as SOORL uses. 


Minor:
- Page 2: ""Since the model...the new model estimates"": should this be part of the previous sentence?
- Page 5: ""There are reasonable evidence"" -> ""There is reasonable evidence""
- Page 5: "". we define a set of..."" -> "". We define a set of...""
- Page 8: ""any function approximation methods"" -> ""method""","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('good experimental results', 'good performance', 'sample efficient method'), they express significant reservations about the paper's overall significance and applicability. The reviewer states that the work has 'limited significance' and suggests it would be more suitable as a workshop paper rather than a full conference paper. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, balancing criticism with praise, and using phrases like 'I believe' and 'I think' to soften direct criticisms. They also provide a structured review with clear pros and cons, and offer specific, constructive feedback for improvement, which contributes to the polite tone.",-20.0,50.0
"This paper studies a very simple and intuitive method to boost the training speed of deep neural networks. The authors first train some light weighted proxy models, using these models to rank the data according to its uncertainty, and then pick the most uncertain subset to train the final model. Experiments on CIFAR10/SVHN/Amazon Review Polarity demonstrates the effectiveness.

In general, I think the authors did a decent job in showing that such a simple idea could surprisingly work well to boost NN training. I believe it will inspire future works on speeding up NN training. However, to form a solid ICLR publication, plenty of future works need to be done.

1)	I will not be fully convinced if an idea aiming to speed up, is only verified on small scale dataset (e.g., CIFAR10). It will be much better if there are large scale experiments conducted such as on ImageNet and WMT neural machine translation. 

2)	Please well position some related works. First, it would be more interesting and informative if some baselines in section 2 (especially those in “Optimization and Importance Sampling’), are compared with. Second, there are important related works omitted such as L2T [1], which also talks/shows the possibly of using partial training data to achieve speed up.

3)	Some writing issues: it would be better to *clearly* demonstrate the final accuracy of different models (i.e. ResNet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text. I’m also note sure about the meaning of `epoch’ in Table 1: does it mean how many epochs the proxy model is trained? If so, I can hardly get the intuition of why smaller epochs works better. I noted a conjecture raised by the authors in the last sentence of paragraph “comparing different proxies”. However, I cannot catch the exact meaning. 

[1] Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018
","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's decent job and potential to inspire future work, but also points out significant improvements needed. The first paragraph is generally positive, while the following paragraphs highlight areas for improvement. The politeness score is 70 (fairly polite) due to the use of respectful language throughout. The reviewer uses phrases like 'I think,' 'I believe,' and 'Please,' which maintain a courteous tone. They also provide constructive feedback and specific suggestions for improvement rather than harsh criticism. The reviewer balances positive comments with areas for improvement, maintaining a professional and respectful tone throughout the review.",50.0,70.0
"
Summary: This paper tries to tackle the option discovery problem, by building on recent work on successor representation and eigenoptions. Although this is an extreme important problem, I feel the paper fails to deliver on its promise. The authors propose a way of clustering states via their SR/SF representation and they argue that this would lead to the discovery of subgoals that are fundamentally different from the popular choices in literature, like bottleneck states. They argue that this discovery procedure would lead to states “better for exploration”, “provide greater accessibility to a larger number of states”. Both of which sound promising, but I felt the actual evaluation fails to show or even assess either of these rigorously. Overall, after going through the paper, it is not clear what are the properties of these discovered subgoal states and why they would be better for exploration and/or control.

Clarity: Can be improved significantly! It requires several reads to get some of the important details. See detailed comments.

Originality and Significance: Very limited, at least in this version. The quantitative, and in some cases qualitative, evaluation lacks considerably. The comparison with the, probably most related, method (eigenoption) yield some slight improvement. But unfortunately, I was not conceived that this grain of empirical evidence would transfer to other scenarios. I can’t see why that would that be the case, or in which scenarios this might happen. At least those insights seem to be missing from the discussion of the results. 


Detailed comments and questions:

1) Section 3.1: Latent Learning. There are a couple of design choices here that should have been more well explained or motivated:
i) The SR were built under the uniformly random policy. This is a design choice that might work well for gridworld/navigation type of domains but there are MDP where the evaluation under this particular policy can yield uninformative evaluations. Nevertheless this is an interesting choice that I think deserved more discussion, especially the connection to previous work on proto-value functions and eigenoptions. For instance, if both of these representations -- eigenoptions and the proposed successor option model -- aim to represent the SR under the uniform policy, why does know do (slightly) better than the other? Moreover, how would these compare under a different policy (non-uniform). 
ii) The choice of reward. The notation is a bit confusing here, as it’s somewhat inconsistent with the definitions (2-4). Also, more generally, It is not clear throughout if we are using the discounted or undiscounted version of SR/SFs -- (2-3) introduce the discounted version, (4) seems to be undiscounted. Not clear if (5) refers to the discounted or undiscounted version. Nevertheless, I am guessing this was meant as a shaping reward, thus \gamma=1 for (5). But if that’s the case, according to eq. (2), most of the time I would expect \psi_s(s_{t+1}) and \psi_s(s_{t}) to have the same value. Could you explain why that is not true (at least in your examples)?
iii) Termination set: Q(s,a)<=0. This again seems a bit of an arbitrary choice and it’s not clear which reward this value function takes into account. 

2) Figure 2: The first 2 figures representing the SRs for the two room domain: the values for one of the rooms seems to be zero, although one would expect a smoother transition around the ‘doorway’, otherwise the shaping won’t point in the right direction for progression. Again, this might suggest that more informative(control) policy might give you more signal.  

3) Section 3.2: ‘The policy used for learning the SR is augmented with the previously learnt options‘. Can you be more precise about how this is done? Which options used? How many of them? And in which way are they used? This seems like a very important detail. Also is this augmented policy used only for exploration? 

4) SRmin < \sum_{s’} ψ(s, :) < SRmax. Is this meant to be an expectation over all reachable next states or all states in the environment? How is this determined or translated in a non-tabular setting. Not sure why this is a proxy to how ‘developed’ this learning problem or approximation is. Can you please expand on your intuition here?

5) Section 3.3. The reward definition seems to represent how much the progress between \phi(s_t+1) - \phi(s) aligns with the direction of the goal. This is very reminest of FuN [2] -- probably a connect worth mentioning and exploring.

6) Figure 4: Can you explain what rho is? It seems to be an intermediate representation for shared representation \phi. Where is this used?

7) Experiments:
“a uniformly random policy among the options and actions (typically used in exploration) will result in the agent spending a large fraction of it’s time near these sub-goals”. Surely this is closely linked to the termination condition of the option and the option policy. How is this assessed?

“in order to effectively explore the environment using the exploration policy, it is important to sample actions and options non-uniformly”. It would be good to include such a comparison, or give a reason why this is the case. It’s also not clear how many of the options we are considering in this policy and how extensive their horizons will be. This comes back to the termination condition in Section 3.1 which could use an interpretation. 

“In all our experiments, we fix the ratio of sampling an option to an action as 1:19.” This seems to be somewhat contradictory to the assumption that primitive actions are not enough to explore effectively this environment. 

Figure 8. I think this experiment could use some a lot more details. Also it would be good to guide the reader through the t-SNE plot in Figure 8a. What’s the observed pattern? How does this compare to the eigenoption counterpart.

8) General comment on the experiments: There seems to be several stages in learning, with non-trivial dependencies. I think the exposition would improve a lot if you were more explicit about these: for instance, if the representation continually refined throughout the process; when the new cluster centers are inferred are the option policies learnt from scratch? Or do they build on the previous ones? Does this procedure converge -- aka do the clusters stabilize?

9) Quantitative performance evaluation was done only for the gridworld scenarios and felt somewhat weak. The proposed tasks (navigation to a goal location) is exactly what SFs are trained to approximate. No composition of (sub)tasks, nor tradeoff-s of goals were studied [1,3] -- although they seem natural scenario of option planning and have been studied in previous SFs work. Moreover, if the SFs are built properly, in these gridworlds acting greedily with respect to the SFs (under the uniformly random policy) should be enough to get you to the goal. Also, probably this should be a baseline to begin with.

References:
[1] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and ´ David Silver. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4055–4065, 2017.

[2] Vezhnevets, A.S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D. and Kavukcuoglu, K., 2017, July. FeUdal Networks for Hierarchical Reinforcement Learning. In International Conference on Machine Learning (pp. 3540-3549).

[3] Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek, A. and Munos, R., 2018, July. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning (pp. 510-519).
","The sentiment score is -60 because the review is predominantly negative. The reviewer states that the paper 'fails to deliver on its promise', has 'very limited' originality and significance, and lacks in quantitative and qualitative evaluation. However, it's not entirely negative as the reviewer acknowledges the importance of the problem and some slight improvements over related methods. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I feel', 'I was not convinced', and 'Can you explain' rather than making blunt negative statements. They also provide detailed feedback and suggestions for improvement, which is constructive and polite. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.",-60.0,20.0
"Summary

The authors consider RL with safety constraints, which is framed as a multi-reward problem. At a high-level, this involves finding the Pareto front, which optimally trades off objectives. The paper primarily introduces and discusses a discretization scheme and methods to model the Q-value function as a NIPWC (non-increasing piecewise constant function). NIPWC are stored as values over discrete partitions of state-action spaces. To do so, the authors introduce two data structures DecRect and ContDecRect to store Q function values over geometric combinations of subsets of state-action space.

The authors discuss how to execute elementary operations on these data structures, such as computing max(f(x), g(x)), weighted sums, etc. The goal is to use these operations to compute Bellman-type updates to compute optimal value/policy functions for multi-reward problems. The authors also present complexity analysis for these operations. 

Pro
- Extensive discussion and analysis of discrete representations of Q-functions as NIPWCs. 

Con
- A major issue with this work is that it is very densely written and spends a lot of time on developing the discretization framework and operations on NIPWC. However: 
- There is no clear practical algorithm to solve (simple) multi-reward RL problems with the authors' approach.
- No experiments to demonstrate a simple implementation of these techniques.
- Even though multi-reward settings are the stated problem of interest, authors don't discuss Pareto front computations in much detail, e.g., section 4.3 computing non-dominated actions is too short to be useful.
- The discussion around complexity upper bounds is too dense and uninsightful. For instance, the bounds in section 5 all concern bounds on the Q-value as a function of the action, which results in upper bounds as a function of |A|. But in practice, the action is space is often small, but the state space is high-dimensional. Hence, these considerations seem less relevant. 

Overall, this work seems to present an interesting computational scheme, but it is hard to see how this is a scalable alternative. Practical demonstrations would benefit this work significantly.

Reproducibility
N/A 
","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('Pro' section), they express significant concerns and criticisms in the 'Con' section. The review highlights major issues such as the lack of a clear practical algorithm, absence of experiments, and insufficient discussion on key aspects. The overall tone suggests that the work has potential but falls short in several important areas. The politeness score is 20 because the reviewer maintains a professional and objective tone throughout, presenting both pros and cons without using harsh language. They offer constructive criticism and suggestions for improvement, which is a polite approach in academic reviews. However, the score is not higher as the review doesn't include explicitly polite phrases or soften the criticisms significantly.",-50.0,20.0
"This paper provides a new method that approximates the confidence distribution of classification probability, which is useful for novelty detection. The variational inference with Dirichlet family is a natural choice.

Though it is principally insightful to introduce the “higher-order” uncertainty, I do see the fundamental difference from the previous research on out-of-distribution detection (Liang, Lee, etc.). They are aimed at the same level of uncertainty.  Consider a binary classier, the only possible distribution of output y is Bernoulli- a mixture of Bernoulli is still Bernoulli.   

In ODIN paper,  the detector contains both the measurement of the extent to which the largest unnormalized output of the neural network deviates from the remaining outputs (U1 in their notation) and another measurement of the extent to which the remaining smaller outputs deviate from each other (U2 in their notation).  In this paper, the entropy term has the same flavor as U2 part?

I am a little bit concerned with the VI approach, which introduces extra uncertainty.  I do not understand why there is another balancing factor eta in equation 6, which makes it no longer a valid elbo. Is the ultimate goal to estimate the exact posterior distribution of p(z) through VI, or purely some weak regularization that enforces uniformity?  Could you take advantage of some recent development on VI diagnostics and quantify how good the variational approximation is?

In general, the paper is clear and well-motivated, but I find the notation sometimes confusing and inconsistent. For example, the dependency on x and D is included somewhere but depressed somewhere else.  alpha_0 appears in equation 4 but it is defined in equation 7. 

I am impressed by the experiment result that the new method almost always dominates best known methods, previously published in ICLR 2018. But I am not fully convinced why it works theoretically.  I would recommend a weak accept.  
","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper's contributions and is 'impressed' by the experimental results, they also express several concerns and recommend only a 'weak accept'. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, asks questions rather than making blunt criticisms, and acknowledges the paper's strengths. They use phrases like 'I am impressed' and 'the paper is clear and well-motivated', which contribute to a polite tone. However, the score is not extremely high as the review maintains a professional, rather than overtly friendly, tone.",20.0,60.0
"Summary:  This paper studies deep multi-task learning. Prior papers have studied various knowledge sharing approaches for deep multi-task learning including hard and soft sharing schemes. And some soft sharing schemes have used tensor decompositions (including TT, and Tucker). This paper fuses this line of work with the recently proposed Tensor-Ring decomposition in order to obtain Tensor Ring (TR)-based soft sharing for multi-task learning. The results show some improvement over prior deep MTL methods based on other tensor factorisation methods.

Strengths:
+ Nice extension of existing line of work tensor-factorisation based MTL.
+ More flexibility for controlling shared/unshared portions of weights compared to DMTRL. 
+ Improves on previous methods results.
+ Experiments evaluate how MTL methods relate with various amounts of training data on each task.

Weaknesses:
- Novelty/significance is limited. 
- Writing. Many things are not clearly and intuitively explained. Some claims are not adequately justified. 
- Introduces more hyper parameters to tune.
- Results may rely on hyper parameter tuning. 

Comments:
1. Novelty.  Existing studies already established the template of different tensor factorisation methods (TT, Tucker) being possible to plug into deep networks for different kinds of soft-sharing MTL. Meanwhile, TR decomposition is taken off the shelf; (and as it’s been applied for compression before, this is not the first time TR decomposition has been used in a CNN context either). Therefore this is an A+B paper and a high bar should be met for the additional analysis, insight, or performance improvements that should provided.
2. Lots of writing issues:
2.1 Many things are not explained transparently enough at best (or major over-claim at worst). For example: 
2.1.1 Paper claims the benefit that each task can have its own I/O dimensionality. However if TR-decomp is “circularly connected” TT-decomp (Fig 1), then this seems not to happen automatically. So it should be unpacked more clearly how this is achieved. 
2.1.2 Paper claims favourable ability to use more private cores than TT, where only one core is private. However circular TT would also seem to have one private core by default (the core with a task axis). So I suspect something else is going on, but this is completely unclear and should be explained more transparently. Furthermore it should be justified if whatever modifications do enable these properties are definitely a unique property of TR-decomp, or could also be applied to TT-decomp. 
2.1.3 Statement “TRMTL generalizes to allow the layer-wise weight to be represented by a relatively lager number of latent cores” unclear: generalises what? larger number of cores than what? Than TT? The previous presentation suggests TT and TR should have same number of cores.  
2.1.4 Statements like “TR enjoys the property of circular dimensional permutation invariance” are made without any explanation about what is the implication of this for neural networks and multi-task learning.  
2.2 Many claims are inaccurate or not adequately backed up by theory or experiment. EG: (i) Paper claims to include DMTRL as a special case. But it only subsumes DMTRL-TT, not DMTRL-Tucker. Because TR-decomp does not include Tucker-decomp as an exact special case.  (ii) Sentences “TR-ranks are usually smaller than TT-ranks” are assertions without verification. 
2.3 Sentences are taken verbatim from other papers, plagiarism. For example: “TR model is more flexible than TT, because TR-ranks can be equally distributed in the cores, but TT-ranks have a relatively fixed pattern”  is verbatim from Zhao’16 TR-decomp paper. 
3. Hyperparameters: This paper apparently gains some practical benefit due to the notion of shared/unshared cores. However, this also introduces  additional hyper parameters (E.g., each layers private proportion “c”) to tune besides the ranks. Unlike the rank that can be pre-estimated by reconstruction error, this one seems to require tuning by cross-validation. This is not scalable. 
4. Hyperparameters+Tuning: Hyperparameters Private proportion, “sharing pattern”, IO dimension seem to be tuned by accuracy.( “We test different sharing patterns and report the ones with the best accuracies”). This is even less scalable, and additional tuning makes it unsurprising it surpasses other models performance.
5. Insight & Analysis. All the core selection & public/private core selection are treated as black box optimisation. No insight is given about what turns out to be useful to share or not, and how consistent this is, etc.
","The sentiment score is -40 because the review is overall critical, pointing out several weaknesses and issues with the paper, though it does acknowledge some strengths. The reviewer notes limited novelty, writing issues, and concerns about hyperparameter tuning. However, they also mention some positive aspects like improvements over previous methods and nice extension of existing work, preventing the score from being more negative. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'nice extension' and frame criticisms as 'weaknesses' rather than using harsh language. The reviewer also provides detailed, constructive feedback, which is a polite approach to peer review. However, the overall critical nature of the review prevents the politeness score from being higher.",-40.0,20.0
"This paper proposed to improve the system resource efficiency for super resolution networks. 

First, I am afraid all the techniques considered in this paper have been investigated in previous works. Thus no new idea is proposed in this work. Also, it is also not clear why these improvement is particularly suitable for the task of super resolution. In my viewpoint, these techniques actually can be used to improve a variety of network architectures in both high-level and low-level vision tasks.

Second, the experimental results are also weak. As this work is aiming to address the super resolution tasks, at least visual comparisons between the proposed methods and other state-of-the-art approaches should be included in the experimental part. But unfortunately, no such qualitative results are presented in the manuscript. 

Finally, the presentation of the paper should also be carefully proofread and revised.
","The sentiment score is -70 because the review is predominantly negative. The reviewer states that no new ideas are proposed, the experimental results are weak, and the paper needs careful proofreading. There are no positive comments about the paper's content or contributions. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without any softening phrases or positive reinforcement. Phrases like 'I am afraid,' 'it is also not clear,' and 'no such qualitative results are presented' convey criticism without much attempt to be polite or constructive. The reviewer does not offer specific suggestions for improvement or acknowledge any potential merits of the work, which contributes to the slightly impolite tone.",-70.0,-20.0
"This paper proposed a combination of graph neural networks and conditional random field to model the correlation between node labels in the output.  In typical graph neural nets the predictions for nodes are conditionally independent given the node representations.  This paper proposes to use a CRF to compensate for that.  In terms of the approach, the authors used GCNs to produce unary potentials for the CRF, and have the pairwise potentials on each edge to model the correlation of labels of neighboring nodes.  Learning is done by optimizing pseudo-likelihood and the energy loss, while inference is performed through a couple heuristic processes.

Combining neural nets with CRFs is not a new idea, in particular this has been tried before on image and sequence CRFs.  It is therefore not surprising to see an attempt to also try it for graph predictions.  The main argument for using a CRF is its ability to model the correlations of output labels which was typically treated as independent.  However this is not the case for deep neural networks, as it already fuses information from all over the input, and therefore for most prediction problems it is fine to be conditionally independent for the output, as the dependence is already modeled in the representations.  This is true for graph neural networks as well, if we have a deep graph neural net, then the GNN itself will take care of most of the dependencies between nodes and produce node representations that are suitable for conditionally independent output predictions.  Therefore I’m not convinced that CRFs are really necessary for solving the prediction tasks tried in this paper.

The learning and inference algorithms proposed in this paper are also not very convincing.  CRFs has been studied for a long time, and there are many mature algorithms for learning them.  We could do proper maximum conditional likelihood learning, and use belief propagation to estimate the marginals to compute the gradients.  Zheng et al. (2015) did this for convnets, we could also do this for graph CRFs as belief propagation can be easily converted into message passing steps in the graph neural network.  Pseudo-likelihood training makes some sense, but energy loss minimization doesn’t really make sense and has serious known issues.

On the other hand, the proposed inference algorithms does not have good justifications.  Why not use something standard, like belief propagation for inference again?  Our community has studied graphical models a lot in the last decade and we have better algorithms than the ones proposed in this paper.

Lastly, the experiments are done on some standard but small benchmarks, and my personal experience with these datasets are that it is very easy to overfit, and most of the effort will be put in to prevent overfitting.  Therefore more powerful models typically cannot be separated from overly simple models.  I personally don’t care a lot about the results reported on these datasets.  Besides, there are a lot of questions about the proposed model, but all we get from the experiment section are a few numbers on the benchmarks.  I expect studies about this model from more angles.  One more minor thing about the experiment results: the numbers for GraphSAGE are definitely wrong.

Overall I think this paper tackles a potentially interesting problem, but it isn’t yet enough to be published at ICLR due to its problems mentioned above.","The sentiment score is -60 because the reviewer expresses significant skepticism about the paper's approach and contributions. They state they are 'not convinced' by the necessity of the proposed method, criticize the learning and inference algorithms as 'not very convincing', and question the experimental results. However, they do acknowledge it tackles a 'potentially interesting problem', preventing an extremely negative score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I'm not convinced' and 'I expect' rather than harsh language. They also acknowledge some positive aspects, like the potential interest of the problem. However, the overall critical nature of the review prevents a higher politeness score.",-60.0,20.0
"REVISION: thanks for the clarification. I have slightly increased my rating (to 4).

This paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.

First, there are many sentences which are not well-formed or are ambiguous (in pretty much all the sections). Then there are terms which are introduced without being first clearly explained or defined. Finally, there are issues with the mathematical clarity as well, with many notations which are used without being explained or defined. Sometimes one can figure out the missing information later (e.g., fig 1 talks about mutual information objectives without stating if we want to maximize or minimize it, but later in the text we figure that out) but it makes reading very difficult.

What is a 'transformed one' (on page 2)
What is a 'geometric intrinsic reward'?
Where are the intrinsic rewards defined?
What is a 'non-parametric classifier'? A neural net? an kernel SVM?

There are also some mathematical problems:
- if f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)
- what are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?
- in eqn 2, I do not think that the log q_c term maximizes the mutual information between actions and (G(t),G(t+1)), i.e. it would be missing an entropy term
- what is Z_c in eqn 2?

","The sentiment score is -50 because the reviewer expresses significant concerns about the clarity and presentation of the paper. They mention issues with sentence structure, undefined terms, and mathematical clarity. The reviewer states that these problems make it difficult to conduct a proper review, which indicates a negative sentiment. However, they do acknowledge that the paper tackles 'a very interesting subject', which prevents the score from being even lower. The politeness score is 20 because the reviewer maintains a professional tone throughout, avoiding harsh language or personal attacks. They use phrases like 'thanks for the clarification' and frame their criticisms as observations rather than accusations. However, the directness of the criticisms and the lack of positive reinforcement prevent the score from being higher on the politeness scale.",-50.0,20.0
"This paper studies extrapolation error in off-policy batch reinforcement learning (RL), where the extrapolation error refers to the overestimation of the value for the state-action pairs that are not in the training data.

The authors propose batch-constrained RL, where the policy is optimize under the constraint that, at each state, only those actions that have been taken in that state in the training data are allowed.  This is then extended to continuous space, where it allows only the state-action pairs that are close to a state-action pair in the training data.  When there is no such action for a given state, the action that is closet to a feasible action at that state is selected.

It makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?  Is it because the experiments are performed under rather deterministic settings?  How often are no state-action pairs found in the neighbor?  Is there any mechanism for recovering from ""not in the batch""?

The paper would be much stronger if it study this challenge of ""not in the batch"" more in depth.  Technical contributions in the present paper are rather limited.

A key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.  Although this assumption is stated, it would be nice to emphasize by also stating that the theorems do not hold when this assumption does not hold.  The assumption becomes less important for continuous case, because of approximation.  It might be interesting to study the performance of the proposed approach when the assumption does not hold in the continuous case.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the intuitive sense of the approach, they express several concerns and limitations of the study. The reviewer suggests that the paper would be stronger with more in-depth analysis and notes that the technical contributions are limited. However, it's not entirely negative as they recognize some merits of the work. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive statements. They use phrases like 'It would be nice' and 'It might be interesting' which contribute to a polite tone. The review maintains a professional and objective stance, focusing on the content rather than making personal comments.",-20.0,60.0
"This paper proposes a hierarchical variational autoencoder for modeling paragraphs. The model creates two levels of latent variables; one level of sentence-level latent variables and another single global latent. This avoids posterior collapse issues and the authors show convincing results on a few different applications to two datasets.

Overall, it is an impressive result to be able to convincingly model paragraphs with a useful global latent variable. Apart from some issues with confusing/incomplete notation (see below), my main criticism is that the authors fail to compare their approach to ""A Hierarchical Latent Structure for Variational Conversation Modeling"" by Park et al. As far as I can tell, the approaches are extremely similar, except that Park et al. may not learn the prior parameters and also use a hierarchical RNN encoder rather than a CNN (which may be irrelevant). They also are primarily interested in dialog generation, so the lower-level of their hierarchy models utterances in a conversation rather than sentences in general, but I don't see this as a major difference. I'd encourage the authors to compare to this and potentially use it as a baseline. More generally, it would have been nice to see more ablation experiments (e.g. convolutional vs. LSTM encoder). Finally, I know that space is tight, but other papers on global-latent-variable models tend to include more demonstrations that teh global variable is capturing meaningful information, e.g. with attribute vector arithmetic. The authors could include results of manipulating review sentiment via attribute vector arithmetic, for example.


Specific comments:

- ""The Kullback-Leibler (KL) divergence term ... which can be written in closed-form (Kingma & Welling, 2013), encourages the approximate posterior distribution qφ(z|x) to be close to the multivariate Gaussian prior p(z)."" The prior is not always taken to be a multivariate Gaussian. You should add a sentence stating that the VAE prior is often taken to be a diagonal-covariance Gaussian for convenience.
- 3.2 has a few things which are unclear. In the second paragraph, you define z as the sampled latent code which is fed through an MLP ""to obtain the starting state of the sentence-level LSTM decoder"". But then LSTM^{sent} appears to be fed z at every timestep. LSTM^{sent} is also not defined - am I to assume that its arguments are the previous state and current input, so that z is the input at every timestep? Also, you write ""where h^s_0 is a vector of zeros"" which makes it sound like the starting state of the sentence-level LSTM decoder is a vector of zeros, not the output of the MLP which takes z as input. In contrast, LSTM^{word} takes three arguments as input. Which are the ""state"" and which are the ""input"" to the LSTM?
- I don't see any description of your CNN encoder (only the LSTM decoder in section 3.2, 3.3 only covers the hierarchy of latent variables, not the CNN architecture). What is its structure? Figure 1 shows a CNN encoder generating lower-level sentence embeddings and a high-level global embedding. How are those computed? It is briefly mentioned in 4.1 under ""Datasets"" but this seems insufficient.
- p_\theta(x | z) is defined as the generating distribution, but also as a joint distribution of z_1 and z_2. Unless I am missing something I think you are overloading the notation for p_\theta.
- I don't think enough information is given about the AAE and ARAE baselines. Are they the same as the flat-VAE, except with the KL term replaced by the an adversarial divergence between the prior and approximate posterior?","The sentiment score is 50 (slightly positive) because the reviewer describes the paper's results as 'impressive' and 'convincing', but also raises several criticisms and suggestions for improvement. The overall tone is constructive rather than overtly negative or enthusiastic. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms constructively (e.g. 'I'd encourage the authors to...'), and acknowledges space constraints. The reviewer maintains a professional tone while providing detailed feedback.",50.0,75.0
"This paper presents a novel deep learning module for recurrent processes. The general idea and motivation are generally appealing but the experimental validation is a mess. Architectures and hyper-parameters are casually changed from experiment to experiment (please refer to Do CIFAR-10 Classifiers Generalize to CIFAR-10? By Recht et al 2018 to understand why this is a serious problem.) Some key evaluations are missing (see below). Key controls are also lacking. This study is not the first to look into recurrent / feedback processes. Indeed some (but not all) prior work is cited in the introduction. Some of these should be used as baselines as opposed to just feedforward networks. TBut with all that said, even addressing these concerns would not be sufficient for this paper to pass threshold since overall the improvements are relatively modest (e.g., see Fig. 4 right panel where the improvements are a fraction of a % or left panel with a couple % fooling rates improvements) for a module that adds significant computational cost to an architecture runtime. As a side note, I would advise to tone down some of the claims such as ""our network could outperform baseline feedforward networks by a large margin”...

****
Additional comments:

The experiments are all over the place. What is the SOA on CIFAR-10 and CIFAR-100? If different from VGG please provide a strong rationale for testing the circuit on VGG and not SOA. In general, the experimental validation would be much stronger if consistent improvements were shown across architectures.

Accuracy is reported for CIFAR-10 and CIFAR-100 for 1 and 2 feedback iterations and presumably with the architecture shown in Fig. 1. Then robustness to noise and adversarial attacks tested on ImageNet and with a modification of the architecture. According to the caption of Fig. 4, this is done with 5 timesteps this time! Accuracy on ImageNet needs to be reported ** especially ** if classification accuracy is not improved (as I expect). 

Then experiments on fine-grained with ResNet-34! What architecture is this? Is this yet another number of loops and feedback iterations? When reporting that ""Our model can get a top-1 error of 25.1, while that of the ResNet-34 model is 26.5.” Please provide published accuracy for the baseline algorithm.

For the experiment on occlusions, the authors report using “a multi-recurrent model which is similar to the model mentioned in the Imagenet task”. Sorry but this is not good enough.

Table 4 has literally no explanation. What is FF? What are unroll times?

As a side note, VGG-GAP does not seem to be defined anywhere.

When stating ""We investigated VGG16 (Simonyan & Zisserman, 2014), a standard CNN that closely approximate the ventral visual hierarchical stream, and its recurrent variants for comparison.”, the authors probably meant “coarsely” not “closely"".","The sentiment score is -70 because the review is predominantly negative. The reviewer points out numerous issues with the paper, including problems with experimental validation, lack of key evaluations and controls, and modest improvements. The phrase 'the experimental validation is a mess' is particularly critical. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism ('the experimental validation is a mess') and sarcastic remarks ('Sorry but this is not good enough'). The reviewer also advises to 'tone down some of the claims', which could be perceived as slightly condescending. However, the review is not overtly rude, maintaining some level of constructive criticism.",-70.0,-20.0
"The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks.

Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g.

""LSTM: state memory and memory of a single external event""

to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs & DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. 

Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). ","The sentiment score is -70 because the reviewer expresses significant dissatisfaction with the paper. They state that they 'did not find the paper particularly well written' and that the taxonomy was 'not illuminating'. They also criticize the authors' simplifications as 'confusing' and 'misleading'. The reviewer finds little value in the paper's predictions and reductions, describing them as 'shady' and not 'realistic'. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. Phrases like 'Unfortunately I did not find...' and 'I actually felt...' are direct in their negative assessment. The use of 'Nit:' at the end also adds a slightly dismissive tone. However, the reviewer does not use overtly rude language, keeping the score from being lower.",-70.0,-20.0
"I raised my rating. After the rebuttal.

- the authors address most of my concerns.
- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.
- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.

-------------------------------------------------------------
This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters. The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.

1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters. 
- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters. After all, newton method and natural gradients method are not used in experiments.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed. 

2. No need to write so much decorated bounds in section 3. The convergence analysis is on Z, not on parameters x and hyper-parameters theta. So, bounds here can not be used to explain empirical observations in Section 5. 

3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?

4. Authors have done a good comparison in the context of deep nets.  However,
- could the authors compare with changing step-size? In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates. Is it better to decay learning rates for toy data sets? It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems. 
- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001"". What are reasons for these?
- In Section 5.2, it is said lambda is tuned by grid-search. Tuning a good lambda v.s. tuning a good step-size, which one costs more?
","The sentiment score is slightly positive (20) because the reviewer mentions raising their rating and that the authors addressed most of their concerns. However, they still have some reservations about the theory part and suggest improvements. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, offering constructive feedback without harsh criticism. They use phrases like 'please focus and explain more' and 'could authors explain', which are polite ways of requesting clarification. The reviewer also acknowledges the good comparisons done by the authors, balancing criticism with positive remarks.",20.0,50.0
"This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent.  In other words, representations where there is no information conveyed by combinations of latents that is not conveyed by considering each latent in isolation.  As the resultant target is intractable to evaluate, a number of approximations are employed for practical training.

The high-level idea is quite interesting, but the paper itself is quite a long way of convincing me that this is actually a good approach.  Moreover, the paper is a long way of the level of completeness, rigor, clarity, and polish that is required to seriously consider it for publication.  In short, the work is still at a relatively early stage and a lot more would need to be done for it to attain various minimum standards for acceptance.  A non-exhaustive list of specific examples of its shortfalls are given below.

1. The paper is over a page and a half under length, despite wasting large amounts of space (e.g. figures 3 and 4 should be two lines on the same plot)

2. The experimental evaluation is woefully inadequate.  The only quantitative assessment is to compare to a single different approach on a single toy dataset, and even then the metric being used is the one the new method uses to train for making it somewhat meaningless.

3. The introduction is completely generic and says nothing about the method itself, just providing a (not especially compelling) motivation for disentanglement in general.  In fact, the motivation of the introduction is somewhat at odds with the work -- correctly talking about the need for hierarchical representations which the approach actually actively discourages.

4. There are insufficient details on the algorithm itself in terms of the approximations that are made to estimate the synergistic mutual information.  These are mostly glossed over with only a very short explanation in the paragraph after equation 15.  Yes there are algorithm blocks, but these are pretty incomprehensible and lack accompanying text.  In particular, I cannot understand what A_w is supposed to be.  This is very important as I suspect the behavior of the approximation is very different to the true target.  Similarly, it would be good to provide more insight into the desired target (i.e. Eq 15).  For example, I suspect that it will encourage a mismatch between the aggregate posterior and prior by encouraging higher entropy on the former, in turn causing samples from the generative model to provide a poor match to the data.

5. The repeated claims of the approach and results being ""state-of-the-art"" are cringe-worthy bordering on amusing.  Writing like this serves no purpose even when it justified, and it certainly is not here.

6. There are a lot of typos throughout and the production values are rather poor.  For example, the algorithm blocks which are extremely messy to the point where they are difficult to follow, citep/citet mistakes occur almost every other citation, there is a sign error in Equation 16.


This is a piece of work in an exciting research area that,  with substantial extra work, could potentially result in a decent paper due to fact that the core idea is simple and original.  However, it is a long way short of this in its current state.  Along with addressing the specific issues above and improving the clarity of the work more generally, one thing in particular that would need to address in a resubmission is a more careful motivation for the method (ideally in the form of a proper introduction).  

Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.  Forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.  As you say in the first line of your own introduction, hierarchy and composition are key parts of learning effective and interpretable representations and this is exactly what you are discouraging.  A lot of the issue here is one of the disentanglement literature at large rather than this paper (though I do find it to be a particularly egregious offender) and it is fine to have different opinions.  However, it is necessary to at least make a sensible case for why your approach is actually useful.  

Namely, is there actually any real applications where such a simplistic disentanglement is actually useful?  Is there are anyway the current works helps in the longer vision of achieving interpretable representations?  When and why is the synergistic information a better regularizer than, for example, the total correlation?  The experiments you have do not make any inroads to answering these questions and there are no written arguments of note to address them.  I am not trying to argue here that there isn't a good case to be made for the suggested approach in the context of these questions (though I am suspicious), just that if the work is going to have any lasting impact on the community then it needs to at least consider them.","The sentiment score is -70 because the reviewer expresses significant criticism and skepticism about the paper. They state that the paper is 'quite a long way of convincing me that this is actually a good approach' and that it's 'a long way of the level of completeness, rigor, clarity, and polish that is required to seriously consider it for publication.' The reviewer lists numerous shortcomings and suggests that substantial extra work is needed. However, they do acknowledge that the core idea is 'simple and original,' preventing the score from being even lower. The politeness score is -20 because while the reviewer provides detailed feedback, some of the language used is quite harsh. For example, they describe claims in the paper as 'cringe-worthy bordering on amusing' and use phrases like 'woefully inadequate' to describe the experimental evaluation. The reviewer does attempt to soften some criticisms by acknowledging the potential of the work and providing constructive suggestions, which prevents the score from being even lower.",-70.0,-20.0
"The paper studies approximation and estimation properties of CNNs with residual blocks in the context
of non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),
and leveraging previous approximation results for such functions.
Explicit risk bounds are obtained for regression functions in Barron and Holder classes.

The main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs
contains a class of ""block-sparse"" fully-connected networks, with appropriate constraints on various size quantities.
This result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk
(Theorem 2, which mostly follows Schmidt-Hieber (2017)),
as well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.

The construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,
even with a filter size that is arbitrarily fixed.
However, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,
since they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,
which is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).
In particular, CNNs are typically used when there is some relevant inductive bias such as equivariance
to translations (and invariance with pooling operations) to take advantage of,
so removing this inductive bias by approximating fully-connected architectures seems a bit twisted.
The approach of reducing the function class to be approximated would seem more relevant here,
as in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of
the present paper can be useful in such a scenario as well.

Separately, the presentation of the paper could be significantly improved,
for instance by introducing relevant notions more clearly in the introduction and related work sections,
and by providing more insight and discussion of the obtained results in the main paper.

More specific comments:
- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs
- Section 2: Explain what is ""s"" in the Barron class, or at least point to the relevant definition in the paper
- Section 3.1:
  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat
  * why is the estimator ""regularized""?
- Definition 2: shouldn't it be D_m^(0) = D instead of 1?
- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.
- Section 4.2: M_1 is the Lipschitz constant of what function?
- Section 5.1: ""M = 1"" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.
- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting contributions, they express several concerns about the practical relevance of the approach and the need for improved presentation. The reviewer points out that the obtained CNN architectures seem 'quite unrealistic compared to most practical use-cases of CNNs' and suggests that the approach of reducing the function class to be approximated would be more relevant. They also mention that 'the presentation of the paper could be significantly improved.' However, the review is not entirely negative, as it recognizes the main contribution and the interesting construction in Theorem 1. The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'The construction of Theorem 1 is interesting' and provide constructive feedback. The reviewer also offers specific suggestions for improvement without using harsh or dismissive language. The use of phrases like 'could be significantly improved' and 'it would be helpful to' indicate a polite approach to criticism.",-20.0,50.0
"The paper presents an approach for an approach to addressing multi-goal reinforcement learning, based on what they call ""one-step path rewards"" as an alternative to the use of goal conditioned value function. 
The idea builds on an extension of a prior work on FWRL. 
The paper presents empirical comparison of the proposed method with two baselines, FWRL and HER. 
The experimental results are mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method. 
The idea of the proposed method is relatively simple, and is not theoretically justified. 

Based on these observations, the paper falls short of the conference standard. ","The sentiment score is -60 because the review is generally negative. The reviewer states that the paper 'falls short of the conference standard' and that the experimental results are 'mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method.' They also mention that the idea is 'not theoretically justified.' However, it's not entirely negative as they do acknowledge that the paper presents an approach and builds on prior work, hence not scoring at the extreme negative end. The politeness score is 0 (neutral) because the language used is professional and objective, without being particularly polite or rude. The reviewer presents their criticisms directly but without using harsh or insulting language, maintaining a neutral, academic tone throughout the review.",-60.0,0.0
"This paper presents an instruction-following model consisting of two modules: a
goal-prediction model that maps commands to goal representations, and an
execution model that maps goal representations to policies. The second module is
trained without command supervision via a goal exploration process, while the
first module is trained supervisedly in a metric learning framework.

This paper contains an important core insight---much of what's hard about
instruction following is generic planning behavior that doesn't depend on the
semantics of instructions, and pre-learning this behavior makes it possible to
use natural language supervision more effectively. However, the paper also
contains a number of serious evaluation and presentation issues. It is obviously
not ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence,
etc.) and should not have been submitted to ICLR in its present form.

SUPERVISION AND COMPARISONS

I found comparisons between supervision conditions in this paper difficult to
understand. It is claimed that the natural language instruction following
approaches described in the first paragraph ""require a large amount of human
supervision"" in the form of action sequences. This is not exactly true, as some
approaches (e.g. Artzi 2013), can be trained with only task completion signals.
More problematically, all these approaches are contrasted with reinforcement and
imitation learning approaches, which are claimed to use ""little human
supervision"". In fact, most of the approaches listed in this section use exactly
the same supervision---either action sequences (imitation learning) or task
completion signals (reinforcement learning). Indeed, the primary distinction is
that the ""NLP-style"" approaches are typically evaluated on their ability to
generalize to new instructions, while the ""RL-style"" approaches are evaluated on
the (easier) problem of fitting the complete instruction distribution as quickly
as possible.

This confusion carries into the evaluation of the approach proposed in this
paper, which is compared to RL and IL baselines. It's hard to tell from the
text, but it appears that this is an ""RL-style"" evaluation setting, where we
only care about rapid convergence rather than generalization. But the baselines
are inadequately described, and it's not clear to me that they condition on the
commands at all. More significantly, it's not clear what an evaluation based on
""timesteps"" means for a behavior-cloning approach---is this the number of
distinct trajectories observed? The number of gradient steps taken? Without
these explanations it is impossible to interpret the experimental results.

GENERALITY OF PROPOSED APPROACH

Despite the advantages of the high-level two-phase model proposed, the specific
implementation in this paper has two significant shortcomings:

- No evidence that it works with real language: despite numerous claims
  throughout the paper that the model is designed to interpret ""human
  instructions"", it is revealed on p7 that these instructions consist of one or two
  5-way indicator features. This is an extremely impoverished instruction space,
  especially compared to the numerous papers cited in the introduction that make
  use of large datasets of complex natural-language strings generated by human
  annotators. The present experiments do not support the use of the word ""human""
  anywhere in the paper.

- No support for combinatorial action spaces. Even if we set aside the
  distinctions between human-generated instructions and synthetic command
  languages like used in Hermann Hill & al., the goal -> policy module is
  defined by a buffer of cached trajectories and goal representations. While
  this works for the simple environments considered in this paper, it cannot
  generalize to real-world instruction-following scenarios where the number of
  distinct goal configurations is too large to tractably enumerate. Again, this
  is a shortcoming that existing approaches do not suffer from (given
  appropriate assumptions about the structure of goal space), so the lack of
  comparisons is problematic.

CLARITY

The whole paper would benefit from copy-editing by an experienced English
speaker, but a few sections are particularly problematic:

- The first paragraph of 4.1.1 is extremely difficult to understand What does
  the fingertip do? What exactly is the action space?

- The end of the second paragraph is also difficult to understand; after reading
  it I still don't know what the extra ""position"" targets do.

- 4.1.4 is cut off mid-way through a sentence.

- last sentence of 4.2

The figures are also impossible to interpret: three of the four are captioned
""overview of the proposed framework"", and none are titled.","The sentiment score is -60 because the reviewer expresses significant concerns about the paper, including 'serious evaluation and presentation issues', stating it is 'obviously not ready to publish', and pointing out multiple shortcomings. However, they do acknowledge an 'important core insight', preventing the score from being even lower. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I found... difficult to understand' rather than more accusatory language, and provide specific, constructive feedback. The reviewer also acknowledges positive aspects before diving into criticisms, which is a polite approach. However, some statements like 'should not have been submitted to ICLR in its present form' are quite blunt, preventing a higher politeness score.",-60.0,20.0
"Summary: The authors present a network which facilitates cross-domain
learning for SLU tasks where the the goal is to resolve intents and
slots given input utterances. At a high level, the authors argue that
by fine-tuning a pre-trained version of the network on a small set of
examples from a target-domain they can more effectively learn the
target domain than without transfer learning.

Feedback:

* An overall difficulty with the paper is that it is hard to
distinguish the authors' contributions from previous works. For
example, in Section 3.1, the authors take the model of Goyal et al. as
a starting point but explain only briefly one difference
(contatenating hidden layers). In Section 3.2 the contributions
becomes even harder to disentangle. For example, how does this section
relate to other word-embeddings papers cited in this section? Is the
proposed method a combination of previous works, and if not, what are
the core new ideas?

* Some sections are ad-hoc and should be justified/explained
better. For example, the objective, which ultimately determines the
trained model behaviour uses a product of experts formulation, yet the
authors do not discuss this. Similarly, the overarching message, that
by fine-tuning a suitable model initialisation using small amounts of
data from the target domain is fairly weak as the authors do not
detail exactly how the model is fine-tuned. Presumably, given only a
small number of examples, this fine-tuning runs the risk of
overfitting, unless some form of regularisation is applied, but this
is not discussed.

* Lastly, there are some curious dips in the plots (e.g., Figure 2 bottom left, Figure 3 top left, bottom left), which deserve more explanation. Additionally, the evaluation section could be improved if the scores were to show error-bars. 

Minor: All plots should be modified so they are readable in grey-scale.","The sentiment score is -50 because the review is generally critical, pointing out several areas for improvement and expressing concerns about the paper's clarity and contributions. However, it's not entirely negative as it acknowledges the authors' work and provides constructive feedback. The politeness score is 20 because while the reviewer maintains a professional tone and offers specific suggestions for improvement, the language is direct and doesn't include many polite phrases or softening language. The reviewer uses phrases like 'An overall difficulty with the paper is...' and 'Some sections are ad-hoc and should be justified/explained better,' which are straightforward but not particularly polite. The review does, however, avoid harsh or rude language, maintaining a respectful tone throughout.",-50.0,20.0
"This paper presents a framework to train an end-to-end multi-lingual multi-speaker speech recognition system. Overall, the paper is quite clear written.
- Strengthens:
+ Experimental results show consistent improvements in speech recognition performance and language identification performance.

- Weakness:
+ I'm not sure whether the framework is novel. The authors have just mixed training data from several languages to train an end-to-end multi-speaker speech recognition system.
+ I don't see the real motivation why the authors want to make the task harder than needed. The example provided in figure 1 is very rare in reality.
+ The authors claimed that their system can recognise code-switching but actually randomly mixing data from different languages are not code-switching.
+ In general, it would be better to have some more analyses showing what the system can do and why.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths (experimental results showing improvements), they express several significant weaknesses and doubts about the novelty and motivation of the work. The overall tone is more critical than positive. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, avoiding harsh criticism or personal attacks. They begin with a positive comment about the paper being 'quite clear written' and use neutral language to express their concerns, such as 'I'm not sure' and 'It would be better to have'. The review maintains a constructive tone while pointing out areas for improvement.",-20.0,50.0
"The paper proposes W2GAN, a GAN where the objective function relies on a W2 distance. Authors state that the discriminator approximate the W2 distance, and that the generator follows an OT map. 
While I did not see any flaws in the development, the paper is quite bushy and hard to follow. Some questions are still open, for instance in the end of the experiments, authors state that the model has ""a strong theoretical advantages"": can you provide more details about those advantages?
The experiments do not show any clear advantages of the method regarding competitors. Regarding Table 1, why are there some points with no arrows? W2-OT seems not to perform better: are there some other advantages (computational?) to use the method? In Figure 1, it is quite difficult to evaluate the results on a single image with no comparisons. Again, providing a strong evaluation of the method would help to strengthen the paper. 

There are some weird statements and typos mistakes that should be corrected. For example in the first 2 pages: (abstract) ""other GANs also approximately following the Optimal Transport"", (Introduction) ""An optimal map has many important implications such as computing barycenters"", ""high-dimenisonal"", ""generator designed"", ""consideral"", ""although the theoretical arguments do not scale immediately"".
The layout of the bibliography should be deeply reviewed.

","The sentiment score is slightly negative (-20) because while the reviewer doesn't see any flaws in the development, they express concerns about the paper being 'bushy and hard to follow', lack of clear advantages over competitors, and the need for stronger evaluation. The reviewer also points out several typos and weird statements that need correction. However, the tone is not entirely negative, as they ask for more details and clarifications rather than outright rejecting the work. The politeness score is slightly positive (20) because the reviewer uses neutral language and phrases their criticisms as questions or suggestions rather than harsh statements. They acknowledge the work done ('I did not see any flaws in the development') and provide specific examples of issues to be addressed, which is constructive. The reviewer maintains a professional tone throughout, even when pointing out errors.",-20.0,20.0
"The paper applies graph convolutional networks to Penn Treebank language modeling and provides analysis on the attention weight patterns it uses.

Clarity: the paper is very clearly written!

The introduction states that existing CNN language models are ""not easily interpretable in that they do not explicitly learn the structures of sentences"". Why is this? The model in this paper computes attention values which is interpreted by the authors as corresponding to the structure of the sentence but there are equivalent means to trace back feature computation in other network topologies as well.

My biggest criticism is that the evaluation is done on a very small language modeling benchmark which is clearly out of date. Penn Treebank is the CIFAR10 of language modeling and any claims on this dataset about language modeling are highly doubtful. Models today have tens and hundreds of millions of parameters and training them on 1M words is simply a regularization exercise that does not enable a meaningful comparison of architectures.

The claims in the paper could be significantly strengthened by reporting results on at least a mid-size dataset such as WikiText-103, or better even, the One Billion Word benchmark.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('very clearly written'), they express significant criticism about the evaluation methodology and dataset choice. The reviewer suggests that the claims could be 'significantly strengthened' by using more up-to-date and larger datasets, indicating that the current work has notable limitations. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positives ('very clearly written') and framing criticisms as suggestions for improvement rather than harsh judgments. The reviewer maintains a professional tone, even when expressing concerns about the paper's methodology.",-20.0,60.0
"
-------------
Summary
-------------
The authors propose to train a policy while concurrently learning a dynamics model. In particular, the policy is updated using both the RL loss (rewards from the environment) and the ""consistency constraint"", which the authors introduce. This consistency constraint is a supervised learning signal, which compares trajectories in the environment with trajectories in the imagined world (produced with the dynamics model). 

---------------------
Main Feedback
---------------------
I feel like there might be some interesting ideas in this work, and the results suggest that this approach performs well. However, I had a difficult time understanding how exactly the method works, and what its advantages are. These are my main questions:

1) At the beginning of Section 4 the authors write ""The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen"". Do you actually do this? This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path. Does the reward signal always come from the true environment?
2) Is the dynamics model used for anything else than action-selection during training? Planning? If not, I don't really understand the results and why this works at all (k=20 being better than k=5, for example).
3) Is the dynamics model pre-trained in any way? I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3). If the agent chooses its actions based on the state that is predicted by the dynamics model, this should throw off the learning of the policy at the beginning (when the dynamics model hasn't learned anything sensible yet).

-----------------------
Other Questions
-----------------------
4) How exactly does training without the consistency constraint look? Is this the same as k=1?
5) Could the authors comment on the evaluation protocol in the experimental section? Are the results averages over multiple runs? If so, it would help to see confidence intervals to make a fair assessment of the results. 
6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so? If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?

---------
Clarity
---------
The title and introduction initially gave me a slightly wrong impression on what the paper is going to be about, and several things were not followed up on later in the paper.
Title:
8) ""generative models"" reminds of things like a VAE or GAN; however, I believe the authors mean ""dynamics models"" instead
9) ""by interaction"" is a bit vague as to what the contribution is (aren't policies and dynamic models in general trained by interacting with the environment?); the main idea of the paper is the consistency constraint
Abstract / Introduction:
10) The authors talk about humans carrying out ""experiments via interaction"" to help uncover ""true causal relationships"". This idea is not brought up again in the methods section, and I don't see evidence that with the proposed approach, the policy does targeted experiments to uncover causal relationships. It is not clear to me why this is the intuition that motivates the consistency constraint. 
11) As the authors state in the introduction, the hope of model-based RL is better sample complexity. This is usually achieved by using the model in some way, for example by planning several steps ahead when choosing the current action. Could the authors comment on where they would place their proposed method - how does it address sample complexity?
12) In the introduction, the authors discuss the problem of compounding errors. These must be a problem in the proposed method as well, especially as k grows. Could the authors comment on that? How come that the performance is so good for k=20?
13) The authors write that in most model-based approaches, the dynamics model is ""learned with supervised learning techniques, i.e., just by observing the data"" and not via interaction. There's two things I don't understand: (1) in the existing model-based approaches the authors refer to, the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference? (2) The auxiliary loss ""which explicitly seeks to match the generative behaviour to the observed behaviour"" is just a supervised learning loss as well, so how is this different?

For me, it would help the readability and understanding of the paper if some concepts were introduced more formally.
14) In Section 2, it would help me to see a formal definition of the MDP and what exactly is optimised. The authors write ""optimise a reward signal"" and ""maximise its expected reward"", however I believe it should be the expected cumulative reward (i.e., return). 
15) The loss function for the dynamics model is not explicitly stated. From the text I assume that it is the mean squared error for the per-step loss, and a GAN loss for the trajectory-wise loss.
16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined? Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?
17) In 2.3 the variable z_t is not formally introduced. What does it represent?

------------------------
Other Comments
------------------------
18) I find it problematic to use words such as ""hallucination"" and ""imagination"" when talking about learning algorithms. I would much prefer to see formal/factual language (like saying that the dynamics model is used to do make predictions / do planning, rather than that the agent is hallucinating). 

-- edit (19.11.) ---
- updated score to 5
- corrected summary","The sentiment score is -30 because while the reviewer acknowledges some interesting ideas and good results, they express significant confusion about the method and list numerous questions and concerns. The overall tone suggests skepticism about the paper's clarity and contributions. The politeness score is 50 because the reviewer uses respectful language throughout, framing criticisms as questions or suggestions, and acknowledges potential merits of the work. They use phrases like 'Could the authors comment on...' and 'It would help me to see...' which maintain a constructive tone. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a professional, matter-of-fact style.",-30.0,50.0
"This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. 

The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezuma’s revenge, Pitfall!, and Private eye over a factor of 2. 

It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. 

================================
I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. ","The sentiment score is -50 because the review starts positively, highlighting the strong experimental results, but then shifts to a more negative tone. The reviewer mentions a weak point and ultimately decreases their score after reading other reviews, indicating overall dissatisfaction. The final paragraph expresses agreement with other reviewers' criticisms, suggesting the paper has significant limitations. The politeness score is 20 because the language used is generally professional and respectful. The reviewer acknowledges both strengths and weaknesses without using harsh language. They express their concerns in a constructive manner, such as calling the weak point 'minor' and describing the idea as 'interesting' even while decreasing the score. However, the politeness is not extremely high, as the review is quite direct in its criticisms, particularly in the final paragraph.",-50.0,20.0
"As a paper on how to prioritize the use of neurons in a memory this is an excellent paper with important results. 

I am confused by the second part of the paper an attached GAN of unlimited size. It may start out small but there is nothing to limit its size over increased learning. It seems to me in the end it becomes the dominate structure. You start the abstract with ""able to learn from a stream of data over an undefined period of time"". I think it would be an improvement if you can move this from an undefined time/memory size to a limited size for the GAN and then see how far that takes you. ","The sentiment score is 70 (positive) because the reviewer starts by calling it an 'excellent paper with important results', which is highly positive. However, the score is not higher because the reviewer expresses confusion about the second part of the paper and suggests an improvement, indicating some reservations. The politeness score is 50 (somewhat polite) because the language is generally respectful and constructive. The reviewer uses phrases like 'I am confused' and 'I think it would be an improvement' rather than harsh criticisms. However, the tone is more neutral than overtly polite, hence the moderate positive score. The reviewer offers suggestions for improvement in a straightforward manner without excessive courtesy language.",70.0,50.0
"The paper proposes using structured matrices, specifically circulant and diagonal matrices, to speed up computation and reduce memory requirements in NNs. The idea has been previously explored by a number of papers, as described in the introduction and related work.  The main contribution of the paper is to do some theoretical analysis, which is interesting but of uncertain impact.

The experiments compare performance against DeepBagOf`Fframes (DBOF) and MixturesOfExperts (MOE). However, there are other algorithms that are both more competitive and more closely related. I would like to see head-to-head comparisons with tensor-based algorithms such as Novikov et al: https://papers.nips.cc/paper/5787-tensorizing-neural-networks, which achieves huge compression ratios (~200 000x), and other linear-algebra based approaches. 

AFTER READING REBUTTAL
I've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. 

I strongly agree with authors when they state: ""We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings"".  I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper's contributions and interesting theoretical analysis, they also express some reservations about its impact and comparisons. The reviewer's tone becomes more positive after reading the rebuttal, increasing their score and expressing strong agreement with some points. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and shows willingness to adjust their opinion based on the authors' rebuttal. They also express hope for future research in the field, which adds a collegial tone to the review.",20.0,50.0
"In this paper, a framework for lifelong learning based on Bayesian neural network is proposed. The key idea is to combine iterative pruning for multi-task learning along with the weight regularization. The idea of iterative pruning was first considered by Mallya et al., 2018 and weight regularization was considered for Bayesian neural network by Nguyen et al., 2018.

Pros: 
- Combination of two idea seems novel. I like the idea of considering the weight parameter as the ""global"" random variables and the mask parameters as the task-specific random variables. 

Cons: 
- In general, there is lack of explanation/justification on the combination of two ideas. Especially, there is lack of explanation on how to apply the whole algorithm (e.g., text states that complete algorithm is in Algorithm 3., but there is no Algorithm 3. in the paper). 

- I do not understand how equation (6) is developed, and why hyper-parameters are need for ""regularization of weights"", comparing with the Variational Continual Learning (VCL, Nguyen et al., 2018). More explanation seems necessary for justification of the algorithm.

- More stronger baselines need to be considered for the experiments. Why is there no comparison with the existing continual learning algorithms? At the very least, comparison with the VCL or Elastic Weight Consolidation (EWC, Kirkpatrick et al., 2017) seems necessary since one of the key idea is about regularization for weights.


In general, I think it is a nice idea to combine two existing approaches. However, the algorithm lacks justification in general and experimental results are not very persuasive.   ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they express more concerns ('Cons') and conclude that the algorithm lacks justification and the experimental results are not very persuasive. The overall tone suggests the paper needs significant improvements. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, acknowledging the novelty of the idea and providing constructive criticism without harsh or rude comments. They use phrases like 'I like the idea' and 'I think it is a nice idea' which contribute to a polite tone, while also clearly stating areas that need improvement.",-20.0,50.0
"The authors propose to use importance resampling (IR) in place of importance sampling (IS) for policy evaluation tasks. The method proposed by the authors definitely seems valid, but it isn’t quite clear when this is applicable.

IR is often used in the case of particle filters and other SMC is often used to combat the so-called “degeneracy problem” where a collection of particles (or trajectories) comes to degenerate such that all the mass is concentrated onto a single particle. This does not seem to be the case here, as the set of data (the replay buffer) does not seem to be changing over time. In particular, since the target policy and the behavior policy are fixed, the bigger issue seems to be that the distribution itself will not change over time.

Finally, the results are given for somewhat simple problems. The first two settings show that the difference between IR/IS can be very stark, but it seems like this is the case when the distributions are very different and hence the ESS is very low. The IR methods seem like they can eliminate this deficiency by only sampling from this limited subset, but it is also unclear how to extend this to the policy optimization setting.

Overall I have questions about where these results are applicable. And finally, as stated a moment ago, it is unclear how these results could be extended to the setting of off-policy policy optimization, where now the resulting policies are changing over time. This would necessitate updating the requisite sampling distributions as the policies change, which does seem like it would be difficult or computationally expensive (unless I am missing something). Note that this is not an issue with IS-based methods, because they can still be sampled and re-weighted upon sampling.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the validity of the proposed method, they express several concerns and questions about its applicability and extensibility. The review points out limitations and unclear aspects of the work, which contributes to the overall negative sentiment. However, it's not extremely negative as the reviewer does recognize some merits of the work.

The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'The method proposed by the authors definitely seems valid' and 'Overall I have questions' rather than making harsh criticisms. The reviewer also frames their concerns as questions or areas for clarification rather than outright dismissals. However, the score is not extremely high as the language, while polite, is not overly deferential or complimentary.",-20.0,50.0
"Summary: The paper considers federate learning of neural networks, i.e. data are distributed on multiple machines and the allocation of data points is potentially inhomogenous and unbalanced. The paper proposes a method to combine multiple networks trained locally on different data shards and form a global neural network. The key idea is to identify and reuse neurons that can be used for all shards and add new neurons to the global network if necessary. The matching and combination process is done via MAP inference of a model using a Beta-Bernoulli process. Some experiments on federated learning demonstrate the performance of the proposed method.

General evaluation (+ pro/ - con, more specific comments/questions below):
+ the paper is very well-written -- the BBP presentation is light but very accessible. The experimental set up seems sound.
+ the matching procedure is novel for federated training of neural networks, as far as I know, but might not be if you are a Bayesian nonparametric person, as the paper pointed out similar techniques have been used for topic models.
- the results seem to back up the claim that the proposed is a good candidate for combining networks at the end of training, but the performance is very similar or inferior to naive combination methods and that the global network is way larger than individual local network and nearly as large as simply aggregating all neurons together.
- the comparison to recent federated learning methods is lacking (e.g. McMahan et al, 2017) (perhaps less communication efficient than the proposed method, but more accurate).

Specific comments/questions/suggestions:
- the MAP update for the weights given the assignment matrix is interesting and resembles exactly how the Bayesian committee machine algorithm of Tresp (2000) works, except that the variances are not learnt for each parameter but fixed for each neuron. On this, there are several hyperparameters for the model, e.g. variance sigma_j -- how are these tuned/selected?
- the local neural networks are very mall (only 50 neurons per layer). How do they perform on the test set on the homogeneous case? Is there a performance loss by combining these networks together?
- the compression rate is not that fantastic, i.e. the global network tends to add new neuron for each local neuron considered. Is this because it is in general very hard to identify similar neuron and group them together? In the homogeneous case, surely there are some neurons that might be similar. Or is it because of the MAP inference procedure/local optima?","The sentiment score is slightly positive (20) because the reviewer acknowledges several positive aspects of the paper, such as it being well-written and presenting a novel approach. However, they also point out significant limitations, which balances out the positive aspects. The politeness score is high (80) as the reviewer uses respectful language throughout, clearly separating positive and negative points, and phrasing criticisms as suggestions or questions rather than direct attacks. They use phrases like 'the paper is very well-written' and frame negatives as 'the results seem to' or 'the comparison... is lacking', which maintains a polite tone while still conveying critical feedback.",20.0,80.0
"The contributions of this paper are in the field of LSTM, where the authors explore the interpretability of LSTM with multivariate data obtained from various and disparate applications. To this end, the authors endow their approach with tensorized hidden states and an update process in order to learn the hidden states. Furthermore, the authors develop a mixture attention mechanism and a summarization methods to quantify the temporal and variable importance in the data. They validate the forecasting  and interpretability performance of their approach with experiments. 

The parer is interesting, well structured and and clearly written. Also, the addressed topic of interpretability is pertinent. However, I have several concerns.

1. In the related work the authors state that “In time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model ” . This is not always right - it is not imperative to add the auto-regressive terms, this is optional and depends on the way we want to formulate our time series forecasting approach and the known constraints.  
2. In section 3 — Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. And I have several concerns with this for time series forecasting. Because, the past is not always a predictor of the future even - particularly in time series context and in industrial settings. And in the occasions where the past allows to predict the future we do not necessarily need to use LSTM to forecast (the notion of persistency in forecasting is enough).  Therefore, the power of LSTM in forecasting would have been convincing if you omit the target series in your multi-variable input.
3. In Network Architecture section the authors develop tensorized hidden state and an update scheme. This idea is interesting, I think it would also be good to know what is the algorithmic complexity of this approach? 
4.  In section 3.3 the authors state that ""In the present paper, we choose the simple normalized summation function eq.(9). "" Could the authors justify the reason behind this choice? I am not convinced of the reason behind this, especially the authors mention, right after,  that ""It is flexible to choose alternative functions for f_{agg}""

5. In the experiment section, concerning the prediction performance the authors present a table showing their results, I believe it would have been more compelling to present the prediction results with graphs showing the normalized cumulative errors, as an example.

6. With regard to the interpretation of the results, the authors show the variable importance as a function of the epoch number, it would be equally important to correlate the same figure with the associated prediction results/normalized cumulative errors as a function of the epoch number - this will allow to assess the importance of the interpretability.

I think it would be important to further justify the pertinence of this work in terms of interpretability (the statement in the introduction ""the interpretability of prediction models is essential for deployment and
knowledge extraction"" seems to be limited) for example what does it bring knowing the variance importance  as a function of the epoch number. As an example, the Pearson correlation coefficient can help select relevant features to a model, and restrict the number of inputs to the relevant ones - can we draw inspiration from this and explain what the authors are proposing in terms of interpretability... Here the idea is to have a motivation presenting the merits of this work, which I think is missing - particularly with the experiments presented here.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'interesting, well structured and clearly written' and the topic as 'pertinent', they express 'several concerns' and provide multiple critiques and suggestions for improvement. This indicates a generally critical stance despite some positive aspects. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges positive aspects of the paper, and frames criticisms as suggestions or questions (e.g., 'Could the authors justify...', 'I think it would be important to...'). The reviewer maintains a professional tone without using harsh or dismissive language, even when expressing concerns.",-20.0,60.0
"This paper proposes to learn a dynamics model (state to pixels using Generative Adversarial Networks), use this model in conjunction with Monte Carlo Tree Search, model-free reinforcement learning (Q-learning) and a reward prediction network essentially combining model-free with model-based learning. The proposed approach is empirically evaluated on a small subset of the Atari games and theoretical analysis for the bias-variance tradeoff are presented.

It is highly appreciated that this paper presents an idea and discusses why the proposed approach does not result in high performance. This is very valuable and useful for the community. On a high level it would be very useful if Figure 1 would be show less examples but present them much larger since it is almost impossible to see anything in a printout. Further, the caption does not lend itself to understand the figure. Similarly Figure 2 would benefit from a better caption. 

The first part of the discussion (7), the individual building blocks, should be mentioned much earlier in the paper. It would be further useful to add more related work on that abstraction level. This would help to communicate the main contribution of this paper very precisely.

On the discussion of negative results: It is very interesting that Dyna-Q does not improve the performance and the hypothesis for why this is the case seems reasonable. Yet, it would be very useful to actually perform an experiment in a better controlled environment for which e.g. the dynamics model is based on the oracle and assess the empirical effect of different MCTS horizons and rollout estimates. Further, this scenario would allow to further quantify the importance and the required “quality” of the different learning blocks.

In its current form the paper has theoretical contributions and experimental results which cannot be presented in the main paper due to space constraints. Albeit the appendix is already very extensive it would be very useful to structure it into the theoretical derivation and then one section per experiment with even more detail on the different aspects of the experiment. The story of the main paper would benefit from referencing the negative results more briefly and better analyzing the different hypothesis on toy like examples. Further, the introduction could be condensed in order to allow for more in detail explanations and discussions without repetition later on.

As argued in the paper it is clear that image generation is a very expensive simulation mechanism which for games like pong which depend on accurate modeling of small aspects of the image are in itself difficult. Therefore, again although really appreciated, the negative results should be summarized in the main paper and the hypothesis concluded better analyzed. The extensive discussion of hyper parameters and approaches for individual components could be in the appendix and the main paper focuses on the hypothesis analysis.
","The sentiment score is slightly negative (-20) because while the reviewer appreciates the paper's discussion of negative results, they also point out several areas for improvement. The reviewer suggests restructuring the paper, condensing the introduction, and providing more detailed analysis of hypotheses. They also mention that some figures are difficult to understand and could be improved.

The politeness score is moderately positive (60) because the reviewer uses respectful language throughout. They begin by stating that the paper is 'highly appreciated' and that it provides 'very valuable and useful' information for the community. The reviewer offers constructive criticism and suggestions for improvement rather than harsh criticism. Phrases like 'it would be very useful' and 'it is very interesting' maintain a polite tone while providing feedback.

Overall, the review balances constructive criticism with positive acknowledgment of the paper's contributions, maintaining a professional and respectful tone throughout.",-20.0,60.0
"Summary: 
The paper considers finding the most adversarial random noise given multiple classifiers. They formulate the problem as the standard min-max game and apply the multiplicative weight updates. The technical contribution is to clarify the computational complexity of implementing/approximating the response oracle. The authors show experimental results.

Comments: 
I am afraid that the main technical result is already known:

Yoav Freund Robert E. Schapire: Adaptive game playing using multiplicative weights, Games and Economic Behavior, 29:79-103, 1999.

The paper shows that a multiplicative update algorithm can approximately solve the min-max game. If you use the result, you can readily obtain the main results of the present paper. 

After rebuttal:
I read the authors comments and I understood the technical contribution more and raised my score.  Implementing/appriximating the response oracle is non-trivial. For MWU, I still think that the above paper should be cited (citing the Adaboost paper is not enough) since the paper shows MWU solves the min-max game.  
","The sentiment score is slightly negative (-20) because the reviewer initially expresses concern that the main technical result is already known, which is a significant criticism. However, after the rebuttal, the reviewer acknowledges a better understanding of the technical contribution and raises their score, which mitigates the negativity somewhat. The politeness score is slightly positive (20) because the reviewer uses polite language throughout, such as 'I am afraid that...' and 'I read the authors comments and I understood...'. They also provide constructive feedback and suggestions for improvement, which is a polite approach to reviewing. The reviewer maintains a professional tone, even when expressing criticism, and shows willingness to reconsider their initial assessment based on the authors' rebuttal.",-20.0,20.0
"This paper is poorly written, and looks like it was not proof-read. 
Presentation of the problem at hand is presented over so many times that it becomes confusing.
Authors ought to better define the image description space of the objects and the haptic space. 
More interesting would have been a good explanation of the different sensors used in the anthropomorphic hand  and the vector built to represent the different sensed objects.
The most expected contribution of this work is barely explained: how the haptic sensors' values/object labels vectors were built and fed to the predictor network, what their values looked like for the various objects, how these vectors clustered for the various objects etc.

Among the many evident weaknesses:
- Domain specific concepts and procedures of most importance to this work are not explained: ""... measure various physical properties of objects using the bio-tac sensor using five different exploration procedures (EP)"".  Page 3, Paragraph 1. Bio-tac sensor and most importantly exploration procedures (EP) should be presented more clearly.
- Incomplete and out of nowhere sentences are common: ""The SHAP procedure
was established for evaluating prosthetic hands and arms. With this idea in mind, prior work (?)
built a prosthetic arm which could ..."" Page 4, Paragraph 1.
- Many references are not well introduced and justified: ""We then trained the network using
ADAM (Kingma & Ba (2014)) with an initial learning rate set to 1e-4."" Page 4, Paragraph 6. In the same paragraph,  authors explain using ""The groundtruth predictions were per-channel averaged haptic forces"" without having defined those channels (that one can guess but shouldn't). Concepts have to be clearly defined prior to their use.


","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'poorly written,' 'confusing,' and lists many 'evident weaknesses.' There are no positive comments, only criticisms and suggestions for improvement. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and critical. The reviewer uses phrases like 'poorly written,' 'barely explained,' and 'incomplete and out of nowhere sentences are common,' which are direct and somewhat harsh criticisms. However, the reviewer does offer specific recommendations for improvement, which prevents the score from being even lower on the politeness scale.",-80.0,-20.0
"This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. 

Pro:

The paper is clear and well written and the contribution is relevant to ICLR. NTP systems by combining the advantages of neural models and symbolic reasoning are a promising research direction. Even though the results presented are lower than previous studies, they present the advantage of being interpretable.

Cons:

I'm not convinced by the model used to integrate textual mentions. The evaluation proposed in section 6.3 proposes to replace training triples by textual mention in order to evaluate the encoding module. However, it seems to me that, in this particular case, these mentions are very short sentences.  This could explained why such a simplistic model that simply average word embeddings is sufficient. I wonder if this would still work for more realistic (and thus longer) sentences.

Minor issues:

-Page 1: In particular [...] (NLU) and [...] (MR) in particular, ...
","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's clarity, relevance, and potential of the research direction. However, they also express some concerns about the model's effectiveness for longer sentences. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'The paper is clear and well written' and 'I'm not convinced' rather than harsh or dismissive language. The reviewer also offers specific suggestions for improvement, which is a polite way to provide feedback.",50.0,75.0
"This paper introduces a new way of interpreting the VQ-VAE, 
and proposes a new training algorithm based on the soft EM clustering. 

I think the technical aspect of this paper is written concisely. 
Introducing the interpretation as hard EM seems natural for me, and the extension
to the soft EM training is sound reasonable. 
Mathematical complication is limited, this is also a plus for many non-expert readers. 

I'm feeling difficulties in understanding the experimental part.
To be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. 
I'm just wondering why this happens, given clean and organized technical sections...

First, I'm confusing what is the main competent in the Table 1. 
In the last paragraph of the page 6, it reads; 
""Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10).""
However, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? 

Second, terms ""VQ-VAE"", (soft?)""EM"" and ""our {model, approach}"" are used in a confusing manner. 
For example, in Table 1, below the row ""Our Results"", there are:
- VQ-VAE
- VQ-VAE with EM
- VQ-VAE + distillation
- VQ-VAE with EM + distillation

The ""VQ-VAE"" is not the proposed model, correct? 
My understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to ""VQ-VAE with EM"". 

Third, a paragraph ""Robustness of EM to Hyperparameters"" is mis-leading. 
The figure 3 does not show the robustness against a hyperparameter. 
It shows the BLEU against the number of ""samples"" (in fact, there is no explanation about what the ""samples"" means). 
I think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. 
The figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. 
However, there is no explanation which hyperparameter is tested to assess ""the robustness to hyperparameters"". 

Fourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. 
In fact, there is a short paragraph that mentions about the SVHN results, 
but it only refers to the appendix. 
I think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. 
However, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. 
So, the authors should include the image reconstruction results in the main body of the paper. 
Otherwise, claims about the image reconstructions should be removed from the abstract, etc. 


+ Insightful understanding of the VQ-VAE as hard EM clustering
+ Natural and reasonable extension to soft-EM based training of the VQ-VAE
-- Unorganized experiment section. This simply ruins the quality of the technical part. 


## after feedback

Some of my concerns are addressed the feedback. 
Considering the interesting technical parts, I raise the score upward, to the positive side. ","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper's technical merits and interesting aspects, they also express significant concerns about the experimental section. The initial critique is quite harsh, but the reviewer raises their score 'to the positive side' after feedback. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, balancing criticism with praise. They use phrases like 'I think' and 'I'm feeling difficulties' to soften critiques, and acknowledge positive aspects such as 'insightful understanding' and 'natural and reasonable extension'. The reviewer also explains their reasoning clearly, which is a polite approach to criticism.",20.0,50.0
"This paper tries to draw connections between rate distortion theory and DNNs and use some intuitions from that domain to draw conclusions about robustness and generalization of the DNNs. 

The paper is mostly written in a storytelling narrative with very little rigor. In my opinion, this lack of rigor is problematic for a conference paper that has to be concise and rigorous. Moreover, the story is not told in a cohesive way. In most parts of the paper, there is not much relationship between the consecutive paragraphs. And even within most of the paragraphs, I was lost in understanding what the authors meant. I wish the paper would have been self-contained and made concrete definitions and statements instead of very high-level ideas that are difficult to judge. In the current state, it is very difficult for me to say what exactly is the contribution of the paper in terms of the story other than some loosely related high-level ideas. I feel like most parts the story that the authors are telling is already told by many other papers in other forms(papers that authors have cited and many other ones).


","The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's lack of rigor, cohesiveness, and clarity. They state that it's 'difficult to judge' the paper's contributions and suggest that much of the content may not be novel. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their criticism is direct and unmitigated. Phrases like 'very little rigor', 'problematic', and 'very difficult for me to say what exactly is the contribution' convey a harsh assessment without softening language. The reviewer does attempt some politeness by using phrases like 'In my opinion' and 'I wish', but overall the tone is more critical than polite.",-70.0,-20.0
"Revised Review:

The authors of this work has taken my concerns, and concerns of other reviewers, and revised their paper during the rebuttal period. They have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the Appendix section), and committed to open-sourcing the platform post publication. For these reasons I believe this work is now at a state that should be published at ICLR, and I revised my score from 5 to 7. I hope other reviewers can reread the work and post their updated comments.

I'm excited about the work, because it incorporates good ideas from A-Life / evolution / open-endedness communities, to introduce new paradigms and new ways of thinking to the RL community. I look forward to using this environment in my own research going forward, regardless of whether this work gets accepted or not. Good luck!

Minor comment: On page 4, the section 5 Experiments, I think ""Technical details"" should be in bold font before the sentence ""We run each experiment using 100 worlds."" so it is distinguished from being part of that sentence.

Original Review:

The authors present a new game environment inspired by MMORPGs. The environment supports a ""massive"" number of agents, each have different neural net brains (*) and have some foraging and combat skills. They use distributed RL to train the policies (using REINFORCE) and over time can observe the dynamics of the population of these artificial life agents interact with each other, where the only reward is survival. There are many interesting insights, such as looking at how multi-agent cooperative (and deceptive) strategies emerge, and how some agents with different niche skills co-evolve with agents with other niche skills. They also plan to open source the platform and I have high hopes that this will be a fantastic research environment. While I'm very optimistic about this work and direction, there are issues with this particular paper, and I feel it is not ready for publication in its current form. While I have no doubt that the software project will be great, as a reviewer I'm evaluating this particular paper, and I want to highlight flaws about the paper and what can be done to fix it during the rebuttal/review period.

My recommendations to improve the article:

(1) Writing - I really enjoyed this work, but frankly, the writing is horrible. It took me days of effort to decipher every paragraph and understand all the terms and what is going on. The article reads like it is written by the person who programmed the game, and played MMORPGs almost every day of his childhood and adult life, so someone who is not reading the article thru the lens of the author might have an incredibly tough time digesting the content. For instance, there are sentences like ""It adds melee, ranged, and magic-based combat""... ""Melee, ranged, and magic combat have maximum Manhattan distance of effect of 1, 2, and 3 cells respectively. They do 10, 2, and 1 damage respectively""... ""This prevents uninteresting 'spawn killing' and is a common technique in human games"". These are only a small selection of samples. There are also terminology like ""#ent and #pop"" which I feel should be replaced by $N_{ent}$ and $N_{pop}$ for a paper. In contrast, older works related to population-based RL training like [2], or RL in games like [3] are examples of clear and understandable writing. I highly recommend you give the draft to someone outside of your team, who is sufficiently isolated from this project (or perhaps to a professional writer if your lab has one), to go over each paragraph, and make the writing more clear. This would benefit the work in the long term as people refer back to the paper when they run your code.

(2) Diagrams - While the diagrams look interesting, IMO they are poorly made. When I look at Figure 1, 4, and 9, it is really difficult to understand what is going on. I recommend redoing the diagrams, perhaps get some inspiration from distill.pub or OpenAI blog posts. There are things that are not clear, like what the inputs are into each agent, and how the training works. I recommend having some pseudocode snippets (like the Gym framework) to explain parts of the overall picture in more detail as figures.

Given a work of this magnitude, I'm personally okay that they went over 8 pages, as long as it is properly used for clarity.

Discussion:

Concepts from Artificial Life and Evolution has been introduced in this work. There is some confusion between what is ""learning"" and what has been ""evolved"" in your setup. Some readers coming from the evolution, or biology fields (who I bet will find your paper interesting to read and experiment with) might interpret ""learning"" to be weight changes during a life time, while ""evolution"" would be changes to the weight parameters from one generation to the next, but I think in policy-gradient RL, ""learning"" means weight changes after an agent dies and is reborn. Should consider clarifying in the introduction, the definition of learning, and whether it is inter-life or intra-life.

You cited some of Stanley's talks on open-endedness, but I wonder if you considered their work [1] where they proposed that having a minimum criteria condition might encourage diversity of solutions. For instance, perhaps in your environment, an agent doesn't have to be the very best, but only manage to survive, to move on to the next generation, which might cause very interesting multi-agent population dynamics. A parallel to modern life is that people (at least those in wealthy nations) live with such a good social safety net that people don't really have to be the best ""agent"" to reproduce and survive, and this might explain the large diverse cultures and ideas we end up with as human species, compared to other animals (where the current game is probably a suitable model of). An experiment to explore an experiment where only the very weakest agents die, but leaving agents with mediocre foraging and combat skills still live on (and pursue their own interests, whatever they may be) will be super interesting, and I encourage you to explore these ideas of open-endedness.

Bugs: In the appendix, the citation for OpenAI Five needs fixing.

Currently it pains me that I can only assign a score of 5 of this work (NOTE: this has been since revised upwards to 7 upon reading revision after rebuttal period), since I don't think the current writing is up to standards. In my opinion, it deserves a score of 7-8. If you work on points (1) and (2) and submit a revised draft with much better writing, visualization, figures to explain the work, I'll happily revise my score and improve it by 1-3 points depending on how much improvement is made.

[1] Brand and Stanley. ""Minimal Criterion Coevolution: A New Approach to Open-Ended Search"" (GECCO 2017) http://eplex.cs.ucf.edu/papers/brant_gecco17.pdf
[2] https://arxiv.org/abs/1703.03864
[3] https://arxiv.org/abs/1804.03720

(*) well, sort of, due to compute limits they are clustered to some extent to their species, so agents within a species have identical brains, unlike the real world.
","The sentiment score is 50 (moderately positive) because the reviewer expresses excitement about the work and its potential, while also noting significant issues that need addressing. They revised their score upwards after the authors made improvements, indicating a positive shift. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and expresses their opinions diplomatically. They acknowledge the work's potential value while clearly stating areas for improvement. The reviewer also offers specific suggestions and encouragement, which contributes to the polite tone. The slightly lower politeness score accounts for some direct criticisms, such as calling the writing 'horrible', which, while honest, could be perceived as somewhat blunt.",50.0,75.0
"Strengths:

Well written paper, covers most of the relevant related work
Technique is conceptually easy to understand (~ adversarial training)

Weaknesses:

Unclear set of desiderata properties for a watermarking technique
No formal guarantees are verified, the mechanism is only tested
Attacks tested are not tailored to the technique proposed

Feedback and rebuttal questions:

This submission is easy to read and follow, and motivates the problem of watermarking well in light of intellectual property concerns. The technique proposed exploits unused capacity in the model to train it to associate specific inputs (computed adversarially) to specific outputs (the keys). Watermarking succeeds when the bit error rate between the predicted signature and the expected one is zero. This approach is conceptually easy to understand. 

The experimental setup used to evaluate the approach is however limited. First, it is unclear why desiderata stated in Section 3.1 and summarized in Table 1 are necessary and sufficient. Would you be able to justify their choice in your rebuttal? For instance, the “security” requirement in Table 1 overlaps with “fidelity”. Similarly, the property named “integrity” really refers to only a subset of what one would typically describe as integrity. It basically calls for a low false positive or high precision. 

The attack model described in Section 3.2 only considers three existing attacks: model fine-tuning, parameter pruning and watermark overwriting. These attacks do not consider how the adversary could adapt and they are not optimal strategies for attacking the specific defensive mechanism put in place here. For instance, could you explain in your rebuttal why pruning the smallest weights in the architecture in the final architecture would help with removing adversarial examples injected to watermark the model? Similarly, given that adversarial subspaces have large volumes, it makes sense that multiple watermarks could be inserted simultaneously and thus watermark overwriting attacks would fail.

If the approach is based on exploring unused capacity in the model, the adversary could in fact attempt to use a compression technique to preserve the model’s behavior on the task and remove the watermarking logic. For instance, the adversary could use an unlabeled set of inputs and have them labeled by the watermarked model. Because these inputs will not be “adversarial”, the watermarked model’s decision surface used to encode the signatures will remain unexplored during knowledge transfer and the resulted compressed or distilled model would solve the original task without being watermarked. Is this an attack you have considered in your experiments and if not could you elaborate why one may exclude it in your rebuttal?

Minor comments: 

P3: Typo “Verifiabiity”
P5: Could you add a reference or additional experimental results that justify why transferable keys would be located near the decision boundaries? 
","The sentiment score is slightly positive (20) because the reviewer acknowledges strengths such as the paper being well-written and the technique being conceptually easy to understand. However, they also point out several weaknesses and areas for improvement, which tempers the overall positive sentiment. The politeness score is high (80) as the reviewer uses respectful language throughout, asks questions politely, and frames criticisms constructively. They use phrases like 'Would you be able to justify...?' and 'Could you explain...?' which maintain a courteous tone. The reviewer also balances critique with positive feedback, demonstrating professional courtesy. The minor comments at the end are presented neutrally and helpfully, further contributing to the polite tone.",20.0,80.0
"This paper presents a generalization of TransE to Riemannian manifolds. While this work falls into the class of interesting recent approaches for using non-Euclidean spaces for knowledge graph embeddings, I found it very hard to digest (e.g. the first paragraph in Section 3.3). Figure 3 and 4 confused me more than helping me to understand the method. Furthermore, current neural link prediction methods are usually evaluated on FB15k and WN18. In fact, often on the harder variants FB15k-237 and WN18RR. For FB15k and WN18, Riemannian TransE seems to underperform compared to baselines -- even for low embedding dimensions, so I have doubts how useful this method will be to the community and believe further experiments on FB15k-237 and WN18RR need to be carried out and the clarity of the paper, particularly the figures, needs to be improved. Lastly, I would be curious about how the different Riemannian TransE variants compare to TransE in terms of speed?

Update: I thank the authors for their response and revision of the paper. To me, results on WN18RR and FB15k-237 are inconclusive w.r.t. to the choice of using Riemannian as opposed to Euclidean space. I therefore still believe this paper needs more work before acceptance.","The sentiment score is -50 because the reviewer expresses several concerns and doubts about the paper's clarity, usefulness, and experimental results. They mention finding it 'very hard to digest', being confused by figures, and noting that the method underperforms compared to baselines. The update also indicates that the results are 'inconclusive' and the paper 'needs more work'. However, it's not entirely negative as they acknowledge it as an 'interesting recent approach'. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone. They use phrases like 'I would be curious about' and 'I thank the authors for their response', which add a degree of politeness. However, the criticism is direct and doesn't employ many softening phrases, keeping the score only slightly positive.",-50.0,20.0
"Summary

The paper presents a novel approach for learning a generative model where different factors of variations can be independently manipulated. The method is build upon  the GAN framework where the latent variables are divided into different subsets (chunks) which are expected to encode information about high-level factors of variation. To this end, a Siamese Network for each chunk is trained with a contrastive loss minimizing the distance between generated images sharing the same factor (the latent variables in the chunk are equal), and maximizing the distance between pairs where the latent variables differ. Given that the proposed model fails in this fully-unsupervised setting, the authors propose to add weak-supervision into the model by forcing the Siamese networks to  focus only on particular aspects of generated images (e.g, color, edges, etc..). This is achieved by applying  a basic transformation  over the input images in order to remove specific information. The evaluation of the  proposed model is carried out using the MS-Celeb dataset where the authors provide qualitative results.


Methodology

*Disentangling generative factors without explicit labels is a challenging and interesting problem. The idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3]. However, the use of Siamese networks in this context is novel and sound.

*As shown in the reported results, the proposed method fails to learn meaningful factors in the unsupervised setting. However, the authors do not provide an in-depth discussion of this phenomena. Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios. 

*The strategy proposed to introduce weak-supervision is too ad-hoc. I agree that using cues such as the average color of an image can be useful if we want to model basic factors of variation. However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.

*As far as I understand, the transformations applied to the input images (e.g, edge detection) must be differentiable (given that it is necessary to backpropagate the gradient of the contrastive loss through the generator network). If this is the case, this should be properly discussed in the paper. Moreover, given that the amount of differentiable transformations is reduced, this also limits the application of the proposed method for more interesting scenarios. 

*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior. How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?  Have the authors considered to use categorical or binary variables? The use of the contrastive loss sounds more appropriate in this case.


Experimental results

*The experimental section is too limited. First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion. For example, when the edge extractor is used, what kind of information is modeled by the latent variables? Is it consistent across different samples?

Moreover, it is not clear why the authors have limited the evaluation to the case where only two “chunks” are used. In principle, the method could be applied with many more subsets of latent variables and then manually inspect them to check it they are semantically meaningful (see [2]) 

*As previously mentioned, there are many recent works addressing the same problem from a fully-unsupervised perspective [1,2,3]. All these works provide quantitative results evaluating the learned representations by using them to predict real labels (e.g, attributes in the CelebA data-set). The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation. This could clarify the advantages of the weakly-supervised strategy compared to unsupervised approaches.

Review summary

+The addressed problem (learning disentangled representations without explicit labeling) is challenging and interesting.

+The idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.

- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.

-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications

-The experimental section do not clarify the benefits of the proposed approach. In particular, the qualitative results are too limited and no quantitative evaluations is provided.


[1] Variational Inference of Disentangled Latent Concepts from Unlabelled Observations (Kumar et al, ICLR 2018)

[2] Beta-vae: Learning basic visual concepts with a constrained variational framework. (Higgins et. al, ICLR 2017)

[3] Disentangling Factors of Variation by Mixing Them. (Hu et. al, CVPR  2018)
","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('novel approach', 'challenging and interesting problem'), they express significant concerns about the methodology and experimental results. The review highlights several limitations and shortcomings, indicating an overall negative sentiment. However, it's not extremely negative as the reviewer does point out some strengths. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'it would be necessary to give more insights' and 'I agree that...'. They provide constructive criticism without using harsh language. The reviewer balances critique with acknowledgment of positive aspects, which contributes to the polite tone.",-30.0,50.0
"This paper proposed to use graph convolutional neural networks for link prediction. The authors proposed to use the dual graph to simultaneously learn node and edge embeddings. The label of the edges (positive or negative) are used as supervised signal for training the GCNs. Experiments on a few small data set prove the effectiveness of the proposed approaches.

Strength:
- important problem

Weakness:
- the novelty of the proposed method is very marginal
- the experiments are quite weak

Details:
- the novelty of the proposed method seems to be very marginal, which simply applies the GCN for link prediction. The existing GCN based method for recommendation shares similar ideas (e.g., Yin et al. 2018, PinSage), though dual hypergraph is not used. But the essential idea is very similar. 
- the data sets used in the experiments are too small
- the node embedding based methods should be compared for link prediction, e.g., DeepWalk, LINE, and node2vec.
","The sentiment score is -50 because the review is predominantly negative. While it acknowledges the importance of the problem, it points out significant weaknesses such as marginal novelty and weak experiments. The reviewer uses phrases like 'very marginal' and 'quite weak' to describe the paper's contributions and experiments. The politeness score is 20 because the reviewer maintains a professional tone and uses neutral language. They provide specific feedback and suggestions for improvement without using harsh or rude language. The reviewer balances criticism with acknowledgment of the paper's strengths, which contributes to a slightly positive politeness score.",-50.0,20.0
"The paper ‘Generative model based on minimizing exact empirical Wasserstein distance' proposes
a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying
on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.
Comparisons with other variants of Wasserstein GAN is proposed on MNIST.

I see little novelty in the paper. The derivation of the primal version of the problem is already 
given in  
Cuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).

Using optimal transport computed on batches rather the on the whole dataset is already used in (among
others)
 Genevay, A., Peyré, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS
 Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  

Also, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on 
batches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very 
well described in the paper (Section 3): 
https://openreview.net/pdf?id=S1m6h21Cb

Computing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be
proved and discussed.

Finally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).


Typos:
 Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup ","The sentiment score is -60 because the reviewer expresses significant criticism and sees 'little novelty' in the paper. They point out multiple issues, including lack of originality, incorrect claims, and insufficient experimental validation. The tone is predominantly negative, though not entirely dismissive. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the critique is quite direct and lacks softening phrases or positive reinforcement. The reviewer bluntly states criticisms without much attempt to cushion the feedback, which comes across as somewhat impolite in academic discourse. However, the language isn't explicitly rude, hence the score isn't extremely low.",-60.0,-20.0
"This paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). 
Specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the MDP itself in a gradual manner, guided by the rate of learning.
It also proposes a couple of variants of imitation rewards, RTGD and ATD inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (CSD, SSD), which are based on either consecutive or single states, which constitute the baseline methods for comparison.
The authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. 
Some moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. 
There is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.
The empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. 
Also the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('moderately interesting observations', 'reasonable' ideas), they express several criticisms. These include questioning the generalizability of the results, doubting the extent of the paper's contribution, and noting the lack of theoretical support. The overall tone suggests skepticism about the paper's significance.

The politeness score is moderately positive (50) because the reviewer maintains a professional and objective tone throughout. They use neutral language to express their criticisms, avoiding harsh or dismissive phrasing. The reviewer acknowledges positive aspects of the work and provides specific, constructive feedback. However, the score is not higher as the review lacks overtly polite or encouraging language, maintaining a more neutral, academic tone.",-20.0,50.0
"Overview:

This paper proposed an approach for zero-shot phoneme recognition, where it is possible to recognise phonemes in a target language which has never been seen before. Rather than just training a phoneme recogniser directly on background data and then applying it to unseen data, phonetic features are first predicted, allowing phonemes not in the source language set to be predicted.

Main strengths:

The paper's main strength lies in that this is a very unexplored area that could assist in the development of speech technology where it is currently not possible. The proposed model (Section 2) has also not been considered in prior work.

Main weaknesses:

The paper's main weakness is in some of its claims and that it misses some very relevant literature. Detailed comments together with a minimal list of references are given below (but I would encourage the authors to also read a bit more broadly). But in short I do not think it is that easy to claim that this is the first paper to do zero-shot learning on speech; many of the zero-resource studies where unlabelled audio is used could be seen as doing some for of zero-shot matching. Specifically [5] is able to predict unseen phoneme targets.  Multilingual bottleneck features can be applied to languages that have never been seen before [2], and the output of phoneme recognisers trained on one language have long been applied to get output on another unseen language. The first one-shot learning speech paper [4] (to my knowledge) is also not mentioned at all. The approach in the paper also still relies on some text data from the target language; if this then can be described as ""zero-shot"" learning, then I think many of these previous studies c also make this claim.

Overall feedback:

There is definitely value in this work, but it should be much better situated within the broader literature. Below I give some editorial suggestions and also outline some suggestions for further experiments.

Detailed comments, suggestions and questions:

- Abstract: It would be useful to have some details of the ""baseline model"" here already, especially since it is such a new task.
- Introduction: ""... but they can hardly predict phones or words directly due to their unsupervised nature."" This is a strong statement that maybe requires more justification. On the one hand, the statement is true, and the high word error rates in e.g. [3] can be cited. On the other hand, it has been shown that at the phone-distinction level, these models perform quite well and sometimes outperform supervised models [1]. Since this paper also considers phone error rate as a metric, I think care should be taken with such statements.
- Introduction: ""While zero-shot learning has attracted a lot of attention in *the* computer vision community, this setup has hardly been studied in speech recognition research especially in acoustic modeling."" Definitely look at some of the studies mentioned below, and also [4] specifically.
- ""However, we note that our model can be combined with a well-resourced language model to recognize words."" How would this be done, since I think this is actually quite a challenging task.
- Section 2: ""... useful the original ESZSL architecture ..."" -> ""... useful in the original ESZSL architecture ...""
- Section 2.2: I assume the small text corpus is at the phone level (and not characters directly)? This should be clarified, and it could raise the question of whether this approach is truly ""zero-shot"".
- Section 3.2: ""We used EESEN framework ..."" -> ""We used the EESEN framework ...""
- Section 4: You could look at the recent work in [2], which uses multilingual bottleneck features trained on 10 languages and applied to multiple unseen languages. It would be interesting to also train your approach on multiple languages instead of only English.

Missing references:

1. M. Heck, S. Sakti, and S. Nakamura, ""Feature Optimized DPGMM Clustering for Unsupervised Subword Modeling: A Contribution to Zerospeech 2017,"" in Proc. ASRU, 2017.
2. E. Hermann and S. J. Goldwater, ""Multilingual bottleneck features for subword modeling in zero-resource languages,"" in Proc. Interspeech, 2018.
3. H. Kamper, K. Livescu, and S. Goldwater, An embedded segmental k-means model for unsupervised segmentation and clustering of speech,"" in Proc. ASRU, 2017.
4. B. M. Lake, C.-Y. Lee, J. R. Glass, and J. B. Tenenbaum, ""One-shot learning of generative speech concepts,"" in Proc. CogSci, 2014.
5. O. Scharenborg, F. Ciannella, S. Palaskar, A. Black, F. Metze, L. Ondel, and M. Hasegawa-Johnson, ""Building an ASR system for a low-resource language through the adaptation of a high-resource language asr system: Preliminary results,""in Proc. ICNLSSP, 2017.

Edit: Based on the rebuttal I've changed my rating from 4 to 5.","The sentiment score is 50 (slightly positive) because while the reviewer acknowledges the paper's strengths and potential value, they also point out significant weaknesses and missing literature. The overall tone is constructive, suggesting improvements rather than outright rejection. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and provides detailed suggestions for improvement. They use phrases like 'I would encourage the authors' and 'There is definitely value in this work,' which maintain a positive and supportive tone even while critiquing. The reviewer also acknowledges the authors' rebuttal by changing their rating, showing openness to dialogue.",50.0,75.0
"
Summary:
========
Theis paper proposes a method for learning decomposable representations in the context of a language modeling task. Using holographic reduced representations (HRR), a word embedding is composed of a role and a filler. The embedding is then fed to an LSTM language model. There is also an extension to chunk-level representations. Experimentally, the model achieves perplexity comparable to a (weak) baseline LSTM model. The analysis of the learned representations shows a separation into syntactic and semantic roles. 

The paper targets an important problem, that of learning decomposable representations. As far as I know, it introduces a novel perspective using HRR and does so in the context of language modeling, which is a core NLP task. The analysis of the learned representations is quite interesting. I do have some concerns with regards to the quality of the language model, the clarity of some of the model description, and the validity of using HRR in this scenario. Please see detailed comments below. 

Comments:
=========
1. Section 2 refers to Plate (1995) for the conditions when the approximate decoding via correlation holds. I think it's important to mention these conditions and discuss whether they apply to the language modeling case. In particular, Plate mentions that the elements of each vector need to be iid with mean zero and variance 1/n (where n is the length of the vector). Is this true for the present case? Typically, word embeddings and LSTM states are do not exhibit this distribution. Are there other conditions that are (not) met?
2. Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation. On the other hand, if I understand correctly, this means that word embeddings are not shared between roles, because s^w_i is also a role-specific vector (not just a word-specific vector). Is that a cause of concern? 
3. It's not clear to me where in the overall model the next word is predicted. Figure 1b has an LSTM that predicts filler embeddings. Does this replace predicting the next word in a vanilla LSTM? Equation 5 still computes a word score. Is this used to compute the probability of the next word as in equation 2?  
4. Comparison to other methods for composing words. Since much of the paper is concerned with composing words, it seem natural to compare the methods (and maybe some of the results) to methods for composing words. Some examples include [2] and the line of work on recursive neural networks by Socher et al., but there are many others. 
5. Perplexity results:
- The baseline results (100.5 ppl on PTB) are very weak for an LSTM. There are multiple papers showing that a simple LSTM can do much better. The heavily tuned LSTM of [1] gets 59.6 but even less tuned LSTMs go under 80 or 80 ppl. See some results in [1]. This raises a concern that the improvements from the HRR model may not be significant. Would they hold in a more competitive model? 
- Can you speculate or analyze in more detail why the chunk-level model doesn't perform well, and why adding more fillers doesn't help in this case? 
6. Motivation: 
- The introduction claims that the dominant encoder-decoder paradigm learns ""transformations from many smaller comprising units to one complex emedding, and vice versa"". This claim should be qualified by the use of attention, where there is not a single complex embedding, rather a distribution over multiple embeddings. 
- Introduction, first paragraph, claims that ""such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability"" - what do you mean by these concepts and how exactly is the current approach limited with respect to them? Giving a bit more details about this point here or elsewhere in the paper would help motivate the work. 
7. Section 3.3 was not so clear to me:
- In step 1, what are these r_i^{chunk}? Should we assume that all chunks have the same role embeddings, despite them potentially being syntactically different? How do you determine where to split output vectors from the RNN to two parts? What is the motivation for doing this?
- In prediction, how do you predict the next chunk embedding? Is there a different loss function for this? 
- Please provide more details on decoding, such as the mentioned annealing and regularization. 
- Finally, the reliance on a chunker is quite limiting. These may not be always available or of high quality. 
8. The analysis in section 4.3 is very interesting and compelling. Figure 2 makes a good point. I would have liked to see more analysis along these lines. For example, more discussion of the word analogy results, including categories where HRR does not do better than the baseline. Also consider other analogy datasets that capture different aspects. 
9. While I agree that automatic evaluation at chunk-level is challenging, I think more can be done. For instance, annotations in PTB can be used to automatically assign roles such as those in table 4, or others (there are plenty of annotations on PTB), and then to evaluate clustering along different annotations at a larger scale. 
10. The introduction mentions a subset of the one billion word LM dataset (why a subset?), but then the rest of the papers evaluates only on PTB. Is this additional dataset used or not? 
11. Introduction, first paragraph, last sentence: ""much previous work"" - please cite such relevant work on inducing disentangled representations.
12. Please improve the visibility of Figure 1. Some symbols are hard to see when printed. 
13. More details on the regularization on basis embeddings (page 4) would be useful. 
14. Section 3.3 says that each unique word token is assigned a vectorial parameter. Should this be word type? 
15. Why not initialize the hidden state with the last state from the last batch? I understand that this is done to assure that the chunk-level models only consider intra-sentential information, but why is this desired? 
16. Have you considered using more than two roles? I wonder how figure 2 would look in this case. 


Writing, grammar, etc.:
====================== 
- End of section 1: Our papers -> Our paper
- Section 2: such approach -> such an approach; HRR use -> HRR uses; three operations -> three operations*:*
- Section 3.1: ""the next token w_t"" - should this be w_{t+1)? 
- Section 3.2, decoding: remain -> remains 
- Section 3.3: work token -> word token 
- Section 4.1: word analogy task -> a word analogy task; number basis -> numbers of basis
- Section 4.2: that the increasing -> that increasing 
- Section 4.3: no space before comma (first paragraph); on word analogy task -> on a word analogy task; belong -> belongs
- Section 4.4: performed similar -> performed a similar; luster -> cluster 
- Section 5: these work -> these works/papers/studies; share common goal -> share a common goal; we makes -> we make; has been -> have been  

References
==========
[1] Melis et al., On the State of the Art of Evaluation in Neural Language Models
[2] Mitchell and Lapata, Vector-based Models of Semantic Composition
","Sentiment score: The review starts with a generally positive tone, acknowledging the paper's importance and novelty. However, it quickly moves to express 'concerns' about various aspects of the work. The reviewer lists numerous issues and suggestions for improvement, indicating a somewhat negative overall sentiment despite the initial positive remarks. This mix of positive and negative elements, with a slight tilt towards the negative, justifies a score of -20.

Politeness score: The reviewer maintains a professional and respectful tone throughout. They use polite phrases like 'Please see detailed comments below' and frame criticisms as suggestions or questions rather than direct attacks. The reviewer also acknowledges the interesting aspects of the work. However, the politeness is not overly effusive, maintaining a professional distance. This balanced, respectful approach warrants a politeness score of 70.",-20.0,70.0
"The paper proposes to integrate sequential information into imitation learning techniques.  The assumption is that mostly all the IL techniques are learning a policy which depends on state at time t, while the information contained in this state may be not sufficient to choose the right action (actually, this is the POMDP setting, the notion of POMDP not appearing in the paper....). The authors thus propose to use a recurrent neural network to encode the state by aggregating past information, instead of just using the features of the state at time t. They thus instantiate this idea on different methods and show that, on some problems, this approach can increase the quality of the final policy.

Actually, the contribution of the paper is a simple extension of existing methods: using a RNN instead of a simple NN in imitation learning models. First of all, when dealing with classical environments such as Atari, many papers propose to use the last N frames as a state encoding (instead of the last frame), following the same intuition. The studied setting thus corresponds to the PO-MDP case and using a RNN in POMDP is for example what is done in  [Merel etal. 2017]. Moreover, the problem of imitation learning (and particularly inverse RL) in POMDP has been of the interest of many papers like [Choi et al. 2008] for instance and many more, and it is unclear what is the positioning of this paper w.r.t existing works. Since the paper proposes just to encode history with a RNN, the proposed solution lacks of originality, and the contribution of the paper in term of model is quite low.  But the authors explain how this can be instantiated in three different settings (IRL, GAIL and BC) -- note that the section concerning the use of Adaboost is not clear and could be better described -- which can be of the interest of the community. 
Concerning the experiments, I don't understand what is the split between training and testing data. Is it pairs of state-action coming from the experts ? or trajectories ? Moreover, I don't understand why these environments correspond to POMDP cases and the authors have to give details on that. For instance, mountain-car is clearly not a POMDP problem in its classical shape, nor Acrobot. As if, it makes the experiments very difficult to reproduce. The interest of using the RNN to encode history does not seem clear for each of the cases since it often degrades the final performance, so I don't know exactly what insights I can extract from the paper.

Pro:
* The approach is proposed for IRL, GAIL and BC

Cons:
* Lack of positionning w.r.t POMDP litterature
* Lack of details in the experiments, and lack of good experimental results
* Low contribution in term of model


[Merel et al. 2017]  Learning human behaviors from motion capture
by adversarial imitation
[Choi et al.] Inverse Reinforcement Learning in Partially Observable
Environments","The sentiment score is -40 because the review is generally critical of the paper's contribution, citing a 'lack of originality' and 'low contribution in terms of model'. However, it's not entirely negative as it acknowledges some potential interest to the community. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and provide specific feedback. They also mention some positive aspects ('Pro:') alongside the criticisms ('Cons:'). The reviewer maintains a respectful tone throughout, even when pointing out shortcomings, which contributes to the slightly positive politeness score.",-40.0,20.0
"Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.

I am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That’s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.

On the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (>2) cases, they should make it as clear as possible.
 
A second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.

Moderate concern

“In order to show this major limitation of GRUs …” but then a 2-gru is used, which means that it’s not a general problem for GRUs with higher dim, right?  Also, won’t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.

Minor

GRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write ‘two GRUs’, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.

Side note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.
","The sentiment score is slightly positive (20) because while the reviewer expresses deep conflict about the paper's significance, they also praise its quality, clarity, and novelty. They acknowledge potential benefits like changing the language of RNNs and attracting dynamical systems specialists to deep learning. The politeness score is high (80) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and expressing concerns in a constructive manner. They use phrases like 'extremely well written' and offer balanced perspectives on the paper's merits and limitations without using harsh or dismissive language.",20.0,80.0
"This paper proposes that models for different tasks in multi-task learning cannot only share hidden variables but also gradients.

Pros:
- The overall framework is theoretically motivated and intuitive. The idea of passing gradients for multi-task learning is interesting and the execution using fast weights is plausible.
- The experiments are extensive and cover three different task combinations in different domains.
- The results are convincing and the additional analyses are compelling.

Cons:
- I would have liked to see a toy example or at least a bit more justification for the ""pretend-to-share"" problem that models ""collect all the features together into a common space, instead of learning shared rules across different tasks"". As it is, evidence for this seems to be mostly anecdotal, even though this forms the central thesis of the paper.
- I found the use of Read and Write ops confusing, as similar terminology is widely used in memory-based networks (e.g. [1]). I would have preferred something that makes it clearer that updates are constrained in some way as ""writing"" implies that the location is constrained, rather than the update minimizing a loss.

Questions:
- How is the weight list of task similarities \beta learned when the tasks don't share the same output space? How useful is the \beta?
- Could you elaborate on what is the difference between pair-wise gradient passing (PGP) and list-wise gradient passing (LGP)

[1] Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.","The sentiment score is 70 (positive) because the review starts with a clear statement of the paper's proposition and lists several pros, including the 'theoretically motivated and intuitive' framework, 'extensive' experiments, and 'convincing' results. The cons are presented as constructive feedback rather than severe criticisms. The politeness score is 80 (polite) due to the balanced and professional tone throughout. The reviewer uses respectful language, acknowledging the paper's strengths while offering suggestions for improvement. The use of phrases like 'I would have liked to see' and 'I found... confusing' express personal opinions without being harsh. The inclusion of questions at the end demonstrates engagement with the work and invites further discussion, which is a polite way to address potential issues.",70.0,80.0
"This paper introduces an approach to pruning the parameters of a trained neural network. The idea is inspired by the Optimal Brain Surgeon method, which relies on second derivatives of the loss w.r.t. the network parameters. Here, the corresponding Hessian matrix is approximated using the Fisher information to make the algorithm scalable to very deep networks.

Strengths:
- The method does not require hyper-parameter tuning.
- The results show the good behavior of the approach.

Weaknesses:

Novelty:
- In essence, this method relies on the work of Marten & Grosse to approximate the Hessian matrix used in the Optimal Brain Surgeon strategy. This is fine, but not of great novelty.

Method:
- It is not clear to me why the notion of binary parameters gamma is necessary. Instead of varying the gammas from 1 to 0, why not directly zero out the corresponding network parameters w?
- In essence, the objective function in Eq. 5 adds an L_1 penalty on the gamma parameters, which would be related to an L_1 penalty on the ws. Note that this strategy has been employed in the past, e.g., Collins & Kohli, 2014, ""Memory Bounded Deep Convolutional Networks"".
- It is not clear to me how zeroing out individual parameters will truly allows one to reduce the model afterwards. In fact, one would rather want to remove entire rows or columns of the matrix W_l, which would truly correspond to a smaller model. This was what was proposed by Wen et al., NIPS 2016 and Alvarez & Salzmann, NIPS 2016, ""Learning the Number of Neurons..."".
- In the past, when dealing with the Hessian matrix, people have used the so-called Pearlmutter trick (Pearlmutter, Neural Computation 2014, ""Fast exact multiplication by the Hessian"". In fact, in this paper, the author mentions the application to the Optimal Brain Surgeon strategy. Is there a benefit of the proposed approach over this alternative strategy?

Experiments:
- While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer.  This does not guarantee entire channels to be removed. As such, I would not know how to make the model actually smaller in practice. It would seem relevant to show the true gains in memory usage and in inference speed (both measured on the computer, not theoretically).

Summary:
I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. However, novelty of the approach is limited, and I am not convinced of its actual benefits in practice.
","The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper (e.g., no hyper-parameter tuning required, good results), they express significant concerns about the novelty, methodology, and practical benefits of the approach. The overall tone is more negative than positive, but not extremely negative. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging positive aspects and framing criticisms as personal opinions (e.g., 'It is not clear to me...', 'I am not convinced...'). They also use phrases like 'I do appreciate...' which adds to the politeness. However, the review is not overly effusive or extremely polite, maintaining a professional tone.",-30.0,50.0
"I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\lambda^2 + n - 1) (\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. 
Conversely, it’s possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldn’t necessarily result in good generalization, which seems to agree with the experimental observation. 
Now given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC?
The experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.","The sentiment score is -50 because the reviewer expresses significant concerns and disagreements with the paper's main arguments. They question the validity of the nonlinearity coefficient (NLC) definition and its relationship to generalization, providing counterexamples that challenge the paper's claims. However, the score is not extremely negative as the reviewer acknowledges some 'interesting bits' in the experiment section. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I do not understand' and 'one can wonder' rather than making blunt accusations. The reviewer also provides detailed explanations for their concerns, which is a constructive approach. The language is not overtly polite, but it avoids rudeness and maintains a neutral, academic tone.",-50.0,20.0
"The paper presents an empirical study of how accuracy and robustness vary with increasing training data for four different data sets and CNN architectures. The main conclusion of the study is that while training accuracy generally increases with increasing training data, provided sufficient training data is available for training the network in the first place, the robustness on the other hand does not necessarily increase, and may even decrease.

Similar findings were presented previously in Su et al., 2018. Hence, the current paper contains incremental and marginal new findings versus the existing literature. The paper would also have been a lot stronger and significantly advanced our scientific understanding of the problem if the authors had made some attempt at trying to explain their findings theoretically. In its current form the paper does not contain sufficient contributions for acceptance.","The sentiment score is -50 because the reviewer acknowledges the paper's empirical study but criticizes it for containing 'incremental and marginal new findings' and not having 'sufficient contributions for acceptance'. This indicates a negative sentiment, though not extremely harsh. The politeness score is 0 (neutral) because the reviewer uses professional language without being particularly polite or rude. They state their criticisms directly but without using inflammatory language. The reviewer acknowledges the work done but clearly expresses dissatisfaction with the paper's novelty and depth, maintaining a neutral, professional tone throughout.",-50.0,0.0
"This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues.

*****

The clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not.

Moreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me.

*****

The paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in ""learning classifiers from only positive and unlabeled data"", KDD 2008; the latter problem setting was proposed in ""presence-only data and the EM algorithm"", Biometrics 2009 and formalized in ""analysis of learning from positive and unlabeled data"", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method.

The huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See ""learning from corrupted binary labels via class-probability estimation"", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled ""on the minimal supervision for training any binary classifier from only unlabeled data"" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting.

Furthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see ""learning classifiers from only positive and unlabeled data"" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in ""presence-only data and the EM algorithm"" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)<=P(X) following ""estimating the class prior and posterior from noisy positives and unlabeled data"", NIPS 2016. BTW, ""mixture proportion estimation via kernel embedding of distributions"" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later.

In summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue.

*****

The novelty is to be honest incremental and thus below the bar of ICLR. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN.","The sentiment score is -80 because the review is highly critical, pointing out numerous issues with the paper including lack of clarity, poor logic, missing important citations, and fundamental misunderstandings of PU learning. The reviewer states the paper has 'incremental' novelty and 'poor' significance, indicating strong negative sentiment. The politeness score is -20 because while the reviewer uses some polite phrases like 'I guess' and 'I am not sure', the overall tone is quite blunt and critical. The reviewer directly states issues without much softening language, using phrases like 'clarity is really an issue' and 'lacking some basic knowledge of PU learning is another big issue'. While not overtly rude, the language is more direct and critical than would be considered polite in academic discourse.",-80.0,-20.0
"
MuMoMAML: Model-Agnostic Meta-Learning for Multimodal Task Distributions

This paper proposed multi-modal MAML, which alleviates the single initialization limitation of MAML by modulating task prior with MAML. Below are some comments.

Pros:
1. Overall, the paper is clear written. 
2. By using modulation, there is no need to explicitly control/know the number of modes in advance.
3. The multi-MAML baseline is good for an ablation study, though it is only on a synthetic regression task.
4. MUMOMAML combines the strength of both gradient-based and model-based meta-learners.

Cons.
1. The novelty of the paper seems to be the combinations of MAML and FiLM, which seems a bit limited.
2. I wonder whether the proposed method is mostly useful when there is a clear mode difference as in the synthetic regression/RL tasks of the paper. Moreover, the paper only shows tasks with only two-three modes, what happen when there is a large number of modes?
3. What's the results on the mini-Imagenet? The Omniglot seems to be saturated already.
4. Why tau is not updated in the inner loop of Algorithm 1?

Minor:
1. page 4, 'in to' -> 'into'
2. In page 5, in 'based on the input data samples and then
infers the parameter to modulate the prior model', what does the `input data samples' refers to? Is it the training data of a meta-learning task?
3. Do you stop gradient to the learner in MUMOMAML?","The sentiment score is slightly positive (20) because the reviewer begins by listing several pros of the paper, including clear writing, good methodology, and combining strengths of different approaches. However, they also list several cons and areas for improvement, which tempers the positive sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They present both pros and cons in a balanced manner, and phrase their criticisms as questions or suggestions rather than harsh criticisms. The use of 'I wonder' and phrasing criticisms as questions (e.g., 'Why is tau not updated...?') contributes to the polite tone. The reviewer also acknowledges the paper's strengths before presenting criticisms, which is a polite approach in academic reviewing.",20.0,50.0
"Post-rebuttal update:
The authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).

I did spot one typo on page 4 - ""exterme"", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.

---

The authors investigate the hidden state dynamics of RNNs trained on a single task that mixes (but clearly separates) pattern recognition and memorisation. The authors then introduce two curricula specific to the task, and study how the trained RNNs behave under different deviations from the training protocol (generalisation). They show that under the curriculum that exhibited the best generalisation, there exist more robust (persisting for long time periods) fixed/slow points in the hidden state dynamics. They then extend the optimisation procedure developed by Sussillo & Barak for continuous-time RNNs in order to find these points. Finally, they use this method to track the speed of these points during the course of training, and link spikes in speed to one of the curricula which introduces new classes over time.

Understanding RNNs - and in particular how they might ""generalise"" - is an important topic of research. As done previously, studying RNNs as dynamical systems is a principled way to do so. In this line of work some natural objects to look into are fixed points and even slow points (Sussillo & Barak) - how long they can persist, and how large the basins of attraction are. While I believe the authors did a reasonable job following this through, I have some concerns about the experimental setup. Firstly, only one task is used - based on object classification with images - so it is unclear how generalisable these findings are, given that the authors' setup could be extended to cover at least another task, or at least another dataset. MNIST is a sanity check, and many ideas may fail to hold when extended to slightly more challenging datasets like CIFAR-10.

Secondly, as far as I can tell, the results are analysed on one network per setting, so it is hard to tell how significant the differences are. While some analyses may only make sense for single networks, e.g. Figure 3, a proper quantification of some of the results over several training runs would be appropriate.

Finally, it is worth investigating LSTMs on this task. This is not merely because they are more commonly used than GRUs, but they are strictly more powerful - see ""On the Practical Computational Power of Finite Precision RNNs for Language Recognition"", published at ACL 2018. Given the results in this paper and actually the paper that first introduces the forget gate for LSTMs, it seems that performing these experiments solely with GRUs might lead to wrong conclusions about RNNs in general.

There are also more minor spelling and grammatical errors throughout the text that should be addressed. For example, there is a typo on the task definition on page 2 - ""the network should *output* a null label.""","The sentiment score is 50 (moderately positive) because the reviewer begins by acknowledging the authors' efforts to address previous concerns and improve the paper, upgrading their rating to 'accept'. However, they still point out some remaining issues and areas for improvement. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, commends the authors on their revisions, and frames criticisms constructively. They use phrases like 'I would have liked to see' and 'it would make sense to' rather than making demands. The reviewer also acknowledges the importance of the research topic and the reasonableness of the authors' approach, balancing critiques with positive feedback.",50.0,75.0
"UPDATE (after author response):

Thank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.

There's one point that the reviewers didn't clearly address:  ""It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions."" Please consider adding such an experiment.

The current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that ""Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd."" Showing that the method outperforms other methods would definitely strengthen the paper.

Section 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5.

---------

The paper proposes a new loss for training deep latent variable models. The novelty seems a bit limited, and the proposed method does not consistently seem to outperform existing methods in the experiments. I'd encourage the authors to add more experiments (see below for suggestions) and resubmit to a different venue.

Section 4:
- q(z) seems to be undefined. Is it the aggregated posterior?
- How is equation (1) related to ELBO that is used for training VAEs?

Some relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.

VEEGAN: Reducing mode collapse in GANs using implicit variational learning
https://arxiv.org/pdf/1705.07761.pdf

Distribution Matching in Variational Inference
https://arxiv.org/pdf/1802.06847.pdf


Section 5.1:
- The quantitative comparison measures MSE in pixel space and inception score, neither of which are particularly good measures for measuring the quality of how well the conditionals match. I’d encourage the authors to consider other metrics such as log-likelihood.

- It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.

Section 5.4: 
- The error bars seem quite high. Is there a reason why the method cannot reliably reduce mode collapse?

Minor issues:
- CIFAT-10 -> CIFAR-10
","Sentiment score: The review starts with some positive acknowledgment of improvements (-20 instead of lower) but maintains several criticisms and suggestions for further work, indicating an overall slightly negative sentiment. The reviewer encourages resubmission to a different venue, suggesting the current work is not yet sufficient.

Politeness score: The language used is generally polite and constructive. The reviewer uses phrases like 'Thank you for updating', 'Please consider', and 'I'd encourage the authors to', which are courteous ways of providing feedback. However, the directness of some criticisms prevents a higher score.

The scores are based on the overall tone, specific language used, and the balance between positive acknowledgments and critical feedback throughout the review.",-20.0,60.0
"Quality: 
- In 4.4, the authors have vigorously explored the space of hyperparameters. However, they do not describe how to determine the hyperparameters, e.g., set aside a validation set from a part of the training set and determine the hyperparameters using this validation set, while the authors split the two datasets into only training and test sets, respectively. Without this procedure, the results may overfit to the test set via repeated experiments. Even though the used datasets are of few-million, this procedure guarantees a minimum requirement for a reliable outcome from the proposed model. I firmly recommend the authors to update their results using a validation set to determine the hyperparameters and then report on the test set. Please describe these experimental details to ensure that the performed experiments are valid.
   
Clarity:
- Overall, the writing can be improved via proof-reading and polishing the sentences. In Introduction section, ""there is little work applying..."" can be specified or rephrased with ""it is underexplored to apply"", and ""input features are not independent"" can be specified on what there are not independent. Moreover, the last two sentences in the second paragraph in the Introduction section is unclear what the authors want to argue: ""The combinations in linear models are then made by cross product over different fields. Due to the sparsity problem, the combinations rely on much manual work of domain experts.""
- The authors use top-k restriction (Shazeer et al., 2017) to consider sparse relationships among the features. For this reason, have you tried to use the L1 loss on the probability distributions, which are the outputs of softmax function?
- In 4.5, the authors said, they ""are in most concern of complementarity."" What is the reason for this idea and why not the ""relevance""?
- In Table 4, I'm afraid that I don't understand the content (three numbers in parenthesis) of the third column. How does each input x_i or x_j, or a tuple of them get their own CTR?

Originality and significance:
- They apply self-attention to learn multiple categorical features to predict Click-Through-Rate (CTR) with a top-k non-zero similarity weight constraint to adapt to their categorical inputs. Due to this, the scientific contribution to the corresponding community is highly limited to providing empirical results on the CTR task.
- The authors argue that ""most of current DNN-based models simply concatenate all feature embeddings""; however, this argument might be an over-simplified statement for the existing models in section 2.
- Similar works can be found but missed to cite: [1] proposes a general framework to self-attention to exploit sequential (time-domain) and parallel (feature-domain) non-locality. [2] learns bilinear attention maps to integrate multimodal inputs using skip-connections and multiple layers on top of the idea of low-rank bilinear pooling.

Pros:
- Strong empirical results on two CTR tasks using the previous works of self-attention and top-k restriction techniques.

Cons:
- This work fairly lacks its originality since the proposing method heavily relies on the two previous works, self-attention and top-k restriction. They apply them to multiple categorical features to estimate CTR; however, their application seems to be monotonic without a novel idea of task-specific adaptation.

Minor comments:
- In Figure 1, ""the number of head"" -> ""the number of heads"".


[1] Wang, X., Girshick, R., Gupta, A., & He, K. (2018). Non-local Neural Networks. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'18).
[2] Kim, J.-H., Jun, J., & Zhang, B.-T. (2018). Bilinear Attention Networks. In Advances in Neural Information Processing Systems 32 (NIPS'18).","The sentiment score is -30 because while the reviewer acknowledges some strengths ('strong empirical results'), they express significant concerns about the methodology, originality, and clarity of the paper. The reviewer recommends major revisions, such as updating results using a validation set, which indicates a somewhat negative sentiment. However, it's not entirely negative as they see some value in the work. The politeness score is 50 because the reviewer uses respectful language throughout, such as 'I firmly recommend' and 'Please describe', showing consideration for the authors. They also balance criticism with positive remarks. However, the tone is not overly polite or deferential, maintaining a professional, objective stance.",-30.0,50.0
"This paper proposes a justification to one observation on VAE: ""restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization"". The explanation given in this work is based on Gaussian mean-field approximation.

I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example

- the sentence under eq. (2)
- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?

It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods. Somehow, those connections are not clear to me.

Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.

As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z). 

However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D should be: D <- \theta -> \tilde{\theta} (if it is a generative model) or D -> \theta -> \tilde{\theta} (if a discriminative model). Either case, I don't think we can have the inequality in eq. (5).  ","The sentiment score is -60 because the review is generally negative, pointing out several issues with the paper. The reviewer mentions having trouble understanding parts of the paper, notes sentences that don't make sense, and identifies a major problem with the paper's use of the data processing inequality. However, it's not entirely negative as the reviewer acknowledges the paper's attempt to explain an observation on VAE. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone. They use phrases like 'I had trouble to understand' and 'It looks like' rather than making blunt accusations. The reviewer also provides specific examples of issues, which is helpful. However, the use of bold text for the word 'wrong' could be seen as slightly impolite, preventing a higher politeness score.",-60.0,20.0
"Summary:
The paper presents a novel combinatorial search algorithm for the discrete target propagation framework developed in Friesen & Domingos (2018). Experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.

  Pros:
-	I like the goal of the work and think that if the targeted problem were to be solved it would be an interesting contribution to the field.
-	The proposed search algorithm is reasonable and works OK.
-	The paper is mostly well written and clear.
-	The experiments are reasonably thorough.

  Cons:
-	The paper states that it is a feasibility study on search methods for learning hard-threshold networks, however, it only evaluates the feasibility of one combinatorial search method. 
-	It’s not made clear whether other approaches were also investigated or what the authors learned from their exploration of this approach.
-	The actual algorithm is not very well explained, despite being the main contribution of the paper.
-	The datasets and models are small and not necessarily representative of the requirements of the field.
-	Scalability remains a serious concern with the proposed approach.
-	It’s not clear to me that the paper presents a sufficient enough contribution to warrant acceptance.

Overall, I like the direction but do not feel that the paper has contributed enough to warrant acceptance. The authors should use the experiments they’ve run and also run more experiments in order to fully analyze their method and use this analysis to improve their proposed approach. 


Questions and comments:

1.	Did you try alternative local search algorithms or did you just come up with a single approach and evaluate it? What did you learn from the experiments and the development of this algorithm that will let you create a better algorithm in the next iteration?

2.	I think that it is unfair to say that “it suggests a simpler, independent justification for the performance improvements obtained by their method.” in reference to the work of Friesen & Domingos (2018), given that the straight-through estimator is not well justified to begin with and their work in fact provides a justification for it. I do agree that it is important to investigate alternative heuristics and approaches within the discrete target propagation framework, however.

3.	Sections 2 and 3 do not clearly define what L_i is and where it comes from. Since these do not normally exist in a deep network they should be clearly defined.

4.	“step 2(b)” is not well defined in section 3.1.1. I assume that this refers to lines 4-8 of Algorithm 1? The paper should explain this procedure more clearly in the text. Further, I question the locality of this method, as it seems capable of generating any possible target setting as a neighbor, with no guarantee that the generated neighbors are within any particular distance of the uniform random seed candidate. Please clarify this.

5.	I believe that a negative sign is missing in the equation for T_i in ‘Generating a seed candidate’. For example, in the case where |N| = 1, then T_i = sign(dL/dT_i) would set the targets to attain a higher loss, not lower. Further, for |N|=1, this seems to essentially reduce to the heuristic method of Friesen & Domingos (2018). 

6.	In the ‘Setting the probabilities’ section:
(a) All uses of sign(h) can be rewritten as h (since h \in {-1, +1}), which would be simpler.
(b) The paper contradicts itself: it says here ‘flip entries only when sign(dL/dh) = sign(h)’ but Algorithm 1 says ‘flip entries only when sign(dL/dh) != sign(h)’. Which is it?
(c) What is the value of a_h in the pseudocode? (i.e., how is this computed in the experiments)

7.	In the experiments, the paper says that ‘[this indicates] that the higher dimensionality of the CIFAR-10 data manifold compared to MNIST may play a much larger role in inhibiting the performance of GRLS.’ How could GRLS overcome this? Also, I don’t agree with the claim made in the next sentence – there’s not enough evidence to support this claim as the extra depth of the 4-layer network may also be the contributing factor.

8.	In Table 2, why are some numbers missing? The paper should explain what this means in the caption and why it occurs. Same for the bolded numbers.

9.	The Loss Weighting, Gradient Guiding, Gradient Seeding, and Criterion Weighting conditions are not clearly defined but need to be to understand the ablation experiments. Please define these properly.

10.	The overall structure of the algorithm is not stated. Algorithm 1 shows how to compute the targets for one particular layer but how are the targets for all layers computed? What is the algorithm that uses Algorithm 1 to set the targets and then set the weights? Do you use a recursive approach as in Friesen & Domingos (2018)?

11.	In Figure 2, what dataset is this evaluation performed on? It should say in the caption. It looks like this is for MNIST, which is a dataset that GRLS performs well on. What does this figure look like for CIFAR-10? Does increasing the computation for the heuristic improve performance or is it also flat for a harder dataset? This might indicate that the initial targets computed are useful but that the local search is not helping. It would be helpful to better understand (via more experiments) why this is and use that information to develop a better heuristic.

12.	It would be interesting to see how GRLS performs on other combinatorial search tasks, to see if it is a useful approach beyond this particular problem.

13.	In the third paragraph of Section 4.2, it says ‘The results are presented in Table 3.’ I believe this should say Figure 3. Also, the ordering of Figure 3 and Table 3 should be swapped to align with the order they are discussed in the text. Finally, the caption for Table 3 is insufficiently explanatory, as are most other captions; please make these more informative.

14.	In Section 4.3:
(a), the paper refers to Friesen & Domingos (2018) indicating that zero loss is possible if the dataset is separable. However, what leads you to believe that these datasets are separable? A more accurate comparison would be the loss for a powerful non-binarized baseline network. 
(b) Further, given the standard error of GRLS, it’s possible that its loss could be substantially higher than that of FTPROP as well. It would be interesting to investigate the cases where it does much better and the cases where it does much worse to see if these cases are informative for improving the method.

15.	Why is there no discussion of training time in the experiments? While it is not surprising that GRLS is significantly slower, it should not be ignored either. The existence of the Appendix should also be mentioned in the main paper with a brief mention of what information can be found in it.

16.	In Algorithm 1, line 2 is confusingly written. Also, notationally, it’s a bit odd to use h both as an element and as an index into T.

17.	There are a number of capitalization issues in the references.

18.	The Appendix refers to itself (“additional hyperparameter details can be found in the appendices”).
","Sentiment score: The review starts with a balanced summary, acknowledging both pros and cons. However, the overall sentiment leans negative, as evidenced by statements like 'do not feel that the paper has contributed enough to warrant acceptance' and 'Scalability remains a serious concern'. The reviewer likes the direction but has significant reservations, resulting in a slightly negative score of -20.

Politeness score: The reviewer maintains a professional and respectful tone throughout. They use phrases like 'I like the goal of the work' and 'The paper is mostly well written and clear', which are encouraging. Even when pointing out issues, the language is constructive rather than harsh, such as 'The authors should use the experiments they've run and also run more experiments'. The questions and comments are framed politely, often starting with 'Please clarify' or 'It would be interesting to see'. This results in a politeness score of 60, indicating a notably polite tone while still maintaining critical feedback.",-20.0,60.0
"Paper Summary: This paper studies the zero-shot learning problem with deep generative models. More specifically, it proposed a hybrid framework that combines VAEs (more precisely, the variational hetero-encoder or VHE) and GANs all together. The entire model is composed of an image encoder (Weibull upward-downward variational encoder), a text decoder (Poisson Gamma belief network), and an image generator (generative adversarial network). Once learned, the generative models can be directly used for zero-shot classification and various image generation applications. In the experiments, two benchmark datasets CUB and Oxford-Flowers are used.

==
Novelty/Significance:
Zero-shot learning is a challenging task and he main motivation of the paper (using generative model) is interesting. The text representation in the paper is simply bag-of-words which limits the application to some extent. In a broader context, image captioning using generative model seems quite relevant.

Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space, Wang et al. In NIPS 2017.

==
Quality:
Overall, reviewer feels this is a very interesting work. However, the results from the paper is quite mixed. It is not yet convincing whether the proposed approach is the state-of-the-art in zero-shot learning or text-to-image generation. 

First, this paper demonstrates the power of generative models in text-to-image generation and other applications. However, reviewer feels that the zero-shot classification result is weak. In Table 1 and Table 2, it seems that GAZSL (Zhu et al. 2018) outperforms the proposed approach. 

Q1: In Table 2, is it possible to report the top-5 accuracy on CUB-easy and top-1 accuracy on Oxford-Flower dataset? Otherwise, it is not very convincing that proposed approach is better than the state-of-the-art approach GAZSL.

Second, the text-to-image generation results look reasonably good. But the resolution and quality of generated images are far from state-of-the-art. One suggestion is to train the VHE model with an improved image generator.

StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, Zhang et al. In CVPR 2017.

AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks, Xu et al. In CVPR 2018.

Also, reviewer would expect to see an improved image generator can lead to a better ZSL performance.

Typo: In the title: Zero-Short → Zero-Shot.

","The sentiment score is 50 (slightly positive) because the reviewer describes the work as 'very interesting' and acknowledges its potential, but also expresses some reservations about the results being 'quite mixed' and 'not yet convincing'. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement. The reviewer also acknowledges the paper's strengths before discussing its limitations. The use of phrases like 'reviewer feels' and 'one suggestion is' contribute to the polite tone. However, it's not extremely polite as it does directly point out weaknesses and asks for additional results.",50.0,70.0
"This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.

This paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. 

The paper also suffers from methodological problems:
-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. 
-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.

Finally, the paper suffers clarity issues. 
-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). 
-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).
-- The authors do not address hyper-parameter tuning. 
-- What is the gold standard for the ""special word retrieved"" data? how is it computed?


Other comments: 
-- Page 2: ""In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning."": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.
-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. 
-- Page 5: how is F1 defined?

Minor comments: 
-- Page 2: "" ... despite the small training data size (100K instances)."": 100K is typically not considered a small training set (for most tasks at least)
-- Page 5: ""... most of the constituent documents ..."": was this validated in any way? how?
-- The word ""extremely"" is used throughout the paper without justification in most cases.


Typos and such:
page 1: ""... a relevant knowledge to the above Winograd Schema example, **does** not present ... "": should be ""is""
page 5: ""In the previous sections, we ***show*** ..."": showed
page 7: ""For example, with the ***test*** ..."": ""test instance""
","The sentiment score is -70 because the reviewer expresses significant criticism and does not recommend acceptance. They state the paper 'is not particularly novel, and suffers from methodology and clarity problems.' The reviewer lists multiple issues with the paper's methodology, clarity, and novelty. However, they do acknowledge some positive aspects like 'impressive' results, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'The authors might also benefit from...' and provide constructive feedback. The reviewer avoids harsh language, but also doesn't use overtly polite phrasing, keeping the tone mostly neutral with a slight lean towards politeness.",-70.0,20.0
"The authors introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification.
The main idea is to learn task-independent representations, while other parameters, including the attention mechanism, are being fine-tuned for each specific task after pretraining. 
The authors find that, for few-shot text classification tasks, their proposed approach outperforms several important baselines, e.g., random initialization and MAML, in certain settings. In particular, ATAML performs better than MAML for very few training examples, but in that setting, the gains are significant. 

Comments:
- I am unsure if I understand the contributions paragraph, i.e., I cannot count 3 contributions. I further believe the datasets are not a valid contribution, since they are just subsets of the original datasets.
- Using a constant prediction threshold of 0.5 seems unnecessary. Why can't you just tune it?
- 1-shot learning is maybe theoretically interesting, but how relevant is it in practice? ","The sentiment score is slightly positive (20) because the reviewer acknowledges the proposed approach outperforms baselines in certain settings, particularly for very few training examples. However, the reviewer also raises some concerns and questions, which tempers the overall positivity. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout, offering constructive feedback without using harsh language. The reviewer uses phrases like 'I am unsure' and poses questions rather than making blunt criticisms, which contributes to the polite tone. The review is balanced, offering both positive observations and areas for improvement without being overly critical or dismissive.",20.0,50.0
"The paper introduces HR-TD, a variation of the TD(0) algorithm. The variant is meant to ameliorate a problem of ‘over-generalization’ with conventional TD. This problem is briefly characterized, but primarily it is presumed to be established by prior work. The algorithm is simple and a series of experiments are presented with it applied to Mountain Car, Acrobot, and Atari Pong, with both linear function approximation and neural networks (DDQN). It is claimed that the results establish HR-TD as an improvement over TD. However, I found the results unconvincing because they were statistically insufficient, methodologically flawed, and too poorly presented for me to be confident of the meaning of numbers reported. In addition, it is not hard to imagine very simple problems where the HR-TD technique would be counterproductive, and these cases were not included in the experimental testbeds.

The first weakness of the paper is with its characterization of the problem that it seeks to solve: over-generalization. This problem is never really characterized in this paper. It instead refers instead to two other papers, one published only in a symposium and the other with no publication venue identified.

The second weakness of the paper is the claim that it has done a theoretical analysis in Section 4.4. I don’t see how this section establishes anything of importance about the new method.

The problem with the main results, the empirical results, is that they do not come close to being persuasive. There are many problems, beginning with there simply not being clear. I read and reread the paragraphs in Section 5.1, but I cannot see a clear statement of what these numbers are. Whatever they are, to assess differences between them would require a statistical statement, and there is none given. Moreover to give such a statistical statement would require saying something about the spread of the results, such as the empirical variance, but none is given. And to say something about the variance one would need substantially more than 10 runs per algorithm. Finally, there is the essential issue of parameter settings. With just one number given for each algorithm, there are no results or no statement about what happens as the parameters are varied. Any one of these problems could render the results meaningless; together they surely are.

These problems become even greater in the larger problems.

A nice property of HR-TD is that it is simple. Based on that simplicity we can understand it as being similar to a bias toward small weights. Such a bias could be helpful on some problems, possibly on all of those considered here. In general it is not clear that such a bias is a good idea, and regular TD does not have it. Further, HR-TD does not do exactly a bias to small weights, but something more complicated. All of these things need to be teased apart in careful experiments. I recommend small simple ones. 

How about a simple chain of states that are passed through reliably in sequence leading to a terminal state with a reward of 1000 (and all the other rewards 0). Suppose all the states have the same feature representation. If gamma=1, then all states have value 1000, and TD will easily learn and stick at this value even for large alpha, but HR-TD will have a large bias toward 0, and the values will converge to something significantly less than the true value of 1000. 

That would be an interesting experiment to do. Also good would be to compare HR-TD to a standard bias toward small weights to see if that is sufficient to explain the performance differences.","The sentiment score is -70 because the review is predominantly negative. The reviewer finds the results 'unconvincing', points out multiple weaknesses in the paper, and states that the empirical results 'do not come close to being persuasive'. However, it's not entirely negative as the reviewer acknowledges some positive aspects like the simplicity of HR-TD. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I found' and 'I recommend' rather than making blunt accusations. The reviewer also provides constructive suggestions for improvement, which is a polite approach. However, some phrases like 'These problems become even greater' and 'All of these things need to be teased apart in careful experiments' have a slightly stern tone, preventing a higher politeness score.",-70.0,20.0
"The paper describes an imitation reinforcement learning approach where
the primitive actions of the agent are augmented with the most common
sequences of actions perform by experts. It is experimentally shown
how this simple change has clear improvements in the performance of
the system in Atari games. In practice, the authors double the number
of primitive actions with the most frequent double actions perform by
experts. 

A positive aspect of this paper comes from the simplicity of the
idea. There are however several issues that should be taken into
account:
- It is not clear how to determine when the distribution of action
  pair saturates. This is relevant for the use of the proposed approach.
- The total training time should consider both the initial time to
  obtain the extra pairs of frequent actions plus the subsequent
  training time used by the system. Either obtained from a learning
  system (15 hours) or by collecting traces of human experts (< 1
  hour?). 
- It would be interesting to see the performance of the system with
  all the possible pairs of primitive actions and with a random subset
  of these pairs, to show the benefits of choosing the most frequent
  pairs used by the expert.
- This analysis could be easily extended to triplets and so on, as
  long as they are the most frequently used by experts.
- The inclusion of macro-actions has been extensively studied in
  search algorithms. In general, the utility of those macros depends on
  the effectiveness of the heuristic function. Perhaps the authors
  could revise some of the literature.
- Choosing the most frequent pairs in all the game may not be a
  suitable strategy. Some sequences of actions may be more frequent
  (important) at certain stage of the game (e.g., at the beginning/end
  of the game) and the most frequent sequences over all the game may
  introduce additional noise in those cases.

The paper is well written and easy to follow, there are however, some
small typos:
- expert(whose => expert (whose
% there are several places where there is no space between a word and
% its following right parenthesis 
- don't need train => don't need to train
- experiments4. => experiments.
- Atmost => At most
","The sentiment score is slightly positive (20) because the reviewer acknowledges the simplicity of the idea as a positive aspect and the paper is described as well-written and easy to follow. However, the reviewer also lists several issues and suggestions for improvement, which tempers the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and frames suggestions as 'interesting' or 'should be taken into account' rather than using demanding language. The reviewer also acknowledges both positive and negative aspects of the paper, which contributes to a polite tone. The mention of typos is done in a matter-of-fact way without harsh criticism.",20.0,60.0
"This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks. The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward. This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers). The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.

Strengths:
+ The paper is generally clear and readable.
+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.

Major concern:
- My biggest concern is that the technical contributions of the paper are not clear at all. The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control). The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)). The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either. Overall, the paper does not make a compelling case for the novelty of the problem or approach.

Other concerns:
- For the cart-pole task, the paper states that the reward is modified ""to exclude any cost objective"". Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this). I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.
- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent. However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied). 

Typos:
- Pg. 5, Section 3.4: ""...this is would achieve...""
- Pg. 6: ...thedse value of 90...""","The sentiment score is -50 because while the reviewer acknowledges some strengths ('generally clear and readable', 'realistic model'), they express a 'major concern' about the lack of clear technical contributions. They state that the approach is not novel and 'does not make a compelling case for the novelty of the problem or approach'. This significant criticism, along with other concerns raised, indicates a generally negative sentiment, though not extremely so. The politeness score is 60 because the reviewer uses professional and respectful language throughout. They begin by noting strengths, use phrases like 'I would have liked to have seen' instead of demanding changes, and offer constructive feedback. The tone is critical but not harsh or rude, maintaining a polite and professional demeanor while expressing concerns.",-50.0,60.0
"This paper presents a joint optimization approach for the continuous weights and categorical structures of neural networks. The idea is the standard stochastic relaxation of introducing a parametrised distribution over the categorical parameters and marginalising it. The method then follows by alternating gradient descent on the weights and the parameters of the categorical distribution.

This exact approach was proposed in https://arxiv.org/abs/1801.07650 by Shirakawa et al. The only innovation in this work is that it uses categorical distributions with more than two values. This is a minor innovation.

The experiments are however interesting as the paper compares to the latest hyper-parameters optimization strategies for neural nets on simple tasks (eg CIFAR10) and gets comparable results. However, given that this is the biggest contribution of the paper, it would have been nice to see results in more complex tasks, eg imagenet or translation.

I very much enjoyed the simplicity of the approach, but the question of innovation is making wonder whether this paper makes the ICLR bar of acceptance. The paper is also hard to read because of many English typos.   ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting experiments', 'enjoyed the simplicity'), they express significant concerns about the paper's innovation and readability. The reviewer questions whether the paper meets the acceptance criteria for ICLR, which indicates a leaning towards rejection. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positive aspects and expressing concerns in a professional manner. They use phrases like 'I very much enjoyed' and 'it would have been nice to see', which maintain a polite tone even while critiquing. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the polite tone.",-20.0,50.0
"This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. Overall, I found the writing very clear, the main idea sound, and paper generally well executed, but I have serious concerns about the significance of the contributions that lead me to recommend rejection. It would be very useful to me if the authors would provide a concise list of what they consider the main contributions to be and why they are significant. As I see it, the paper does three main things:

1. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions. The simplifications and gradient derivations are well known and appear in many places (e.g. http://jonathan-huang.org/research/dirichlet/dirichlet.pdf, https://arxiv.org/pdf/1405.0099.pdf) and should not be considered contributions in the age of automatic differentiation (see Justin Domke's blog post on autodiff).

2. In section 3, the authors consider the unique challenges of using the proposed networks. They propose targeted activation functions that will improve the stability of learning. I found this to be the most interesting portion of the paper and the most significant contribution. Unfortunately, it is short on details and empirical results are referenced that do not appear in the paper (i.e. the second to last paragraph on page 5). If I were to rewrite this paper, I would focus on answering the question ""What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?"", replacing section 2 with an expanded section 3.

3. In section 4, the authors evaluate the proposed networks on a collection of synthetic and real tasks. In the end, the results are mixed, with the Dirichlet network performing best on the XENON1T task and the standard softmax network performing best on the CIFAR-100 task. In general, I don't mind mixed results and I appreciate that the authors included both sets of experiments; however, it is important that there is a convincing argument for why one would prefer the proposed solution even when accuracy is the same (e.g. it is faster, it is interpretable, etc.). The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. This may be true, but they only perform evaluations on tasks where the primary goal is accuracy. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that.

In summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or (2) present a convincing evaluation that strongly motivates the proposed model's use or that provides some novel insight into the model's behavior. I think that the authors are on their way to achieving (1), but do not achieve (2). I would suggest finding an application that requires uncertainty estimates for the distribution and centering the paper around that application.

Minor comments:

- Figure 2 (right) should include a y-axis label (e.g. ""parameter value"").

- In Figure 3 (right), it is not obvious what the ""Sigmoid"" line corresponds to. 

- It is not clear what the authors are trying to show in section 4.1. The EL activation function is smooth and monotone and the likelihood is convex, so there should be no question that the distribution will concentrate around y.

- Section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.","The sentiment score is -50 because while the reviewer found some positive aspects ('writing very clear', 'main idea sound', 'paper generally well executed'), they ultimately recommend rejection due to 'serious concerns about the significance of the contributions'. This indicates an overall negative sentiment, though not extremely harsh. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledges positive aspects, and provides constructive feedback. They use phrases like 'It would be very useful to me if...' and 'I appreciate that the authors included...', which are polite ways of offering criticism. However, the review is not overly deferential, maintaining a professional tone, hence the moderate positive score rather than a very high one.",-50.0,50.0
"This paper presents Partially Mutual Exclusive Softmax (PMES), a relaxation of the full softmax that is commonly used for multi-class data. PMES is designed for positive-unlabeled learning, e.g., language modeling, recommender systems (implicit feedback), where we only get to observe positive examples. The basic idea behind PMES is that rather than considering all the non-positive examples as negative in a regular full softmax, it instead only considers a ""relevant"" subset of negatives. Since we actually don't know which of the negatives are more relevant, the authors propose to incorporate a discriminator which attempts to rate each negative by how hard it is to distinguish it from positives, and weight them by the predicted score from the discriminator when computing the normalizing constant for the multinomial probability. The motivation is that the negatives with higher weights are the ones that are closer to the decision boundary, hence will provide more informative gradient comparing to the negatives that are further away from the decision boundary. On both real-world and synthetic data, the authors demonstrate the PMES improves over some other negative sampling strategies used in the literature. 

Overall the idea of PMES is interesting and the solution makes intuitive sense. However, the writing of the paper at the current stage is rather subpar, to the extend that makes me decide to vote for rejection. In details:
 
1. The motivation of PMES from the perspective of mutual exclusivity is quite confusing. First of all, it is not clear to me what exactly the authors mean by claiming categorical distribution assumes mutual exclusivity -- does it mean given a context word, only one word can be generated from it? Some further explanation will definitely help. Further more, no matter what mutual exclusive means in this context, I can hardly see that PSME being fundamentally different given it's still a categorical distribution (albeit over a subset).

The way I see PMES from a positive-unlabeled perspective seems much more straight-forward -- in PU learning, how to interpret negatives is the most crucial part. Naively doing full softmax or uniform negative sampling carry the assumption that all the negatives are equal, which is clearly not the right assumption for language modeling and recommender systems. Hence we want to weight negatives differently (see Liang et al., Modeling user exposure in recommendation, 2016 for a similar treatment for RecSys setting). From an optimization perspective, it is observed that for negative sampling, the gradient can easily saturate if the negative examples are not ""hard"" enough. Hence it is important to sample negatives more selectively -- which is equivalent to weighting them differently based on their relevance. A similar approach has also been explored in RecSys setting (Rendle, Improving pairwise learning for item recommendation from implicit feedback, 2014). Both of these perspectives seem to offer more clear motivation than the mutual exclusivity argument currently presented in the paper.

That being said, I like the idea of incorporating a discriminator, which is something not explored in the previous work.  

2. The rigor in the writing can be improved. In details:

* Section 3.3, ""Multivariate Bernoulli"" -> what is presented here is clearly not multivariate Bernoulli

* Section 3.3, the conditional independence argument in ""Intuition"" section seems no difference from what word2vec (or similar models) assumes. The entire ""Intuition"" section is quite hand-wavy.

* Section 3.3, Equation 4, 5, it seems that i and j are referred both as binary Bernoulli random variables and categorical random variables. The notation here about i and j can be made more clear. Overall, there are ambiguously defined notations throughout the paper. 

* Section 4, the details about the baselines are quite lacking. It is worth including a short description for each one of them. For example, is PopS based on popularity or some attenuated version of it? As demonstrated from word2vec, a attenuated version of the unigram (raised to certain power < 1) works better than both uniform random, as well as plain unigram. Hence, it is important to make the description clear. In addition, the details about matrix factorization experiments are also rather lacking. 

3. On a related note, the connection to GAN seems forced. As mentioned in the paper, the discriminator here is more on the ""cooperative"" rather than the ""adversarial"" side. 

Minor:

1. There are some minor grammatical errors throughout. 

2. Below equation 3, ""\sigma is the sigmoid function"" seems out of the context.

3. Matt Mohaney -> Matt Mahoney ","The sentiment score is -60 because the reviewer explicitly states they are voting for rejection due to subpar writing, which indicates a strongly negative sentiment. However, they do mention some positive aspects like the idea being interesting, which prevents the score from being even lower. The politeness score is 20 because the reviewer uses generally polite and professional language, offering constructive criticism and suggestions for improvement. They avoid harsh or rude phrasing, instead using phrases like 'can be improved' and 'it is worth including'. However, the directness of some criticisms (e.g., 'subpar', 'rather lacking') prevents the score from being higher.",-60.0,20.0
"This paper proposes a new variational recurrent model for learning sequences. Comparing to existing work, instead of having latent variables that are dependent on the neighbors, this paper proposes to use independent latent variables with observations that are generated from multiple latent variables. 
The paper further combined the proposed method with multiple existing ideas, such as the shared/prviate representation from VAE-CCAE, adding the hierarchical structure, and prior updating. 

Pros:
The proposed method seems technical correct and reasonable. 
There are many extensions which are potentially useful for many applications 
There are many experimental results showing promising performance. 

Cons:
The framework is very incremental. It is novel but limited. 
The paper claim that the main point to use the simpler variations distribution is to speed up the inference. But no speed comparisons are shown in the experiments section. 
The evaluation shows that prior updating (one extension) seems contributes to the biggest performance gain, not the main proposed method. 

","The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as technical correctness, potential usefulness, and promising experimental results. However, the cons mentioned, including the incremental nature of the work and lack of speed comparisons, prevent a higher positive score. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, presenting both pros and cons objectively without using harsh language. The reviewer's critique is constructive and balanced, avoiding overly negative or confrontational statements.",20.0,50.0
"I like the idea of trying to qualitatively illustrate the behavior of SGD when optimizing parameters of complex models, such as Deep and Conv Nets, but I think that the contribution is not very substantial. The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. 

It would helpful to illustrate the difference between ""crossing"" and ""moving over"" a barrier with a simple figure. 

The experimental validation is interesting, although I think it is limited and perhaps the conclusions that can be drawn from it are not so surprising. I believe it would have been interesting to study other important factors that affect the behavior of SGD, such as learning rate and type of momentum. For example, a larger learning rate might allow for more crossing of barriers. Also, different SGD algorithms (ADAGRAD, ADAM, etc...) would behave considerably differently I expect. At the moment these important factors are overlooked. 

It is not clear to me why we would want to avoid larger batch sizes. A larger batch size allows for a lower variance of stochastic gradients, and therefore faster convergence. I think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful SGD works, such as SAGA and the like. I agree that a smaller batch-size is preferable at the beginning of the optimization, but again this is a well known fact (again, see SAGA) and it is for computational reasons mostly (being far away from the (local) mode, a noisy gradient is enough to move in the right direction - no need to spend computations to use an accurate gradient). There is no guarantee that the local optimum close to initialization is a bad local optimum in general, so I don't think that using a large batch size at the beginning is a bad idea for this reason - again it is just computational. 

Another thing missing I think is the discussion around why it is potentially a good thing to cross the barrier, either at the beginning of the exploration or towards convergence to a local optimum. At the moment, the paper seems to report the behavior of SGD without key insights on the importance of crossing or avoiding crossing barriers.

As a concluding remark - there has been a lot of work on the connections between diffusions and MCMC algorithms (see e.g., the Metropolis Adjusted Langevin Algorithm - MALA) and a lot of the considerations made in the paper are somewhat known. That is, random walk/diffusion type MCMC (and even gradient-based MCMC like Hybrid Monte Carlo) struggle a lot in non-convex problems and they hardly move across modes of a posterior distribution (equivalent to crossing barriers of potential). So I'm not at all surprised that SGD does not cross barriers during optimization and I would challenge the statement in the introduction saying ""Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process.""","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('I like the idea', 'The experimental validation is interesting'), they express several criticisms and limitations of the work. They state that the contribution is 'not very substantial', the conclusions are 'not so surprising', and important factors are 'overlooked'. The overall tone suggests that the reviewer sees potential in the work but finds it lacking in several areas.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'It would be helpful', 'I believe it would have been interesting', and 'I think this point requires elaboration' to suggest improvements rather than making harsh criticisms. The reviewer also acknowledges positive aspects of the work before presenting criticisms. However, the score is not higher because the review is quite direct in pointing out limitations and doesn't use many overtly polite phrases or softening language.",-30.0,50.0
"The authors suggest a reward shaping algorithm for multi-agent settings that adds a shaping term based on the TD-error of other agents to the reward. In order to implement this, each agent needs to keep tack of two different value estimates through different DQN networks, one for the unshaped reward and one for the shaped reward. 

Points of improvement and questions: 
-Can you please motivate the form of the reward shaping suggested in (2) and (3)? It looks very similar to simply taking \hat{r}_a = r_a + sum_{a' not a} z_{'a}. Did you compare against this simple formulation? I think this will basically reduce the method to Value Decomposition Networks (Sunehag ‎2017) 
-The results on the prisoners dilemma seem miss-leading: The ""peer review"" signal effectively changes the game from being self-interested to optimising a joint reward. It's not at all surprising that agents get higher rewards in a single shot dilemma when optimising the joint reward. The same holds for the ""Selfish Quota-based Pursuit"" - changing the reward function clearly will change the outcome here. Eg. there is a trivial adjustment that adds all other agents rewards to the reward for agent i that will will also resolve any social dilemma.
-What's the point of playing an iterated prisoners dilemma when the last action can't be observed? That seems like a confounding factor. Also, using gamma of 0.9 means the agents' horizon is effectively limited to around 10 steps, making 50k games even more unnecessary. 
-""The input for the centralized neural network involves the concatenation of the observations and actions, and optionally, the full state"": This is not true. For example, the Central-V baseline in COMA can be implemented by feeding the central state along (without any actions or local observations) into the value-function. It is thus scalable to large numbers of agents. 
-The model seems to use a feed-forward policy in a partially observable multi-agent setting. Can you please provide a justification for this choice? Some of the baseline methods you compare against, eg. QMIX, were developed and tested on recurrent policies. Furthermore, independent Q-learning is known to be less stable when using feedfoward networks due to the non-stationarity issues arising (see eg. ""Stabilising Experience Replay"", ICML 2017, Foerster et al). In it's current form the concerns mentioned outweigh the contributions of the paper.","The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper and expressing concerns that 'outweigh the contributions of the paper.' However, it's not entirely negative as it acknowledges the authors' work and provides constructive feedback. The politeness score is 20 because the reviewer uses polite language such as 'Can you please...' and 'Please provide...', but the overall tone is direct and critical. The reviewer maintains a professional tone without using overly harsh language, but also doesn't go out of their way to soften criticisms or offer praise, keeping the review mostly neutral in terms of politeness.",-50.0,20.0
"In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.

As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:

* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.
* Different motivation/derivation of NES.
* Concept of adversarial distributions.
* Regression network for good initialization.
* Introduction of accuracy-iterations plots.

My main concerns are as follows:
* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement “However, existing black-box attacks are weaker than their white-box counterparts” is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].
* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.
* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.

Hence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.

[1] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” (https://arxiv.org/abs/1804.08598) 
[2] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)
[3] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (https://arxiv.org/abs/1805.09190)
[4] Athalye et al. (2017) “Synthesizing Robust Adversarial Examples” (https://arxiv.org/pdf/1707.07397.pdf)
[5] Madry et al. {2017) “Towards Deep Learning Models Resistant to Adversarial Attacks” (https://arxiv.org/pdf/1706.06083.pdf)","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some contributions of the paper, they express several major concerns and state that a 'major revision of the manuscript will be necessary.' The overall tone suggests that the paper has some value but falls short in several important areas. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, acknowledges the authors' contributions, and provides detailed, constructive feedback. They use phrases like 'I like the idea' and 'it would be great if' which contribute to a polite tone. However, the critique is direct and doesn't use overly deferential language, keeping the score from being extremely high.",-20.0,60.0
"Paper Summary - This paper presents an approach for fine-grained action recognition and video captioning. The authors train a model using both classification and captioning tasks and show that this improves performance on transfer learning tasks. The method is evaluated on the Something-Something v2 dataset as well as a new dataset (proposed in this paper). The authors also evaluate the benefit of using fine-grained action categories vs. coarse-grained action categories on transfer learning.

Paper Strengths
-  Comparing fine-grained vs. coarse-grained action categories for transfer learning is well motivated. Evaluating just this aspect in the context of video classification is helpful (Section 5.1). Establishing the baseline using linear classifiers for feature transfer makes the feature transfer result more robust. The authors have also done a good job of evaluating their method in the coarse-grained and fine-grained settings (Table 1, 2).
- The architectural and experimental design in this paper is well illustrated.
- The 20bn kitchen dataset has interesting categories about intention - pretending to use, using, and using & failing.
- The ablation in Table 1 is helpful in understanding the contribution of 3D vs. 2D convolutions.

Paper Weaknesses
- I believe this paper tries to do too much and as a result fails to show results convincingly. There are too many results and not much focus on analyzing them. In my opinion, the experimental setup in the paper is weak to fully support the authors' claims.
- I now analyze the main contributions of this paper as outlined by the authors in Section 1.
    - Label granularity and feature quality: To me this is the most interesting part of this paper and most related to its title. However, this is also the most under-analyzed aspect. The only result that the authors show is in Sec 5.1 and Fig 5. Apart from using the provided fine-grained vs. coarse-grained labels for evaluation, the authors do not perform many experiments in this domain and neither do they analyze these results. For example, the gain using fine-grained labels is not significant in Figure 5 (2Channel - CG vs. 2Channel - FG). The authors do not explain this aspect. Another missing baseline from Figure 5 is ""2Channel - Captions & CG actions"". This baseline is needed to understand the contribution of FG vs CG actions when also using captioning as additional supervision.
    - Baselines for captioning: The authors do not provide any details for this task. If the intent is to establish baselines there needs to be more effort on analyzing design decisions - e.g. decoding, layers in LSTM. Captioning metrics such as CIDER and SPICE are missing.
    - Captions as a source for transfer learning: This is poorly analyzed in this paper. 1) Can the captions be converted to ""tags"" and then used for supervision? What is the benefit of producing the full sequential text description over this simple approach? 2) Captions for transfer learning are only analyzed in Figure 5 without much explanation. It is hard to claim that captioning is the reason for performance gains without really analyzing it completely.
    - 20bn-kitchenware dataset - This dataset is explained in just one paragraph in Section 6. What is the motivation behind collecting this dataset as opposed to showing transfer learning on some other dataset?
- Missing references
        - There has been work in understanding the effect of fine-grained categories in ImageNet transfer learning - What makes ImageNet good for transfer learning? Huh et al. What is the insight provided over this work?
- Minor comments
    - Section 1: Figure 4 is referenced in points 1 & 3. I think you mean Figure 5.
","The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper (e.g., 'well motivated', 'well illustrated', 'helpful'), they also express significant criticisms. The review states that the paper 'tries to do too much and as a result fails to show results convincingly' and has a 'weak' experimental setup. The reviewer also points out several missing analyses and baselines. These criticisms outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I believe' and 'In my opinion' to soften criticisms, and they provide specific, constructive feedback. The reviewer also acknowledges the paper's strengths before discussing its weaknesses. However, the score is not higher as the review is quite direct in its criticisms, without excessive softening language.",-30.0,50.0
"The paper proposes a generative infection cascade model based on latent vector representations. The main idea is to use all possible paths of infections in the model. 

Comments:
- The papers clarity could be much improved. It is not easy to follow, is overflowing with notation, and lengthy. Sec. 2.1 for example can easily be made much more concise. Secs. 3.1 and 3.2 are especially confusing. In the first equation in Sec. 3, what is \phi with and without sub/superscript? In Eq. (2), what is k - a probability, or an index? And what is the formal definition ""infection"" and ""future"" in the description of k stating that it is ""the probability that u infects v in the future""?

- The authors mention that the actual infectors in a diffusion process are rarely observed. While this might be true, in many types of data include infection attempts. This should be worthwhile to model - there are many works on reconstructing cascades from partial data.

- The authors note (rightly) the Eq. (9) is hard to solve, and propose a simple lower bound based on (what I think is) a decomposition assumption.  Unless I misunderstood, this undermines the contribution of the structure of past infections. Could the authors please clarify?

- The results mention 5 (tables?), but only 4 are available, of which one appears floating on the last page.

- Why are methods discussed in the introduction (e.g., DeepCas, Wang 2017a,b 2018) not used as baselines?

Minor:
- Wang 2017a and Wang 2017b are not the same Wang
- Several occurrences of empty parentheses - ()
- ","The sentiment score is -30 because the review is generally critical, pointing out several areas for improvement such as clarity, model assumptions, and missing comparisons. However, it's not entirely negative as it acknowledges some positive aspects like the main idea being interesting. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language such as 'could be much improved' and 'please clarify'. They also offer constructive feedback and suggestions rather than just criticism. The reviewer maintains a professional tone throughout, even when pointing out issues.",-30.0,20.0
"The paper suggest a shrinkage-based estimator (James-Stein estimator) to compute policy gradients in reinforcement learning to reduce the variance by trading some bias. Two versions are suggested: The on-policy gradients is shrinked either towards (i) model based gradient, or towards (ii) a delayed average of previous on-policy gradients. Empirically, both methods have better performance than the baseline.

The paper is clearly written and well motivated. Some details are lacking that would be of interest to the reader and to make the results reproducible. For example how is \hat Q estimated? The trick that is referred to in the end of page about only simulating short horizon trajectories deserves more detail. I would suggest providing more details, in the text and/or in the two algorithms.

The authors claim that JS estimator for gradient estimation in RL has not been used before. I am also not aware of any other work, but have also not been looking after that line of work. The paper seems to be a good contribution to the ever increasing literature of how to improve deep RL.

Minors:
\hat \theta on RHS in eq (7) should be \bar \theta ? Otherwise, what is \hat \theta?
section 4.2 Ww -> We

======= After revision =========

I still think this is a very interesting, novel and relevant idea that desires attention. However, on the same time, I agree with the points raised by the other two reviewers which are all well-motivated and relevant concerns. Therefore, I join the view that the paper is not yet ready for publication but I do encourage the authors to improve their work and resubmit to another venue.","The sentiment score is slightly negative (-20) because while the reviewer initially praises the paper as 'clearly written and well motivated' and a 'good contribution', they ultimately conclude that it's 'not yet ready for publication'. This indicates a overall negative sentiment, albeit a mild one. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and providing constructive feedback. They use phrases like 'I would suggest' and 'I encourage the authors to improve their work', which are polite ways of offering criticism. The reviewer also balances negative points with positive ones, showing consideration for the authors' efforts.",-20.0,60.0
"
[pros]
- It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases.
- It also proposes use of the penalty term in terms of the Lipschitz constant  of the discriminative function.

[cons]
- Some of the arguments on the Wasserstein distance and on WGAN are not sound.
- Theorem 3 does not make sense.
- The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).

[Quality]
I found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below.

[Clarity]
The main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions.

[Originality]
Despite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\phi,\varphi,\psi$ and the form of the gradient penalty, $\max \|\nabla f(x)\|_2^2$ in this paper versus $E[(\|\nabla f(x)\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal.

[Significance]
This paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning.

Detailed comments:

In Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation.

It is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the ""critic"" $f$ via a multilayer neural network with weight clipping.

One can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading.

On optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\sim P_g}[f(x)]-E_{x\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. 

I do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes ""$\forall x \not= y$"", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\|x-y\|$ to obtain $f(x)-f(y)=k\|y-x\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \not= y$ such that $f(y)-f(x)=k\|x-y\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\|x-y\|$ under the Lipschitz condition.

Appendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\partial J_D/\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as ""arg min"" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function.

Page 5, line 36: $J_D(x)$ appears without explicit definition.

Page 23, lines 34 and 38: Cluttered expression $\frac{\partial [}{\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.","The sentiment score is -30 because the review is generally critical, pointing out several weaknesses and issues with the paper. The reviewer states that the paper's quality is 'not high', criticizes some arguments as 'not sound', and notes that a key theorem 'does not make sense'. However, the score is not extremely negative as the reviewer also acknowledges some positive aspects, such as the paper's significance in stimulating further research. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I found some weaknesses' and 'I do not agree' rather than harsh or dismissive language. The reviewer also balances criticism with positive comments about clarity and significance. However, the score is not higher as the review is quite direct in its criticisms without much softening language.",-30.0,50.0
"Summary: 
This paper expands on the work on 'emergent communication' with 2 innovations: 
- The architecture has a separate 'message channel' that processes the incoming and outgoing messages mostly independently of the hidden state of the agent. There are also dedicated architecture elements for the interaction between the hidden state and the message stream. 
- The outgoing message is gated with a 'speak' action: only when the agent takes the speak-action at time step t is a message sent out at timestep t+1. 

Comments for improvement: 
-The paper proposes a rather complicated architecture, with many moving part. In the paper's current form it is extremely hard to see which part of this architecture contribute to the success of the method. A set of ablation studies on the different components would indeed be very helpful. 
-Using the word 'thought' to describe the hidden state of the agent is rather distracting.
-Equation (1): This just seems to be the policy gradient term for a factorised action space across 'environment action' and 'communication action'. The only obvious difference is that the policy here is shown to condition on the state representation s_t, rather than on the input. Is that intended?
-The paper suffers from a lot of undefined notation, e.g. the s_t above. Please clarify.
-In Figure 2b) the MCU is shown to produce the action a_t as an output. That seems like a mistake. 
-Figure 4): The results seem to be extremely unstable, which is a well known issue for independent learning. Recent work (MADDPG, COMA) has shown that centralised critics can drastically avoid these instabilities and improve final performance. Did you compare against using a centralised critic, V(central state), rather the V(observation)? Also, using a single seed on this kind of unstable learning process renders the results highly non-conclusive. 
-In Figure (5), what are the red-arrows? Do these correspond to the actual actions taken by the agents or are they simply annotations? It would be good to see how far the communication range is by comparison. Also, why is there a blob of 'communicating' agents far from the enemy? 
-Are different methods in the large scale battle task trained in self-play and then pitched against other methods in a round-robin tournament after training has finished or are they trained against each other? 
-In Figure 6 (a), why are average rewards changing over the course of training? I would expect this to be a zero-sum setting in self-play. 
-I couldn't find any supplementary material referenced in the text for the details. Instead the paper seems to have another copy of the paper itself attached in the pdf. This makes it hard to evaluate the paper given that few details around training are provided in the main text. 

Overall I am concerned that the learning method used in the paper (independent baseline) is known to be unstable and to produce poor results in the multi-agent setting (see COMA and MADDPG). This raises the concern that the communication channel is mostly useful for overcoming the issues introduced from having a decentralised critic.","The sentiment score is -50 because while the reviewer acknowledges some innovations in the paper, they express several significant concerns and criticisms. The reviewer points out multiple issues with the methodology, results, and presentation, suggesting that the paper needs substantial improvements. The overall tone is more negative than positive, but not entirely dismissive.

The politeness score is 20 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'Comments for improvement' and 'Please clarify' which are polite ways of offering criticism. The reviewer also acknowledges the paper's innovations at the beginning. However, the language is not overly polite or deferential, maintaining a neutral, academic tone for the most part.",-50.0,20.0
"[Summary:]
This paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce “miracle representations” and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.

[Strengths:]
The paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. 

[Weaknesses:]
- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].
- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. 
- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].

[Specific comments:]
- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. 
-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.
-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.
-The use of the term “miracle” keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one “miracle X” was defined and used as these are all essentially saying the same thing.

References
[1]https://arxiv.org/abs/1803.02999
[2]https://arxiv.org/abs/1801.05558

","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's clear motivation, readability, and interesting ideas in the 'Strengths' section. However, they also point out several weaknesses and areas for improvement, which tempers the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement without using harsh or dismissive language. The reviewer maintains a professional tone, balancing positive feedback with areas of concern, and uses phrases like 'I am wondering' and 'it would have been nice' to soften their critiques.",20.0,60.0
"This paper presents an algorithm for finding a polytope of adversarial examples. This means that within a convex hull, you can move around freely and get a new adversarial example at each point, while still maintaining misclassification. It then couples this with a method of generating nearest neighbor patch-based images in an effort to create ""macro-level different"" examples. The premise is interesting, but the implications are questionable and I do not find the work in macro-level differences to be sound. This could be based in misunderstandings, so please let me know if you think that is the case.

Strengths:
- The notion of the polytope is interesting and the algorithm for finding such polytope seems perfectly reasonable.
- I think the goal of macro-level adversarial examples is interesting.

Weaknesses:
- First of all, the 5 corners of the polytope all look the same to me (for instance fig 2). This is not encouraging, because it means that every single point in the polytope will also look exactly like the corners. To be frank, this means the polytope is not that interesting and has only found an extremely small pocket of adversarial examples. If you use a regular method of finding a single adversarial example, I'm sure the outcome wouldn't change within some ball around the sample (perhaps with very small radius, but nonetheless). In fact, a comparison between that ball's volume and the volume of the polytope would be interesting.
- The implication of these polytopes is not at all clear if it doesn't really allow us to generate adversarial example of a new flavor. The investigation into macro-level differences does not help the case, as I will explain.
- I am not at all convinced that there is any meaning to the examples with ""macro-level differences.""  It's a bit unclear to me how many patches are used per image, but assuming that a patch is centered over each pixel,  it would mean that we have as many control parameters as we have pixels, which assuming the pixels each have three color values, is just 1/3 of the original degrees of freedoms. Now, the patches probably do constrain what we can paint a bit, but since the patches are applied with a pyramid, it means the center pixel will contribute more than any other for a given patch, so I'm not so sure. I'm not convinced that we can't come up with linear combinations of these patches that produce highly non-natural images with ""micro-level"" adversarial patterns. In fact, I think section 4.1 and figure 7 provide evidence to the contrary. Let me explain:
    - Section 4.1: Why do you need a total variation penalty at all if you have constructed a patch-based drawing method that is supposed to be unable to produce unnatural high-frequency patterns? If you only had a handful of patches and they were all non-overlapping, then this would be impressive and.
    - Figure 7: We can clearly see high-frequency patterns that create the shadow of an obelisk in 7(a). I think the same is true for ""erase"", although the pattern is not as recognizable. The examples actually look more suspicious than regular adversarial examples, since it looks like the original image has simply been blurred, which means the adversarial perturbations are more clear. I understand that these patterns were created using a complicated scheme of natural patches, but I think you made this method too powerful. The one interesting quality is the bottom right of the trimaran which looks like a shark - however, that is a singular occurrence in your examples and it certainly feels like the high-frequency patterns will contribute much more to class than the shark itself.
- Please let me know if I am misinterpreting the importance of the results in Figure 7, since this is an important culmination of this work.

Other comments:
- Some of notation is a bit confusing. In (1), why is p not bold but x and t are bold? They are all vectors. In Algorithm 1, x is not bold anymore.
- Algorithm 1 also seems quite unnecessary to include so explicitly.
- Isn't a bounded polytope called a ""simplex""? Perhaps there is a distinction that I'm not aware of, but the absence of the word ""simplex"" throughout the whole paper surprised me a bit. Perhaps this is a perfectly correct omission due to differences that I'm not aware of.

Minor comments:
- abstract, ""We propose a way to finding"" -> either ""to->""of"" or ""find""
- page 3, ""and we can generate new colliding example"" -> ""a new colliding example""
- page 3, ""taking arbitrary an convex combinations"" -> ""combination""
- page 3, ""Given a target x"", I think you mean ""Given a target t""
- page 5, ""As many gradient-based method"" -> ""methods""
- page 8, ""carton""? ""rubber""? Those are not in figure 7(b).
- page 10, ""are crucial to less non-robust"" ? This sentence (which is the final sentence of the conclusion and thus has a certain level of importance) is not something that is novel to your paper. The impact of non-linearities on adversarial examples have been well-studied.","The sentiment score is -50 because the review is generally critical of the paper, pointing out several weaknesses and questioning the implications and soundness of the work. However, it does acknowledge some strengths and interesting aspects, preventing it from being extremely negative. The politeness score is 20 because while the reviewer maintains a professional tone and uses phrases like 'please let me know' and 'I think,' there are also some blunt criticisms such as 'To be frank, this means the polytope is not that interesting.' The reviewer also offers constructive feedback and asks for clarification on potential misunderstandings, which adds to the politeness.",-50.0,20.0
"The authors explore the possibility of using an end-to-end approach for predicting pharmacological assay outcome using fluorescence microscopy images from the public Cell Painting dataset. In my view, the primary contributions are the following: an interesting and relatively new application (predicting assay outcomes), enriching the CellPainting dataset with drug activity data, and a comparison of several relevant methods and architectures. The technical novelty is weak, and although the authors demonstrate that end-to-end holistic approaches outperform previous segmentation-and-feature-extraction approaches, this result is not surprising and has been previously reported in closely related contexts.


OVERVIEW

The authors evaluate the possibility of using and end-to-end deep learning approach to predict drug activity using only image data as input. The authors repurpose the CellPainting dataset for activity prediction by adding activity data from online ChEMBL databases. If made available as promised, the dataset will be a valuable resource to the community. The authors compare a number of previous approaches and state-of-the-art image classification network architectures to evaluate the use of CNNs instead of more classical image analysis pipelines. The comparison is a strong point of the paper, although some details are lacking. For example, the authors claim that GapNet is the quickest method to train, and while they report the number of hyperparameters and time per epoch, the number of epochs trained is never mentioned. 

The authors propose an architecture (GapNet) for the assay prediction task. While the way Global Average Pooling is used to extract features at different stages in the network might be new, it is a straightforward combination of GAP and skip connections. Little insight into why this approach is more efficient or evidence for its effectiveness is provided. Similarly, more explanation for why dilated convolutions and SELU activations would be appreciated. A comparison between GapNet and the same network without the GAP connections could possibly provide a more interesting comparison and might also provide a more pervasive argument as to why GapNet’s should be used. Ultimately, the benefit of using GapNet over the other architectures is not strongly motivated, as training time is less of a concern in this application than predictive power.


RELATED WORK

The authors present previous work in a clear and comprehensive manner. However, the reported finding that “CNNs operating on full images containing hundreds of cells can perform significantly better at assay prediction than networks operating on a single-cell level” is not surprising, and partial evidence of this can be found in the literature. In [1], it was shown that penultimate feature activations from pre-trained CNNs applied to whole-image fluorescence microscopy data (MOA prediction) outperform the baseline segmentation-then-feature extraction method (FNN). Similarly, in [2] (the paper proposing MIL-Net), it is shown that end-to-end whole-image CNN learning for protein localization outperforms the baseline (FNN). In [3] whole image end-to-end learning outperforms whole image extracted features for a phenotyping task. All of these references use fluorescence microscopy data similar to the dataset in this work.

[1] Pawlowski, Nick, et al. ""Automating morphological profiling with generic deep convolutional networks."" bioRxiv (2016): 085118.
[2] Kraus, Oren Z., Jimmy Lei Ba, and Brendan J. Frey. ""Classifying and segmenting microscopy images with deep multiple instance learning."" Bioinformatics 32.12 (2016): i52-i59
[3] Godinez, William J., et al. ""A multi-scale convolutional neural network for phenotyping high-content cellular images."" Bioinformatics 33.13 (2017): 2010-2019.


APPROACH

The authors compile enrich the CellPaining dataset with activity data from various drug discovery assays. In my view, the creation of this dataset is the strongest and most valuable contribution of the paper. The method used to collect the data is described clearly and the choices made when compiling the dataset, including the thresholds and combinations of activity measures seems like a well founded approach.

The authors then identify a number of approaches that are relevant for the problem at hand, binary prediction of drug activity based on image data. These include previous approaches used for cell images and modern image classification networks.


EXPERIMENTS

The different approaches/networks mentioned above were evaluated on a testset. The results indicate that end-to-end CNN approaches outperform all non-end-to-end with no significant difference between the individual end-to-end CNNs. The results are stated clearly and the presentation of different metrics is a nice addition to properly compare the results. It would however contribute valuable information if the authors stated how the confidence intervals of the F1 score are calculated (are the experiments based on several runs of each network or how is it done).


NOVELTY/IMPACT

+ Creation of a new dataset on a new and interesting problem 
+ Useful comparison of modern networks on the task
- GapNet - lacking technical novelty, insight, and performance is unconvincing
- Demonstrates that end-to-end learning outperforms cell centric approach - was this really surprising or even new information?


OTHER NOTES:
* Figure 3 is never mentioned in the main text
* Figure 3 (*’s) are confusing. Do they represent outliers? Statistical significance tests?
* Figure 5 which panel is which?
* Be clear what you mean when you refer to “upper layers” of a network
* An important point not mentioned: in practice, many assays use stains that are closely tied to the readout, unlike the dataset here which provides only landmark stains. The results found here do not necessarily apply in other cases.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'interesting application', 'valuable resource', 'strong point of the paper'), they also point out several weaknesses (e.g., 'technical novelty is weak', 'little insight', 'benefit... is not strongly motivated'). The overall tone suggests that the reviewer sees more areas for improvement than strengths in the paper. The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'in my view' and 'it would contribute valuable information if' rather than making blunt criticisms. The reviewer also balances critiques with positive observations and provides constructive suggestions for improvement, which contributes to the polite tone.",-20.0,50.0
"The idea proposed in this paper is to improve classification accuracy by making use of the context.
E.g. on the north pole we will see polar bears but no penguins, on Antartica we have no polar bears but many penguins.
Hence, if we apply our imagenet-like classifier in the wild, we can improve accuracy by taking into account changes in the prior distribution.

The paper proposes a way to rescale the probabilities to do exactly this and reports improved results on modified versions of 
 CIFAR 10 and imagenet with artificial class skew. To achieve this, an additional trick is introduced where the re-scaling is only used when the model is not very certain of its prediction. And additional motivation for this work is that less compute resources are needed if the problem is simplified by utilizing class skew. 

The core idea of the paper is interesting. However, I am not able to understand what exactly is done and I am 100% confident I cannot re-implement it. The authors already improved upon this in our interactions prior to the review deadline. 
An additional issue is that the paper does not have a good baseline. 
I would not like to dismiss the approach based on its simplicity. An elegant solution is always preferred. However, all the tasks are quite artificial and this limits the ""impact"" of this work. If an ""natural"" application/evaluation where this approach would be possible, it would strengthen the paper greatly. 

For the reasons above I recommend rejection of the manuscript in the current state but I am confident that many of these issues can be resolved easily and if this is done I will update the review.

Missing information
----------------------------
- The original manuscript had a lot of information missing, but much of it has since been provided by the authors.
- In the static class skew experiment, were two passes over the data needed? Or was the Pt(i) pre-set? Would it also be possible to give details about LR, optimizer, LR schedule, batch size, .... for the transfer learning experiments. This would enhance reproducibility. 
- For the imagenet experiments how was Pt(i) set in the if I assume correctly, static setting.

Possible additional baselines:
-----------------------------------------

We could make a simpler rescaling by changing the prior distribution and assuming everything else remains constant.
While this is a simplifying assumption, it is very easy to implement and should take only a couple of minutes to run. 
P(i|x)=1/P(X)*P(X|i)*P(i)
Pt(i|x)=P(i|x)*Pt(i)/P(i)

One could also introduce another baseline where only the most probably classes are considered. Since this approach is clearly sub-optimal since it guarantees some mis-predictions it should serve as a lower bound on the performance that is to be expected. 
","The sentiment score is -30 because while the reviewer acknowledges the core idea as interesting, they recommend rejection due to several issues. The overall tone is critical but not entirely negative, as they express confidence that many issues can be resolved. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledges positive aspects, and offers constructive feedback. They avoid harsh criticism and use phrases like 'I would not like to dismiss the approach' and 'I am confident that many of these issues can be resolved easily', which maintain a polite tone even while pointing out shortcomings.",-30.0,50.0
"This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. There is also an interesting notion of using an embedding based on the action history to aid the agent in avoiding violations. However, I do not believe this paper did a good enough job in situating this work in the context of prior work — in particular (Camacho 2017). There is a significant related work section that does an ok job of describing many other works, but to my knowledge (Camacho 2017) is the most similar to this one (minus the embedding), yet is not mentioned here. It is difficult to find all related work of course, so I would encourage revision with detailed description of the novelty of this work in comparison with that one. I would also encourage an more thoughtful examination of the theoretical ramifications of the reward shaping signal with respect to the optimal policy as (Camacho 2017) do and as is modeled in the (Ng 1999) paper. As of this revision, however, I'm not sure I would recommend it for publication. Additionally, I suggest that the authors describe the reward shaping mechanism a bit more formally, it was unclear whether it fits into Ng's potential function methodology at first pass.

Comments:

+ It would be nice to explain to the reader in intuitive terms what “no-1D-dithering” means near this text. I understand that later on this is explained, but for clarity it would be good to have a short explanation during the first mentioning of this term as well.
+ It would be good to clarify in Figure 1 what . * (lr)^2 is since in the main text near the figure is is just (lr)^2 and the .* is only explained several pages ahead
+ An interesting connection that might be made is that Ng et al.’s reward shaping mechanism, if the shaping function is based on a state-dependent potential then the optimal policy under the new MDP is still optimal for the old MDP. It would be interesting to see how well this holds under this holds under this schema. In fact, this seems like analysis that several other works have done for a very similar problem (see below).
+ I have concerns about the novelty of this method. It seems rather similar to 

Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Decision-making with non-markovian rewards: From LTL to automata-based reward shaping."" In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017.
Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping."" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017.

However, that work proposes a similar framework in a much more formal way. In fact, in that work also a DFA is used as a reward shaping signal -- from what I can tell for the same purpose through a similar mechanism. It is possible, however, that I missed something which contrasts the two works.

Another work that can be referenced:

De Giacomo, Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. ""Reinforcement Learning for LTLf/LDLf Goals."" arXiv preprint arXiv:1807.06333 (2018).

I think it is particularly important to situate this work within the context of those others. 

+ General the structure of the paper was a bit all over the place, crucial details were spread throughout and it took me a couple of passes to put things together. For example, it wasn't quite clear what the reward shaping mechanism was until I saw the -1000 and then had to go back to figure out that basically -1000 is added to the reward if the constraint is violated. I would suggest putting relevant details all in one place. For example, ""Our reward shaping function F(x) was  { -1000, constraint violation, 0 otherwise}"". ","The sentiment score is -50 because the reviewer expresses significant concerns about the paper's novelty and its positioning within existing literature. They state, 'I'm not sure I would recommend it for publication,' which is a clear negative sentiment. However, they also acknowledge some positive aspects like the 'interesting notion' of using embeddings, so it's not entirely negative. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'I would encourage' and 'It would be nice to explain,' which are polite ways of giving feedback. The reviewer also acknowledges the difficulty in finding all related work, showing empathy. While critical, the tone remains professional and courteous throughout.",-50.0,50.0
"This paper presents an end to end rl approach for hierarchical text classification. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy. It is based on capturing the global hierachical structure during training and prediction phases as against most methods which either exploit the local information or neural net approaches which ignore the hierarchical structure. It is demonstrated the method particularly works well compared to sota methods especially for macro-f1 measure which captures the label weighted performance. The approach seems original, and a detailed experimental analysis is carried out on various datasets. 

Some of the concerns that I have regarding this work are :
 - The problem of hierarchical text classification is too specific, and in this regard the impact of the work seems quite limited. 
 - The significance is further limited by the scale of the datasets of considered in this paper. The paper needs to evaluate against on much bigger datasets such as LSHTC datasets http://lshtc.iit.demokritos.gr/. For instance, the dataset available under LSHTC3 is in the raw format, and it would be really competitive to evaluate this method against other such as Flat SVM, and HRSVM[4] on this dataset, and those from the challenge.
- The experimental evaluation seems less convincing such as the results for HRSVM for RCV1 dataset are quite different in this paper, and that given HRSVM paper. It is 81.66/56.56 vs 72.8/38.6 reported in this paper. Given that  81.66/56.56 is not too far from that given by HiLAP, it remains a question if the extra computational complexity, and lack of scalability (?) of the proposed method is really a significant advantage over existing methods.
 - Some of the references related to taxonomy adaptation, such as [3] and reference therein,  which are also based on modifying the given taxonomy for better classification are missing.
 - Comparison with label embedding methods such as [1,2] are missing. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better.
[1] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, Sparse Local Embeddings for Extreme Multi-label Classification, in NIPS, 2015.
[2]  H. Yu, P. Jain, P. Kar, and I. Dhillon, Large-scale Multi-label Learning with Missing Labels, in ICML, 2014.
[3] Learning Taxonomy Adaptation in Large-scale Classification, JMLR 2016.
[4] Recursive regularization for large-scale classification with hierarchical and graphical dependencies, https://dl.acm.org/citation.cfm?id=2487644","The sentiment score is slightly positive (20) because the reviewer acknowledges the originality of the approach and the detailed experimental analysis. However, they also express several concerns, which temper the overall positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, presenting their concerns as suggestions for improvement rather than harsh criticisms. They use phrases like 'Some of the concerns that I have' and 'It remains a question', which maintain a polite tone while expressing critiques. The reviewer also acknowledges the potential of the work and provides specific suggestions for improvement, which contributes to the polite and constructive nature of the review.",20.0,60.0
"I feel like I am missing something about this paper, so rather than a review, this is just mainly a long question making sure I understand things properly.  Ignore the score for now, I'll change once I get a clearer picture of what's happening here.

The network you propose in this paper is motivated by solving PDEs where, as in (1), the actual solution as they are computed numerically depends on the current spatial field of the state, as well as difference operators over this field (e.g., both the gradients and the Laplacian terms).  So, I naturally was assuming that you'd be designing a network that actually represented state as a spatial field, and used these difference operators in computing the next state.  But instead, it seems like you reverted to the notion of ""because difference operators can be expressed as convolutions, we use a convolutional network"", and I don't really see anything specific to PDEs thereafter, just general statements about state-space models.

Am I understanding this correctly?  Why not just actually use the PDE-based terms in the dynamics model of an architecture?  Why bother with a generic ResNet? (And I presume you're using a fully convolutional ResNet here?)  Wouldn't the former work much better, and be a significantly more interesting contribution that just applying a ResNet and a generic U-Net as a state estimator?  I'm not understanding why the current proposed architecture (assuming I understand it correctly) could be seen as ""PDE guided"" in all but the loosest possible sense.  Can you correct me if I'm misunderstanding some element here?","The sentiment score is -50 because the reviewer expresses confusion and skepticism about the paper's approach, suggesting they are 'missing something' and questioning the novelty and appropriateness of the methods used. However, it's not entirely negative as they are open to clarification. The politeness score is 50 because the reviewer frames their critique as questions and personal understanding rather than direct criticism. They use phrases like 'Am I understanding this correctly?' and 'Can you correct me if I'm misunderstanding', which maintain a respectful tone. The reviewer also acknowledges their potential misunderstanding, which adds to the politeness. However, the questioning of the paper's contribution and methods prevents it from being extremely polite.",-50.0,50.0
"## Strength

This paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. 

## Weakness

### Not practical

The authors report that “removing individual training examples did not have a measurable impact on model performance”. However, this seems not to be supported by experiments.
First, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? 
Second, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data?
Third, with `""adv"" metric, we need to perform adversarial-example attacks before training, which has little value in practice. 

### Datasets

They only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. 

## Most confusing typos

1. Section 4, paragraph 5, ""However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples."" Is there a missing ""than""? It's confused.
2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). 
","The sentiment score is -40 because while the review acknowledges some strengths, it focuses more on weaknesses and areas of confusion. The reviewer points out issues with practicality, lack of clarity in experimental details, and limitations in dataset choices. The politeness score is 20 because the language is generally professional and constructive, offering specific recommendations for improvement. However, the reviewer doesn't use overtly polite language or praise, maintaining a more neutral tone. The review provides detailed feedback without being harsh, balancing critique with suggestions for enhancement.",-40.0,20.0
"This paper considers adversarial attack and its defense to DQN. Specifically, the authors propose a poisoning attack that is able to fool DQN, and also propose a modification of DQN that enables the use of strong defense. Experimental results are provided to justify the proposed approach.

Detailed comments:

1.  Although the attack approach seems easy to implement, it would be interesting to see why it works. It might make this paper better if the intuition of the UQP is provided. FGSM is a well-known attack for deep learning models. What is the intuition of using the sign of the gradient of the cross-entropy? Since the argmax is a one-hot vector, this cross-entropy seems ill-defined. How to compute the gradient?

2. It would also be interesting to see why taking actions based on the student network enables better defense.  In DADQN, the authors seem to combine a few tricks proposed by existing works together. It might be better to highlight the contribution and novelty of this approach. ","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contributions and experimental results, but also raises questions and suggests improvements. The review starts with a neutral summary and then provides constructive feedback, indicating a generally positive but cautious stance. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, framing suggestions as 'it would be interesting' and 'it might be better,' which is polite but not overly deferential. The reviewer also balances critique with acknowledgment of the paper's merits, maintaining a professional and courteous tone.",20.0,50.0
"Summarization: This paper studies how to inject structured prior knowledge into the teaching model for machine learning. The authors propose a very general framework called `teach to teach’, in which 1) the knowledge is distilled via subset selection that matches the teacher decision distribution 2) the distilled knowledge is then transferred into the teacher model by reweighting the contributions of teacher objective and coherence constraints. Extensive experiments are conducted on image classification, unsupervised domain adaptation and sequence learning.
 
Questions: 1) Can the teacher models, like those in L2T, be successfully transferred? For example, the teacher model trained with task 1 (with prior knowledge 1) successfully applied to task 2?
2) I’m not that clear about the relationship with knowledge distillation (Hinton et. al 2015). Per my understanding, the authors seem to make the distribution of the knowledge (specified by the set prior function F) coherent with teacher model and let the both influence each other (in section 3.2). In that sense I do not know what is the `dark’ knowledge here.
 
Pros: In general I think this paper is a decent work that the structural prior knowledge is elegantly combined with teaching strategy (a.k.a. the teacher models in curriculum learning). The proposed method is intuitive and natural. The empirical verifications are deep and comprehensive to demonstrate the effectiveness of the `teaching to teach’ framework.
 
Cons: 1) I think the authors should compare with self-paced learning with diversity (SPLD) since you also take diversity as a form of structural knowledge.
2) The writing needs to be significantly polished. First, please simply the writing both in terms of general logic and language. I spent quite a few efforts in figuring out the meaning of some notations and complicated terms such as `curriculum-routed’ and `g_i’. Furthermore, I see no reason of putting so much fancy decorations on an essentially iterative algorithm (the bottom part of page 5 and all page 6). Second,  I suggest the authors give more intuitive and concrete examples towards what is the structural prior knowledge at the earlier phase of the paper, rather than putting most of them into appendix. Last but not least, please use more clear citation formats: currently quite a few citations are missing of publishing venues such as Fan et.al 2018 and Furlanello et.al 2018.","The sentiment score is 50 (slightly positive) because the reviewer describes the paper as a 'decent work' with 'intuitive and natural' methods and 'deep and comprehensive' empirical verifications. However, they also list some cons and areas for improvement, balancing out the positive aspects. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'I think' and 'I suggest' which soften their critiques. The reviewer also acknowledges the paper's strengths before discussing its weaknesses, which is a polite approach to feedback.",50.0,60.0
"In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. They main idea is to benefit from binary maps between the query image and the support set (for the case of few-shot learning for the sake of discussion here) to guide the similarity measure. 

I have quite a few concerns;

- After reading the paper two times, I still couldn't find a clear explanation as how the binary map C is constructed. The paper says the cost of M,i,j,k = 1 is C. So what exactly happens given I_t and I_k. My understanding is that a vector representation of each image is obtained and then from those representations the matrix C is constructed (maybe an outer product or something). This does not come out clearly. 

- Nevertheless, I suspect if such a construction (based on my understanding) is the right approach. Firstly, I guess the algorithm should somehow encourage to match more points between the images. Right now the loss  does not have such a term so hypothetically you can match two images because they just share a red pixel which is obviously not right. 

- Aside from the above (so basically regularizing norm(C) somehow), one wonders why matching a point to several others (as done according to matrix C) is the right choice. 

- Related to the issues mentioned before, I may also complain that matching far away points might not be ideal. Currently I do not see how this can be avoided nor a solid statement as why this should not be a problem.  


- Another comment is how the alignment here differs from attention models? They surely resemble each-other though the alignment seems not that rich.


-  last but not least, I have found the language confusing. Some examples,
   -p2 bandwidth signal than the traditional label-only signal : I am very confused by how bandwidth comes to the picture and how this can be measured/justified

  - fig.1, what are \phi and \psi. paper never discussed these.

  - the authors say M is a tensor with 3dimensions. Then the marginalization before eq.1 uses M_{i,\cdot,\cdot} = 1 . What does this really mean?

    ","The sentiment score is -60 because the reviewer expresses 'quite a few concerns' and lists several major issues with the paper, indicating a generally negative view. However, it's not extremely negative as the reviewer engages deeply with the content and offers constructive criticism. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language such as 'I have quite a few concerns' and 'I may also complain' rather than harsh or rude phrasing. The reviewer also uses 'I' statements to frame their criticisms as personal observations rather than absolute facts. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.",-60.0,20.0
"The paper propose an end-to-end technique that applies both spatial and temporal attention. The spatial attention is done by training a mask-filter, while the temporal-attention use a soft-attention mechanism.  In addition the authors propose several regularization terms  to directly improve attention. The evaluated datasets are action recognition datasets, such as HMDB51, UCF10, Moments in Time, THUMOS’14. The paper reports SOTA on all three datasets. 



Strengths:

The paper is well written: easy to follow, and describe the importance of spatial-temporal attention. 

The model is simple, and propose novel attention regularization terms. 

The authors evaluates on several tasks, and shows good qualitative behavior. 


Weaknesses:

The reported number on UCF101 and HMDB51 are confusing/misleading.  Even with only RGB, the evaluation miss numbers of models like ActionVLAD with 50% on HMDB51 or Res3D with 88% on UCF101. I’ll also add that there are available models nowadays that achieve over 94% accuracy on UCF101, and over 72% on  HMDB51. The paper should at least have better discussion on those years of progress. The mis-information also continues in THUMOS14, for instance R-C3D beats the proposed model. 

In my opinion the paper should include a flow variant. It is a common setup in action recognition, and a good model should take advantage of these features. Especially for spatial-temporal attention, e.g., VideoLSTM paper by Li. 

In general spatial attention over each frame is extremely demanding. The original image features are now multiplied by 49 factor, this is more demanding in terms of memory consumption than the flow features they chose to ignore.  The authors reports on 15-frames datasets for those short videos. But it will be interesting to see if the model is still useable on longer videos, for instance on Charades dataset. 

Can you please explain why you chose a regularized making instead of Soft-attention for spatial attention? 

To conclude: 
The goal of spatial-temporal attention is important, and the proposed approach behaves well. Yet the model is an extension of known techniques for image attention, which are not trivial to apply on long-videos with many frames. Evaluating only on rgb features is not enough for an action recognition model. Importantly, even when considering only rgb models, the paper still missed many popular stronger baselines. 

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper (well-written, novel approach, good qualitative behavior), they also point out significant weaknesses. These include misleading or missing comparisons with existing models, lack of flow variant, and concerns about the model's applicability to longer videos. The overall tone suggests the paper has potential but falls short in several important areas.

The politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They begin by highlighting the paper's strengths before moving on to weaknesses. The language used is respectful and objective, with phrases like 'Can you please explain' and 'In my opinion' showing consideration for the authors. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism. However, the score is not higher because the criticism, while politely phrased, is quite direct and extensive.",-20.0,60.0
"The authors propose a hierarchical model of symbolic music that takes explicit advantage of measures and chords to construct the hierarchy. Their model is very similar to SampleRNN (2-level RNN Autoregressive Model) but with an additional cross-entropy loss for chord labels at the higher level and a summarization connection passing back to the high level from the low-level at the end of each bar. They show that given monophonic music with chord labels their model is able to produce reasonably coherent chords and note samples, and improves the NLL over a low-level model alone. 

The core of their approach (using measures as a natural hierarchy for a multi-level RNN) is a good one, but not new in of itself as it was the basis for the prior work of Roberts et al. (http://proceedings.mlr.press/v80/roberts18a/roberts18a.pdf). The authors highlight in section 3.3 that their work is distinguished by the summarization connection, but do not provide any evidence in their results that the connection is useful. They find in Table 1 that connection hurts NLL on the note level, and do not compare summarized to non-summarized models in the listening tests. 

The area for most improvement in the paper is the evaluation, especially the listening tests. The authors compare samples from four models that generate different types of outputs and were trained on different datasets. Because of this, the notion of user preference is completely convoluted with external factors. In particular the comparisons to DeepBach and SequenceTutor are inappropriate and give little information about the quality of the model architecture itself. To be useful comparisons should be restricted to model architectures that are trained on the exact same data as HAPPIER, and output both chords and melodies like HAPPIER does. Given that the novelty of the paper rests on the summarization connections, and they were not shown to help NLL, it would be natural to try and compare the different model variants in the paper and see if the NLL misses some element of larger structure that listeners may care about. My rating is thus based on the lack of novelty and poor quality of evaluation justifying the actual novel aspects of the paper. 

Some minor comments that could also help improve the paper:

* Including NLL for chords is important to compare summarization (does it help in chord prediction?)
* The input representation could use further clarifying. What is the dictionary of chords to predict from? Are they just chord names or individual notes (the figures imply notes, but that doesn't seem what's happening). In Figure 2, clarify the meaning of tick, what 1, 0 means in terms of time progression.
* Provide quantitative evidence for the claims in 4.2 that the notes and chords belong to the same key. Compare real data and generated data for those statistics. 
* Provide explanation for why Note NLL is higher for Summarization.
* Minor notation problems: Eq 1, f should not be a function of n_i. Similar, in Eq 2, p(n_{ij}) should be a function of c_i. Eq 3 doesn't define what the hat represents. ","The sentiment score is -50 because the reviewer expresses significant criticism of the paper, particularly regarding the lack of novelty and poor quality of evaluation. They state that the core approach is 'not new in of itself' and that the authors don't provide evidence for the usefulness of their main contribution (the summarization connection). The reviewer also points out that the evaluation methods are flawed, especially the listening tests. However, the score is not extremely negative as the reviewer does acknowledge some positive aspects, such as the core approach being 'a good one'.

The politeness score is 20 because while the reviewer is critical, they maintain a professional and constructive tone throughout. They offer specific suggestions for improvement and provide detailed explanations for their criticisms. The language used is not harsh or rude, but rather matter-of-fact and focused on the content of the paper. The reviewer also includes some positive comments, such as acknowledging the good core approach. However, the score is not higher as the review is predominantly critical and doesn't use overtly polite language or praise.",-50.0,20.0
"This paper presents a method for learning the Q function from multi-agent demonstrations, such as trajectories of soccer players in a soccer game.  The basic ideas are:

-- The policy class pi(m|s) maps states s to actions m probabilistically.  In particular, the probability class is a mixture of Gaussians, with the mean/covariance functions and the mixture function all being instantiated via neural nets.  The behavior of the individual players are assumed to be conditionally independent given the current state.

-- A hand-designed reward function is used, such as the ball being in a ""strike zone"" of some kind for the team on offense. 

-- The Q function class is also a neural net.

-- The policy pi is learned through maximum likelihood of the state/action pairs of the the pre-collected demonstrations.  This is essentially probabilistic behavioral cloning over the policy class.

-- The Q function is learned via Q-fitting over the demonstration data, i.e., minimizing squared error on the Bellman residual.  This requires the pre-specified reward function, the policy pi, and some details regarding the state transitions (e.g., the non-deterministic transitions of the ball). 

-- The policy pi and the Q function are trained jointly by adding the two objectives.

-- The main empirical evaluation is comparing the quality of the Q function estimation against a naive baseline (random movements) and a non-mixture policy class (i.e., a single mode, rather than a mixture of Gaussians).  The results against the non-mixture policy class do not seem to show much difference in performance.  

-- Some qualitative evaluations are also presented, but it's unclear what insight one is expected to gain from looking at them.


**Clarity**
The paper is reasonably well written.  Some aspects of the logical flow could be improved, but overall it's fine.  Detailed comments are:

-- The proposed approach is essentially imitation learning and not, strictly speaking, reinforcement learning.  Yet the work is positioned as reinforcement learning.  Perhaps it's just a difference in norms of terminology usage.

--  There are more multi-agent RL & IL works than what was discussed in the related works section.  Of course, an exhaustive discussion is not expected, but the writing makes it sound like there hasn't been much work at all.  Examples include:
https://arxiv.org/abs/1807.09936
https://arxiv.org/abs/1706.02275
http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf

-- The authors comment in the related work that team sports cannot yet be simulated.  This statement is sort of true, although it's not clear how true. For instance, the physics engines of many sports video games are very realistic.  Moreover, it doesn't strike me as the most prominent point of contrast with [Hausknecht & Stone 2016].  For instance, a more prominent point of contrast is simply imitation learning vs reinforcement learning.  

-- It's not clear what the authors mean specifically when they refer to ""average performance"" contrasting with [Le et al. 2017b].  Also, desn't that work also condition on game context and situation? 

-- Section 4.1 is written in a somewhat disorganized way.  The exposition alternates between discussing baselines and the evaluation methodology in a way that is confusing. 

-- One of the baselines is described but then left to the appendix for analysis.  This doesn't make sense from a narrative perspective.


**Originality**
From a technical perspective, it's not clear that there's much novelty in this approach.  All the ingredients are pretty standard, and the ingredients are put together in a standard way. 

From an applications perspective, there might be some novelty here but it's not clear.  The application of imitation learning to sports data is not new, but also not saturated either.  The specific idea of learning a better rating system is interesting, but it's not clear how fleshed out this application is in the paper (more comments on this below).


**Significance**
For me, the key question for significance boils down to: ""Does this paper change the way people think about doing X?"" for the most interesting instantiation of X possible.  In this case, it seems X should be computing a rating system for multi-agent spatial multi-agent behavior (as suggested by the title).  But the limitations in the model and the experiments make this a questionable proposition.  

The model class all but ignores the multi-agent aspect (the agents are all independent conditioned on the states). Moreover there was no comparison against a model class that made a worse assumption on how to handle the multi-agent aspect.  So it's not clear how the multi-agent aspect is significant.

The evaluation of the rating approach is not convincing.  The approach essentially performed as well as a single-mixture baseline.  Furthermore, the direction V(s) learning approach is deferred to the appendix, and I'd like to see it included in the main paper.  Finally, it would be nice if there was some attempt to compare with a non-MDP based approach.  For instance, people directly compute things like P(goal|state).  I'd like to see evidence that this style of approach will outperform those approaches, which are the de facto standard in sports analytics right now.


**Overall Quality**
Based on the above comments, it is my opinion that this paper has too many holes in it to be ready for publication at a venue such as ICLR.  As a primarily applications paper, the burden is on the authors to demonstrate reasonably convincing evidence that this line of approach is valuable to the application.","The sentiment score is -60 because the reviewer expresses several significant concerns and criticisms about the paper, including lack of novelty, insufficient evaluation, and questionable significance. The reviewer concludes that the paper 'has too many holes in it to be ready for publication'. However, the score is not extremely negative as the reviewer does acknowledge some positive aspects, like reasonable writing clarity. The politeness score is 20 because the reviewer uses generally professional and respectful language, avoiding harsh or personal criticisms. They offer constructive feedback and suggestions for improvement. However, the tone is not overly polite or deferential, maintaining a neutral, objective stance throughout most of the review.",-60.0,20.0
"This is a paper with scattered potentially interesting ideas. But the execution is limited and the writing poor with critical details lacking.  A major limitation of the paper is that it is not clear what contribution it makes. Some of the analyses are indeed interesting but 1) these analyses are mostly descriptive and 2) they are limited to one particular (outdated) architecture. How would batch norm or residual connections or any of the developments that have happened since AlexNet affect these results?

As a side note, the references/comparisons between AlexNet and recurrent nets (see abstract, etc) are misleading. This is based on the claim that Bowers et al (2014) qualitatively different results but this is for entirely different domains (words). Indeed what could have made potentially the work more relevant would have been to show some kind of benchmarking between AlexNet and alternative architectures (possibly RNNs). As such the current study does not contribute much except for comparing different semi-arbitrary measures of selectivity for one specific (outdated) network architecture trained on a particular problem (ILSVRC).

****
Minor points:

The study is limited to correctly classified images as stated on page 3. This seems like a major confound in a study aimed at understanding the visual representations learned. It seems to me that the conclusions of the paper could be heavily biased because of this (when computing any measure based on inter and intraclass responses).

In general, this is a relatively poorly written paper which would be hard to reproduce. For instance, the image generation for activating units (assuming it is novel) could be interesting but it is not even described with sufficient details so as to reproduce the results.","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper has 'scattered potentially interesting ideas' but criticizes the execution, writing quality, and lack of clear contribution. They also point out major limitations and misleading comparisons. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite direct and critical without much attempt to soften the blow. Phrases like 'execution is limited', 'writing poor', and 'poorly written paper' contribute to the negative tone. The reviewer does acknowledge some potentially interesting aspects, which prevents the scores from being even lower.",-70.0,-20.0
"This work provides theoretical insights on recent learning rate proposals such as Cyclical Learning Rates (Smith et al.). The authors focus on stochastic approximation i.e. how large is the SGD loss as a function of condition number and horizon. The critical contribution is the theoretical benefit of oscillating learning rates over more traditional learning rate schemes. Authors provide novel upper/lower bounds to establish benefit of oscillating LR, support their theory with experiments and provide insights on finite horizon learning rate selection. An important drawback is that results only apply to linear regression which is a fairly simple setup.

I have two important comments regarding this work:
1) I believe proof of Theorem 3 has a bug. In the proof, authors use the inequality
(1-gamma_t lambda^k)^2 < exp(-2lambda^k gamma_t).
Obviously this can only be correct for gamma_t lambda^k<1. However, checking the setup of the problem, it can be seen that for largest eigenvalue and gamma_0, ignoring log factors:

gamma_0L = L/(mu T_e)=kappa / T_e=kappa/T.

Since, no restriction is imposed on T, gamma_0L can be as large as O(kappa) and invalidates the above inequality. So T should be T>O(kappa). I am not sure if this affects the overall statement or the remaining argument.

2) The paper can benefit from more detailed experiments (e.g. Figs 1 and 2). Arguably the most obvious baseline is ""constant learning rate"". However, authors compare to 1/T or 1/sqrt(T) learning rates. It is not at all clear from current experiments, if the proposed approach beats a good constant LR choice.

I am happy to increase my score if the comments above are addressed.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions and theoretical insights, they also point out significant drawbacks and issues. The reviewer mentions an 'important drawback' regarding the limited applicability of the results, and identifies a potential bug in one of the proofs. They also suggest that more detailed experiments are needed. The reviewer's statement 'I am happy to increase my score if the comments above are addressed' implies a current negative leaning.

The politeness score is moderately positive (60) because the reviewer uses respectful and professional language throughout. They acknowledge the paper's contributions before presenting criticisms, and frame their comments as suggestions for improvement rather than harsh criticisms. The use of phrases like 'I believe' and 'The paper can benefit from' softens the critique. The final statement about being willing to increase the score is particularly polite, showing openness to revision.",-20.0,60.0
"This paper analyses the learning dynamics of GANs by formulating the problem as a primal-dual optimisation problem. This formulation assumes a limited class of models -- Wasserstein GANs with discriminators using linear combinations of base functions. Although this setting is limited, it advanced our understanding of a central problem related to GANs, and provides intuition for more general cases. The paper further shows the same analysis can be applied to multi-task learning and distributed learning.

Pros:

* The paper is well written and well motivated
* The theoretical analysis is solid and provide intuition for more complex problems

Cons:

* The primal-dual formulation assumes Wasserstein GANs using linear discriminator. This simplification is understandable, but it would be helpful to at least comment on more general cases.

* Experiments are limited: only results from GANs with LQG setting were presented. Since the assumption of linear discriminator (in basis) is already strong, it would be helpful to show the experimental results from this more general setting.

* The results on multi-task learning were interesting, but the advantage of optimising the mixing weights was unclear compared with the even mixture baseline. This weakens the analysis of the learning dynamics, since learning the mixing did not seem to be important.

It would also be helpful to comment on recently proposed stabilising methods. For example, would spectral normalisation bring learning dynamics closer to the assumed model?","The sentiment score is 50 (slightly positive) because the review begins with acknowledging the paper's contribution to understanding GANs and its solid theoretical analysis. However, it also lists several cons and areas for improvement, balancing the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The cons are framed as suggestions for improvement rather than harsh criticisms. Phrases like 'it would be helpful' and 'it would also be helpful' are used to soften the critique, maintaining a constructive and polite tone.",50.0,80.0
"This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense.

Good things:
- It is a very elegant method. It is also very simple (in a good way).
- The paper is really well written.
- The experiments are carefully conducted and are indeed showing what the authors describe.
- I think the method is potentially of practical use.

Problems:
- I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though.
- The font is too small in many figures. It is impossible to read it. 
- I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network.
- Only makes sense to apply this to fully connected networks.","The sentiment score is 70 (positive) because the reviewer starts by stating the paper is of 'high quality and clarity' and lists several 'Good things' about the paper, including its elegance, well-written nature, and careful experiments. The few 'Problems' mentioned are relatively minor and constructive. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions or questions rather than harsh judgments. The reviewer also balances positive and negative feedback, showing consideration for the authors' work while providing helpful input.",70.0,80.0
"This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. 

In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn’t tested in robots. However, to my understanding the REINFORCE baseline isn’t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. 

Moreover, I think the description of the experiments doesn’t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn’t say what they were. 
Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix).

Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.’s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X )


Comments/Thoughts:

+ I think in the introduction there are some statements that probably need citations. For example, “But the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? 
+ “Yes, 54 environments but no real-world physical robots” —> this and the intro seems like a blogpost at times. That can be fine (some would argue it’s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation.
+ “Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> citation/backing? it might be nice to point to the experiment section here to back it (e.g., ""As will be shown in Section X and in \citet{something}, REINFORCE can be quite sample inefficient"")
+ “In practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to?
+ “regress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section” —> regress to \sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier
+ What is the actual loss function used for the baseline? Is it the same as Pathak et al.?
+ What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? 
+ Was a variance-reducing baseline used in REINFORCE?
+ What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation?
+ “Hence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. “ —> what were the learning rates?


Linguistic/Typos:

Also, some minor, but frequent, grammatical issues/typos that I’ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I’ve tried to point out below. 

+ “This leads to a significantly sample efficient exploration policy. “ —> significantly more (?) sample efficient ?

“Why is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent” —> by the agent?

“Forward model fθF is trained to minimize its loss which amounts to minimizing rti with respect to θF” —> the forward model

“However, policy is optimized to maximize the objective” —> However, the policy

“We can also optimize  for policy parameters θP via differentiable loss function” —> We can also optimize for (the) policy parameters \theta via (a) differentiable loss function?

“To optimize policy to maximize a discounted sum “ —> To optimize the policy

“How good is Forward Prediction Model” —> How good is the forward prediction model

There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. 
","The sentiment score is slightly positive (20) because the reviewer describes the paper as 'interesting' and 'somewhat novel', but also expresses several concerns and recommends revisions. The overall tone suggests the reviewer sees potential in the work but feels it needs improvement. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and frames suggestions politely (e.g., 'I think', 'I suggest'). The reviewer also balances critique with positive comments. However, it's not extremely high as the review is quite direct in pointing out flaws and doesn't use overly deferential language.",20.0,60.0
"i take reviewing very seriously, and it often takes hours per paper. this paper, however, has many typos, grammatical errors, and seems to have been submitted last minute.  therefore, i have read the paper quickly.
that said, i do not understand the results.
clearly, many discretization methods have previously been described, as alluded to by citing the taxonomy paper on the subject.  the authors state they have developed a better approach.  however, i do not see a comparison to the state of the art in the simulations, and i do not follow the results of Table 2, which columns correspond to which particular algorithms? in either case, the proposed approach does not seem to improve the empirical results, nor have theoretical guarantees, so i am not particularly impressed with the results either.","The sentiment score is -70 because the reviewer expresses strong dissatisfaction with the paper's quality and results. They mention numerous errors, lack of understanding of the results, and unimpressive findings. The politeness score is -30 because while the reviewer doesn't use explicitly rude language, their tone is quite dismissive and critical. They mention reading the paper 'quickly' due to its poor quality, which could be seen as disrespectful. The reviewer also uses phrases like 'i do not understand' and 'i am not particularly impressed,' which, while not overtly impolite, convey a negative and somewhat condescending attitude towards the authors' work.",-70.0,-30.0
"This paper studies the problem of robust policy optimization, motivated by the fact that policies that work in simulations do not transfer well to real world. The authors propose to use the diversity measure on the roll out trajectories to select a diverse set of simulator configurations and train policies that can work well for all of those selected configurations. 

Overall, I think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough. 

Pros:
- The paper is easy to follow.
- The idea of using a diverse summary to do robust policy optimization is interesting. 
- The diversity measure on the trajectories instead of the space of configuration parameters also intuitively makes sense since it takes into account that the similarity between two configuration parameters does not typically mean the similarity between their corresponding policies. 

Cons:
- The setting of this paper seems to only work for the fully observable case with state space being in R^d, deterministic dynamics and deterministic policy (otherwise the diversity measure would be stochastic?). It would be good to clarify these in Sec. 2. 
- For the example in Fig 2 and the first experiment, what I don't understand is why the initial state is not part of the policy.
- It is not clear if the reason that EP-OPT performed worse than the proposed approach is only because there are not enough rollouts for EP-OPT. This could be an unfair comparison.
- It would be good to show the comparison to EP-OPT for the second experiment as well. 
- Two experiments may not be enough to verify valid performance since there could be a lot of randomness in the results.
- In page 6, it would be good to clarify that the summary being optimal is only with respect to f(M_s), but not the original problem of finding optimal policy. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting idea', 'easy to follow'), they express significant concerns about the paper's clarity, experimental design, and overall convincingness. The cons outweigh the pros in the review.

The politeness score is moderately positive (60) because the reviewer uses respectful and constructive language throughout. They begin with positive points, use phrases like 'I think' and 'it would be good to', and frame criticisms as suggestions for improvement rather than harsh judgments. The tone is professional and helpful, even when pointing out weaknesses.

The reasoning behind these scores is based on the balance of positive and negative comments, the specific language used, and the overall tone of the review. The reviewer maintains a polite and professional demeanor while still clearly communicating their concerns about the paper's limitations.",-20.0,60.0
"
> Even though the paper details the underlying Markovian setup in Section 2, it is unclear to the reader how this knits with the FFNN architecture, for example what are the Markovian functions at hidden layer and output layer. Are they all conditional probabilities? How do you prove that this is what occurs within each node?

> Why is the functional form of f_\theta in Eq 1? 

> How many hidden layers are in place?

> What is the Stochastic dynamical process in Figure A and how is this tethered to DyMon? 

> The authors mention an nth-order Markovian process implemention but is this not the case with any fully connected neural network implementation? What the reader fails to see is why DyMoN is different to these already-existing architectures.

> In the teapot example, the authors mention a DyMoN architecture. (Page 8). Is this what is used throughout for all the experiments? If yes, why is it generalizable and if not, what is DyMoN’s architecture? You could open the DyMoN box in Figure 10 (1) and explain what DyMoN consists.
 

Section 2 is the crux of the paper and needs more work - explain the math in conjunction to the ‘deep’ architecture, what is the 'deep' architecture and why it is needed at all. Then go on to show/prove that the Markovian processes are indeed being realized. 

","The sentiment score is -50 because the review is generally critical and points out several unclear aspects and shortcomings of the paper, without offering much positive feedback. However, it's not entirely negative as it provides constructive criticism and suggestions for improvement. The politeness score is 0 (neutral) because the language used is direct and professional, without being overly polite or rude. The reviewer asks straightforward questions and points out issues without using harsh language, but also doesn't use particularly courteous phrasing. The review focuses on technical content rather than personal comments, maintaining a neutral tone throughout.",-50.0,0.0
"The paper proposes a data augmentation technique where the input image is sub-sampled by randomly sampling rows and columns without replacement, which the authors call ‘pseudosaccades’. Rather than multiple classifiers, the authors ensemble using multiple ‘pseudosaccades’ as input, with the same network.

Comments:
I think that the proposed augmentation is a neat trick. However, the inner-workings of the method are poorly presented (or not well understood). For eg. In section 3.5, while discussing the effects of the method on individual classes, the authors mention ‘different architectures do tend to be affected by the pseudosaccades differently’ and provide no further insights.

There are no experiments that compare this method with other standard data augmentation techniques. For instance, one could use a similar ensembling technique for transformations like shear, translation, rotation, etc. by randomly sampling their corresponding parameters. I would be interested in experimental results that compare the proposed ensemble with ensembles constructed using these common techniques.

Since there is no reason for this technique to be used in isolation (I found no such motivation in the paper), it would be insightful to have experimental results where this technique is combined with the aforementioned standard augmentation techniques. Will this method’s impact on the accuracy change with these other augmentations? (Ablation studies would be useful).

This is a a form of regularization and can be thought of reverse structured dropout. Also have the authors compared this with Cutout [1, 2]? Similar experiments and comparisons would be insightful.

[1] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
[2] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017.

In summary:
The performance improvements are incremental. The paper lacks sufficient technical contribution. Further, it does not provide comparisons with standard techniques and similar augmentation methods to demonstrate the usefulness of the method.  ","The sentiment score is -50 because while the reviewer acknowledges the proposed technique as a 'neat trick', they express several criticisms and concerns. They point out that the inner workings are poorly presented, there's a lack of comparison with standard techniques, and insufficient technical contribution. The overall tone suggests that the paper needs significant improvements. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I think', 'I would be interested', and 'it would be insightful', which soften the criticism. They also offer specific suggestions for improvement, which is a polite way to provide feedback.",-50.0,50.0
"The paper proposes yet another variant of the celebrated Dropout algorithm. Specifically, the proposed method attempts to address the obvious drawbacks of Dropout: (i) the need to heuristically select the Dropout rate; and (ii) the universality of this selection across a layer. 

As the authors have admitted in the paper (Sec. 1.2), there is a variety of methods already addressing the same problem. They argue that contrary to some of these methods ""jumpout does not rely on additional trained models: it adjusts the dropout rate solely based on the ReLU activation patterns. Moreover, jumpout introduces negligible computation and memory overhead relative to the original dropout methods, and can be easily incorporated into existing model architectures.""

However, this is argument is certainly untrue and rather misleading. The works of Kingma et al. (2015) and Molchanov et al. (2017), that the authors cite, does not introduce additional trained models. In addition, there is additional related work that the authors do not cite, but ought to: 

[1] Yarin Gal, Jiri Hron, Alex Kendall, ""Concrete Dropout,"" Proc. NIPS 2017.
[2] Yingzhen Li, Yarin Gal, ""Dropout Inference in Bayesian Neural Networks with Alpha-divergences,"" Proc ICML 2017.
[3] Harris Partaourides, Sotirios Chatzis, “Deep Network Regularization via Bayesian Inference of Synaptic Connectivity,” J. Kim et al. (Eds.): PAKDD 2017, Part I, LNAI 10234, pp. 30–41, 2017. 

These methods also address a similar problem, without introducing extra networks or imposing extra costs art inference time. Thus, citing them, as well as COMPARING to them, is a necessity for this paper to be convincing.

These crucial shortcoming aside, there are various theoretical claims in this paper that are not sufficiently substantiated. To begin with, the arguments used in the last paragraph of page 4 seem at least speculative; then,  the authors proceed to propose a solution to the alleged problem in the beginning of page 5. They suggest sampling from a truncated Gaussian, but they do not elaborate on why this selection solves the problem; they limit themselves to noting that other selections, such as the Beta distribution, may also be considered in the future. We must also underline that [3] have suggested exactly that; sampling from a Beta. 

Finally, the last two modifications the authors propose seem reasonable, yet they are extremely heuristic. No one knows (which can be guaranteed through theoretical proofs or solid experimental evidence) that without these the method would not work. In addition, previous papers, e.g. [1-3] achieve similar goals in a principled fashion (ie by inferring proper posterior densities); without experimental comparisons, nobody knows which paradigm is best to adopt. 

","The sentiment score is -60 because the review is predominantly negative. The reviewer points out several critical issues with the paper, including misleading arguments, lack of proper citations, insufficient substantiation of theoretical claims, and heuristic modifications without proper justification. The reviewer also suggests that the paper's main argument is 'untrue and rather misleading'. However, it's not entirely negative as the reviewer acknowledges some aspects as 'reasonable', preventing it from being at the extreme negative end.

The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism and dismissive language. Phrases like 'certainly untrue and rather misleading', 'crucial shortcoming', and 'extremely heuristic' come across as somewhat harsh. The reviewer also uses phrases like 'nobody knows' and 'No one knows', which could be seen as slightly confrontational. However, the review isn't overtly rude, maintaining some level of academic decorum, which is why it's not scored lower on the politeness scale.",-60.0,-20.0
"The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method.

There are several aspects of this work that I admire.

- The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed.
- The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions.
- The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier.
- The source code is already included in the submission.

My only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. 

Overall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community.
------
Rev. In light of the rebuttal and the following discussions I have updated my rating to 7.","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer expresses admiration for several aspects of the work, including the evaluation strategy, organization, depth of discussion, and ease of implementation. They use phrases like 'I admire,' 'high quality,' and 'novel combination.' The only concern mentioned is minor and presented constructively. The final paragraph strongly endorses the paper, calling it 'technically sound' with 'strong and insightful' evidence. The politeness score is 80 because the reviewer uses respectful and professional language throughout. They offer praise generously and frame their single concern diplomatically. The tone is consistently courteous, using phrases like 'I think it would make the work more valuable if...' rather than making demands. The review concludes on a very positive note, suggesting quick adoption by the community, which further reinforces the polite and encouraging tone.",90.0,80.0
"This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It’s difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. 

It appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper.

1.    Typo in the training objective (Eq .1):  the second (or the first) ""sup"" should be removed? 

2.    Section 2.3 is very confusing. Particularly, how is the parameter \phi introduced? What’s the detailed update of \phi? 
- ""We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p."" If v_\phi(x) is a distribution, what’s the parametric form of v?  
- ""Generally, this is achieved by structuring the energy function V_\phi:=\log v_\phi."" It seems that V_\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\phi(x), however, why the distribution is self-normalized? Specifically, why \int \exp(V_\phi) dx = 1? Otherwise, how the authors deal with the partition function \int \exp(V_\phi(x)).  
- It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs?

3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode.

If the authors can fully address the concerns above, I will consider changing the scores.  

Other comments:
1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. 

2. Duplicate citations – R2P2 [35] [36]

3. Other related papers:
 - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17
- Tu et al., Learning Approximate Inference Networks for Structured Prediction, ICLR 18","The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'combines a number of ideas', 'address this problem'), they express significant concerns and confusion about key aspects of the work. The reviewer states that 'technical details are not clear', 'Section 2.3 is very confusing', and 'empirical results are fairly weak'. They also request major clarifications and additions, indicating that the paper needs substantial improvement. The politeness score is 20 because the reviewer uses generally respectful language and offers constructive feedback. They use phrases like 'The authors may consider' and 'The paper would benefit from', which are polite ways of suggesting improvements. However, the politeness is not extremely high as the criticism is quite direct in places.",-50.0,20.0
"The authors argue that graph neural networks based on the message passing frameworks are not able to infer the topological structure of graphs. Therefore, they propose to use the node embedding features from DeepWalk as (additional) input for the graph convolution. Moreover, a graph pooling operator is proposed, which clusters node pairs in a greedy fashion based on the l2-distances between feature vectors. The proposed methods are evaluated on seven common benchmark datasets and achieve better or comparable results to existing methods. Moreover, the method is evaluated using synthetic toy examples, showing that the proposed extensions help to infer topological structures.

A main point of criticism is that the authors claim that graph convolution is not able to infer the topological structure of a graph when no labels are present. In fact the graph convolution operator is closely related to the Weisfeiler-Lehman heuristic for graph isomorphism testing and can distinguish most graphs in many practical application. Therefore, it is not clear why DeepWalk features would increase the expressive power of graph convolution. It should be stated clearly which structural properties can be distinguished using DeepWalk features, but no with mere graph convolution.
The example on page 4 provides only a weak motivation for the approach: The nodes v_1 and v_2 should be indistinguishable since they are generated using the same generator. Thus, the problem is the mean/max pooling, and not the graph convolution. When using the sum-aggregation and global add pooling, graphs with two clusters and graphs with three clusters are distinguishable again. Further insights how DeepWalk helps to learn more ""meaningful"" topological features are required to justify its use.

Clustering nodes that are close in feature space for pooling is a reasonable idea. However, this contradicts the intuition of clustering neighboring nodes in the graph. A short discussion of this phenomenon would strengthen the paper in my opinion.

There are several other questions that not been answered adequately in the article.

* The 10-fold cross validation is usually performed using an additional validation set. What kind of stopping criteria has bee use? * It would be helpful to provide standard deviations on these small datasets (although a lot of papers sadly dismiss them).
* I really like the use of synthetic data to show superior expressive power, but I am unsure whether this can be contributed to DeepWalk or the use of the proposed pooling operator (or both). Please divide the results for these toy experiments in ""GEO-DEEP"" and ""GEO-deep no pooling"". As far as I understand, node features in different clusters should be indistinguishable from each other (even when using DeepWalk), so I contribute this success to the proposed pooling operator.
* A visualization of the graphs obtained by the proposed pooling operator would be helpful. How do the coarsened graphs look like? Given that any nodes can end up in the same cluster, and the neighborhood is defined to be the union of the neighboring nodes of the node pairs, I guess coarsened graphs are quite dense.
* DiffPool (NIPS 2018, Ying et al.) learns assignment matrices based on a simple GCN model (and thus infers topological structure from message passing). How is the proposed pooling approach related to DiffPool (except that its non-differentiable)? How does it perform when using only the features generated by a GCN? How does it compare to other pooling approaches commonly used, e.g., Graclus? At the moment, it is quite hard to judge the benefits of the proposed pooling operator in comparison to others.


In summary, the paper presents promising experimental results, but lacks a theoretical justification or convincing intuition for the proposed approach. Therefore, at this point I cannot recommend its acceptance.


Minor remarks:

* p2: The definition of ""neighbour set"" is needless in its current form.
* p2: The discussion of graph kernels neglects the fact that many graph kernels compute feature vectors that can be used with linear SVMs.

-----------
Update:
The comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly.","The sentiment score is -50 because while the reviewer acknowledges some promising experimental results, they ultimately cannot recommend acceptance due to lack of theoretical justification and convincing intuition. The review starts positively but becomes increasingly critical, ending with a negative recommendation. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism and specific suggestions for improvement. They avoid harsh or personal criticisms, instead focusing on the paper's content. The reviewer also acknowledges a misunderstanding and updates their view in response to author comments, showing flexibility and fairness.",-50.0,50.0
"The paper aims at a better understanding of the positive impacts of Batch Normalisation (BN) on network generalisation (mainly) and  convergence of learning. First, the authors propose a novel interpretation of the BN re-parametrisation. They show that an affine transform of the variables with their local variance (scale) and mean (shift) can be interpreted as a decomposition of the gradient of the objective function into a regressor assuming that the gradient is parallel to the variables (up to a shift) and the residual part which is the gradient w.r.t. to the new variables. In the second part of the paper, authors review various normalisation proposals (differing mainly in the subset of variables over which the normalisation statistics is computed) as well as the known empirical findings about the dependence of BN on the batch size. The paper presents an experiment that combines two normalisation variants. A further experiment strives at regularising BN for small batch sizes.

Unfortunately, it remains unclear what questions precisely the authors answer in the second part of the paper and, what is more important, how they are related to the novel interpretation of BN presented in the first part. This interpretation holds for any function and can be possibly seen as a gradient pre-conditioning. However, the authors do not ""extend"" it towards the gradients w.r.t. the network parameters and do not consider the specifics of the learning objectives (a sum of functions, each one depending on one training example only). The main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network. The first one normalises using the statistics over channel and spatial dimensions, whereas the second one uses the statics over the batch and spatial dimensions. The improvements are rather marginal, but, what is more important, the authors do not explain how and why this proposal follows from their new interpretation of BN.

Overall, in my view, this paper is premature and not appropriate for publishing at ICLR in its present form.
","The sentiment score is -70 because the reviewer expresses significant concerns about the paper, stating it is 'premature and not appropriate for publishing at ICLR in its present form.' They point out unclear connections between different parts of the paper and marginal improvements in experiments. The politeness score is 20 because while the reviewer is critical, they use professional and respectful language throughout. They acknowledge the authors' efforts and explain their concerns without using harsh or rude language. The reviewer uses phrases like 'Unfortunately' and 'in my view' to soften their criticism, maintaining a polite tone despite the overall negative assessment.",-70.0,20.0
"The paper casts the problems of value learning and policy optimization, which can be problematic the non-linear setting, into the bilevel optimization framework. It proposes two novel algorithms with convergence guarantees. Although other works with similar guarantees exist, these algorithms are very appealing for their simplicity. A limited empirical evaluation is provided for the value-based method in Acrobot and Mountain Car and in the Atari games Pong and Breakout for the proposed bilevel Actor Critic.

There are a few missing references to similar, recent work, including Dai et al’s saddle-point algorithm (https://arxiv.org/pdf/1712.10285.pdf). Also, the claim that this is “the first attempt to study the convergence of online reinforcement learning algorithms with nonlinear function approximation” can’t be true (even replacing ‘attempt’ by ‘successfully’, there is e.g. Maei et al.’s nonlinear GTD paper, see below).

Although certainly interesting, the claims relating bilevel optimization and the target network are not completely right. E.g. Equation 3.6 as given is a hard constraint on omega. More explicitly, there are no guarantees that either network is the minimizer of the RHS quantity in 3.6.

The two-timescale algorithm is closer in spirit to the use of a target network, but in DQN and variants the target network is periodically reset, as opposed to what the presented theory would suggest. A different breed of “soft target” networks, which is more closely related to bilevel optimization has been used to stabilize training in DDPG (https://arxiv.org/abs/1509.02971).

There was some confusion for me on the first pass that you define two algorithms called ‘online Q-learning’ and ‘actor-critic’. Neither algorithm is actually that, and they should be renamed accordingly (perhaps ‘bilevel Q-Learning’ and ‘bilevel actor-critic’?). In particular, standard Q-Learning is online; and the actor-critic method does not minimize the Bellman residual (i.e. I believe the RHS of 3.8 is novel within policy-gradient methods).

Once we’re operating on a bounded space with continuous operators, Theorem 4.2 is not altogether surprising – a case of Brouwer’s fixed point theorem, short of the result that theta* = omega*, which is explained in the few lines below the theorem. While I do think Theorem 4.2 is important, it would be good to contrast it to existing results from the GTD family of approaches. Also, requiring that |Q_theta(s,a)| <= Qmax is a significant issue -- effectively this test fails for most commonly used value-based algorithms.

The empirical evaluation lacks any comparison to baselines and serves for little more than as a sanity check of the developed theory. This is probably the biggest weakness of the paper, and is unfortunate given the claim of relevance to e.g. deep RL.



Questions

Throughout, the assumption of the data being sampled on-policy is made without a clear argument as to why. Would the relaxation of this assumption affect the convergence results?

Can the authors provide an intuitive explanation if/why bilevel optimization is necessary?

Can you contrast your work with Maei et al., “Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation”?


Suggestions

The discussion surrounding the target network should be improved. In particular, claiming that the DQN target network can be viewed “as the parameter of the upper level optimization subproblem” is a stretch from what is actually shown.

The paper was sometimes hard to follow, in part because the claims are not crisply made. I strongly encourage the authors to more clearly relate their results to existing work, and ensure that the names they use match common usage.

I would have liked to know more about bilevel optimization, what it aims to solve, and the tools used to do it. Instead all I found was very standard two time-scale methods, which was a little disappointing – I don’t think these have been found to work particularly well in practice. This is particularly relevant in the context of e.g. the target network question.

A proper empirical comparison to existing algorithms would significantly improve the quality and relevancy of this work. There are tons of open-source baselines out there, in particular good state of the art implementations. Modifying a standard implementation to optimize its target network along the lines of bilevel optimization should be relatively easy.

Revision:
I thank the authors for their detailed feedback, but still think the work isn't quite ready for publication. After reading the other reviews, I will decrease my score from 6 to 5. Some sticking points/suggestions:
- Some of my concerns remain unanswered. E.g. the actor-critic method 3.8 is driven by the Bellman residual, which is not the same as e.g. the MSPBE used with linear function approximation. There is no harm in proposing variations on existing algorithms, and I'm not sure why the authors are reluctant to do. Also, Brouwer's fixed point theorem, unlike Banach's, does not require a contractive mapping.
- The paper over-claims in a number of places. I highly recommend that the authors make their results more concrete by demonstrating the implications of their method on e.g. linear function approximation. This will also help contrast with Dai et al., etc.","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('interesting', 'appealing for their simplicity'), there are numerous criticisms and suggestions for improvement. The overall tone suggests the paper is not ready for publication in its current state. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh judgments. They use phrases like 'I would have liked to know more' and 'I strongly encourage' which maintain a polite tone while still conveying areas for improvement. The reviewer also thanks the authors for their feedback in the revision section, further demonstrating politeness.",-30.0,50.0
"Quality is good, just a handful of typos.
Claritys above average in explaining the problem setting.
Originality: scan refs...
Significance: medium
Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). Further, for inference, the authors propose an explicit procedure. It seems like a noveel approach to demixing which is exciting.
Cons: The experiments do not push the limits of their method. It's difficult to judge the demixing 'power' of the method because it's difficult to tell how hard the problem is. Their method seems to easily solve it (super low MSE). The classification measure is clearly improved by denoising, which is totally unsurprising-- There should definitely be comparison with other denoising methods.

In general, they don't compare to any other methods. Actually in the appendix, comparisons are provided for a basic compressive sensing problem, but their only comparator is ""LASSO"" with a ""fixed regularization parameter"", and vanilla GAN. Since the authors ""main contribution"" (their words) is demixing, I'm surprised that they did not compare with other demixing approaches, or try on a harder problem. Could you  give some more details about the LASSO approach? How did you choose the L1 parameter?

I have another problem with the demixing experimental setting. On one hand, both the sinusoids and MNIST have ""similar characteristics"" in the sense that they are both pretty sparse, basically simple combinations of primary curves. This actually makes the problem harder for a dictionary learning approach like MCA (referenced in your paper). On the other hand, both signals are very simple to reconstruct. For example, what if you superimposed the grid of digits onto a natural image? Would you be able to train the higher resolution GAN to handle a more difficult setting? The other demixing setting of adding 1's and 2's has a similar problem.

The authors need to provide (R)MSE  results that show how well the method can reconstruct mixture components on average over the dataset. The only comparison is visual, and no comparators are provided.

Conclusions:
I'm actually torn on this paper. On one hand this paper seems novel and clearly contributes to the field. On the other hand, HOW MUCH contribution is not addressed experimentally, i.e. the method is not properly compared with other denoising or demixing methods, and definitely not pushed to its limits. It's hard to assess the difficulty of the denoising problem because their method does so well, and it's hard to assess the difficulty of demixing because of the lack of comparators.

Caveats:
I am knowledgeable about iterative optimization approaches to denoising and demixing, especially MCA (morphological component analysis), but *not knowledgeable about GAN-based approaches*, though I have familiarity with GANs.

*********************
Update after author response:
I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. I think this is an exciting contribution to dually learning component manifolds for demixing.","The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty and potential of the approach, calling it 'exciting' and noting that it 'clearly contributes to the field'. However, they express significant concerns about the lack of comparisons and limited experimental validation, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms constructively (e.g., 'Could you give some more details...'), and acknowledges their own limitations ('not knowledgeable about GAN-based approaches'). The reviewer also provides a balanced view, discussing both pros and cons, and ends on a positive note after the author response, indicating a polite and fair approach to the review.",20.0,60.0
"The authors study the task of sample-based quantitative evaluation applied to GANs. The authors suggest multiple modifications to existing evaluation pipelines: (1) Instead of embedding the samples in the InceptionNet feature space, train a domain-specific encoder. If labeled data is available, add a cross-entropy loss to the encoder training objective so that the class can be predicted. (2) Instead of fitting a single Gaussian in the feature space, fit a GMM instead. This should allow for a more fine-grained “class-aware” distance between the (empirical) distributions. 

Pro: 
Attempt to attack a critical issue in generative modeling. Good overview of competing approaches.
Several ablation studies of evaluation measures and the behavior of FID with respect to the representation space.
The ideas make sense on a conceptual level, albeit suffering from major practical concerns.

Con:
- Clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (>3) times (i.e. deficiencies of FID and IS, etc.), Many statements which should be empirically tested are stated as folklore (last paragraph on page 3). In general the paper merits another polishing pass (mode != model, last paragraph in  section 3, “unmatch”, etc.).
- Why would a VAE capture a good feature space? It is known that the tradeoff between what is stored in the latent space versus the discriminator *completely* depends on the power of the discriminator -- if the discriminator is flexible enough it can just learn the marginal distribution and ignore the latent code. Hence, this subtle issue will likely undermine the entire model comparison.
- Using the predictive distribution as a soft label for CAFD. Interesting idea, but why would one have access to labels in the first place? Why wouldn't one use a conditional GAN if we already have labels? Secondly, why would the modes necessarily correspond to classes?
- Stated issues with FID: Why would you expect FID to be resistant to such drastic transformations as blocking out a significant proportion of pixels with “blocks”? This is a *major* change in the underlying distribution. The fact that humans can “fill in” this gap should have nothing to do with the quality of the underlying model. Arguably, you can also hide one eye, the nose and the mouth and still judge the sample as “good”.

The ideas presented in this paper are conceptually interesting. However, given the drawbacks discussed above I cannot recommend the acceptance of this work.
","The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges some positive aspects ('Pro' section), the 'Con' section is more extensive and critical. The reviewer concludes by stating they 'cannot recommend the acceptance of this work,' which is a clear negative sentiment. The politeness score is 20 because the reviewer maintains a professional tone throughout, acknowledging positive aspects and providing specific feedback. However, the criticism is direct and not particularly softened, keeping the score only slightly positive. The reviewer uses phrases like 'Clarity can be improved' and 'The ideas make sense on a conceptual level, albeit suffering from major practical concerns,' which are polite ways of expressing criticism.",-60.0,20.0
"This paper proposes a curriculum that encourages training on easy examples first and postpones training on hard examples. However, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example. The proposed objective is hence biased at the beginning but they dampen it over time to converge to the true objective at the end.

Positives:
- There is not much work considering each example as an individual subtask.
- The observation that an under-fitted classifier can destroy a good feature extractor is good.

Negatives:
- In the intro it says “[update rule of gradient descent] assumes the top layer, F2, to be the right classifier.”. This seems like a fundamental misunderstanding of gradient descent and the chain rule. The term d output/d F1 takes into account the error in F2.
- The caption of figure 2 says the “... they cannot separate from its neighbors…”. If the loss of all examples in a cluster is high, all are being misclassified. A classifier then might have an easy job fixing them if all their labels are the same or have a difficult job if their labels are random. The second scenario is unlikely if based on the claim of this figure, the entropy has decreased during training. In short, the conclusion made in fig 2 does not necessarily hold given that figure.
- This method is supposed to speed up training, not necessarily improve the final generalization performance of the model. The figures show the opposite outcomes. It’s not clear why. The improvement might be due to not tuning the hyperparameters of the baselines.
- Figure 3 does not necessarily support the conclusion. The fluctuations might be caused by any curriculum that forces a fixed ordering across training epochs. Often on MNIST, the ordering of data according to the loss does not change significantly throughout training.","The sentiment score is -40 because while the review acknowledges some positives ('not much work considering each example as an individual subtask', 'good observation'), it lists more negatives and points out several flaws in the paper's methodology and conclusions. The overall tone suggests skepticism about the paper's claims and results. The politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language to express criticisms ('This seems like a fundamental misunderstanding', 'It's not clear why') rather than harsh or dismissive language. The reviewer also acknowledges positive aspects before presenting criticisms, which is a polite approach in academic reviews.",-40.0,20.0
"*Summary:
This paper analyzes the convergence of ADAM and RMSProp to stationary points
in the non convex setting.
In the second part the authors experimantally compare the performance of these methods to Nesterov's Accelerated method.



*Comments:

-The paper does not tell a coherent story and the two parts of the paper are somewhat unrelated.

-The authors claim that they are the first to analyze adaptive methods in the non-convex setting, yet this was recently done in 
[Xiaoyu Li, Francesco Orabona; On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes]
The authors should cite this paper and compare their results to it.

-The above paper of [Li and Orabona] demonstrates a nice benefit of AdaGrad in the non-convex setting. Concretely they show that in the noisless setting adaptive methods give a faster rate of $O(1/T)$ compared to the standard rate of $O(1/\sqrt{T})$ of SGD.

Unfortunately, the results of the current paper do not illustrate the benefit of adaptive methods over SGD, since the authors provide similar rates to SGD or even worse rates in some situations.
I think that in light of [Li and Orabona] one should expect a $O(1/T)$ rate also for ADAM and RMSProp.


-The experimental part is not so related to the first part. And the experimental phenomena is only demonstrated for the MNIST dataset, which is not satisfying. 


*Summary:
The main contribution of this paper is to provide rates for approaching stationary points.
This is done for ADAM and RMSProp, two adaptive training methods.
The authors do not mention a very relevant reference, [Li and Orabona].
Also, the authors do not show if ADAM and RMSProp have any benefit compared to SGD in the non-convex setting, which is a bit disappointing. Especially since [Li and Orabona] do demonstrate the benefit of AdaGrad in their paper.
","The sentiment score is -60 because the review is predominantly negative. The reviewer points out several significant issues with the paper, such as the lack of coherence, missing important citations, and failure to demonstrate benefits of the methods studied. The only positive aspect mentioned is the main contribution of providing rates for approaching stationary points. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without much effort to soften the criticism. Phrases like 'does not tell a coherent story', 'not so related', and 'not satisfying' contribute to a somewhat impolite tone. The reviewer does not use any particularly polite language or acknowledge positive aspects of the work in detail, which could have balanced the criticism.",-60.0,-20.0
"This paper studies ReLU model, or equivalently, one-layer-one-neuron model, for the classification problem. This paper shows if the data is linearly separable, gradient descent may converge to either a global minimum or a sub-optimal local minimum, or diverges. This paper further studies the implicit bias induced by GD and SGD and shows if they converge, they can have a maximum margin solution. 

Comments:
1. Using ReLU model for linearly separable data doesn't make sense to me. When ReLU is used, I expect some more complicated separable condition. 
2. This paper only studies one-layer-one-neuron model, which is a very restricted setting. It's hard to see how this result can be generalized to the multiple-neuron case.
3. The analysis follows closely with previous work in studying the implicit bias for linear models.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions, they express skepticism about the relevance and generalizability of the study. The first paragraph summarizes the paper's content neutrally, but the comments section raises three critical points that question the paper's approach and significance. The politeness score is neutral (0) as the reviewer's language is neither particularly polite nor rude. They present their criticisms directly but professionally, without using overly harsh or complimentary language. The reviewer states their concerns matter-of-factly, using phrases like 'doesn't make sense to me' and 'It's hard to see how', which are neutral in tone.",-20.0,0.0
"The authors provided a training scheme that ensures network retains old performance as new data sets are encountered (e.g. (a) same class no drift, (b) same class with drift, (c) new class added). They do this by incrementally adding FC layers to the network, memory component that stores previous precomputed features, and the objective is a coupling between classification loss on lower level features and a feature-loss on retaining properties of older distributions. The results aren't very compelling and the approach looks like a good engineering solution without strong theoretical support or grounding. ","The sentiment score is slightly negative (-30) because while the reviewer acknowledges the authors' efforts in providing a training scheme, they express that 'the results aren't very compelling' and describe the approach as lacking 'strong theoretical support or grounding'. This indicates a somewhat negative view of the work's significance and rigor. The politeness score is neutral (0) as the language used is neither particularly polite nor rude. The reviewer states their opinions directly without using overly harsh language, but also without any notably courteous phrasing. They present their critique in a straightforward, matter-of-fact manner typical of academic peer reviews.",-30.0,0.0
"Comments: 

The author(s) provide stability and generalization bounds for SGD with momentum for strongly convex, smooth, and Lipschitz losses. 

This paper basically follows and extends the results from (Hardt, Recht, and Singer, 2016). Section 2 is quite identical but without mentioning the overlap from Section 2 in (Hardt et al, 2016). The analysis closely follows the approach from there. 

The proof of Theorem 2 has some issues. The set of assumptions (smooth, Lipschitz and strongly convex) is not valid on the whole set R^d, for example quadratic function. In this case, your Lipschitz constant L would be arbitrarily large and could be damaged your theoretical result. To consider projected step is true, but the proof without projection (and then explaining in the end) should have troubles. 

From the theoretical results, it is not clear that momentum parameter affects positively or negatively. In Theorem 3, what is the advantage of this convergence compared to SGD? It seems that it is not better than SGD. Moreover, if \mu = 0 and \gamma > 0, it seems not able to recover the linear convergence to neighborhood of SGD. Please also notice that, in this situation, L also could be large. 

The topic could be interesting but the contributions are very incremental. At the current state, I do not support the publications of this paper. 
","The sentiment score is -60 because the reviewer expresses several criticisms and does not support publication of the paper. They state the contributions are 'very incremental', point out issues with the proof and assumptions, and question the advantages of the results. The final statement 'I do not support the publications of this paper' is strongly negative. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without much softening. Phrases like 'has some issues', 'should have troubles', and 'it is not clear' come across as somewhat blunt. The reviewer does not use polite hedging language or positive acknowledgments that are common in more tactful reviews.",-60.0,-20.0
"Summary:
The paper presents a method for ""learning an optimizer""(also in the literature Learning to Learn and a form of Meta-Learning) by using a Variational Optimization for the ""outer"" optimizer loss. The mean idea of the paper is to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weight them using a product of Gaussians formula for the mean. The method is simple and clearly presented. The paper also presents issues with the standard ""learning to learn"" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the ""exponential explosion of gradients"" which I think lacks enough justification as currently presented (see below for details). The ideas are clearly stated, although the work is not groundbreaking, but more on combining several ideas into a single one. 

Experiments: 
The authors evaluate their method on a single task which consists of optimizing a 3-layer convolutional neural network on downsampled images from ImageNet. A key idea, not new to this work, is to optimize the meta-optimizer with respect to the validation dataset rather than the training, which seems to be crucial for any meaningful training to happen. Although the experiments do show so promising results, they seem to be somewhat limited (see below for details). There is also a small ablation study on how do different features presented to the optimizer affect its performance. Given the still small-scale experiments, I'm not sure this is a significant result for the community. 

Conclusion:
As a whole, I think the idea in the paper is a good one and worth investigating further. However, the objections I have on section 2.3 and the experiments seem to indicate that there needs to be more work into this paper to make it ready for publication. 


On section 2.3 and the explosion of gradients:

There is a mistake in the equation on page 4 regarding the ""gradient with respect to the learning rate"". Although the derivation in Appendix A is correct, the inner product in the equation starts wrongly from j=0, where it should in fact start at j = i + 1. To be more clear the actual enrolled equation for dw^T/dt for 3 steps back is:

dw^T/dt = (I - tH^{T-1})(I - tH^{T-2})(I - tH^{T-3}) dw^{T-3} - (I - tH^{T-1})(I - tH^{T-2}) g^{T-3} - (I - tH^{T-1}) g^{T-2} - g^{T-1} 

Hence the product must start at j = i + 1. 
It is correct that in this setting the equation is a polynomial of degree T of the Hessian, however, there are several important factors that the authors have not discussed. Namely, if the learning rate is chosen accordingly such that the spectral radius of the Hessian is less than 1/t then rather than the gradient exploding the higher order term will vanish. However, even if they do vanish for large T since the Hessian plays with smaller and smaller power to more recent gradients (after correcting the mistake in the equation) than the actual T-step gradient will never vanish (in fact even if tH = I then dw^T/dt = g^{T-1}). Hence the claims of exploding gradients made in this section coupled with the very limited theoretical analysis seem to unconvincing that this is nessacarily an issue and under what circumstances they are. 

The toy example with l(w) = (w - 4)(w - 3) w^2 is indeed interesting for visualizing a case where the gradient explosion does happen. However, surprisingly here the authors rather than optimizing the learning rate, which they analyzed in the previous part of the section, they are now optimizing the momentum. The observation that at high momentum the training is unstable are not really surprising as there are fundamental reasons why too high momentum leads to instabilities and these have been analyzed in the literature. Additionally, it is not mentioned what learning rate is used, which can also play a major role in the effects observed. 

As a whole, although the example in this section is interesting, the claims made by the authors and some of the conclusions seem to lack any significant justifications, in addition to the fact that usually large over-parameterized models behave differently than small models. 


Experiments:

I have a few key issues with the experimental setup, which I think need to be addressed:

1. The CNN being optimized is quite small - only 3 layers. This allows the authors to train everything on a CPU. The key issue here, as well with previous work on Learning to Learn, is that it is not clear how scalable is this method to very Deep Networks. 

2. Figure 1 - The setup is to optimize the problem for 10000 iterations, however, I think it is pretty clear even to the naked eye that the standard first-order optimizers (Adam/RMS/Mom) have not fully converged on this problem. Hence I think its slightly unfair to compare their ""final performance"" after this fixed period. Additionally using the curriculum the ""meta""-optimizer is trained explicitly for 10000 iterations. Hence, it is also unclear if it retains its stability after letting it run for longer. From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance - I hope this is the latter as that is the only way to fairly compare the two approaches. 

3. Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction). Given however that these are plots for only 10000 iterations it is still unclear if this is scalable to larger problems. 

","Sentiment score: The review starts with a neutral to slightly positive tone, acknowledging the clear presentation and interesting ideas. However, it then points out several significant issues and limitations, particularly in the theoretical analysis and experimental setup. The reviewer concludes that more work is needed before publication, indicating an overall slightly negative sentiment.

Politeness score: The language used is generally professional and respectful. The reviewer acknowledges the positive aspects of the paper and uses phrases like 'I think' to soften criticisms. They provide detailed explanations for their concerns, which is helpful and constructive. However, they do directly point out mistakes and shortcomings, which slightly reduces the politeness score from very polite to moderately polite.",-20.0,60.0
"* The proposed SGLD-SA algorithm, together with its convergence properties, is very interesting. The introduction of step size $w^{k}$ is very similar to the ""convex combination rule"" in (Zhang & Brand 2017) to guarantee convergence.
  
* It seems that this paper only introduced Bayesian inference in the output layers. It would be more interesting to have a complete Bayesian model for the full network including the inner and activation layers.

* This paper imposed spike-and-slab prior on the weight vector which can yield sparse connectivity. Similar ideas have been explored to compress the model size of deep networks (Lobacheva, Chirkova and Vetrov 2017; Louizos, Ullrich and Welling 2017 ). It would make this paper stronger to compare the sparsification and compression properties with the above work.

* In equation (11) there is a summation from $\beta_{p+1}$ to $\beta_{p+u}$. I wonder where this term comes from, as I thought $\beta$ is a vector of dimension $p$.

Reference:
Zhang, Ziming, and Matthew Brand. ""Convergent block coordinate descent for training tikhonov regularized deep neural networks."" Advances in Neural Information Processing Systems. 2017.

Lobacheva, Ekaterina, Nadezhda Chirkova, and Dmitry Vetrov. ""Bayesian Sparsification of Recurrent Neural Networks."" arXiv preprint arXiv:1708.00077 (2017).

Louizos, Christos, Karen Ullrich, and Max Welling. ""Bayesian compression for deep learning."" Advances in Neural Information Processing Systems. 2017.

","The sentiment score is 50 (slightly positive) because the reviewer starts by praising the proposed algorithm as 'very interesting' and notes its similarity to existing work, which is a positive point. However, they also suggest areas for improvement and raise questions, indicating a balanced view. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, such as 'It would be more interesting' and 'It would make this paper stronger,' offering constructive criticism without being harsh. They also acknowledge the paper's strengths before suggesting improvements. The use of phrases like 'I wonder' when asking for clarification further contributes to the polite tone.",50.0,75.0
"The paper discusses clustering sparse sequences using some mixture model. It discusses results about clustering data obtained from a restaurant loyalty program.

It is not clear to me what the research contribution of the paper is. What I see is that some known techniques were used to cluster the loyalty program data and some properties of the experiments conducted noted down. No comparisons are made. I am not sure what to evaluate in this paper. ","The sentiment score is -50 because the reviewer expresses confusion about the paper's research contribution and states that they are 'not sure what to evaluate,' indicating a negative view of the paper's clarity and significance. However, it's not extremely negative as they don't outright reject the paper. The politeness score is 0 (neutral) because the language used is neither particularly polite nor rude. The reviewer states their concerns directly without using overly harsh language or personal attacks, but also without any notably courteous phrasing.",-50.0,0.0
"The paper ""Neural Distribution Learning for generalized time-to-event prediction"" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. 
 
First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. 

Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like ""Modeling the intensity function of point process via recurrent neural networks"" should be considered in the experiments, though they do not explicitely model censoring but  with slight adapations should be able to work well of experimental data. 


","The sentiment score is -30 because the reviewer expresses several concerns and criticisms about the paper, particularly regarding clarity and comparison with existing work. However, it's not entirely negative as the reviewer acknowledges some positive aspects like the convenience of the HazardNet framework. The politeness score is 20 because the reviewer uses relatively neutral language and phrases criticisms as suggestions for improvement rather than harsh judgments. The reviewer uses phrases like 'should be more clear' and 'it is not clear to me' which are polite ways of pointing out issues. The overall tone is professional and constructive, albeit critical.",-30.0,20.0
"This paper while presenting interesting ideas, is very poorly written. It seems as though the authors were in a rush to submit a manuscript and did not even bother with basic typesetting.
Firstly, the paper spends too much time motivating and re-introducing the model of Arora et.al. Note to the authors here, they cite the same paper from Arora et.al for 2017 twice. The first time the model they refer to was introduced by the paper ""RAND-WALK: A latent variable model approach to word embeddings"", this is probably what the authors mean by the 2016 reference?

Now coming to the experiments, the results are presented in a table that is poorly formatted. The section partitions are not clearly delimited, making for a hard read. Even if we overcome that and look at the results, the presented numbers are incredibly confusing. On the STS 13 and 15 data sets, Ethayarajh 2018's numbers are much better at 66.1 and 79.0. Coming to STS14 Ethayarajh attain 78.4 while the proposed method achieves 78.1. If we discount this for the moment, and look at the results on STS12 where the proposed method achieves 71.4, this is the only data set where the proposed method does better than the other baselines.

So almost on 3 of the 4 datasets Ethayarajh 2018 does better. This makes me question what exactly is the proposed model improving?

Coupled with the fact that there is no motivation to explain results or future work, this makes for a very poorly written paper that is very challenging to read.

It is very likely that there is some merit to the proposed methods that introduce non linearity, but these points simply get lost in the mediocre presentation.","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper is 'very poorly written', criticizes the formatting, questions the results, and concludes that it's a 'very poorly written paper that is very challenging to read'. The only positive aspect mentioned is that there might be 'some merit to the proposed methods', but this is overshadowed by the criticisms. The politeness score is -30 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and critical. Phrases like 'did not even bother with basic typesetting' and 'mediocre presentation' are particularly blunt. However, the reviewer does attempt to provide specific feedback and suggestions, which prevents the score from being even lower.",-70.0,-30.0
"This paper extends the definition of adversarial examples to the ones that are “far” from the training data, and provides two conditions that are sufficient to guarantee the non-existence of adversarial examples. The core idea of the paper is using the epistemic uncertainty, that is the mutual information measuring the reduction of the uncertainty given an observation of the data, to detect such faraway data. The authors provided simulation studies to support their arguments.

It is interesting to connect robustness with BNN. Using the mutual information to detect the “faraway” datapoint is also interesting. But I have some concerns about the significance of the paper:
1.  The investigation of this paper seems shallow and vague. 
    (1). Overall, I don’t see the investigation on the “typical” definition of adversarial examples. The focus of the paper is rather on detecting “faraway” data points. The nearby perturbation part is taken care by the concept of “all possible transformations” which is actually vague.
    (2). Theorem 1 is basically repeating the definition of adversarial examples. The conditions in the theorem hardly have practical guidance: while they are sufficient conditions, all transformations etc.. seem far from being necessary conditions, which raises the question of why this theory is useful? Also how practical for the notion of “idealized NN”?
    (3). What about the neighbourhood around the true data manifold? How would the model succeed to generalize to the true data manifold, yet fail to generalize to the neighbourhood of the manifold in the space?  Delta ball is not very relevant to the “typical” definition of adversarial examples, as we have no control on \delta at all.
2. While the simulations support the concepts in section 4, it is quite far from the real data with the “typical” adversarial examples. 

I also find it difficult to follow the exact trend of the paper, maybe due to my lack of background in bayesian models. 
1. In the second paragraph of section 3, how is the Gaussian processes and its relation to BNN contributing to the results of this paper?
2. What is the rigorous definition for \eta in definition 1?
3. What is the role of $\mathcal{T}$, all the transformations $T$ that introduce no ambiguity, in Theorem 1. Why this condition is important/essential here?
4. What is the D in the paragraph right after Definition 4? What is D’ in Theorem 1?
5. Section references need to be fixed. 



","The sentiment score is -30 because while the reviewer acknowledges some interesting aspects of the paper, they express several significant concerns about its depth, clarity, and relevance to typical adversarial examples. The overall tone is more critical than positive. The politeness score is 20 because the reviewer uses respectful language throughout, framing criticisms as 'concerns' and asking questions rather than making blunt statements. They also acknowledge their own potential lack of background in some areas. However, the review is not overly effusive or deferential, maintaining a professional tone.",-30.0,20.0
"This paper provides a theoretical perspective of the dual learning tasks and proposes two generalizations (multipath/cycle dual learning) that utilize multiple language sets. Through experiments, the paper discusses the relationship between theoretical perspective and actual translation quality.

Overall, the paper is well written and discussed enough. My concern is about Theorem 1 that could be a critical problem.
In the proof of Theorem 1, it discussed that the dual learning can minimize Case 2. This assumption is reasonable if the vanilla translator is completely fixed (i.e., no longer updated) but this condition may not be assumed by the authors as far as I looked at Algorithm 2 and 3 that update the parameters of vanilla translators directly. The proof is constructed by only the effect against Case 2. However, if the vanilla translator is directly updated through dual training, there should exist some random changes in also Case 1 and this behavior should also be included in the theorem.

Correction and suggestions writing:
* It is better to introduce an additional $\alpha$, $\beta$ and $\gamma$ for the vanilla translation accuracy (e.g., $\alpha_0 := p_{ij}p_{ji}^r$) so that most formulations in Section 3 can be largely simplified.
* In Justification of Assumption1 ... ""the probability of each cluster is close to $p_max$"" ->  maybe ""greater than $p_max$"" to satisfy the next inequality.
* Eq. (3) ... $T_{ji}^d(T_{ij}^d(x^{(i)})$ -> $T_{ji}^d(x^{(j)})$ to adjust other equations.
* Section 3 Sentence 1: ""translatorby"" -> ""translator by""
* Section 4.2: ${\rm Pr}_{ X^{(3)} \sim T_{23}^d (X^{(1)}) }$ -> $... (X^{(2)}) }$","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is 'well written and discussed enough', they express a 'concern' about Theorem 1, which they describe as a 'critical problem'. This indicates a significant issue that impacts the overall positive aspects of the paper. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, such as 'Overall, the paper is well written' and provides constructive feedback. They offer suggestions for improvement without using harsh or critical language. The use of phrases like 'It is better to' and 'maybe' when suggesting changes contributes to the polite tone. The reviewer also provides specific, helpful corrections, which is a courteous way to assist the authors in improving their work.",-20.0,60.0
"(Since the reviewer was unclear about the OpenReview process, this review was earlier posted as public comment)

Most claims of novelty can be clearly refuted such as the first sentence of the abstract ""...This work presents a new approach to active anomaly detection..."" and the paper does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.
      
      1. Related Works: ""...active anomaly detection remains an under-explored approach to this problem...""
          - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:
            - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, ""Ai2: Training a big data machine to defend,"" International Conference on Big Data Security, 2016.
        
      2. ""To deal with the cold start problem, for the first 10 calls of select_top..."":
          - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)
        
      3. Many arbitrary hyper parameters as compared to simpler techniques:
          - The number of layers, nodes in hidden layers.
            - The number of instances (k) per iteration
            - The number of pretraining iterations
            - The number of times the network is retrained (100) after each labeling call
            - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).
        
      4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.
      
      5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:
          - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (pradeepr/arxiv npmix v.pdf)"" target=""_blank"" rel=""nofollow"">https://www.cs.cmu.edu/ pradeepr/arxiv npmix v.pdf)
          - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)
          - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)
          - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)
          
      6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.
        - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.
        
      7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).
        
      8. Does not compare against current state-of-the-art Tree-based AAD
          - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.
        
      9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.
      
      10. Does not propose any other query strategies than greedily selecting top.
      
      11. Question: Does this support streaming?
","The sentiment score is -80 because the review is highly critical, pointing out numerous flaws and lack of novelty in the paper. The reviewer refutes claims of originality, criticizes the literature review, and lists many specific issues with the methodology and comparisons. There are no positive comments. The politeness score is -20 because while the language is not overtly rude, the tone is quite harsh and dismissive. The reviewer uses phrases like 'can be clearly refuted', 'deliberately not introduced', and 'incorrect and misleading', which come across as somewhat impolite. However, the reviewer does maintain a professional tone overall and provides detailed, specific feedback, which prevents the score from being lower.",-80.0,-20.0
"First of all, the paper cannot be accepted because it violates the double blind submission policy by including an acknowledgments section.

Nonetheless, I will give some brief comments:

 The paper proposes a probabilistic hierarchical approach to perform zero-shot learning.
Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy.

 The experimental section of the paper is strong enough although more baselines could have been tested. The paper only compares the usual cross entropy loss with their proposed soft-classification framework. 
Nonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task.

 
On the other hand, the theoretical aspect is weak. The proposed method seems to be a straightforward extension of Frome et al., NIPS 2013. The main contribution is that soft probability scores are used to perform classification instead of using only class membership information.

Some weighting strategy is proposed in Section 2.2 but the proposed steps seem very ad hoc with no theoretical justification. The first equation on page 8 has the same problem where some random definition is provided.
","The sentiment score is slightly negative (-20) because the reviewer starts by stating the paper cannot be accepted due to a policy violation, and points out some weaknesses in the theoretical aspect. However, they also acknowledge the strong experimental section and improvements in performance, which prevents the score from being more negative. The politeness score is slightly positive (20) as the reviewer uses professional language throughout, offers 'brief comments' despite the policy violation, and provides balanced feedback on both strengths and weaknesses. They avoid harsh criticism and use phrases like 'could have been' instead of more direct negative statements.",-20.0,20.0
"The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack. In general I find the writing poor, as it is not exactly clear what the focus of the paper is - the evaluation or the new attack? The experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.
	
Major remarks:
- Linking the code and asking the reviewers not to look seems like bad practice and close to violating double blind, especially when considering that the cleavhans library is well known. Should have just removed the link and cleavhans name and state it will be released after review. 

- It is unclear what the focus of the paper is, is it the evaluation methodology or the new attack? While the evaluation methodology is presented as the main topic in title, abstract and introduction most of the paper is dedicated to the attack.

- The evaluation methodology is a good idea but is quiet trivial. Also, curves are nice visually but hard to compare between close competitors. A numeric value like area-under-the-curve should be better.

- The theoretical guarantees is also quiet trivial, more or less saying that if a confident adversarial attack exists then finding the most confident attack will be successful. Besides that the third part of the proof can be simplified significantly.

- The experiments are very lacking. The authors do not compare to any other attack so there is no way to evaluate the significance of their proposed method

- That being said, the max-confidence attack by itself sounds interesting, and might be useful even outside confidence thresholding.

- One interesting base-line experiment could be trying this attack on re-calibrated networks e.g. “On Calibration of Modern Neural Networks” 

- Another baseline for comparison could be doing just a targeted attack with highest probability wrong class.

- I found part 4.2 unclear 

- In the conclusion, the first and last claims are not supported by the text in my mind. 



Minor remarks:

- The abstract isn’t clear jumping from one topic to the next in the middle without any connection.

- Having Fig.1 and 2 right on the start is a bit annoying, would be better to put in the relevant spot and after the terms have been introduced.

-In 6.2 the periodically in third line from the end seems out of place.

","The sentiment score is -60 because the review is predominantly negative. The reviewer criticizes the writing quality, lack of clarity in focus, trivial methodology, and insufficient experiments. Phrases like 'poor writing', 'lacking', and 'trivial' indicate a negative sentiment. However, there are some positive aspects mentioned (e.g., 'good idea', 'sounds interesting'), which prevent the score from being extremely negative. The politeness score is -20 because while the reviewer isn't overtly rude, the tone is quite direct and critical. The use of phrases like 'bad practice', 'very lacking', and 'isn't clear' without much softening language contributes to a somewhat impolite tone. However, the reviewer does offer constructive suggestions and uses some neutral language, preventing the score from being extremely negative in terms of politeness.",-60.0,-20.0
"Pros:

This paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \phi_i, and then is formulated as a GP. 

Cons:

However, there are some key issues.
1. The so-called “infinite width” is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details.
 
[S1] Generalization Properties of Learning with Random Features, NIPS 2017.
 
2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing.

Summary:
The paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results.

Response to rebuttal:
The authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. 

Therefore the paper is very borderline. However, I would like to bump my rating a bit higher.

 
 
 ","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('Pros' section), they express significant concerns ('Cons' section) and describe the paper as 'very borderline'. The overall tone is more negative than positive, but not extremely negative. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They present their concerns as 'issues' rather than failures, and use phrases like 'I would like to bump my rating a bit higher', which shows a willingness to be fair. However, the review is not overly polite or effusive, maintaining a professional tone.",-30.0,20.0
"- Does the paper present substantively new ideas or explore an under-explored or highly novel question? 

The paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. 

- Does the results substantively advance the state of the art?

The results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).

- Will a substantial fraction of the ICLR attendees be interested in reading this paper? 

Yes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper’s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.

- Would I send this paper to one of my colleagues to read? 

  I would definitely send the paper to my colleagues to read. 


- In terms of quality:  

Clear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. 

- I terms of clarity:  

Easy to read.–Experimental evaluation is clearly presented.

- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor’s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).

- In terms of significance:  

 The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   



","The sentiment score is 70 (positive) because the reviewer expresses overall positive views about the paper. They note that the combination of ideas is novel, the results advance the state of the art, and they would definitely send the paper to colleagues. The reviewer also mentions that the paper is interesting and presents a promising direction. However, they do point out some limitations, such as the simplicity of the examples and the question of scalability, which prevents the score from being higher. The politeness score is 80 (polite) because the reviewer uses respectful and professional language throughout. They provide constructive feedback and balance positive comments with areas for improvement. The reviewer's tone is encouraging and supportive, using phrases like 'definitely interesting' and 'promising direction', while also offering suggestions for future work in a considerate manner.",70.0,80.0
"This work proposes a defence based on class-conditional feature distributions to turn deep neural networks into robust classifiers.

At present this work lacks even the most rudimentary evidence to support the claims of robustness, and I hence refrain from providing a full review. In brief, model robustness is only tested against adversarials crafted from a standard convolutional neural network (i.e. in a transfer setting, which is vastly different from what the abstract suggests). Unsurprisingly, the vanilla CNN is less robust than the density-based architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla CNN are. No direct attacks - neither gradient-based, score-based or decision-based attacks - have been used to evaluate robustness. Please check [1] for how a thorough robustness evaluation should be performed.

[1] Schott et al. “Towards the first adversarially robust neural network model on MNIST”.","The sentiment score is -80 because the reviewer states the work 'lacks even the most rudimentary evidence' and they 'refrain from providing a full review', indicating strong disapproval. They also point out significant flaws in the methodology. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive, using phrases like 'unsurprisingly' and suggesting the authors need to check basic evaluation methods. The reviewer makes no attempt to soften their criticism or provide encouragement, which contributes to the negative politeness score.",-80.0,-20.0
"This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.

The proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.

I like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between ""compound questions"" instead of ""multi-hop questions"". In my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.

My only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:

(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).

(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include ""Search-based Neural Structured Learning for Sequential Question Answering"" and ""ComplexWebQuestions"". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.","The sentiment score is 60 (moderately positive) because the reviewer expresses liking the proposed idea and approach, calling it a 'straightforward solution' and praising the clarification of terminology. However, they do have some concerns about the evaluation, which prevents a higher score. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, phrases criticisms constructively as questions or suggestions, and acknowledges positive aspects before raising concerns. They use phrases like 'I like', 'Could you explain', and 'I think the authors should' which maintain a collegial tone.",60.0,80.0
"# Positive aspects of this submission

- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.

- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.

- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.

# Criticism

- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.

- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.

- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.","The sentiment score is 70 (positive) because the review starts with strong praise for the submission, highlighting its novelty, creativity, and usefulness. The reviewer also commends specific aspects like the alternative metrics and model variations. While there are criticisms, they are presented as areas for improvement rather than fundamental flaws. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, balancing praise with constructive criticism. They acknowledge the paper's contributions even when pointing out limitations, using phrases like 'shouldn't overshadow the bigger contributions' which shows consideration for the authors' work. The review maintains a professional and courteous tone throughout, offering specific and helpful feedback without being harsh or dismissive.",70.0,80.0
"The whole model can be simplified by this: using auto-encoders for X and Y's reconstruction, then use Triple GAN loss to match the joint distribution of (X, Y).  However, the deterministic model with GAN loss looks problematic to me.

questions:

1. Although the authors showed strong evidence in their experiment part, they still failed to compare models with Bicycle-GAN, i.e., how Bicycle GAN performs on these two dataset?

2. missing some comparison: why use simplified Triple-GAN loss (i.e. without two regularization terms)  instead of Triangle-GAN, which is addressed to be better? I think the authors need to discuss about this. Also, the authors need to use MMD and other methods mentioned in the original WAE paper.

3. In table 1, without triple-GAN loss, the whole model is deterministic, but the authors can still show the FID score for the generalization ability, which is better than all other cycle-GAN based models, why is that possible? Is this equivalent to claim that auto-encoder has the ability to generate realistic images just by sampling z? 
(If I understand the experiment correctly, the author's synthesized images is generated by $y_hat = G_2(E_1(X))$, no sampling z required)

4. Can the authors show the generalization ability of JWAE? For example, with input X, we can have different correct corresponding Ys, just like Bicycle-GAN did.

","The sentiment score is slightly negative (-20) because the reviewer expresses some concerns about the model and points out several missing comparisons and explanations. The opening statement suggests that the model can be simplified, which implies some criticism. However, the reviewer also acknowledges the 'strong evidence in their experiment part,' which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They phrase their concerns as questions rather than direct criticisms, which is a polite approach. The reviewer also uses phrases like 'I think the authors need to discuss about this' instead of making demands, which contributes to the politeness. The absence of harsh or accusatory language also supports this score.",-20.0,50.0
"This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network's feature vector (output of layer before last) and learn a direct mapping to the GAN's latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. 

Inference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate ""edited"" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN's performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple.

In the experimental part, the authors first compare their approach to the 'naive encoder' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder's latent space is added, and the rest of the discriminator's layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN's is precluded by the fact that different examples are chosen. This oversight does not affect the paper's relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained.

The article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there's a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869.

Overall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm's simplicity, most of the paper is devoted to experiments and discussions.
 
","The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper, describing it as 'novel', 'significant', and 'technically sound'. They highlight the paper's contributions and usefulness in the field. The few criticisms are minor and constructive. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, offering constructive feedback and suggestions without harsh criticism. They acknowledge the paper's strengths while politely pointing out areas for improvement. The tone is professional and courteous, with phrases like 'I wish the authors would have' and 'Overall, I find this paper provides...' indicating a respectful approach to feedback.",80.0,70.0
"The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. 

Clarity: 
Section 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.

Originality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.

Evaluation: 
The evaluation is not very strong for the following reasons:

1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960

2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956

3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.

This makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.

Also, which test set are you using?

Finally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? 
Please reference this figure in the text.","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('nice idea'), they express significant concerns about the evaluation, weak baselines, and limited experiments. The critique outweighs the positive elements. The politeness score is slightly positive (20) as the reviewer uses professional language and offers constructive feedback. They ask questions and make suggestions rather than using harsh criticism. However, the tone is not overly warm or complimentary, maintaining a neutral-to-slightly-positive level of politeness typical in academic reviews.",-30.0,20.0
"Summary
This paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.
The main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.
Specifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.
By performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.

Strength
Learning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.
This paper tackles this problem and showed results on various segmentation problems.

Weakness
The proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.

This paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.
For example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. 
In addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.
For example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.

There are some strong arguments that require further justification. 
- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).
However, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.
- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.

Overall comment
I believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. 
Especially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.
","The sentiment score is -60 because the review is predominantly negative. While the reviewer acknowledges the interesting nature of the problem, they express significant concerns about the novelty, methodology, and conclusions of the paper. The reviewer uses phrases like 'relatively simple', 'closely related to existing approach', 'question the novelty', and 'rather incremental', indicating a negative sentiment. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I believe', 'I question', and 'Overall comment' to soften their criticisms, and they acknowledge the paper's strengths before discussing its weaknesses. The language is not overtly polite, but it avoids rudeness and maintains a constructive tone.",-60.0,20.0
"This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given.

This paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong.

Theorem 3:
""Note that $\theta^i_t$ and $\hat \theta^i_t$ are initialized with the same initial distribution µ0 = ν0 and we can also set $\theta^i_0$ to be independent of $\hat \theta^i_0$, we can have $\gamma(0) = 0$. $\gamma(0) = E \|\theta^i_0 - \hat \theta^i_0 \|^2$."" - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density.

""Then according to the Gronwall Lemma, we have"" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and  $\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case.

""... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption ""We can tune the bandwidth of the RBF kernel to make ∇K ≤ H_∇K, which is omitted in the Assumption due to the space limit."" First, there is a missing norm, since ∇K is a vector and H_∇K is I believe a scalar constant. Second, c1 = H_∇K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement).

I find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations).

""Note the first bullet indicates U to be a convex function and W to be ... "" I think it should be K, not W.

Theorems 3-6 could be lemmas, while there should be a unifying theorem for the bound.

Finally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process.

Other comments:

Example in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative.

It is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data.
","The sentiment score is -50 because the review starts positively but then becomes critical, pointing out several issues with the paper's proofs and theorems. The reviewer expresses difficulty understanding parts of the proof and suggests that some statements are incorrect. This indicates a predominantly negative sentiment, though not extremely harsh. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'Could you please elaborate' and 'I suggest', which are polite ways to express criticism. However, the reviewer is also direct in pointing out perceived errors, which keeps the score from being higher. The language is not overly formal or deferential, but it avoids rudeness or harsh criticism.",-50.0,20.0
"Post-rebuttal
------------------
I have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:
- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.
- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.

=========================================
Pros
-------
[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.

Limitations
----------------
[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. 

[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. 

[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?

[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \lambda^* > 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. 

[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. 


General comments
---------------------------
[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as ""gradient descent in Euclidean space"" (although the authors would need to also precisely explain what they mean by that statement).

[Originality] Some related work that the authors should position their proposed problem/solution against:
- There is some work on ""max-kernel search"" which can perform similarity search with general notions of similarity (than just Euclidean metrics).
- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.

Minor comments/typos
---------------------------------
- The authors should present the precise SL2G algorithm given the graph in the manuscript.
- l^2 --> \ell_2
- gradient decent --> gradient descent
- Table 1, f(q, x) --> f(x, q)","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('I am going to raise my rating by one point', 'presents a novel solution'), there are several criticisms and limitations pointed out. The reviewer mentions that the solution is 'somewhat obvious', the manuscript is 'hard to parse', and there are multiple areas needing clarification or improvement. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I better understand the paper', 'it would be good to clarify', and provide detailed feedback without using harsh language. The reviewer balances critique with recognition of the paper's merits, which contributes to the polite tone.",-20.0,50.0
"I am a researcher in NLP and know little about vision, so I cannot review this paper. I have contacted general chair about this situation. ","The sentiment score is 0 (neutral) because the reviewer is neither positive nor negative about the paper itself. They are simply stating their inability to review it due to lack of expertise in the field. The politeness score is 50 (somewhat polite) because the reviewer is being honest and responsible by admitting their lack of expertise and informing the general chair about the situation. The language used is straightforward and professional, without any rudeness, but also without excessive politeness. The reviewer is taking appropriate action in a respectful manner.",0.0,50.0
"In this paper, the authors relate the architectures of recurrent neural
networks with ODEs and defines a way to categorize the RNN architectures by
looking at non-linearity order and temporal memory scale. They further
propose QUNN, a RNN architecture that is more stable and has less complexity
overhead in terms of input dimension while comparing with LSTM. 

Although this paper provides a new view point of RNN architectures and relates
RNNs with ODEs, it fails to provide useful insight using this view point.
Also, it is not clear what advantage the new proposed architecture QUNN has
over existing models like LSTM or GRU. 

The paper is well presented and the categorization method is well defined.
However, how the order of non-linearity or the length of temporal memory
affect the behavior and performance of RNN architectures are not studied.

It is proved that QUNN is guaranteed existence and its Jacobian eigen values
will always have zero real part. It would be easier to understand if the
authors could construct a simple example of QUNN and conduct at least some 
synthetic experiments.

In general I think this paper is interesting but could be extended in various
ways. 
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The paper is well presented', 'interesting'), they express several criticisms and concerns. The reviewer states that the paper 'fails to provide useful insight' and questions the advantages of the proposed architecture. However, it's not entirely negative as they see potential for extension.

The politeness score is moderately positive (60) because the reviewer uses respectful and professional language throughout. They offer constructive criticism without harsh or rude phrasing. The use of phrases like 'It would be easier to understand if...' and 'I think this paper is interesting but...' show a polite approach to giving feedback. The reviewer also acknowledges positive aspects before presenting criticisms, which is a polite review technique.",-20.0,60.0
"This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. 

It is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? 

Arguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. 

Another way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?
","Sentiment Score (50): The review starts positively, calling the paper 'interesting' and praising its goal and writing. However, it then raises several concerns and suggestions for improvement, balancing the positive aspects with constructive criticism. This mix of positive and negative elements results in a slightly positive overall sentiment.

Politeness Score (75): The reviewer maintains a professional and respectful tone throughout. They use phrases like 'interesting paper', 'well written', and 'interesting take', which are polite and encouraging. Even when raising concerns, the language remains constructive rather than harsh, using phrases like 'It is a little unclear' and 'Perhaps a better thing is to...'. The reviewer also acknowledges the authors' own recognition of limitations. This consistently polite and constructive approach warrants a high politeness score.",50.0,75.0
"The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important.  To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there’s also an experiment on ImageNet
It looks to me that the proposed method is overly complicated. It is also described in a complicated manner.  I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers.
The results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated","The sentiment score is -30 because the reviewer expresses several criticisms and doubts about the paper's approach and results. They describe the method as 'overly complicated' and 'not clear', and state that the improvements shown are 'not great'. However, they do acknowledge some 'interesting ideas', which prevents the score from being more negative. The politeness score is 20 because the reviewer uses relatively neutral language and avoids harsh criticism. They present their concerns as personal observations (e.g., 'It looks to me', 'I don't see') rather than absolute statements. The reviewer also acknowledges positive aspects, such as 'interesting ideas' and 'decent improvement', which contributes to a more polite tone. However, the overall critique is still direct, preventing a higher politeness score.",-30.0,20.0
"- Summary
This paper presents a minor improvement over the previous Deli-GAN framework (Gurumurthy et al CVPR'17). Specifically, the work proposes to use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence. It shows little results (seems to be positive) using this new distance measure other than traditional ones.  Except that, I didn't see any other contribution from this paper.

- Suggestions
The paper is poorly written and seems to be a rush submission to ICLR. For example:
* a lot of grammatical errors throughout the paper
* only 6.5 papes out of 8 pages are utilized
* the introduction is not convincing -- what problem are you going to address? any summary of your methodology? why it is expected to outperform existing frameworks? what distinguishes your work from existing works? and what're your main results? I cannot conclude after reading the intro.
* the results are very minor and not convincing. It seems the authors conducted a very limited set of experiments and concluded that the proposed Deli-Fisher GAN is better. If you claim that the proposed framework can generate better images, at least the framework should be compared to the latest state-of-the-art GANs (e.g. spectral GANs, etc.)
* The writing is not polished. 

Overall, the paper is far from ready to be submitted to ICLR, not mentioning acceptance. I would recommend the authors to conduct more experiments and comparisons and do a better job before submitting it to future conferences.

","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper presents only a 'minor improvement', shows 'little results', and is 'poorly written'. They also mention that the paper is 'far from ready to be submitted'. These comments indicate a strong negative sentiment towards the paper. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite direct and critical. Phrases like 'poorly written', 'rush submission', and 'not convincing' are used without much softening language. However, they do offer some constructive feedback and use 'I would recommend' instead of more forceful language, which prevents the score from being lower.",-70.0,-20.0
"This submission proposes an energy-based method to correct mislabelled examples. Intuitively, the authors claim that contradictions between energy and noisy labels can be used to identify label noise. To make the idea reliable, the authors propose to compute the energy by using learned (commonly shared) features. Experiment results look good. The presentation is also clear. My concerns are as follows:

(1) By learning discriminative features and then correcting the label noise, the authors have implicitly assumed that the label noise strongly dependent on the discriminative features. This assumption may be strong as most labels are provided according to the original instance (features). In the experiment section, it is very unclear about how the noise is generated, e.g., how to select x_i according to variance? How to set the threshold? What is ""col"" in Tables 1 and 2? What are the overall label noise rates? Note that if the threshold for the variance is large. It means that the noise is only added to the discriminative features, making the experiment too ad-hoc. 

(2) The theory of why the residual energy can be used to identify label noise is elusive. How to set the threshold for identifying label noise with a theoretical guarantee is also unclear. Two recent papers on learning with instance-dependent label noise are surprisingly missed, e.g., Menon, Aditya Krishna, Brendan Van Rooyen, and Nagarajan Natarajan. ""Learning from binary labels with instance-dependent corruption."" arXiv preprint arXiv:1605.00751 (2016). and Cheng, Jiacheng, et al. ""Learning with Bounded Instance-and Label-dependent Label Noise."" arXiv preprint arXiv:1709.03768 (2017). The latter one proposes algorithms to identify label noise with theoretical guarantees. The authors should compare the proposed method with them.

(3) There are methods provided for choose the values of hyperparameters. Most of them are empirically set, which is not convincing.

(4) The authors reported that with discriminative features learned by employing noisy data, the proposed method also provide good performance. It would be interesting to see how corrected labels will recursively help better learn the discriminative features. Illustrating figures are preferred.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Experiment results look good. The presentation is also clear.'), they express several significant concerns and criticisms about the methodology, assumptions, and lack of comparisons to existing work. The overall tone suggests the reviewer is not fully convinced by the paper. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They phrase concerns as questions or suggestions rather than direct attacks, and use polite phrases like 'It would be interesting to see...' However, the review doesn't go out of its way to be overly complimentary or deferential either, maintaining a fairly neutral professional tone overall.",-20.0,50.0
"This paper proposes to use n-ary representations for convolutional neural network model quantization. A novel strategy of nested-means clustering is developed to update weights. Batch normalization is also considered in the activation quantization. Experiments on both weight quantization and activation quantization are conducted and show effectiveness.

Strengths:
1.	The idea of nested-means clustering is interesting, which somehow shows its effectiveness.
2.	State-of-the-art experimental results.
3.	The representation is excellent, and it is easy to follow.

Concerns:
1.	Though the experiment study seems solid, an ablation study is still missing. For example, how important is the nested-means clustering technique? What is the effect if replacing it with the original one or with other clustering methods? What will happen if expanding the interval in the quantization of activation? All these kinds of questions are hard to answer without an ablation study.
2.	It is not clear how the weight and activation quantization are addressed together.
3.	If counting the first and last layers, what is the size of the model (the number of parameters)?
4.	Similarly, what are the FLOPs in different settings of experiments? This seems missing.
5.	When discussing the related work about model compression, there are important references missing. I just list two references in the latest vision and learning literature:
[Ref1] X. Lin et al. Towards accurate binary convolutional neural network. NIPS 2017
[Ref2] Z. Liu et al. Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm. ECCV 2018.
","The sentiment score is 50 (slightly positive) because the review begins by highlighting the paper's strengths, including 'interesting' ideas, 'state-of-the-art experimental results', and 'excellent' representation. However, it also lists several concerns, which balance out the positive aspects. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, framing concerns as questions or suggestions rather than direct criticisms. Phrases like 'It is not clear' and 'This seems missing' are used instead of more confrontational language. The reviewer also acknowledges the paper's strengths before discussing concerns, which is a polite approach in academic reviews.",50.0,70.0
"The authors are providing an information theoretic viewpoint on the behavior of DNN based on the information bottleneck.  The clarity of the paper is my main concern.  It contains quite a number of typos and errors.  For example, in section 6, the results of MNIST in the first experiment was presented after introducing the second experiment.  Also, the results shown in Fig 1b seems to have nothing to do with Fig. 1a.  It makes use of some existing results from other literature but it is not clearly explained how and why the results are being used.   It might be a very good paper if the writing could be improved.   The paper also contains some experimental results.  But they are too brief and I do not consider the experiments as sufficient to justify the correctness of the bounds proved in the paper.","The sentiment score is slightly negative (-20) because the reviewer expresses concerns about the clarity of the paper, mentioning 'quite a number of typos and errors' and issues with the presentation of results. However, they also note that 'It might be a very good paper if the writing could be improved,' which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the potential value of the paper while offering constructive criticism. They avoid harsh or accusatory language, instead framing their concerns as observations and suggestions for improvement. The reviewer also balances negative points with positive ones, which contributes to the polite tone.",-20.0,50.0
"This paper proposed a bio-inspired sparse coding algorithm where iterations
for dictionary updates take into account the past updates. It is argued
that time takes a crucial rule in learning.

The paper is quite well written and contains an extensive literature review
demonstrating a good understanding of previous literature in both ML/DL and biological
vision.

The idea of using a ""non-linear gain normalization"" to adjust atom selection
in sparse coding is interesting and as far as I know novel, while providing
interesting empirical results: The system learns in an unsupervised way faster.

Misc:

- Using < > for latex brakets is not ideal. I would recommend: $\langle\,,\rangle$

- ""derivable"" I guess you mean ""differentiable""

- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the
reference to Pedregosa et al. for sklearn is missing.",The sentiment score is 80 (positive) because the reviewer expresses a generally positive view of the paper. They describe it as 'quite well written' with an 'extensive literature review' and 'interesting empirical results'. The idea is called 'interesting and novel'. The politeness score is 70 (polite) due to the use of respectful language throughout. The reviewer offers constructive feedback and suggestions without harsh criticism. They use phrases like 'I would recommend' instead of demanding changes. The only slightly negative comments are phrased as minor suggestions ('Misc:') rather than major criticisms.,80.0,70.0
"This review will unfortunately be very short because I am afraid there is not much to say about this well written paper, which seems to have been sent to the wrong conference. The scientific problem is interesting, namely the detection of topological artifacts in images showing biological phenomena (which I don’t know much about). The relevant literature here is basically literature from this field, which is not machine learning and not even image processing. The contribution of the paper, in terms of machine learning, is to apply a well known neural model (YOLO) to detect bounding boxes of objects in images, which are very specific. The contribution here does not lie in machine learning, I am afraid.

This is thus a purely experimental paper on a single application, namely object detection in specific images. Unfortunately the experiments are not convincing. The results are validated against a “traditional method”, which has never been cited, so we do not know what it is.

The performance gain obtained with YOLO seems to be minor, although the difference in time complexity is quite enormous (to the advantage of YOLO).

The contribution is thus minor and for me does not justify publication at ICLR.

The grant number is mentioned in the acknowledgments, which seems to violate double blind policy.
","The sentiment score is -70 because the reviewer expresses significant concerns about the paper's suitability for the conference and its overall contribution. Phrases like 'unfortunately be very short', 'sent to the wrong conference', 'contribution here does not lie in machine learning', and 'does not justify publication' indicate a strongly negative sentiment. However, it's not entirely negative as the reviewer acknowledges the paper as 'well written' and the problem as 'interesting'. The politeness score is 20 because while the reviewer is direct in their criticism, they use polite language such as 'I am afraid' and 'unfortunately'. They also start with a positive comment about the paper being well-written. However, the overall tone is more matter-of-fact than overtly polite, hence the relatively low positive score.",-70.0,20.0
"The authors proposed a new method improving a previous Bayesian optimization approach for chemical design (Gomez-Bombarelli et al., 2016b) by addressing the problem that data points need to have valid molecular structures. The main contribution is a constrained Bayesian optimization approach that take into account the constraint on the probability of being valid.

My biggest concern of this paper is that it is not really using sequential evaluations to do automatic design of experiments on molecules. The experiments do not seem to fully support the proposed approach in terms of being able to adaptively do sequential evaluations. 

Detailed comments:
1. The term inside the expectation of the EI criterion in Sec 2.5 should be max(0, f(m)-\eta) rather than max(0, \eta - f(m)).
2. The EIC criterion the authors adopted uses Pr(C(m)) if the constraint is violated everywhere with high probability. It seems like Pr(C(m)) does not have the ability to explore regions with high uncertainty. How does this approach compare to Bayesian level set estimation approaches like 
B. Bryan, R. C. Nichol, C. R. Genovese, J. Schneider, C. J. Miller, and L. Wasserman, “Active learning for identifying function threshold boundaries,” in NIPS, 2006 
I. Bogunovic, J. Scarlett, A. Krause, and V. Cevher, “Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation,” in NIPS, 2016.
3. It would be good to explain in more detail how a constraint is labeled to be valid or invalid. 
4. What is the space of m in Sec 2.3 and the space of m in Sec 2.4? It seems like there is a discrepancy: Sec 2.3 is talking about m as a molecule but Sec 2.4 describes f as a function on the latent variable? It would be good to be clear about it.
5. In the experiment, I think it is very necessary to show the effectiveness of the constrained BO approach in terms of how the performance varies as a function of the number of evaluations on the constraint and the function. The current empirical results only support the claim that the constrained BO approach is able to output more valid latent variables and the function values from constrained BO is higher than vanilla BO under the same number of training data. It is also strange why there is a set of test data. 


Typos/format:
1. citation format problems across the paper, e.g. 
""Gomez-Bombarelli et al. (Gomez-Bombarelli et al., 2016b) presented Automatic Chem""
""SMILES strings Weininger (1988) are a means of representing molecules as a character sequence.""
It's likely a problem of misuse of \cite, \citep.
2. no period at the end of Sec 2.4","The sentiment score is -30 because while the reviewer acknowledges the authors' contribution, they express significant concerns about the paper's methodology and experimental support. The opening paragraph is somewhat positive, but the 'biggest concern' mentioned and the numerous detailed comments indicate overall negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'It would be good to explain' and 'I think it is very necessary to show' which are polite ways of pointing out areas for improvement. The reviewer also provides detailed comments and suggestions, which is a courteous way to help the authors enhance their work.",-30.0,50.0
"This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:

- There are versions of attention mechanisms that are spare and differentiable. See here: 
From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification
André F. T. Martins, Ramón Fernandez Astudillo 

- Why is ""rationalizing textual matching"" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.

- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:
D. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018
http://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf

- I don't agree that explanations are sufficient if removing them doesn't degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:
Adversarial Examples for Evaluating Reading Comprehension Systems.
Robin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf

- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.

- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.

- What is the ""certificate of exclusion of unselected parts"" that the proposed method has?

- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it.","The sentiment score is -60 because the review is predominantly critical, pointing out several significant issues with the paper such as lack of human evaluation, no direct comparison to related work, and questionable assumptions. However, it's not entirely negative as it acknowledges the paper's proposal and offers constructive feedback. The politeness score is 20 because while the reviewer maintains a professional tone and offers specific suggestions for improvement, the language is direct and doesn't use many polite phrases or softeners. The reviewer states disagreements plainly ('I don't agree') and points out flaws without much cushioning, but also doesn't use rude or harsh language.",-60.0,20.0
"I don't quite see how the proposed approach addresses non-linear canonical correlation analysis. In particular:

1) The main motivation is the minimization of the conditional mutual information I(X;Y|Z), where X and Y correspond to the two views and Z is latent. First of all, what uncertainty does this expression has when X and Y are observations and Z is given? My understanding is that the main objective of any CCA problem should be to find some transformations, say f(X) and g(Y), with some (to be defined) desirable properties. For example, these would correspond to linear transformations, say Ax and By, for classical CCA. Therefore, should not one be interested in minimizing something like I(f(X);g(Y)|Z)?

2) Assuming that the minimization of the conditional mutual information I(X;Y|Z) would be the goal, I don't quite see why the formulation in equation (6) would actually be equivalent (or be some reasonable approximation)? 

3) It is well known that differential entropy can be negative (e.g., Cover and Thomas, 2006). Why would the conditional mutual information in equation (4) be non-negative? Alternatively, what would negative values of I(X;Y|Z) mean in the CCA context? My understanding is that one should be interested in minimizing I(X;Y|Z), or its variants with transformations, in *absolute value* to ensure some closeness to conditional independence.

4) Expressions in equation (5)-(6) are general and hold with no assumptions whatsoever for any random variables X, Y, Z (given the expectations/integrals exist). It is therefore not clear what are the variables of this minimization problem? (parameters? but what is the parametric model?)

5) Assuming solving (6) is the goal, this formulation as mentioned by the authors is actually is quite a challenging problem involving latent variables. Some form of this approach explanation would 

I can not quite see how the proposed adversarial version would correct or supplement any of these questions.

Other comments:

1) It would be appropriate to cite the probabilistic CCA paper by Bach and Jordan (2005); a better citation for classical CCA would be Hotelling (1936).

2) I find the multiple mentioning of the *non-linear* (in-)dependence confusing. Is this in statistical sense? And how exactly is this related to CCA? Does it have anything to do with the fact that the third and higher order cumulants are zero only for independent variables unless they are Gaussian? Moreover, does this linear independence have any connection with the non-linearity of the proposed CCA approach?

3) What exactly is the *linear correlation criterion* and how does it enter the classical CCA or PCCA formulation (Introduction; bullet point 2)?

4) It would be helpful to introduce the original CCA problem emphasizing that each view, X and Y, are *different* linear transformation of *the same* latent codes z. Moreover, the full description of the models (classical CCA/ PCCA) wouldn't take more than one-two paragraphs and would help the readers to avoid any misunderstanding.

5) Are any assumptions necessary to ensure existence?
","The sentiment score is -60 because the reviewer expresses significant doubts and criticisms about the proposed approach, pointing out multiple issues and gaps in understanding. The review starts with 'I don't quite see how...' and continues to list several major concerns, indicating a generally negative sentiment. However, it's not entirely dismissive, as the reviewer offers suggestions and asks for clarifications, hence not reaching the lowest possible score. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout. They use phrases like 'I don't quite see' and 'It would be helpful' rather than harsh or rude language. The reviewer also offers constructive feedback and suggestions for improvement, which contributes to a slightly positive politeness score.",-60.0,20.0
"The paper studies the problem of question generation from sparql queries. The motivation is to generate more training data for knowledge base question answering systems to be trained on. However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:
- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745
- Globally Coherent Text Generation with Neural Checklist Models
Chloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032
Thus the main novelty claim of the paper needs to be hedged appropriately. Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.

Some other points:
- How is the linearization of the inout done? It  typically matters
- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.
- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used. Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. ""what job did jefferson have"" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone. While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express. What were the guidelines used?
","The sentiment score is -50 because the reviewer expresses significant concerns about the novelty of the paper's approach and points out several issues with the methodology and evaluation. However, it's not entirely negative as the reviewer acknowledges the paper's motivation and provides constructive feedback. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, offering specific suggestions for improvement without using harsh language. The reviewer uses phrases like 'I would propose' and 'Some other points:' which contribute to a polite tone while still clearly communicating the criticisms.",-50.0,50.0
"The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).

There is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.

- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.

- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('experiments... are preliminary but encouraging'), they also point out significant issues ('the setting... does not make any sense'). The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer uses respectful language ('Please reconsider', 'Please try to provide') and offers constructive feedback without harsh criticism. The reviewer balances critique with recognition of potential value, maintaining a professional and courteous tone throughout.",-20.0,50.0
"This paper tries to propose a so-called hybrid algorithm to eliminate the gradient delay of asynchronous methods. The authors propose algorithm 1 and a simplified version algorithm 2 and prove the convergence of algorithm 2 in the paper.  The paper is very hard to follow, especially the algorithm description part. What I can understand is that the authors want to let the fast workers do more local updates until the computation in the slowest worker is done. The idea is similar to EASGD except that it forces the workers to communicate the server once the slowest one has completed their job.

The following are my concerns:
1. Do you consider the overhead in constructing the communication between machines? in your method,  workers are keeping notifying servers that they are done with the computation. 
2. In Algorithm 1 line 9 and line 23, there are two assignments: x_init =x and x_init=ps.x, is there any conflict? 
3. In Algorithm 2,  at line 6 workers wait to receive ps.x, at line 20 server wait for updates. I think there is a bug, and nothing can be received at both ends.
4. The experiments are too weak. There is no comparison between other related methods, such as downpour, easgd.
5. The authors test resnet50 on cifar10,  however, there is no accuracy result. They show the result by using googlenet, why not resnet50? I am curious about the experimental settings.

Above all, the paper is hard to follow and the idea is very trivial. Experiments in the paper are also very weak. ","The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's clarity, methodology, and experimental results. Phrases like 'very hard to follow', 'the idea is very trivial', and 'experiments are too weak' indicate a strong negative sentiment. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and direct. The reviewer doesn't soften their criticisms or use many polite phrases. The use of phrases like 'Do you consider...' and 'I am curious about...' slightly mitigate the negative tone, preventing a lower score. However, the overall impression is of a blunt, somewhat impolite review.",-70.0,-20.0
"This paper presents a new semi-supervised method for bilingual dictionary induction and proposes a new metric to measure isometry between embedding spaces.

Pros:
- The paper proposes to use a new metric, the Gromov-Hausdorff distance to measure how isometric two word embedding spaces are.
- The toy example is useful for motivating the use case of the method.
- The approach achieves convincing results on the dataset.

Cons:
- Beyond the isometry metric, the main innovation as far as I can see seems to be the hubness filtering, which is incremental and not ablated, so it is not clear how much improvement it yields. The weak orthogonality constraint has already been used in [2].
- It is not clear to me what the proposed metric adds beyond the eigenvector similarity metric proposed in [1]. The authors should compare to this metric at least.
- The authors might want to add the results of [3] for an up-to-date comparison.

[1] Søgaard, A., Ruder, S., & Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.
[2] Zhang, M., Liu, Y., Luan, H., & Sun, M. (2017). Adversarial Training for Unsupervised Bilingual Lexicon Induction. In Proceedings of ACL.
[3] Artetxe, M., Labaka, G., & Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.","The sentiment score is slightly positive (20) because the review begins with a neutral summary and lists both pros and cons. The pros highlight the paper's contributions and positive results, while the cons point out areas for improvement without being overly critical. The overall tone suggests the reviewer sees merit in the work but believes there's room for enhancement. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They offer constructive criticism and suggestions for improvement without using harsh or dismissive language. The use of phrases like 'The authors might want to' and 'It is not clear to me' indicate a polite approach to giving feedback. The reviewer also acknowledges the paper's strengths before discussing its limitations, which is a courteous way to structure feedback.",20.0,50.0
"This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.

In active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?

The novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.

The paper is well-written, has good experiments, and has a comprehensive related work section.

Overall, this paper is good, but is not novel or important enough for acceptance.","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('The paper is well-written, has good experiments, and has a comprehensive related work section'), the overall tone is critical. The reviewer points out minimal improvements, questions the computational cost, and ultimately concludes that the paper is 'not novel or important enough for acceptance'. This indicates a negative sentiment, but not extremely so.

The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They provide specific feedback and suggestions, and acknowledge the paper's strengths. The language used is not overly formal or polite, but it's also not rude or dismissive. The reviewer presents their criticisms in a constructive manner, which contributes to the slightly positive politeness score.",-50.0,50.0
"Overview and contributions: The authors propose the ModifAE model that is based on an autoencoder neural network for continuous trait image modifications. ModifAE requires fewer parameters and less time to train than existing generative models. The authors also present experiments to show that ModifAE produces more convincing and more consistent continuous face trait modifications than the current baselines.

Strengths:
1. Nice presentation of the model.
2. Good experiments to justify improved running time and fewer number of parameters.

Weaknesses:
1. I am not completely convinced by the results in Figure 4. It doesn't seem like the model is able to pick up on subtle facial expressions and generate them in a flexible manner. In fact, the images look very very similar regardless of the value of the traits. Furthermore, the authors claim that ""In general, as she becomes more emotional, her smile increases, and as she is made more attractive, her smile increases as well, as smiling subjects are judged as more attractive"". I believe attractiveness and emotions are much more diverse and idiosyncratic than just the size of her smile...
2. From Figure 5 it seems like ModifAE generates images that are lower in quality as compared to StarGAN. Can the authors comment on this point? How can ModifAE be improved to generate higher-quality images?

Questions to authors:
1. Weakness points 1 and 2.
2. This did not affect my rating, but I am slightly concerned by the labelings as seen in Figure 1. Is it reasonable to infer traits like ""trustworthy"", ""attractive"", ""aggressive"", ""responsible"" from images? Are these traits really what we should be classifying people's faces as, and are there any possible undesirable/sensitive biases from the dataset that our models could learn? I would like to hear the author's opinions on the ethical implications of these datasets and models. 

Presentation improvements, typos, edits, style, missing references:
None","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the results and image quality. The reviewer is not completely convinced by the results in Figure 4 and notes that the ModifAE-generated images seem lower in quality compared to StarGAN. These weaknesses outweigh the positive comments about the model presentation and experiments. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, frames criticisms as questions or areas for improvement, and includes positive feedback alongside the critiques. The reviewer also asks for the authors' opinions on ethical implications, showing consideration for broader impacts. The tone is professional and constructive overall, without any rude or harsh language.",-20.0,60.0
"This paper introduces a domain adaptation approach for structured output data, with a focus here on semantic segmentation. The idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence. This is achieved by defining new patch classes via clustering the source patches according to the semantic information, and making use of an adversarial classifier on the predicted patch-class distributions.

Strengths:
- Modeling the structure via patches is an interesting idea.
- The proposed method achieves good results on standard benchmarks.

Weaknesses:

Method:
- The idea of relying on patches to model the structure is not new. This was achieved by Chen et al., CVPR 2018, ""ROAD: Reality Oriented Adaptation..."". In this work, however, the patches were assumed to be in correspondence, which leaves some novelty to this submission, although reduced.
- In essence, the patch-based adversarial alignment remains global; this can be thought of as working at a lower resolution and on a different set of classes, defined by the clusters, than the global alignment. The can be observed by comparing Eq. 3 and Eq. 6, which have essentially the same form. This is fine, but was not clear to me until I reached Eq. 6. In fact, what I understood from the beginning of the paper was an alternative formulation, where one would essentially assign each patch to a cluster and aim to align the distributions of the output (original classes) within each cluster. I suggest the authors to clarify this, and possibly discuss the relation with this alternative approach.
- I am not convinced by the claimed relationship to methods that learn disentangled representations. Here, in essence, the authors just perform clustering of the semantic information. This is fine, but I find the connection a bit far-fetched and would suggest dropping it.

Experiments:
- The comparison to the state of the art is fine, but I suggest adding the results of Chen et al, CVPR 2018, which achieves quite close accuracies, but still a bit lower. The work of Saito et al., CVPR 2018, ""Maximum Classifier Discrepancy..."" also reports results on semantic segmentation and should be mentioned here. I acknowledge however that their results are not as good as the one reported here.
- While I appreciate the ablation study of Section 4.2, it only provides a partial picture. It would be interesting to study the influence of the exact values of the hyper-parameters on the results. These hyper-parameters are not only the weights \lambda_d, \lambda^g_{adv} and \lambda^l_{adv}, but also the number of clusters and the size of the patches used. 

Summary:
I would rate this paper as borderline. There is some novelty in the proposed approach, but it is mitigated by the relation to the work of Chen et al., CVPR 2018. The experiments show good results, but a more thorough evaluation of the influence of the hyper-parameters would be useful.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths ('good results', 'interesting idea'), they also point out several weaknesses and describe the paper as 'borderline'. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive criticism, and acknowledges the paper's merits. They use phrases like 'I suggest', 'I appreciate', and 'I acknowledge', which contribute to a polite tone. However, they don't go out of their way to be overly courteous, maintaining a professional, slightly formal tone.",-20.0,60.0
"The paper presents a variational inference approach for locally linear dynamical models. In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,  enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO. Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.  

Quality: The experiments appear to be well designed and support the main claims of the paper. 

Clarity: The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately. It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way. I also struggled a little to understand what is the difference between forward interpolate and filtering. 

Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted. In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.  

Significance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics. 

Overall, this appears to be a board-line paper with weak novelty. On the positive side, the experimental validation seems well done. The clarity of this paper needs to be strengthened.  

Minor comments: 
- abstract: uncover nonlinear observation? -> maybe change ""observation"" to ""latent dynamics""?

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('experiments appear to be well designed', 'experiments are thoroughly conducted'), they also point out significant weaknesses ('clarity is below average', 'technical novelty of this paper rather weak', 'board-line paper with weak novelty'). The overall tone suggests more concerns than praise. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and balances negative points with positive ones. They use phrases like 'it would be helpful' and 'I struggled a little to understand' rather than harsh criticisms. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism.",-20.0,50.0
"This article presents a novel approach called Integral Pruning (IP) to reduce the computation cost of Deep neural networks (DNN) by integrating activation pruning along with weight pruning. The authors show that common techniques of exclusive weight pruning does compress the model size, but increases the number of non-zero activations after ReLU. This would counteract the advantage of DNN accelerator designs (Albericio et al., 2016; Reagen et al., 2016) that leverage activation sparsity to speed up the computations. IP starts with pruning the weights using an existing technique to mask out weights under a threshold and then fine-tune the network in an iterative fashion to maintain the accuracy. After weight pruning, IP further masks out the activations with smaller magnitude to reduce the computation cost. Unlike weight pruning techniques that use static masks, the authors propose to use dynamic activation masks for activation sparsity in order to account for various patterns that are being activated in DNN for different input samples. In order to do this, the 'winner rate' measure for every layer (or for a group of layers in deep networks like ResNet32) is defined, to dynamically set the threshold for the generation of activation masks which eventually controls the amount of non-zero activation entries. The article empirically analyzes the sensitivity of activation pruning on validation data by setting different winner rates at every layer in DNN and decides upon a set of winner rates accordingly followed by an iteration of fine-tuning the network to maintain its performance. The authors show that their technique produced lower number of non-zero activations in comparison with the intrinsic sparse ReLU activations and weight pruning techniques. 

The topic of reducing network complexity for embedded implementations of DNNs is highly relevant, in particular for the ICLR community.

The IP technique yields significantly reduced number of multiply-accumulate operations (MACs) across different models like MLP-3, ConvNet-5, ResNet32 and AlexNet and on different datasets like MNIST, CIFAR10 and ImageNet. They also depicted that pruning the activations with dynamic activation masks followed by fine-tuning the network yields more sparse activations and negligible loss in accuracy when compared against using static activation masks.
    

Strengths of the paper:
- The motivation to extend compression beyond the weights to activations in order to support the DNN accelerator designs and the technical details are clearly explained. 
- The proposed technique indeed produces sparser activations than intrinsic ReLu sparse activations and can also applied to any network regardless of the choice of activation function.
- The proposed technique is evaluated across different network architectures and datasets.
- The advantage of adapting dynamic activation masks over static ones is clearly demonstrated.

 Weaknesses of the paper:
- The originality of the approach is limited because it is a relatively straightforward combination of existing techniques for weight and activation pruning.
- The ""winner rate"" measure is defined for every layer and should be explored over different values in order to find the equilibrium to reduce the number of non-zero activations and maintain the accuracy. This search of winner rates will become inefficient as the depth of the network increases. However, the authors used a single winner rate for a group of layers in case of ResNet-32 to reduce the exploration of search space but this choice might lead to suboptimal results.
- The authors compare the resultant number of MAC operations against numbers from the weight pruning technique. However, there also exist different works on group pruning techniques like Liu et al. (2017), Huang & Wang (2017), Ye et al. (2018) to prune entire channels / feature maps and thus yield more compact networks. Since these approaches prune the channels, they show a direct impact on the computation complexity and greatly reduce the computation time. A proper and fair comparison would be to compare the numbers of IP against such group pruning techniques. This comparison is highly important to highlight the significance of the approach on speeding up the DNNs and it is missing from the paper.
- At several locations in Section 4, e.g. Sec. 4.1, 4.3, and 4.4. there is no precise statement about the incurred accuracy loss (or no statement at all). The connection to Figures 4 and 5 is not immediately clear and should be made explicit.
		
References:
- Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming.
- Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers
- Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks.
        
Overall Evaluation:
The authors integrate activation pruning along with the weight pruning and show that the number of MAC operations are greatly reduced by their technique when compared to the numbers of weight pruning alone. 	However, I am not convinced regarding the reported number of MAC operations since the number of MAC operations of sparse weight matrices and activations would remain the same as the original models unless some of the filters/activation maps are pruned from the network.  On the other hand, comparisons against group pruning techniques are highly necessary to evaluate the potential impact of the approach on speeding up of DNNs. My preliminary rating is a weak reject but I am open to revise my rating based on the authors response to the above stated major weaknesses.

Minor comments:
- Caption of Fig. 4 should mention the task on which the results were obtained.
- There are occasional grammar errors and typos that should be corrected.","The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper, they ultimately give a 'weak reject' recommendation. The review points out several weaknesses and expresses skepticism about some of the reported results. However, it's not entirely negative as the reviewer is open to revising their opinion based on the authors' response. The politeness score is 60 because the reviewer uses professional and respectful language throughout. They acknowledge the paper's strengths before discussing weaknesses, and phrase criticisms as constructive feedback rather than harsh judgments. The reviewer also uses polite phrases like 'I am open to revise my rating' which contributes to the overall courteous tone.",-30.0,60.0
"Summary: This paper proposes three new techniques for improving Atari performance over APE (Horgan 2018).  Two of them are closely linked in that they deal with improving stability.  Another involves integrating the use of expert trajectories from DQfD.  

I will summarize each: 

Transformed Bellman: This applies a rescaling function (it's basically a monotonically increasing version of the sqrt(x) function) to the Q-function and applies the inverse of the function to the max Q-value of the next state (such that the contracting effect h-function is not ""applied"" multiple times when doing the TD backup).  

Temporal Consistency: This encourages the ""next state"" after where the TD-update is applied to not change too much.  This addresses a problem discussed in (Durugkar 2018).  I think the intuition here is that the state which follows the state with the TD update may be visually similar, but it does not impact the value in the past states, so its value function should not have a highly correlated change with the previous state's change in value function.    

DQfD: Storing an expert replay buffer and an actor replay buffer.  The expert replay buffer is fixed and the actor replay buffer stores the most recent ""actor processes"".  Train with both a supervised imitation loss (only for the highest return episode) and the original TD loss.  Additionally, the pre-training phase is removed and the ratio of expert-learned trajectories is fixed (both seem like steps in the right direction).  

Review: This paper proposes a few changes to DQN training, two of which are aimed at reducing instability, and one is aimed at improving exploration (expert trajectories).  Because all of these changes are well justified and the experiments are fairly thorough, I recommend acceptance.  My main reservation is that the ideas presented are not very strongly thematically linked.  The presence of ablation studies compensates for this to some extent.  

Strengths: 

  -The discussion of related work and comparison to baselines is pretty extensive.  For example I appreciated the ablation study removing ""transformed Q-learning"" and comparison to the pop-art method.  

  -The results, at least for Ape-X DQfD seem impressive to me in that the method works without reward clipping and with a much higher discount factor.  Additionally the results generally outperform DQfD (uses expert trajectories) and Rainbow (no human trajectories).  Additionally evidence was presented that the learned policies often exceed the performance of the human demonstrations (for example in time to achieve rewards).  

Weaknesses: 

  -Two of the techniques: ""transformed bellman"" and ""temporal consistency"" seem well-linked thematically, but the expert demonstration idea seems orthogonal.  I would have preferred splitting that idea out into a separate paper, given that the paper is already 20 pages.  

  -The motivation for temporal consistency just references (Durugkar 2018).  The readability of this paper would be improved if it were discussed more here as well.  I also feel like the analysis could be more thorough here, for example a result using the temporal consistency loss on Baird's counter example really should be shown (like figure 2 in Durugkar's paper).  

-It would be nice to see a visualization or a toy problem with the ""transformed bellman"".  

Questions: 

-Is the ""highest return episode"" idea (3.4) general or is it exploiting the fact that Atari is deterministic?  It seems like in general we'd want to use many high reward episodes, or the highest reward episodes that go into different parts of state space.  It seems like it could be a very bad idea on certain settings (for example if the reward has a lot of randomness).  

-""Proposition 3.1 shows that in the basic cases when either h is linear or the MDP is deterministic, Th
has the unique fixed point h ◦ Q∗"".  From 3.1, it looks if h is linear, then it distributes over r(x,a) + maxh^{-1}(Q) and then it also won't effect which is the max, so it would reduce to h*r(x,a) + max(Q) - which means it's just rescaling the original reward.  So then this result is trivial?  Please correct me if I misunderstood something here.  

-Could an MDP be constructed which causes the transformed bellman operator to perform badly?  I am imagining something where the MDP is just a single step, and there is a stochastic action which behaves like a lottery.  So perhaps there is a 1-in-1-million chance to win 1-billion dollars by taking an action.  If I understand correctly the transformed bellman operator will destroy the large reward here (because in a single step, there is just r(x,a) which h is applied to).  Which would make the action seem bad even though it's actually appealing.  

Notes: 

  -I did not read the proofs in the appendix.  ","The sentiment score is 60 (moderately positive) because the reviewer recommends acceptance and praises the paper's extensive comparisons, impressive results, and thorough experiments. However, they also point out some weaknesses and areas for improvement, tempering the overall positivity. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions for improvement rather than harsh judgments. They use phrases like 'I appreciated' and 'It would be nice to see' which maintain a collegial tone. The reviewer also asks questions in a curious rather than confrontational manner, further contributing to the polite tone.",60.0,70.0
"This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart.

The experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there's little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest.

The writing is not very clear, especially around equations.

Overall the contribution of this paper is marginally incremental:
1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified.
2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf.
3. Reusing previous level/segment computation with gradient fixed is also not a big innovation.

typo:
1. end of page 3, and ""W."" denotes"".
2. The speed experiment should be put in the main text.","The sentiment score is -30 because the review is generally critical, pointing out several limitations and describing the contribution as 'marginally incremental'. However, it does acknowledge some positive aspects like good experimental results on WikiText-103, so it's not entirely negative. The politeness score is 20 because the language is professional and constructive, offering specific recommendations and avoiding harsh language. The reviewer uses phrases like 'The writing is not very clear' instead of more critical alternatives, and provides detailed feedback, which is helpful and respectful. The tone is direct but not rude, maintaining a balance between critique and politeness typical of academic peer reviews.",-30.0,20.0
"The authors claim contributions in three areas:
1) Learning representations on physiological signals. The proposed approach  uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms 
 LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else?

2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. 

3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. 

The experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the “transference” arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare “hospitals”. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. 

It is quite hard to argue from what’s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 “makes sense” as stated in the paper? What is the point that’s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability?

Overall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ","The sentiment score is slightly negative (-30) because while the reviewer finds the problem interesting, they express several concerns and criticisms about the paper's methodology and experimental arguments. They use phrases like 'needs more details', 'needs work', and 'skeptical', indicating areas that require improvement. However, it's not entirely negative as they acknowledge the interesting nature of the problem.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'It would be good to', 'I recommend', and 'Can the authors explain', which are polite ways of suggesting improvements. The reviewer also acknowledges positive aspects, such as finding a part of the paper 'particularly' interesting. While critical, the language is not harsh or rude, but rather aimed at improving the paper.",-30.0,50.0
"This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. 

The authors suggest that one can predict future frames from a vector comprised of an observation encoding and an action. To train the model, they suggest using a linear combination of three different losses: (1) an adversarial loss that encourages the generated sample to look similarly to training data, (2) an InfoGAN-inspired loss that is supposed to maximise mutual information between the conditioning (e.g. action) and the generated sample, and (3) a content loss, taken to be the mean-squared error of the prediction and ground-truth in the VGG feature space.

The major contribution of this work seems to be using these three losses in conjunction, while doing conditional frame prediction at the same time. While interesting, there exist very similar approaches that also use adversarial losses [1] as well as approaches using different means to reach the same goal [2, 3]. None of these are mentioned in the text, nor evaluated against. It is true that [1] is not action-conditional, but adding actions as conditioning could be a simple extension.

Experimental section consists of an ablation study, which evaluates importance of different components of the loss, and a qualitative study of model predictions. With no comparison to state of the art (e.g. [1, 3]), it is hard to gauge how valuable this particular approach is. 
The qualitative evaluation starts with §4.4¶1 “we follow the customary GAN literature to include some qualitative results for illustration”, as if there was no other reason for including samples than to follow the custom. Since the paper is about action-conditional prediction, it would be interesting to see predictions conditioned on the same initial sequence but different actions, which are not present, however. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting.

The paper is clearly written, but some claims in the text are not supported by any citations (e.g. §1¶2 “More recently, several papers have shown that forward modelling…” without a citation).  Some claims are misleading (e.g. §1¶3 says that by using adversarial training we don’t need to use task-specific losses and it does not put constraints on input modality. While true, using MSE loss is equally general). Some other claims are not supported at all or may not be true (e.g. §3.2¶1 “ResNet … aims at compressing the information in the raw observation” - to the best of my knowledge, there is no evidence for this).

To conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements. I recommend to reject this paper.

[1] Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., & Levine, S. (2018). Stochastic Adversarial Video Prediction. CoRR, abs/1804.01523.
[2] Eslami, S.M., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N.C., King, H., Hillier, C., Botvinick, M.M., Wierstra, D., Kavukcuoglu, K., & Hassabis, D. (2018). Neural scene representation and rendering. Science, 360, 1204-1210.
[3] Denton, E.L., & Fergus, R. (2018). Stochastic Video Generation with a Learned Prior. ICML.
[4] Buesing, L., Weber, T., Racanière, S., Eslami, S.M., Rezende, D.J., Reichert, D.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., & Wierstra, D. (2018). Learning and Querying Fast Generative Models for Reinforcement Learning. CoRR, abs/1802.03006.","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer recommends rejecting the paper, stating that the approach is not novel, the experimental evaluation is lacking, and the text contains unsupported statements. They criticize the lack of comparison to state-of-the-art methods and point out several weaknesses in the paper's claims and methodology. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'While interesting...' and 'The paper is clearly written, but...' which soften the criticism. However, the overall message is still quite negative, which prevents a higher politeness score.",-80.0,20.0
"The paper aims at justifying the performance gain that is acquired by the use of ""composite"" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).

I found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.

Example from the Abstract:

""The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from other’s intelligence and diligence, and the other is saving the efforts in data preparation and resources
and time in training""

The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., ""components"") in the input of a network then you have ""more information"", and this cannot be bad. Here are the corresponding claims in the Abstract:

""we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.""

""if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved.""

However, this argument seems to be just about expressiveness; adding more features can be statistically problematic. 

Furthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.

Finally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.


The motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don't see any connection between this and the use of pre-training.

Other examples unclear statements from the intro:

""One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.""

""Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once.""

There are many typos in the paper including this one about X for the XOR function:
""Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR""


","The sentiment score is -70 because the reviewer expresses significant criticism throughout the review. They describe the paper as 'lacking in terms of writing and in terms of clarity', question the validity of the main arguments, and point out numerous issues with examples and statements. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their criticism is direct and unmitigated. They use phrases like 'lacking', 'invalid', and 'simplistic' without softening the blow. The reviewer also doesn't offer any positive comments or constructive suggestions for improvement, which contributes to the overall negative tone. The presence of phrases like 'I found' and 'I don't see' slightly mitigates the harshness, preventing an even lower politeness score.",-70.0,-20.0
"This is a well written paper which proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to existing Bayesian filters, the paper also proposes two different versions of the [differentiable] Unscented Kalman Filter. Performance of the different filters and noise models is evaluated on two real-world robotic problems: Visual Odometry and visual tracking of an object pushed by the robot.
While the general idea of learning the noise variances through backpropagation are straightforward extensions of existing work on differential Bayesian filters, the questions that the paper explores are important to make end-to-end learning of Bayesian filter more common. The results will help future research select the correct differential filter for their use case, and insight in potential benefits (or lack thereof) by learning heteroscedastic or homoscedastic process noise, and/or observation noise.
A downside is that the paper does not further explore how to weigh different loss terms which are apparently important to successfully train such models. Also unfortunate is the footnote which states that the current results are incomplete and will be updated, hence as a reviewer I am not sure which results and conclusions are valid right now.


Pros:
+ clearly written
+ useful experiments for those seeking to select a differential Bayesian filter, and learning (heteroscedastic) noise from data.
+ experiments on real-world use cases rather than toy problems

Cons:
- Incomplete experiments according to footnote, thus results and conclusions might change after this review.
- Unclear what the effect of the selected process / observation model is on the learned noise


Below are more detailed comments and questions:
* p6. Footnote: ""due to time constraints, ..., results will be updated"" Is this acceptable? I have never seen such a notice when reviewing. So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review?
* If I understand correctly, the paper 'only' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. I did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models.
* I would have liked to see in the paper more explanation on how the process and observations models were selected and validated  in the experiments, since I expect that the validity of these functions affects the learned noise variances. Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data?
* Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood.

* Section 1:
	* ""Our experiments show that ... "" This may be a matter of taste, but I did not expect to see the main conclusions already in the introduction. They should appear in the abstract to help out the quick reader. In the introduction, it appears as if you are talking about some separate preliminary experiments, and which you base some conclusions that will be used in the remainder of this paper.

* Section 3:
	* So, mostly empirical study, since heteroscedastic noise models were already used?
	* ""Previous work evaluated ... "" please add citations

* Section 4.1:
	* ""train a discriminative neural network o with parameters wo to preprocess the raw sensory data D and thus create a more compact representation of the observations z = o(D;wo)."" At this point in the paper, I don't understand this. How is z learned, via supervised learning (what is the target value for z)? Or is z some latent representation that is jointly optimized with the filters? This only became somewhat clearer in Sec. 5.2 on p.8 where it states that ""We ... train a neural network to extract the position of the object, the contact point and normal as well as ..."". So if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?).

* Section 4.2:
	* ""we predict a separate Qi for every sigma point and then compute Q as the weighted mean"" → So, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points?

* Section 4.3:
	* Equation 14: inconsistent use of boldface script: should use bold sigma_t, and bold l_t ?
	* ""In practice, we found that during learning ... by only increasing the predicted variance"" →  This is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn't we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason?

* Section 5.1.1
	* ""... of varying length (from 270 to over 4500 steps) ..."" it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond.

* Section 5.1.2:
	* Table 1: How are the parameters of the filters in the ""no learning"" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the 'no learning' parameters served as the initial condition for the learning approaches?
	* Table 1, first row column Q+R: ""0.2"" → Is there a missing zero here, i.e. ""0.20""? Otherwise, the precision of reported results in this table is not consistent. Hard to say: is the mean of R+Q 0.2, and slightly lower than R+Qh, or could it be as high as 0.24 ?
	* ""learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF"". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error).

* Section 6: 
	* ""Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise."" I don't see on what presented results these conclusions were drawn, as this is the first time the word ""outlier"" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.
","The sentiment score is 60 (positive) because the review starts with praise, calling it a 'well written paper' and highlighting its usefulness and real-world applications. The reviewer also lists several pros. However, it's not extremely positive due to some cons and critical comments mentioned. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames most negative points as suggestions for improvement rather than harsh criticisms. The reviewer also acknowledges potential limitations of their own review due to incomplete results. The language is professional and courteous throughout, with phrases like 'I would have liked to see' and 'Is it possible to add' when making suggestions.",60.0,80.0
"It has previously been observed that training deep networks using large batch-sizes leads to a larger generalization gap compared to the gap when training with a relatively small batch-size. This paper proposes to add noise sampled from diagonal ""empirical"" Fisher matrix to the large batch gradient as a method for closing the generalization gap. The authors motivate the use of empirical Fisher for sampling noise by arguing that the covariance of gradients from small batch-sizes can be seen as approximately equal to a scaled version of the Fisher matrix. It is then pointed out that using the Fisher matrix directly to sample noise could in principle close the generalization gap but would lead to slow converegence similar to SGD with a small batch-size. The authors then claim that the convergence speed is better when noise is sampled from the diagonal Fisher matrix instead of the full Fisher matrix. This claim is proven in theory for a convex quadratic loss surface and experiments are conducted to empirically verify this claim both in the quadratic setting are for realistic deep networks. Finally an efficient method for sampling noise from the diagonal empirical Fisher matrix is proposed.

Comments:
I think the paper is very well written and the results are presented clearly. In terms of novelty, I found the argument about convergence using diagonal Fisher being faster compared with full Fisher quite interesting, and its application for large batch training to be insightful. 

As a minor comment, for motivating theorem 3.1, it is pointed out by the authors that the diagonal Fisher acts as an approximation of the full Fisher and hence their regularization effects should be similar while convergence should be faster for diagonal Fisher. As a caveat, I think the authors should also point out that the convergence rate would be best when C is set to 0 in the result of the theorem. This implies no noise is used during SGD updates. However, this would imply the regularization effect from the noise will also vanish which would lead to poor generalization. 


However, there is a crucial detail that makes the main argument of the paper weak. In the main experiments in section 4.3, for the proposed large batch training method, the authors mention that they use a small batch-size of 128 for the first 50 epochs similar to Smith et al (2017) and then switch to the large batch-size of 4096, at which point, the learning rate is linearly scaled proportional to the large batch-size with a warmup scheme similar to Goyal et al (2017) and ghost batch normalization is used similar to Hoffer et al (2017). The former two tricks have individually been shown on their own to close the generalization gap for large batch-size training on large datasets like ImageNet. This paper combines these tricks and adds noise sampled from the diagonal Fisher matrix on top when switching to large batch-size after epoch 50 and reports experiments on smaller datasets-- MNIST, Fashion MNIST and the CIFAR datasets. Finally, the accuracy numbers for the proposed method is only marginally better than the baseline where isotropic noise is added to the large batch-size gradient. For these reasons, I do not consider the proposed method a significant improvement over existing techniques for closing the generalization gap for large batch training.

There is also a statement in the paper that is problematic but can be fixed by re-writing. In the paper, empirical Fisher matrix, as termed by the authors in the paper, refers to the Fisher matrix where the target values in the dataset is used as the output of the model rather than sampling it from the model itself as done for computing the true Fisher matrix. This empirical (diagonal) Fisher matrix is used to sample noise which is added to the large batch gradient in the proposed method. It is mentioned that the covariance of the noise in small batch SGD is exactly same as the empirical Fisher matrix. This claim is premised on the argument that the expected gradient (over dataset) is unconditionally roughly 0, i.e., throughout the training. This is absolutely false. If this was the case, gradient descent (using full dataset) should not be able to find minima and this is far from the truth. Even if we compare the scale of expected gradient to the mini-batch gradient (for small batch-size), the scale of these two gradients at any point during training (using say small batch-size SGD) is of the same order. I am saying the latter statement from my personal experience. The authors can verify this as well.

Overall, while I found the theoretical argument of the paper to be mostly interesting, I was dissapointed by the experimental details as they make the gains from the proposed method questionable when considered in isolation from the existing methods that close the generalization gap for large batch training.","The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'very well written', 'results are presented clearly', 'interesting' theoretical arguments), they ultimately express significant disappointment with the experimental details and question the paper's main contributions. The reviewer states that the proposed method is not a 'significant improvement over existing techniques' and that the gains are 'questionable'. This criticism outweighs the initial positive comments, resulting in a moderately negative overall sentiment.

The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, even when expressing criticism. They use phrases like 'I think', 'As a minor comment', and 'I found' to soften their critiques. The reviewer also acknowledges positive aspects before presenting criticisms. However, the score is not higher because the criticism, while politely expressed, is still direct and substantial, particularly in the final paragraph where the reviewer states they were 'disappointed' by the experimental details.",-50.0,50.0
"This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution.

Strengths:
- The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter.
- The results show the good behavior of the approach.

Weaknesses:

Method:
- One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this.
- In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors.
- While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.

Experiments:
- In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. 
- Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)).
- Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult.
- Many compression methods report results on ImageNet. This would make this paper more convincing.
- While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression.

Related work:
- It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning.
- The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez & Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates.
- The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error.

Summary:
I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.

After Response:
I appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue.
","The sentiment score is -50 because while the reviewer acknowledges some strengths of the paper, they ultimately conclude that there are 'too many points to be corrected or clarified and too many missing experiments for this paper to be accepted'. This indicates a generally negative sentiment, though not extremely so. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's strengths and using phrases like 'I appreciate' and 'I would encourage'. However, they also provide direct criticism without excessive softening, keeping the tone professional rather than overly polite. The reviewer maintains a balanced and constructive tone, offering specific suggestions for improvement while clearly stating their concerns.",-50.0,50.0
"Thank you for an interesting read.

The paper proposes an approximate training technique for energy-based models (EBMs). More specifically, the samples used negative phase gradient in EBM training is approximated by samples from another generator. This ""approximate generator"" is a composition of a decoder (which, with a Gaussian prior on latent variable z, is trained to approximate the data distribution) and another EBM in latent space. The authors show connections to WGAN training, thus the name EnGAN. Experiments on natural image generation and anomaly detection show promising improvements, although not very significant.

From my understanding of the paper, the main contribution of the paper comes from section 4, which proposes a latent-space MCMC scheme to improve sample quality. I have seen several papers fusing EBMs and GAN training together and to the best of my knowledge section 4 is novel (but with problems, see below). Section 3's recipe is quite standard, e.g. as seen in Kim and Bengio (2017), and in principle contrastive divergence also uses the same idea. The idea of estimating of the entropy term for the implicit distribution p_G with adversarial mutual information estimation is something new, although quite straight-forward.

Although I do agree that MCMC mixing in x space can be much harder than MCMC mixing in z space, since I don't think the proposed latent-space MCMC scheme is exact (apart from finite-time simulation, rejection...), I don't see theoretically why the method works.

1. The MCMC method essentially samples z from another EBM, where that EBM(z) has energy function -E_{\theta}(G(z)), and then generate x = G(z). Note here EBM(z) != p(z). The key issue is, even when p_G(x) = p_{\theta}(x), there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_{\theta}(x). You can easily work out a counter example by considering G is an invertible transformation. Therefore I don't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space.

2. Continuing point 1, with Algorithm 1 that only fits p_G(x) towards p_{\theta}(x), I am confident that the negative phase gradient is still quite biased. Why not just use the latent-space MCMC sampler composited with G as the generator, and use these MCMC samples to train both the decoder G and the mutual information estimator?

3. I am not exactly sure why the gradient norm regulariser in (3) make sense here? True that it would be helpful to correct the bias of the negative phase, but why this particular form? We are not doing WGAN here and in general we don't usually put a Lipschitz constraint on the energy function. I've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN, but most of them are just empirical observations...
Also the Omega regulariser is computed on which x? On data? Do you know whether the energy is guaranteed to be minimized at data locations? In this is that appropriate to call Omega a regulariser?

The presentation is overall clear, although I think there are a few typos and confusing equations:

1. There should be a negative sign on the LHS of equation 2.
2. Equation 3 is inconsistent with the energy update equation in Algorithm 1. The latter one makes more sense.
3. Where is the ratio between the transition kernels in the acceptance ratio equation? In general for Langevin dynamics the transition kernel is not symmetric.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting read', 'promising improvements'), they raise several significant concerns and criticisms about the theoretical foundations and methodology. The overall tone suggests skepticism about the paper's contributions. The politeness score is moderately positive (60) as the reviewer uses polite language throughout ('Thank you for an interesting read', 'I do agree that...'), frames criticisms as questions or suggestions, and maintains a professional tone. However, they don't go out of their way to be overly courteous, keeping the focus on the technical content.",-20.0,60.0
"Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers.  While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was.  For example, how the optimization was done is not discussed at all.  At the same time, I'm not an expert in group theory, so it's possible that the paper has technical novelty or significance which I did not appreciate.  

Strengths: 

 -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.  

Weaknesses: 
  
-I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3).  

-The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references.  

  -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I'm missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper).  In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset.  

  -It's interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better.  For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it's important to confirm).  


Questions: 

-Section 3.1 makes reference to ""learning parameters"".  I assume that this is done in the usual way with backpropagation and then SGD/Adam or something?  

-How is it guaranteed that W is orthogonal in the learning procedure?  
","The sentiment score is -50 because the reviewer expresses several criticisms and weaknesses of the paper, such as finding it 'fairly weak experimentally' and 'difficult to determine what the exact algorithm was'. However, they do acknowledge some strengths and the possibility that they might have missed some technical novelty due to lack of expertise in group theory, which prevents the score from being more negative. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'I found' and 'I had a difficult time understanding' which soften the criticism. They also acknowledge their own potential limitations in understanding the paper. The language is not overtly polite, but it maintains a respectful tone throughout, hence a slightly positive score.",-50.0,20.0
"This paper trains an information retrieval (IR) model by contrasting the joint query-document distributions, p(q, d) with negative samples drawn from a resampling of the product of marginals, p(q) x p(d). They use a second discriminator to provide the re-weighting (I believe picking to top negative sample from the other model) and train this other model in a way that mirrors the first. They also attempt to point out some theoretical problems with a competing model, IRGAN, which uses a generator that is trying to model the joint.

While I like the proposal idea, I think the paper has too many problems to warrant publication. First, the story is very disappointing. The authors phrase most of the paper as a critique of IRGAN, but this critique falls short. Really this is more of a paper about where to get negative samples when training a model of the joint (or the log-ratio in this case). Using negative samples from real data with noise contrastive estimation [1] is found in numerous works in NLP [2][3], and has gained some recent attention in the context of representation learning [4][5]. The first algorithm proposed is essentially doing a sort of ranking loss on negative samples, which mirrors similar works [6]. In fact, the generator in IRGAN could be viewed as just a parametric / adaptive negative sampling distribution in the context of NCE for the ultimate purpose of learning an estimate of the log-ratio. The most interesting thing I think of this work here is the co-training, i.e., using another model to help re-sample, and I think this idea should be explored in more detail.

Second, the paper spends far too much time revisiting prior work than addressing their own model, doing more analysis, providing more insight.

Third, the paper is just poorly written. The notation is confusing, some of the equations are unclear (I have no idea how ""r"" is used in any of this), and the arguments of the baseline in IRGAN don't really doesn't make any sense.

Notes:
P1
I don't really follow why IRGAN is so central to this work. Good ideas aren't difficult to motivate, especially if empirically everything works out.
P2
I'm having trouble with claims, especially more recently, about GAN instability, particularly since numerous approaches [7][8] seem to have more or less solved the problem.

The use of ""|"" in G is awfully confusing.
P3
Almost 2 pages of unnecessary background

P4
Why are we using ""|"" in functions? What's wrong with "",""?
theta = \theta
I don't understand the point of the quote (in italics).
What happened to ""r"" in all of this?
The last two equations and their relationship could be more clear.

You use italics, so is this supposed to be a quote? But then you have a section which attempts to show this.
P5
I have no idea what's supposed to be going on in 5). The samples from the real joint don't factor in the generator gradient, or at least it's absolutely not clear that this pops out of the baseline? Then you switch from log (1 - x) to - log x and there's some claim about this violating the adversarial objective?

It took me more than a few reads to figure out what the equation at the bottom of P5 is doing: is this resampling? It's fairly unclear.

[1] Gutmann, Michael U., and Aapo Hyvärinen. ""Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.""
[2] Mnih, Andriy, and Koray Kavukcuoglu. ""Learning word embeddings efficiently with noise-contrastive estimation."" 
[3] Mikolov, Tomas, et al. ""Distributed representations of words and phrases and their compositionality.""
[4] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" 
[5] Hjelm, R. Devon, et al. ""Learning deep representations by mutual information estimation and maximization.""
[6] Faghri, Fartash, et al. ""VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.""
[7] Miyato, Takeru, et al. ""Spectral normalization for generative adversarial networks.""
[8] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. ""Which Training Methods for GANs do actually Converge?."" ","The sentiment score is -70 because the reviewer explicitly states 'the paper has too many problems to warrant publication' and provides several critical points. They mention disappointment with the paper's story, excessive focus on prior work, and poor writing quality. The few positive aspects ('I like the proposal idea') are heavily outweighed by the criticisms. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism ('the paper is just poorly written') and dismissive language ('I don't really follow why IRGAN is so central to this work'). The reviewer doesn't use overtly rude language but doesn't soften their criticisms either, leading to a somewhat impolite tone.",-70.0,-20.0
"This paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node
is NP-complete. This is an extension of the result of Blum and Rivest'88. The original theorem was proved for 
threshold activation units and the current paper proves the same result for ReLU activations. The authors do this
by reducing the 2-affine separability problem to that of fitting a neural network to data. The reduction is well 
written and is clever. This is a reasonable contribution although it does not add significantly to the current state of the art. 
  ","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper as a 'reasonable contribution' and praises the reduction as 'well written and clever'. However, they also note that it 'does not add significantly to the current state of the art', tempering the positive sentiment. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits without harsh criticism. They provide a balanced view, highlighting both strengths and limitations in a professional manner. The tone is constructive and objective, avoiding any rude or dismissive language.",50.0,60.0
"Summary: 
This paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.


Comments:

There is a previously known result quite related to this paper: 

Ishibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. 

Theorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. 

Combined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. 

For this reason, I am afraid that the main technical result is quite weak.

After Rebuttal:
I read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some value in the paper, they express significant concerns about the novelty and strength of the main technical result. The initial summary is neutral, but the comments section raises doubts about the paper's contribution, stating 'the main technical result is quite weak' and suggesting that the main result can be 'readily derived' from existing work. The 'After Rebuttal' section shows a slight improvement in the reviewer's opinion, but still agrees with another reviewer's concerns.

The politeness score is moderately positive (40) because the reviewer uses respectful and professional language throughout. They frame their criticisms as personal opinions ('I am afraid that...') rather than harsh statements of fact. The reviewer also shows willingness to consider the authors' rebuttal and adjust their opinion. However, the politeness is not extremely high as the criticism, while politely phrased, is still direct and substantial.",-20.0,40.0
"#update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.

------------------ 

The authors provide a study on learning to refer to 3D objects. The authors collect a dataset of referential expressions and train several models by experimenting with a number of architectural choices.

This is an interesting study reporting results on the effect that several architectural choices have generating referential expressions. Overall, while I appreciate all the experiments and results, I don't really feel I've learned something from this paper. 

First and foremost, the paper, from the title already starts to build up expectations about the 3d nature of the study, however this is pretty much ignored at the rest of the paper. I would expect the paper to provide  some results and insights regarding the 3D nature of the dataset and how this affects referential expressions, however, there is no experiment that has used this 3d-ness in any way. Even the representations of the objects are stripped down to essentially 2D (a single-view of a 3D object used to derived VGG features is as 3D as any image dataset used for similar studies, right?). 
My major question is then: why should all this research take place in a 3D dataset? Is it to validate that research like this is at all possible with 3D objects? 

Moreover, all interesting aspects of referential expressions are stripped out since the authors experiment only with this geometric visual property (which has again nothing to do with 3d-ness, you could totally get that out of images). An interesting study would be to have all objects in the same image and have referential expressions that have to do with spatial expressions, something that the depth or a different view of the of the object could play a role.

Given the fact that there are no technical innovations, I can't vouch for accepting this paper, since there has been quite a lot of research on generating  referential expressions on image datasets (e.g., Kazemzadeh., 2014 and related papers). However, learning to refer to 3D objects is a very interesting topic, and of great importance given the growing interest of training agents in 3D virtual environments, and I would really encourage the authors to embrace the 3d-ness of objects and design studies that highlight the challenges and opportunities that the third dimension brings.


Kazemzadeh et al.: ReferIt Game: Referring to Objects in Photographs of Natural Scenes","The sentiment score is -60 because the reviewer expresses significant concerns about the paper's contributions and novelty, stating they 'cannot increase my score' and 'can't vouch for accepting this paper'. They also mention that they haven't really learned something from the paper. However, it's not entirely negative as they do acknowledge it as an 'interesting study' and encourage further research. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I appreciate all the experiments' and 'I would really encourage the authors', which soften the criticism. The reviewer also provides constructive feedback and suggestions for improvement, which is a polite approach to criticism.",-60.0,20.0
"This paper proposed a new method for face completion using progressive GANs. The novelty seems very limited compared with previous methods. The results did not significantly outperform previous methods such as CTX in terms of visual quality. In addition, some of the features for the proposed method were not evaluated properly. 

1. The frequency attention module is not convincing. The visualization of the attention features look like normal feature in a neural network. Also, in Figure 8, the quality of results with and without FAM look very similar. These 4 images were selected from 3000 test images, but the difference is too small to show the benefit of FAM. 

2. In figure 8, it is unclear how the performance changes with each loss. Probably the results without L_bdy, L_rec, L_feat should be analyzed separately. 

3.  In figure 6, the results compared to CTX look similar. And the figure is too small to see the details. For example, from row 1, the result by CTX seems even better. 

4. How many images were used in the user study? Did each subjects evaluate the entire test set 3009 images? 
","The sentiment score is -60 because the review is predominantly negative. The reviewer states that the novelty is 'very limited', results did not 'significantly outperform' previous methods, and features were 'not evaluated properly'. They also point out several weaknesses in the paper's methodology and presentation. However, it's not entirely negative, as they acknowledge it as a 'new method', hence not the lowest possible score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use neutral language like 'seems', 'unclear', and phrase criticisms as questions or suggestions rather than direct attacks. However, the overall tone is more matter-of-fact than overtly polite, hence a slightly positive but not high score.",-60.0,20.0
"This manuscript applies transfer learning for protein surface prediction. The problem is important and  the idea is novel and interesting. However, the  transfer learning model is unclear. 
Pros:  interesting and novel idea
Cons:  unclear transfer learning model, insufficient experiments. 

Detail: section 4 describes the transfer learning model used in the work, but the description is unclear. It is unknown the used model is a new model or existing model. Besides, in the experiments, the proposed method is not compared to other transfer learning methods.  Thus, the evidence of the experiments is not enough. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the novelty of the idea, they express significant concerns about the clarity of the transfer learning model and insufficient experiments. The pros are outweighed by the cons in this review. The politeness score is moderately positive (50) as the reviewer uses neutral language and balances positive and negative feedback without using harsh or critical tones. They present their concerns as observations rather than direct criticisms, maintaining a professional and respectful tone throughout the review.",-20.0,50.0
"This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments:

1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.  

Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018

2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. 


Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017.

Johnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016.

Kim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016.

3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth? 
","The sentiment score is -30 because while the reviewer acknowledges some improvements over baseline methods, they express significant concerns about the novelty and comprehensiveness of the work. The phrase 'limited novelty' and critiques of the comparison methods indicate a generally negative sentiment, though not extremely so. The politeness score is 50 because the reviewer uses professional and respectful language throughout, starting with positive aspects before moving to criticisms. They use phrases like 'Please see the following comments' and pose questions rather than making blunt statements. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a neutral professional tone.",-30.0,50.0
"This paper solves Flappy bird by combining DQN and probabilistic programming. I think this is in general a good avenue to explore.

However I found the paper to be poorly written. For example, notation is not properly introduced, there are many mathematical mistakes and typos in the written text and citations. This makes it very hard to understand what is actually going on.

It is also not clear what is the probabilistic program and what are we conditioning on? What is the inference algorithm? Maybe it's useful to expand more on how this ties to the ""RL as inference"" framework (see e.g. Levine, 2018). It seems like we are doing rejection sampling where the condition is ""no collision"". As a result, I'm not sure whether sampling from prior is a competitive baseline.

For the DQN experiment, the learning curve seems very noisy in a way that it's unclear whether a fair conclusion can be drawn only from one run (as it appears to be done).

The experiments also feel a bit contrived to make a strong case for probabilistic programming + DQN.","The sentiment score is -50 because while the reviewer acknowledges that the paper explores a 'good avenue,' they express significant concerns about the paper's quality. They describe it as 'poorly written' with 'many mathematical mistakes and typos,' making it 'very hard to understand.' The reviewer also questions the methodology and experimental design. These criticisms outweigh the initial positive comment, resulting in a negative overall sentiment.

The politeness score is 20 because the reviewer maintains a professional tone throughout, avoiding harsh language or personal attacks. They use phrases like 'I think' and 'I found,' which soften the criticism. However, the review is direct in its criticisms without excessive politeness, hence a slightly positive but not highly polite score.",-50.0,20.0
"Summary.
The paper proposes a vehicle’s trajectory planner that iteratively predict next-step (longitudinal and latitudinal) position of an ego-vehicle. Instead of using a raw image, a set of handcrafted features (i.e., the status of traffic lights, route, roadmap, etc) are mapped onto a fixed-size of bird-eye view map, which is then fed into the recurrent neural network. Additional regularizing loss terms are explored for the robustness of the model. The effectiveness of the method is demonstrated in simulation and real-world experiment.

Strengths.
- Impressive demonstrations in simulation and real-world experiments.
- The paper is generally well-written and easy to follow.

vs. Existing motion planning approaches.
There exists a large volume of papers on vehicle motion planning, which has largely been explored for controlling self-driving vehicles. Some of them successfully demonstrated their effectiveness for navigating a vehicle in typical driving scenarios, including “slowing down for a slow car”.
A notable survey may include:

[1] Paden et al., “A survey of motion planning and control techniques for self-driving urban vehicles,” IEEE Transactions on intelligent vehicles, 2016. 

However, the paper provides neither any works of literature on existing motion planners nor any types of comparison with them. This makes hard to judge the proposed learning-based motion planner outperforms others including conventional optimization-based methods. 

Missing data collection details.
This work depends hugely on its own human-designated oracle-like map, which provides driving-related features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. Generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status). Section 6.2 should be explained more in detail.

A weak novelty of using intermediate-level input/output representation.
There exist similar approaches that utilized similar representations to determine a vehicle’s behaviour, examples may include:

[1] Lee et al., “Convolution Neural Network-based Lane Change Intention Prediction of Surrounding Vehicles for ACC,” IEEE ITSC 2017.
[2] We et al., “Modeling trajectories with recurrent neural networks,” IJCAI, 2017.

Missing evaluation details.
In Section 6.2, (though not mentioned) it seems that a training dataset is collected from 60-days of real-world driving (given the context). But, in the testing phase, it seems that the authors used a simulator to evaluate different driving scenarios with various initial condition (i.e., speed, heading angle, position, etc). Can authors clarify details of the evaluation environment?

Minor concerns.
A paragraph of contribution summary (in Introduction section) will help. 
Typos (e.g., Section 2 line 17: ‘off of’)","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths ('Impressive demonstrations', 'well-written'), they raise several significant concerns. These include lack of comparison with existing methods, missing data collection details, weak novelty, and unclear evaluation details. The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging strengths before criticisms, and phrases concerns as suggestions or questions rather than direct criticisms (e.g., 'Can authors clarify...?'). The reviewer also offers helpful suggestions for improvement, which contributes to the polite tone.",-20.0,60.0
"This paper studied the information bottleneck principle for deep learning. In the paper by (Schwatz-Ziv & Tishby 17'), it is empirically shown that the mutual information I(X;T) between input X and internal layers T decreases, which is called a compression phase. In this paper, the author found that the compression phase is not always happening and the shape of the curve of I(X;T) highly depends on the ""bining size"" which is used for estimating mutual information by (Schwatz-Ziv & Tishby 17'). Then the authors proposed to use a noisy DNN to make sure the map X->T is stochastic, then proposed a guaranteed mutual information estimator. Then some empirical results are shown.

I think the problem in (Schwatz-Ziv & Tishby 17') do exist and their result is highly questionable. However, I have some major question about this paper.

1. In this paper a noisy DNN was proposed. However, how do you choose the noise level \beta? If I understand correctly, the noise level plays a similar role of the bining size in (Schwatz-Ziv & Tishby 17'). Noise level goes to zero is similar to bining size goes to zero. I wish to see a figure about how different \beta affects the curve of I(X;T) (similar to Figure 1 but let \bet change). 

    In Figure 4(d) there is a plot showing different \beta will affect the mutual information, but the x-axis is ""weight"". I wonder that how the curve of mutual information change w.r.t \beta, if the x-axis is training epochs. Do your statement stable about \beta? 

2. I think Section 3 and Theorem 1 are interesting and insightful. But I notice that in Section 10 you mentioned that this will be a separate paper. Is it OK to put them together in this paper?

3. The paper by (Schwatz-Ziv & Tishby 17') has not pass a peer-review process and it is still a preprint. This paper is nothing but only saying some deficiencies of (Schwatz-Ziv & Tishby 17') (except Section 3 and Theorem 1 which I think should be an independent paper). I think such a paper should not be published as a conference paper before (Schwatz-Ziv & Tishby 17') pass a peer-review process.

So totally I think this paper should not be accepted by ICLR at this point. I think Section 3 and Theorem 1 should become an independent paper, and the DNN approach can be an application of the mutual information estimator.","The sentiment score is -60 because the reviewer expresses significant concerns about the paper and ultimately recommends against acceptance. They state that the paper 'should not be accepted by ICLR at this point' and raise several major issues. However, it's not entirely negative as they do find some parts interesting and insightful. The politeness score is 20 because the reviewer uses generally respectful language and frames criticisms as questions or suggestions rather than harsh statements. They acknowledge the value of some aspects of the work. However, the overall tone is still critical, preventing a higher politeness score.",-60.0,20.0
" Summary: 
%%%%%%%%%%%%%%%
The paper explores ways to adapt the learning rate rule through a new minimax formulation.
The authors provide regret bounds for their method in the online convex optimization setting.

Comments:
%%%%%%%%%%%%%%%
-I found the motivation of the approach to be very lacking.
Concretely, it is not clear at all why the minimax formulation even makes sense, and the authors do not explain this issue.

-While the authors provide regret guarantees for their method, the theoretical analysis does not reflect when is their approach  beneficial compared to standard adaptive methods. Concretely, their bounds compare with the well known bounds of AdaGrad. 
It is nice that their approach enables to extract AdaGrad as a private case. But again, it is not clear what is the benefit of their extension.

-Finally, the experiments do not illustrate almost any benefit of the new approach compared to standard adaptive methods.


Summary
%%%%%%%%%%%%%%%
The paper suggests a different approach to adapt the learning rate.
Unfortunately, the reasoning behind the new approach is not very clear.
Also, nor theory neither experiments illustrate the benefit of this new approach over standard methods.
","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the motivation is 'very lacking', the theoretical analysis doesn't show benefits over existing methods, and the experiments 'do not illustrate almost any benefit'. These are significant criticisms that suggest the paper has major flaws. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and doesn't use many polite phrases or softening language. Phrases like 'it is not clear at all' and 'Unfortunately, the reasoning behind the new approach is not very clear' are quite blunt criticisms. The reviewer doesn't offer much encouragement or positive feedback, which contributes to the slightly impolite tone.",-70.0,-20.0
"
This paper consider the connections between SGD and distributionally robust optimization. There has long been observed a connection between robust optimization and generalization. Recently, this has been explored through the lens of distributionally robust optimization. e.g., in the papers of Namkoong and Duchi, but also many others, e.g., Farnia and Tse, etc. Primarily, this paper appears to build off the work of Namkoong. 

The key connection this paper tries to make is between SGD and DRO, since SGD in sampling a minibatch, can be considered a small perturbation to the distribution. Therefore the authors use this intuition to propose a weighted version of SGD (WSGD) whereby high variance weights are assigned to mini batch, thus making the training accomplish a higher level of distributional robustness. 

This idea is tested on a few data sets including CIFAR-10 and -100. The results compare WSGD with SGD, and they show that the WSGD-trained models have a lower robust loss, and also have a higher (testing) accuracy. 

This is an interesting paper. There has been much discussion of the role of batch size, and considering it from a different perspective seems to be of interest. But the connection of the empirical results to the theoretical results seems tenuous. It’s not clear how predictions of the theory match up. This would be useful to understand better. More generally, a simpler presentation of the key results would be useful, so as to allow the reader to better appreciate what are the main claims and if they are as substantial as claimed. Overall the writing needs significant polishing, though this is only at a local level, i.e, it doesn’t obscure the flow of the paper. ","The sentiment score is 50 (slightly positive) because the reviewer describes the paper as 'interesting' and acknowledges its potential value, but also points out areas for improvement. The reviewer sees merit in the approach but feels the connection between theory and empirical results needs strengthening. The politeness score is 75 (fairly polite) because the reviewer uses respectful language throughout, acknowledging the paper's contributions while offering constructive criticism. Phrases like 'This is an interesting paper' and 'This would be useful to understand better' indicate a polite and professional tone. The reviewer also balances positive comments with suggestions for improvement, maintaining a courteous approach even when pointing out weaknesses.",50.0,75.0
"This paper proposes a distributed policy gradient method for learning policies with large, collaborative, homogeneous swarms of agents. 

Formalism / objective: 
The setting is introduced as a ""collaborative Markov team"", so the objective is to maximise total team reward, as expressed in equation (3). This definition of the objective seems inconsistent with the one provided at line (14): Here the objective is stated as maximising the agent's return, L_n, after [k] steps of the agent updating their parameters with respect to L_n, assuming all other agents are static. I think the clearest presentation of the paper is to think about the algorithm in terms of meta-learning, so I will call this part the 'inner loop' from now on. 
Note (14) is a very different objective: It is maximising the return of an agent optimising 'selfishly' for [k] steps, rather than the ""collaborative objective"" mentioned above. This seems to break with the entire premise of collaborative optimisation, as it was stated above. 
My concern is that this also is reflected in the experimental results: In the food gathering game, since killing other agents incurs ""a small negative reward"", it is never in the interest of the team to kill other team-mates. However, when the return of individual agents is maximised both in the inner loop and the outer loop, it is unsurprising that this kind of behaviour can emerge. Please let me know if I am missing something here. 

Other comments: 
-The L_n(theta, theta_n) is defined and used inconsistently. Eg. compare line (9), L_n(theta_n, theta), with line below, L_n(theta, theta_n). This is rather confusing 
-In equation (10) please specific which function dependencies are assumed to be kept? My understanding is that \theata_n is treated as a function of theta including all the dependencies on the policies of other agents in the environment? 
-Related to above, log( pi_\theta_n ( \tan_n)) in line 16 is a function of all agents policies through the joint dependency on \theta. Doesn't that make this term extremely expensive to evaluate? 
-Why were the TRPO_kitchensink and A3C_kitchensink set up to operate on the minimum reward rather than the team reward as it is defined in the original objective? It is entirely possible that the minimum reward is much harder to optimise, since feedback will be sparse. 
-The survival game uses a discrete action space. I am entirely missing MARL baseline methods that are tailored to this setting, eg. VDN, QMIX, COMA etc to name a few. Even IQL has not been tried. Note that MADDPG assumes a continuous action space, with the gumble softmax being a common workaround for discrete action spaces which has not been shown to be competitive compared to the algorithms mentioned above. 
-Algorithmically the method looks a lot like ""Learning with Opponent Learning Awareness"", with the caveat that the return is optimised after one step of 'self-learning' by each agent rather than after a step of 'Opponent-learning'. Can you please elaborate on the similarity / difference? 
-Equation (6) and C1 are presented as contributions. This is the standard objective that's commonly optimised in MARL when using parameter sharing across agents.","The sentiment score is -50 because the review is predominantly critical, pointing out several inconsistencies and issues with the paper. The reviewer expresses concerns about the objective function, inconsistent definitions, and missing baseline comparisons. However, it's not entirely negative as the reviewer asks for clarification and suggests the author might be missing something, indicating openness to explanation. The politeness score is 20 because while the reviewer is direct in their criticisms, they use polite language such as 'Please let me know if I am missing something here' and 'Can you please elaborate on the similarity / difference?'. The tone is professional and constructive, focusing on the content rather than making personal comments. The reviewer also acknowledges potential misunderstandings on their part, which adds to the politeness.",-50.0,20.0
"This problem of interest in this paper is Curriculum Learning (CL), in the context of deep learning in particular. CL refers to learning a non-random order of presenting the training examples to the learner, typically with easier examples presented before difficult ones, to guide learning more effectively. This has been shown to both speed up learning and lead to better generalization, especially for more challenging problems. In this paper, they claim that their contribution is to decompose the problem of CL into learning two functions: the scoring function and the pacing function, with the role of the former being to estimate the difficulty of each training example and the latter to moderate the schedule of presenting increasingly more challenging examples throughout training.

Overall, I found it hard to understand from reading the paper what exactly is new versus what is borrowed from previous work. In particular, after reading Weinshall et al, I realized that they have already proposed a number of things that are experimented with here: 1) they proposed the approach of transfer learning from a previously-trained network as a means of estimating the ‘scoring function’. 2) they also distinguish between learning to estimate the difficulty of examples, and learning the schedule of decreasing difficulty throughout learning, which is actually stated here as the contribution of this paper. In particular, in Section 3 of Weinshall et al, there is a sub-section named “scheduling the appearance of training examples” where they describe what in the terminology of this paper would be called their pacing function. They experiment with two variants: fixed, and adaptive, which are very similar to two of the pacing functions proposed here.

Bootstrapping:
A component of this work that didn’t appear in Weinshall et al, is the bootstrapping approach to estimating the scoring function. In general, this involves using the same network that is being trained on the task to estimate the difficulty of the training examples. The authors explain that there are two ways to do this: estimate how easy each training example is with respect to the ‘current hypothesis’ (the weights of the network at the current step), and with respect to the ‘final hypothesis’, which they estimate if I understand correctly as the network at the end of training. The latter would necessitate first training the network in the standard way, and then using it to estimate how easy or hard each example is, and using those estimates to re-train the network from scratch using that curriculum. They refer to the former as self-paced learning and to the latter as self-taught learning. I find these names confusing in that they don’t really convey what the difference is between the two. Further, while self-paced learning has been studied before (e.g. Kuman et al), I’m not sure about self-taught learning. Is this a term that the authors here coined? If not, it would be useful to add a reference. 

Using easy / hard examples as judged by the current / final hypothesis:
When using the current hypothesis, under some conditions, Weinshall et al showed that choosing harder examples is actually more beneficial than easy examples, similar in spirit to hard negative mining. On the other hand, when using the final hypothesis to estimate examples’ difficulty, using a schedule of increasing difficulty is beneficial. Based on this, I have two comments: 1) It would therefore be useful to implement a version that uses the current hypothesis to estimate how easy each example is (like the self-paced scoring function) but then invert these estimates, in effect choosing the most challenging instead of the easiest ones as is done for anti-curriculum learning. This would be a hybrid between the current self-paced scoring function and anti-curriculum scoring function that would essentially implement the hard negative mining technique in this context. 2) It would be useful to comment on the differences between the self-paced scoring function used here, and that in Kumar et al. In particular, in this case using a curriculum based on this scoring function seems to harm training but in Kumar et al, they showed it actually increased performance in a number of different cases. Why does one work but the other doesn’t?

Experiments:
The experiments are presented in a subset of 5 classes from CIFAR-10 (also used by Weinshall et al.), but also in the full CIFAR-10 and CIFAR-100 datasets. They used both a small CNN (same as in Weinshall et al) as well as a VGG architecture. Overall, their results are comparable to what was previously known: using a curriculum computed by transfer leads to improved learning speed and final performance (though sometimes very slightly) compared to the standard training, and the training with a random curriculum. Further, the benefit is larger when the task is harder (as measured by the final vanilla-trained performance). By computing the distances between the gradients obtained from using a curriculum (via the transfer scoring function) and no curriculum confirms that these two training setups indeed drive the learning in different directions; an analysis similar to Weinshall et al. Also, since, as was previously known and they also observe, the benefit of CL is larger at the beginning of training, they propose a single-step pacing function that performs similarly to other pacing functions while is simpler and more computationally effective. The idea is to decrease only once the proportion of easy examples used in mini-batches, via a step function. Therefore at the start many easy examples are used, and after this threshold is surpassed, few easy examples are used.
 
Overall, I don’t feel the contribution of this paper is large enough to recommend acceptance. The main points that guided this decision are: 
1) The relationship with previous work is not clear. In particular, Weinshall et al seems to have already proposed a few components that are claimed to be the contribution of this paper, as elaborated on above. The authors should mention that the transfer scoring function was borrowed from Weinshall et al, clarify the differences between their pacing functions from those in Weinshall et al., etc. 
2) The usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently. An additional experiment is proposed above to add another ‘data point’ to this discussion. 
3) self-paced learning is presented as something that doesn’t work and wasn’t expected to work. However, in the past successes were shown with this method, so it would be useful to clarify the difference in setup, and justify this difference.
4) It seems that the experiments resulted to similar conclusions to what was already known. While it’s useful to confirm these findings on additional datasets, I didn’t feel that there was a significant insight gained from them.
","The sentiment score is -60 because the reviewer expresses significant concerns about the paper's contribution and novelty. They state that they 'don't feel the contribution of this paper is large enough to recommend acceptance' and list several major issues. However, it's not entirely negative as they acknowledge some useful aspects of the work. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I found it hard to understand' and 'It would be useful to' rather than harsh criticism. They also acknowledge the positive aspects of the work. However, the review is not overly polite, maintaining a neutral, critical stance appropriate for peer review.",-60.0,20.0
"This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., “evolutionary layer”. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.
The paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.
-	In page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. 
-	Given the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.
-	Based on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a naïve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. 
-	There are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. 
*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. 
*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.
*** Less Parameter Tuning. Again experiments could clarify this issue.
*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. 
-	To calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 ‘Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis’). 
-	One of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 ‘Massive Multimodality, Deception, and Genetic Algorithms’). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.
","The sentiment score is -50 because while the reviewer acknowledges that the paper 'touches an interesting topic and proposes a sound method', they also point out 'several issues with the paper' including 'ungrounded and untested claims' and 'many unclear points in the method'. The review lists multiple specific criticisms and areas for improvement, indicating a generally negative sentiment despite recognizing some positive aspects.

The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns, such as 'It is not clear why...' and 'There are no motivations or discussion for this.' The reviewer also acknowledges positive aspects of the work before presenting criticisms. However, the score is not higher because the review is direct in its criticisms without using many softening phrases or overtly polite language.",-50.0,50.0
"In the paper , the authors proposed the use of autoencoder for Model-X knockoffs. The authors proved that, if there exists latent factors, and if the encoders and the decoders can approximate conditional distributions well, the autoencoder can be used for approximating Model-X knockoff random variables: one can find relevant features while controlling FDR (Theorem 2.2).

I think the theoretical part is good, and the experimental results seem to be promising.

My concern is the gap between theory and practice. In the manuscript, the authors used VAE for approximating conditional distributions. The question is how we can confirm that the trained encoder and decoder satisfy the assumptions in Theorem 2.2. If the trained models violate the assumptions, the control of FDR is no longer guaranteed, which may lead to false discoveries. As long as this gap remains unfilled, we cannot use the procedure reliably: we always need to doubt whether the encoders and decoders are trained appropriately or not. I think this gap is unfavorable for scientific discovery where only rigid procedures are accepted.
How we can use the proposed procedure reliably, e.g. for scientific discovery? Is there any way to confirm that the encoders and decoders are appropriate? Or, is there any way to bypass the gap so that we can guarantee the FDR control even for inappropriate models?","The sentiment score is slightly positive (20) because the reviewer acknowledges the good theoretical part and promising experimental results. However, they express significant concerns about the gap between theory and practice, which prevents a higher positive score. The politeness score is high (80) as the reviewer uses respectful language throughout, phrases criticisms constructively, and asks thoughtful questions rather than making harsh judgments. They use phrases like 'I think' and 'My concern is' which maintain a polite tone while expressing their views.",20.0,80.0
"## Summary ##

The authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem.


## Assessment ##

I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. 

 I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.

They both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.


## Specific Comments and Questions ##

1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \sum{x_j} % 2$.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this?
4. How were the hyperparameters (learning rate, AdaGrad $\delta$, Adam $\beta_1, \beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently.
5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. 
6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.","The sentiment score is -50 because the reviewer expresses significant doubts about the paper's novelty and methodology. They state that the proposed method 'Cakewalk' is not different enough from existing methods to warrant acceptance, and they have concerns about the assumptions and experimental setup. However, it's not entirely negative as they acknowledge some positive aspects and offer constructive feedback. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'I don't think' and 'I have concerns' rather than harsh language. They also offer specific suggestions for improvement, which is a polite way to provide criticism. The reviewer balances negative feedback with positive acknowledgments and constructive suggestions, maintaining a respectful tone overall.",-50.0,20.0
"In the manuscript entitled ""Neural Causal Discovery with Learnable Input Noise"" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).  The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (""learnable noise risk"") with a flexible functional approximation (neural network).  Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.  The simulation and real data experiments are interesting and seem well applied.

A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.  In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.  Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.  In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.  Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).  ","The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting aspects of the work and its potential applications, but also expresses concerns about the manuscript's positioning and clarity. The reviewer describes the method as 'novel' and the experiments as 'interesting and seem well applied,' which are positive points. However, the concerns raised about the manuscript's relationship to different fields and the lack of clarity on the value of learnt relationships balance out the positive aspects, resulting in a mildly positive overall sentiment.

The politeness score is moderately high (60) because the reviewer uses respectful and professional language throughout. They begin by summarizing the work objectively and acknowledge its strengths before presenting their concerns. The concerns are framed as suggestions for improvement rather than harsh criticisms. Phrases like 'A concern I have is...' and 'it would seem valuable to...' indicate a constructive approach to feedback, maintaining a polite tone throughout the review.",20.0,60.0
"This paper studies the problem of optimizing non-decomposable metric in classification. This topic has been discussed in several recent works mainly under deterministic classifier context, the authors discuss the possibility of training a neural network and learn the model by gradient-based methods, which could result in randomized classifier; and conducted experiments to compare the performance with other existing methodologies. I have the following concerns after reading it.

1.The main idea of the paper has shown in other related works and the authors didn’t convince me why their work solves something that could not be solved in existing work. The related work section missed some relevant recent work including Ref[1], in which the method is also gradient-based and can be applied to neural networks. The well-behaved notion used in Definition 2 seems much weaker than the assumptions shown in Ref[1,2] to guarantee existence or uniqueness of the Bayes classifier, the authors could spend some effort to discuss why they require less assumptions.

2.For the theory part, all the convergence results are proved in an asymptotic way without further discussion in the sample complexity. This becomes problematic for this work because (as shown in eq (7)) mini batch size goes to infinity is an unrealistic assumption in neural network training. Also when the class is unbalanced, empirical mean converging to population also slows down significantly which is required in Eq (4) and other places. I would like to see more discussion on the sample complexity either theoretically or experimentally.

3.The experiments lack details for reproducing the results or generalizing the gain to other problems. For example, batch size, learning rate or how the size of the network influence the performance metrics. This information will be useful for others who want to apply the proposed method.
 
There are some minor formatting issues like the leading space in \citep. Please fix those.

Based on the above reasons, I’ll give this paper a 5.

[Ref 1] Yan, B., Koyejo, S., Zhong, K. & Ravikumar, P.. (2018). Binary Classification with Karmic, Threshold-Quasi-Concave Metrics. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:5531-5540
[Ref 2] Narasimhan, H., Kar, P., & Jain, P. (2015, June). Optimizing non-decomposable performance measures: a tale of two classes. In International Conference on Machine Learning (pp. 199-208).","The sentiment score is -50 because the review expresses several concerns and criticisms about the paper. The reviewer points out missing related work, weak theoretical foundations, and lack of experimental details. The overall score of 5 given by the reviewer also indicates a negative sentiment. However, it's not extremely negative as the reviewer acknowledges some positive aspects like the paper's attempt to apply the method to neural networks. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or rude expressions. They use phrases like 'I have the following concerns' and 'I would like to see more discussion,' which are polite ways to express criticism. The reviewer also provides constructive feedback and suggestions for improvement, which adds to the politeness. However, the score is not extremely high as the review is primarily focused on criticisms rather than praise.",-50.0,20.0
"The paper proposes a Siamese network model for mapping the space of natural compounds to a latent representation space; furthermore, it utilizes this representation to compute a similarity score between an already known compound and a newly discovered one. 

Some comments:
- Comparison with LDA requires more details: have you used the same number of topics for both small and large datasets?  How are you training it? 
- Page 4 the definition of micro-averaging is missing: “Micro-averaging for precision and recall is formally defined as: “ 
- Figure 3 does not show a well defined clusters of various compound families because it’s using same color for different families in some cases (according to the caption of Figure 3). I wonder if you can somehow show for which compound families the colors have been repeated or maybe show fewer compound families. 

The problem of finding similar compounds to a novel compound from NMR spectra is an interesting applied problem; however, technical novelty of the paper is not significant. Given the level of technical novelty, I believe the paper is more suitable for a more applied conference/journal in the fields of chemistry or bioengineering.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting nature of the problem, they express concerns about the technical novelty and suggest the paper might be more suitable for a different type of conference or journal. This implies the paper may not meet the standards for the current venue. The politeness score is moderately positive (50) as the reviewer uses neutral language and phrases their criticisms as suggestions or questions rather than direct criticisms. They also begin with a neutral summary of the paper and end with a constructive suggestion, which maintains a respectful tone throughout.",-20.0,50.0
"The proposed method is too simplistic, the model being succinctly described in less than one page with many errors in the given math expressions. Only the model is given. The optimization problem, as given in (1) is not explained. the authors need to stud the optimization problem, to derive its resolution, and to describe the obtained algorithm.

The authors’ main motivation is to “maps the input convolutional features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, …”. However, in the proposed method is operating in this way. The model is a simple transformation, and nothing ensures the compactness of the feature space, neither the separability of the classes.

It is difficult to understand the “arm CNNs with radial basis feature transformation”. There are two figure in the paper that seek to show this modification of CNN, but this is not enough because nothing is said in the text, which makes these images difficult to understand. Moreover, the figures have notations different than those in the  main body, such as F_{l-1} as opposed to F_{i,j,K}.

What is the transformation to be learned ? Is it T as given in the text before (1), or P as given in (3). In (1), it seems that it is a mix of both, namely T* = argmin_P ! Moreover, it is written “To enforce T’s positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T ′T”.  Decomposing T as T’T, means that T is very very special.

Equation (4) is not correct. The summation is on i, which is not in the expressions, but in the result with F_{i,j,K}.

With the exception of Tables 3 and 4, most experiments are on comparing the conventional versus the proposed method. The authors need to compare to other methods available in the literature on defense against adversarial attacks. Moreover, it is not clear why the author compare the proposed method to ADVT (adversarial training) in Table 4, and not in Table 3.

Some references are incomplete. For example, the second reference is missing the publication type, volume, … 
","The sentiment score is -70 because the review is predominantly negative. The reviewer criticizes the method as 'too simplistic', points out 'many errors', and states that key aspects are 'not explained' or 'difficult to understand'. They also highlight missing comparisons and incomplete references. The few neutral statements don't offset the overall negative tone. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the feedback. Phrases like 'too simplistic' and 'difficult to understand' are blunt. The reviewer doesn't use polite phrases or acknowledge any positives, which contributes to the slightly impolite tone. However, they do maintain a professional, academic style which prevents the score from being lower.",-70.0,-20.0
"The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning. My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small. 

Also, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost. 

In chapter 2, related work. The authors state that ""tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data. 

To me these two reasoning statements are not particularly convincing. One could also say:

NN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...

Actually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's 'interesting algorithm and direction', they express concerns about the limited number of datasets used and suggest additional comparisons. The reviewer also challenges some of the authors' statements, indicating a critical stance. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, starting with positive acknowledgment and framing criticisms as suggestions or personal opinions ('My concern is...', 'would be good to...', 'To me...'). The reviewer maintains a professional tone without using harsh or dismissive language, even when disagreeing with the authors' points.",-20.0,50.0
"This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.

Generally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.

1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.

2.  About the \epsilon_{1,2,3} in formula (3). \epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the ""Markov chains are considered"" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.

3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \Psi and \Delta? Why \Delta can be used to predict sentiment intensity in your experiment? It seems that \Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.

So I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.","The sentiment score is 50 (slightly positive) because the reviewer expresses general approval of the paper's idea and experiment, stating 'Generally I like the idea' and 'I think the experiment is interesting.' However, they also raise 'some major questions' and suggest improvements, indicating a mix of positive and critical feedback. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames their concerns as questions rather than direct criticisms. They also end on a positive note, stating 'the idea in this paper is interesting' and suggesting the paper could be accepted with revisions. The reviewer maintains a professional and courteous tone while providing detailed feedback.",50.0,75.0
"The paper proposes a modification of GAIL (Ho & Ermon, 2016) to make use of non-expert data. The non-expert data is used by training a classifier to classify between roll-outs of the current policy, expert demonstrations and non-expert demonstrations. Similar to GAIL, the policy is iteratively updated using TRPO with a cost that is given by the log probability of predicting the policy. The use of non-expert data acts as regularization in order to learn better features similar to universum prescription (Zhang & LeCun, 2017).  

The paper is well-written and very clear. The general problem setting is interesting, but I think it is of rather little significance, because I do not see many clear applications. The evaluation focuses on simulated robots, however gathering non-expert data on real robots would be very expensive, so the approach would not make a lot of sense here (even if we replace TRPO a more sample efficient rl method). The paper mentions the game of Go, but learning a policy on such large state spaces is not feasible without major modification and significant computational effort. However, the paper also mentions autonomous driving, which might be a more convincing application, because we can have a lot of demonstrations that we do not want to label as expert trajectories. I think the paper would profit a lot from having an experiment where the importance of making use of non-expert data becomes evident.

The approach seems sound, although I think that we can not expect much benefit from using the unlabelled data in the proposed way. By not making any assumptions on the non-expert data, they do not carry any information about the objective; the information that they carry about the system dynamics is not exploited for the RL update. Instead, the use of non-expert data is restricted to learning better features for discriminating between the agent and the expert. However, non-expert data is typically not cheaper than policy roll-outs and better features could also be learned by using more samples from the policy. The experiments also show only slight benefits, especially when comparing the final performance (instead of total returns) and accounting for the additional system interactions needed for generating the non-expert data. To make the comparison fairer, we could consider using a few more system interactions (variable K in the paper) per iteration for standard GAIL, so that the total number of function evaluations would match those of M-GAIL after a certain number of iterations. Especially if K is appropriately tuned, it is not clear whether we could still show an advantage of M-GAIL. It would also be interesting to show, whether we can benefit from using the policy roll-outs of previous iterations as non-expert data for the current iteration (in the traditional IL setting where no non-expert data is available a priori).

The main weakness of the paper is, that the novelty seems marginal. Instead of doing binary classification with cross-entropy loss, we're doing three-class classification with cross-entropy loss and use it for binary classification (by throwing away the auxiliary logit for predicting the non-expert class). Did I miss any other difference to GAIL? We can argue whether the policy objective is different (to me, H_\phi of M-GAIL corresponds to the discriminator of GAIL and the objective is exactly the same), however, even if we call it a minor modification, we would have very little novelty in the approach. As the paper does also not compensate for this with very good results or thorough theoretical analysis, I think that the contribution is too minor.

I do not see the point of section 4.4. and the related appendix A2. For all I understand, it proves that when using lambda=0 (standard GAIL, right?), the proof of Fu et al. (2018) for GAIL is valid (i.e. we learn a completely useless reward function that does not carry any additional information compared to the policy), and when using lambda!=0 we learn something different. I don't see the the purpose of this statement and I don't think that it needs to be proven. The paper argues, that for small lambda we can treat the discriminator logits as approximations of these (completely useless) reward functions--without providing any bound. As I do not see why this would be useful, I think the section should be removed. 

Minor:
Typo: ""[...]due to its dependent[sic] on the linearity of reward functions and good feature engineering"" 
","The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('well-written and very clear'), they express significant concerns about the paper's significance, novelty, and results. They state the problem is 'of rather little significance', the novelty seems 'marginal', and the experiments show 'only slight benefits'. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They avoid harsh or dismissive language, instead using phrases like 'I think' and 'It would be interesting to show'. However, the review is not overly effusive in its praise, maintaining a professional tone.",-40.0,50.0
"The paper proposes a method MixFeat for regularizing deep neural networks models, aiming at avoiding overfitting in training. The MixFeat interpolates, based on a careful selected mixing ratio, the hidden states (feature maps) of two randomly selected examples. Unlike MixUp, the MixFeat does not interpolate the labels of the two selected examples and the feature interpolation processes are conducted in the hidden space. Experiments on both Cifar10 and Cifar100 show that the networks with MixFeat improve their predictive accuracy as well as outperform networks with Mixup as regularizer.   

The paper is well written and easy to follow, and the experimental results on both Cifar10 and Cifar100 show promising results. Nevertheless, the idea of interpolating pairs of latent features for network regularization is not very novel. Additional, the experimental section is a bit weak in its current form. 

Main Remarks:

1.	MixFeat is very similar to Manifold-Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), where both feature maps and labels of a pair of examples are mixed, so Manifold-Mixup would be a valid comparison baseline to MixFeat. In addition, the proposed method is similar to SMOTE (where features are mixed in the input space). In this sense, performance of SMOTE may be a useful comparison baseline as well.
2.	In the experimental section, the choice of parameter for Mixup seems arbitrary to me and may not be the optimal one. For example, for the Cifar10 and Cifar100 datasets, the original paper highlights that Alpha equals to one is a better choice to obtain better accuracy for ResNet. Also, as highlighted from AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), MixUp is quite sensitive to the choice of Alpha and suboptimal Alpha value easily leads to underfitting. 
3.	Some claims are not well justified. For example, the authors claim that MixFeat can reduce overfitting even with datasets with small sample size, but did not provide any training cost or errors in Figure6 to support that claim. 
4.	MixFeat is closely related to MixUp, and I would like to see more experiments with MixUp as baseline in terms of regularization effect. For example, it would be useful to include MixUp in Figures 4 and 6.

Minor remarks: 

1.	What were the parameters for MixFeat used for Table 1?
2.	Is the proposed method robust to adversarial examples as shown in MixUp and ManiFold-Mixup?
3.	How the incorrect labels are generated in Section 3.2.1 is not very clear to me.
4.	Since MixFeat is similar to Mixup, I wonder if MixFeat has the problem of “manifold intrusion” as suggested in AdaMixUp when generating samples from image pairs?  How sensitive is MixFeat to the parameters Theta and Pi? Would learning mixing policies as suggested by AdaMixUp make sense here?

============after rebuttal============

I really appreciate the authors' rebuttal, which has addressed some of my concerns.
Nevertheless, I agree with the other reviewers about the main weakness of the paper. That is, why the proposed method works and what are its advantages over similar strategies, such as Mixup, AdaMixup and Manifold Mixup, are not clear.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'easy to follow', 'promising results'), they express several criticisms and concerns about the novelty, experimental design, and justification of claims. The overall tone suggests the paper needs significant improvements. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers constructive criticism, and acknowledges positive aspects. They use phrases like 'I would like to see', 'I wonder if', and 'I really appreciate the authors' rebuttal', which contribute to a polite tone. However, the criticism is direct, preventing a higher politeness score.",-20.0,60.0
"This paper study the model-based approach in deterministic low dimensional continuous control. As far as I am concerned and I understood, the main contribution of this paper is in substituting one-step-ahead prediction model with a multiple-step prediction model, resulting in a more accurate prediction model. I was not able to find points beyond this. I would be happy if the authors could clarify it. ","The sentiment score is slightly negative (-20) because the reviewer expresses some uncertainty about the paper's contributions beyond the main point they identified. The phrase 'I was not able to find points beyond this' suggests a lack of substantial content or novelty in the paper. However, the score is not deeply negative as the reviewer acknowledges a main contribution and seems open to clarification. The politeness score is moderately positive (50) due to the reviewer's use of respectful language. They express their concerns politely, using phrases like 'As far as I am concerned' and 'I would be happy if the authors could clarify'. The tone is constructive rather than critical, inviting the authors to provide more information rather than outright dismissing the work.",-20.0,50.0
"This paper empirically studies the robustness of equivariant CNNs to rotations as well as adversarial perturbations. It also studies their sample efficiency, parameter efficiency, and the effect of rotation- and adversarial augmentation during training and/or testing. 

The main findings are:
1) Rotation-equivariant networks are robust to small rotations, even if equivariance to small rotations is not directly built into the architecture
2) Applying rotational data augmentation increases robustness to rotations
3) Equivariant networks are more sample efficient than CNNs and outperform them for all dataset sizes.
4) Applying rotational data augmentation decreases robustness to adversarial perturbations, and this effect is more pronounced for GCNNs.

If true, this is a valuable addition to the literature. It is one of the first independent validations of claims regarding sample complexity and accuracy made by the authors of the various equivariant network papers, performed by a party that does not have their own method to promote. Many of the findings do not have an obvious explanation, so the data from this paper could conceivably prompt new theoretical questions and investigations.

The authors chose to highlight one finding in particular, namely that GCNNs become more sensitive to adversarial perturbations as they are trained on more heavily rotation-augmented data. However, this appears to be true for both CNNs and GCNNs, the difference being only in degree (see fig 4, 10, 11). This is not apparent from the text though, as e.g. the abstract notes that ""robustness to geometric transformations in these models [equivariant nets] comes at the cost of robustness to small pixel-wise perturbations"".

Since HNets, GCNNs and RotEqNets should be exactly equivariant to 90 degree rotations (and some others, perhaps), it is surprising that figure 1 shows a continuing decline in performance with bigger and bigger random rotations. If the network is made rotation invariant through some pooling layer at the end of the network, one would expect to see a decline in performance up to 45 degrees, followed by an increase back to baseline at 90 degrees, etc. 

Polar transformer networks achieve good results in fig. 1, but I wonder if this is still true for rotations around points other than the origin.

Since CNNs and GCNNs differ in terms of the number of channels at a certain number of parameters, and differ in terms of number of parameters at a certain number of channels, it could be that channel count or parameter count is the more relevant factor, rather than equivariance. So it would be good to make a scatterplot where each dot is a network (either CNN or GCNN, at various model sizes), the x-axis is parameter count (or in another plot, 2d channel count), and the y-axis corresponds to the accuracy. This can be done for various choices of augmentation / perturbation. The type of network (CNN or GCNN) could be color coded. If indeed the CNN/GCNN variable is relevant, that should be clearly visible in the plot, and similarly if the parameter count or channel count is relevant. One could also do a linear regression of accuracy or log-accuracy or something using CNN/GCCN, param-count, channel-count as covariates, and report the variance explained by each. 

In several plots, e.g. fig 4, 8, the y-axes do not have the same range, making it hard to compare results between subplots. 

The experiments have some weaknesses. For one thing, it seems like each accuracy value reported comes from a single training run. It would be much preferable to plot mean and standard deviation / error bars. Another weakness is that all experiments are performed on MNIST. Even just a simple validation of the main findings on CIFAR would significantly strengthen the paper.

Because of the limited scope of the experiments, it is not clear to me how generalizable and robust the experimental results are. With deep network performance it can be hard to know what the relevant hyperparameters are, and so we may well be reading tea leaves here.

It is also unfortunate that no explanation for the observed phenomena is available. However, it is conceivable that the findings presented in this paper could help researchers who are trying to understand adversarial attacks / robustness, so it is not a fatal flaw. I am certainly glad the authors did not make up some unsupported story to explain the findings, as is all too common in the literature these days.

Overall, I consider this a borderline paper, and am tending towards a reject. My main considerations are:
1. Uncertainty about generalizability
2. Uncertainty about usefulness to practitioners or theorists (admittedly, this is hard to predict, but no clear use-case is available at this point)
3. A lot of data, but no clear central finding of the paper","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('valuable addition to the literature', 'independent validations'), they ultimately lean towards rejecting it ('tending towards a reject'). They list several concerns and weaknesses, which outweigh the positive elements. The politeness score is moderately positive (60) as the reviewer uses respectful and constructive language throughout. They acknowledge the potential value of the work and provide detailed, specific feedback for improvement. The reviewer avoids harsh criticism and uses phrases like 'It would be good to...' and 'It is unfortunate that...' instead of more direct negative statements. They also commend the authors for not making unsupported claims. The overall tone is professional and aimed at improving the paper rather than dismissing it outright.",-20.0,60.0
"The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors rather than the dimension of the embedding vectors. 

The list of my concerns or cons of this paper is:

- For the main theorem, i.e., Theorem 1, 
a.) why is it intuitive that the size of the training dataset required for learning a norm constrained graph embedding is O(C|A|_2). This is not that intuitive to me. Later, the authors argue that graphs are usually sparse and average node degree is usually smaller than the embedding size, thus it is easily overfitting the training data. However, I would say, in practice, the positive training pairs are not restricted to 1-hop neighbors, but could also be 2 or more hops, in that case, it won't easily overfit. 
b.) the main result from the theorem is that the error gap of norm constrained embeddings scales as O(d^-0.5(lnn)0.5), but I did not see how this is related to the norms of the embedding vectors and how is this evidenced in the empirical studies? It might be better to show a plot of error gap vs. d and/or n. 
c.) how is this analysis related to the later claim that \lambda_r controls the model capacity of linear graph embedding?

- The linear graph embedding framework considered in this paper assumes that each node only has one set of embeddings, but in practice, one node usually has two sets of embeddings as context node or a center node. How would this affect the whole analysis and claims?

- How would the claims or analysis in this paper be generalized to non-linear graph embedding frameworks?

- For the experiments, 
a.) In Figure 3, the y label of (b) is missing, and the Average L2 norm of (c) cannot reflect the Generalization performance 
b.) In Figure 4(a), why after overfitting, we can still observe that the test accuracy increases?
c.) In Figure 5, why the test precision first increases and then decrease with more regularization?","The sentiment score is slightly negative (-30) because while the reviewer acknowledges the paper's main contribution, they list several concerns and criticisms without balancing them with positive aspects. The review focuses primarily on limitations and areas needing clarification or improvement. However, it's not entirely negative as the reviewer recognizes the paper's theoretical and empirical contributions. The politeness score is moderately positive (60) because the reviewer uses neutral, professional language throughout. They frame their criticisms as 'concerns' and use phrases like 'It might be better' instead of more direct criticisms. The reviewer also asks questions for clarification rather than making accusatory statements. The tone is constructive and aimed at improving the paper rather than dismissing it outright.",-30.0,60.0
"The paper studies fatigue monitoring of EEG driving simulator experiments using various EEG analysis algorithms, one also based on ranking. The data used was from a prior experiment. 

The paper is written in a rather confusing manner, which makes the assessment of originality and significance a hard task for the reviewer. A novel algorithm Bdrank (based on raking is defined) and compared to 2 other algorithms; unclear why with these and not with others. The paper ignores a large portion of the literature, starting with Kohlmorgen et al 2007, Blankertz group, Lee group etc. 
The results  are only somewhat interesting, no understanding of the underlying physiological processes is given. 

Overall, I consider the paper somewhat preliminary. ","The sentiment score is -50 because the review is generally negative, with phrases like 'written in a rather confusing manner', 'ignores a large portion of the literature', and 'somewhat preliminary'. However, it's not entirely negative as it acknowledges some interesting results. The politeness score is -20 because while not overtly rude, the language is quite direct and critical without much softening. Phrases like 'confusing manner' and 'hard task for the reviewer' come across as somewhat impolite. The reviewer doesn't use particularly polite language or offer much encouragement, focusing mainly on criticisms.",-50.0,-20.0
"The paper proposes a iterative approach at inference time to improve object detections. The work relies on updating the feature activations and perform new feed forward passes to obtain improved results. 

Pros:
(+) The idea of iterative inference is potentially effective
(+) The paper is well written and clear
(+) The authors show results on compelling benchmarks
Cons:
(-) Reported improvements are very small
(-) Important baselines are missing


First, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more FLOPs at test time. For N-stages, the approach requires xN more operations for forward passes  and xN for backward passes. This is a serious shortcoming as it adds compute time per image for the inference stage and cannot be parallelized. 

The authors show small improvements for AP on COCO. From their analysis, it seems that the biggest gains come from N=1 stages, while the improvement added for N>1 are miniscule (Table 1). Note that the authors show results on COCO minival (5k images) and from my experience there, it's expected to see a +/- 0.2% AP between different trained models of the same architecture. The authors report a +0.46% gain. 

In addition, the authors do not provide results for other baseline approaches that have similar FLOPs at test time, such as iterative bounding box regression and input scale augmentation. Note that both these approaches do not add any parameters and require no additional memory, but add to the FLOPs at test time. From my personal experience, test time augmentations can add +1.5% to the final performance. Concretely, look at Mask R-CNN arXiv Table 8 last two rows. Test time augmentations add 1.5% on top of an already enhanced model. Empirically, the better the model the harder it is to get gains from inference tricks! And still test time augmentations boost performance significantly.

Given the small gains and the lack of competing baselines, it is hard to make a case for accepting the paper. ","The sentiment score is -60 because while the reviewer acknowledges some positive aspects ('pros'), the overall tone is critical and skeptical. The reviewer points out significant shortcomings, such as small improvements, missing baselines, and increased computational cost. The conclusion suggests rejecting the paper, which strongly indicates a negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, acknowledging both pros and cons. They provide constructive criticism and explain their concerns in detail without using harsh or dismissive language. However, the review is not overly polite or praising, maintaining a neutral to slightly positive tone in terms of politeness.",-60.0,50.0
"This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.

It is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.

The proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.

Overall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me.","The sentiment score is -70 because the reviewer expresses significant concerns about the paper's assumptions, describing them as 'unrealistic' and 'too many'. The reviewer states that the assumptions are 'very far from true in any real neural networks' and that the paper 'makes too many unrealistic assumptions'. These criticisms indicate a strongly negative sentiment towards the paper's approach and validity. However, the score is not at the extreme negative end because the reviewer acknowledges the difficulty of the problem and shows some understanding for making strong assumptions in general. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'It is unclear' and 'I'm OK with' rather than more aggressive language. The reviewer also acknowledges the difficulty of the problem, which shows some empathy. However, the politeness is not extremely high as the criticism is direct and doesn't include many softening phrases or positive comments about the paper's strengths.",-70.0,20.0
"The submission explores Breiman's dilemma: training margin is not always a good predictor of test error.

In particular, the authors show that:

- For under-parametrized CNNs, the training prediction margin is a good predictor of the test error.
- For over-parametrized CNNs, the training prediction margin is not a good predictor of the test error.

Throughout the submission, I suspect that the authors compute the ""functional margin"", that is, the difference between the largest label score and the second largest score, for correctly classified examples. Functional margins ignore the smoothness of the underlying function, a critical factor for generalization. For instance, the function f(x) = 1[x > 0] has large functional margin, but any perturbation around the x-origin would drastically change the prediction. For this reason, I think the authors should consider the ""geometrical margin"" instead, which is unfortunately difficult to compute for general neural networks. Their theory tries to reflect on this issue by using spectrally-normalized bounds, but the practice ignores this issue completely (as far as I can tell).

Therefore, we may be looking at the wrong statistic to predict generalization error. Is Breiman dilemma solved by re-defining margin properly? Geometrical margin can be computed in closed-form for linear classifiers, so perhaps this would be a first step in this investigation.

","The sentiment score is slightly positive (20) because the reviewer acknowledges the authors' work and findings, but also expresses concerns and suggests improvements. The review starts by summarizing the authors' contributions, which is positive. However, the reviewer then points out potential issues with the methodology and suggests alternative approaches, indicating a mix of positive and critical feedback. The politeness score is moderately high (60) because the reviewer uses respectful language throughout, avoiding harsh criticism. They use phrases like 'I suspect' and 'I think' to soften their suggestions, and frame their concerns as opportunities for further investigation rather than outright flaws. The reviewer also asks a thought-provoking question at the end, encouraging further exploration of the topic in a constructive manner.",20.0,60.0
"[Summary]
This paper proposes an extension of the dual learning framework, with a guider network and multiple languages included: (1) Each language $i$ has a guider network $GN_i$, that can be used to reconstruct the source sentence from either the output of the encoder or the output of the decoder. (2) Multiple languages are used in this framework, where each language also has a $GN_i$  for guiding the training according to the reconstruction error. The authors work on MultiUN dataset to verify their algorithms.
 
[Clarity]
This work is not easy to follow. My suggestions to revise the paper are shown as follows:
(1) Please use the \begin{equation}…\end{equation} environment to clearly describe your framework and training objectives, with each notation, function and hyper-parameter clearly defined. Actually, I do not find the training objective function in this paper.
Besides, currently, in this paper, there are many undefined notations and typos, for example, (1) in section 3.1, first paragraph, what is the $n$? Then in Eqn.(1) ,what is $N$ and $M$? Also, it is very confusing to use subscripts $i$ and $j$ to distinguish the hidden states from the encoder and decoder. (2) What is the mathematical definition of $ISE_i$? (3) In page 5, 3rd line, “then ISD_i is used to reconstruct Si = GNi(ISE_i , \theta)…” Should the ISE_i be ISD_i?
(2) Please use \begin{algorithm}…\end{algorithm} to tell the readers how your framework works.
 
[Details]
1. The first question is “why this problem”. In the 3rd paragraph of page 1, you mentioned that “However, the best direction to update parameters heavily relies on the quality of sampled translations ... which may be far from real translations Y due to inaccurate translations existing in the sampled ones……” But in practice, dual learning as well as back-translation [ref1] works well for many language pairs. In particular, the dual learning and back-translation works for the unsupervised NMT [ref2], where no labeled data is available. Therefore, I am not fully convinced by this claim and then, the motivation of this work. What’s more, this paper does not work on standard WMT dataset, while previous dual learning and back-translation work on that most commonly used dataset. Therefore, the comparison between the guider network and dual learning are not fair.
2. I am not sure how the BDE in Eqn. (1) is related to the NMT translation quality. Any reference or theoretical/empirical proofs? 
3. It is hard to reproduce such a complex NMT system with NMT, GN and an RL scheduler. Any open-source code or any simple solutions?
4. Do you use a single-layer LSTM or a deep LSTM? Transformer [ref3] is the state-of-the-art NMT system. Why don’t you choose this system? Also, you do not work on WMT dataset to verify your GLF-2L (Table 1). Therefore, I cannot justify whether the proposed algorithm is efficient compared to the current NMT algorithms. I am not convinced by the experimental results.
5. The connection/difference between this work and (Tu et al 2017) should be discussed clearly, and you should implement (Tu et al 2017) as your baseline.  Besides, for the 3-language setting, no multilingual baseline is implemented.
 
[Pros & Cons]
(+) This paper tries to extend dual learning from word level to hidden state level;
(+) Multiple languages are involved in this framework;
(-) Experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used WMT datasets;
(-) The paper is not easy to follow. (See [clarify] part for details);
(-) Training process is a little complex; not easy to implement;
 
References
[ref1] Edunov, Sergey, et al. ""Understanding back-translation at scale."" EMNLP 2018
[ref2] Lample, Guillaume, et al. ""Phrase-Based & Neural Unsupervised Machine Translation."" EMNLP 2018
[ref3] Vaswani, Ashish, et al. ""Attention is all you need."" Advances in Neural Information Processing Systems. 2017.
 ","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('This paper tries to extend dual learning from word level to hidden state level' and 'Multiple languages are involved in this framework'), the overall tone is critical. The reviewer expresses significant concerns about the paper's clarity, experimental design, and convincingness of results. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'Please use...' and 'My suggestions to revise the paper are...', which are polite ways to offer criticism. However, the directness of some criticisms ('I am not fully convinced', 'It is hard to reproduce such a complex NMT system') prevents a higher politeness score. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the slightly positive politeness score.",-50.0,20.0
"Strength: 

Intuition that multiple sources of uncertainty are relevant to adversarial examples 

Weaknesses:

Threat model is unclear
No adaptive adversaries are considered
Attack parameters could be better justified

The intuition presented at the beginning of Section 4 is interesting. There are indeed multiple sources of uncertainty in machine learning, and the softmax probability only captures confidence partially. In particular, estimating the support of training data for a particular prediction and the density of that support is conceptually relevant to understanding and mitigating outlier test points like adversarial examples. 

Given that the approach is motivated as a defense (see Section 7 for instance), it needs to be evaluated in a realistic adversarial setting. In particular, it would greatly strengthen the paper if a clear threat model was specified. In your rebuttal, would you be able to formulate clearly what adversarial capabilities and goals were assumed when designing this defense? 

All experiments are performed on a binary variant of CIFAR-10. In addition, all pairs chosen for the experiments are well-separated: dogs are semantically further apart from airplanes than they are from horses. Would you be able to clarify in your rebuttal how the approach would generalize to multi-class classification? 

Perhaps the strongest limitation of the evaluation is that it does not consider adaptive adversaries. This goes back to the threat model point raised previously. Adaptive strategies will be put forward by adversaries aware of the defense being deployed (security should not be obtained through obscurity). For instance, the adversary could modify their attack to have it minimize the difference between logits on the training and adversarial data. This would help evading detection by the proposed scheme. However, results from Section 6 are shown for attacks that do not attempt to reduce the L1 difference between adversarial and training data. 

Some attack parameters could also be better justified. The naming convention for the perturbation sizes reads a bit imprecise and is perhaps more confusing than it is informative. Furthermore, could you explain in your rebuttal why epsilon is larger than 1.0 for the FGSM---when the inputs where normalized between 0 and 1?

Details: 

Page 1: Typo in “defence”
Page 2: Notation s_i is overloaded multiple times making it difficult to parse expressions
Page 2: Citation to Kull et al. should use \citep after “Beta calibration”
Page 3: Citation to Rozsa et al. should use \citep after “PASS score”
Page 5: Generally-speaking, it’s best to compute attacks at the logit layer rather than the probabilities to avoid numerical instabilities, which can then lead to gradient masking. However, the following sentence suggests the opposite: “The attacks were all white-box attacks and performed on the network which included a final softmax layer in its structure.”
","The sentiment score is -30 because while the reviewer acknowledges some strengths ('interesting intuition'), they primarily focus on weaknesses and limitations of the paper. The review points out several major issues like unclear threat model, lack of adaptive adversaries, and unjustified attack parameters. However, it's not entirely negative as it offers constructive feedback and suggestions for improvement. The politeness score is 50 because the reviewer uses respectful language throughout, poses questions rather than making blunt criticisms ('would you be able to clarify...'), and acknowledges the paper's strengths. The tone is professional and constructive, avoiding harsh language. However, it doesn't go out of its way to be overly polite or complimentary either, maintaining a neutral-positive tone.",-30.0,50.0
"Summary:
The paper introduces a new approach for interpreting deep neural networks called step-wise sensitivity analysis. The approach is conceptually quite simple and involves some interesting ideas, but I have some serious concerns whether the output produced by this method carries any meaning at all. If the authors were able to refute my concerns detailed below, I would raise my score substantially.


Strengths:
+ Potentially interesting heuristic to identify groups of feature channels in DNNs that encode image features in a distributed way


Weaknesses:
- Using the magnitude of the gradient in intermediate layers of ReLU networks is not indicative of importance
- No verification of the method on a simple toy example


Details:


Main issue: Magnitude of the gradient as a measure of importance.

I have trouble with the use of the gradient to identify ""outliers,"" which are deemed important. Comparing the magnitude of activations across features does not make sense in a convnet with ReLUs, because the scale of activations in each feature map is arbitrary and meaningless. Consider a feature map h^l[i,x,y,f] (l=layer, i=images, x/y=pixels, f=feature channels), convolution kernels w^l[x,y,k,f] (k=input channels, f=output channels) and biases b^l[f]:

h^l[i,:,:,f] = ReLU(b^l[f] + \sum_k h^(l-1)[i,:,:,k] * w^l[:,:,k,f])

Assume, without loss of generality, the feature map h^l[:,:,:,f] has mean zero and unit variance, computed over all images (i) in the training set and all pixels (x,y). Let's multiply all ""incoming"" convolution kernels w^l[:,:,:,f] and biases b^l[f] by 10. As a result, this feature map will now have a variance of 100 (over images and pixels). Additionally, let's divide all ""outgoing"" kernels w^(l+1)[:,:,f,:] by 10.

Simple linear algebra suffices to verify that the next layer's features h^(l+1) -- and therefore the entire network output -- are unaffected by this manipulation. However, the gradient of all units in this feature map is 10x as high as that of the original network. Of course the gradient in layer l-1 will be unaltered once we backpropagate through w^l, but because of the authors' selection of ""outlier"" units, their graph will look vastly different.

In other words, it is unclear to me how any method based on gradients should be able to meaningfully assign ""importance"" to entire feature maps. One could potentially start with the assumption of equal importance when averaged over all images in the dataset and normalize the activations. For instance, ReLU networks with batch norm and without post-normalization scaling would satisfy this assumption. However, for VGG-16 studied here, this is not the case.

On a related note, the authors' observation in Fig. 4b that the same features are both strongly positive and strongly negative outliers for the same class suggests that this feature simply has a higher variance than the others in the same layer and is therefore picked most of the time. Similarly, the fact that vastly different classes such as shark and German Sheppard share the same subgraphs speaks to the same potential issue.


Secondary issue: No verification of the method on simple, understandable toy example.

As shown by Kindermans et al. [1], gradient-based attribution methods fail to produce the correct result even for the simplest possible linear examples. The authors do not seem to be aware of this work (at least it's not cited), so I suggest they have a look and discuss the implications w.r.t. their own work. In addition, I think the authors should demonstrate on a simple, controlled (e.g. linear) toy example that their method works as expected before jumping to a deep neural network. I suppose the issue discussed above will also surface in purely linear multi-layer networks, where the intermediate layers (and their gradients) can be rescaled arbitrarily without changing the network's function.


References:
[1] Kindermans P-J, Schütt KT, Alber M, Müller K-R, Erhan D, Kim B, Dähne S (2017) Learning how to explain neural networks: PatternNet and PatternAttribution. arXiv:170505598. Available at: http://arxiv.org/abs/1705.05598","The sentiment score is -60 because the reviewer expresses 'serious concerns' about the paper's methodology and questions whether the output has 'any meaning at all'. The review points out significant weaknesses and issues with the approach, indicating a largely negative sentiment. However, the reviewer does mention some strengths and potential interest in the topic, preventing the score from being extremely negative. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I have trouble with...' and 'I suggest...' rather than harsh or dismissive language. The reviewer also offers constructive feedback and suggests ways for the authors to improve their work, which contributes to the slightly positive politeness score.",-60.0,20.0
"The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones. There is a budget issue here and one does not want to just keep the whole training set of all known previously observed classes but rather one wants to consider a maximum memory budget allowed to store what is necessary for an optimal incremental learning (typical examples, statistics etc). There is also a privacy issue preventing from storing original training samples.

This is a relevant problem that is has gain interest in the last few years. It is related to topics such as few shot learning and meta few shot learning (with respect ti the number of examples per class that are kept, which is limited) and somehow to budget learning . Yet these topics and associated references are surprisingly not evoked in the text.

The paper is rather well written but it strongly lacks precision about the proposed method. A description of the ICARL state of the art method is missing and would have been mandatory since the proposed work appears to build on iCARL method. Actually the description of the method is very short since the dedicated section (§4) is mainly used to describe a rather standard convolutional auto encoder architecture. At the end one tries to guess what the proposed method consists in. As far as i understand it is based on iCARL method where selected examples of past observed classes are not stored as is but in their encoded form (by the convolutional autoencoder). At the end my understanding of the proposed approach is that it consists in an incremental progress of a state of the art method, then an incremental work with limited innovation.

By the way i am not sure of the meaning of pseudo exemplar as used in the proposed method. Are these drawn following a distribution computed on training samples ? Or are these pseudo exemplar because you use reconstructed samples from encodings (by the CAE).  

When looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing. As written in the paper the proposed system outperforms iCARL in case the examples are encoded in the same dimension as original examples (hence no benefit on the storage side) but reaches similar performance when using less storage capacity. ","The sentiment score is slightly negative (-30) because while the reviewer acknowledges the relevance of the problem and that the paper is well-written, they express several criticisms. These include the lack of precision about the proposed method, missing description of the ICARL method, limited innovation, and unconvincing experimental results. The politeness score is slightly positive (20) as the reviewer uses generally respectful language, acknowledges some positive aspects, and phrases criticisms as observations rather than direct attacks. They use phrases like 'rather well written' and 'as far as i understand' which maintain a polite tone while expressing concerns.",-30.0,20.0
"This paper demonstrates that CNNs are more robust to class-relevant label noise. They argue that real-world noise should be class-relevant.

Pros:

1. The authors find a new angle to exploit robust learning with noisy labels.

2. The authors perform numerical experiments to demonstrate the effectiveness of their proposal. And their experimental result support their previous claims.

Cons:

We have two questions in the following.

1. Basic definition: in learning with noisy labels, there are two basic models. First, most research focuses on class-conditional noise (CCN) model [1]. Second, recent research explore a bit on instance-dependent noise (IDN) model [2, 3]. As far as I know, there is no class-irrelevant label noise and class-relevevant label noise. In CCN mode, people would like to use symmetric noise and asymmetric noise as a basic benchmark to conduct experiments.

2. Motivation: The authors want to claim CNNs are more robust to such realistic label noise than class-irrelevant label noise. However, they make one mistake. They do not have a clear definition about realistic label noise. In my mind, I believe Clothing1M [4] should be realistic label noise dataset.

By the way, in learning with noisy labels, there are two kinds of research. First, people propose new robust methods for CCN model. Second, people propose new robust methods for IDN models. Proposing new setting should be encouraged. However, the setting and conclusion should be reasonable.

References:

[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 1988.

[2] A. Menon, B. Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. Machine Learning, 2018.

[3] J. Cheng, T. Liu, K. Ramamohanarao, D. Tao. Learning with bounded instance-and label-dependent label noise. arxiv 1709.03768, 2017.

[4] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some pros of the paper, they raise significant concerns and questions about the basic definitions and motivation of the research. The reviewer points out potential mistakes and suggests that the conclusions may not be reasonable. However, the score is not deeply negative as the reviewer does recognize some positive aspects.

The politeness score is moderately positive (60) because the reviewer uses professional and respectful language throughout. They frame their criticisms as questions rather than direct accusations, and they acknowledge the pros of the paper before discussing the cons. The reviewer also provides helpful references and suggests that 'proposing new settings should be encouraged,' which shows a constructive attitude. The language is not overly formal or deferential, hence not scoring higher, but it maintains a polite and professional tone throughout.",-20.0,60.0
"The paper presents some interesting observations related to the connection between the universal adversarial attacks on CNNs and spectral properties. While most of the results are empirical, the authors present two theorems to justify some of the observations. However, the paper is poorly written and very hard to read. Rather than providing too many plots/results in the main paper (maybe use supplementary matl.), the empirical results should be better explained to help the readers. Similarly, the implications of the theorems are not really clear and bit hand-wavy.    

xxxxxxxxxxxxxx

It seems that the authors provided a generic response to all the reviewers and I am not sure if they acknowledge the lack of clarity and lot of hand-wavy explanations in the paper. This issue has been raised by other reviewers too and is quite critical for becoming a good paper worthy for ICLR. Therefore, I am unable to update my score for this paper. However, I do appreciate the comparison with Moosavi-Dezfooli et al. (CVPR'17), this is a good addition as suggested by another reviewer. ","The sentiment score is -50 because while the reviewer acknowledges some interesting observations and theoretical contributions, they express significant concerns about the paper's clarity, organization, and explanations. The phrase 'poorly written and very hard to read' indicates a strong negative sentiment. However, the reviewer does note some positive aspects, preventing the score from being extremely negative. The politeness score is 0 (neutral) because the reviewer's language is direct and professional without being overtly polite or rude. They express criticisms plainly but without harsh language. The reviewer's appreciation of the authors' response in the second paragraph shows a degree of politeness, but this is balanced by the continued criticism, maintaining an overall neutral tone.",-50.0,0.0
"This paper presents a method for predicting future frames of a video (or unseen views of a 3D scene) in a ""jumpy"" way (you can query arbitrary viewpoints or timesteps) and ""consistent"" way (when you sample different views, the scene will be consistent). They use a VAE that encodes the input video in a permutation invariant way, which is achieved by summing the per-frame latent vectors. Then, they sample a latent vector using a DRAW prior. This latent vector can then be used to render the video/scene from different times/viewpoints via an LSTM decoder. They test the model on several toy datasets: they compare to video prediction methods on a dataset of moving shapes, and 3D viewpoint prediction on a 3D MNIST ""dice"" dataset.

Pros:
- The idea of developing new methods for viewpoint and video synthesis that allow for ""jumpy"" and ""consistent"" predictions is an important problem.

- The paper is fairly well written.

- The design of the model is reasonable (it is a natural extension of VAE viewpoint/future prediction methods).

Cons:
- All of the experiments were done on toy datasets. These are also not well-established toy datasets, and seem tailored to debugging the model, so it is not particularly surprising that the method worked. Since the main contribution is not very novel from a technical perspective (it is more about designing a model from existing, well-established components), this is a significant limitation. 

- The paper suggests with experiments that GQN generates predictions that are less consistent across samples, but it is not clear exactly which design decisions lead to this difference. Why is this model more jumpy and consistent than GQN?

- The paper claims that JUMP trains more reliably than several video prediction methods in Figure 5. Yet, in the 3D viewpoint synthesis task, they suggest that JUMP had trouble with convergence, i.e.: ""We ran 7 runs for each model, and picked the best 6/7 runs for each model (1 run for JUMP failed to converge)."" This is confusing for two reasons. First, why was this evaluation protocol chosen (i.e. running 7 times and picking the 6 best)? If it was a post-hoc decision to remove one training run, then this should be clarified, and the experiment should be redesigned and rerun. Second, is the implication that JUMP is more stable than video prediction methods, but not necessarily more stable than GQN for viewpoint prediction?

- The paper should consider citing older representation learning work that deals with synthesizing images from multiple viewpoints. For example: 

M. Tatarchenko, A. Dosovitskiy, T. Brox. ""Multi-view 3D Models from Single Images with a Convolutional Network"". ECCV 2016.

- There is insufficient explanation of the BADJ baseline. What architectural changes are different?

- The decision to use DRAW, instead of a normal VAE prior, is unusual and not explained in much detail. Why does this improve the visual fidelity of the samples?

Overall:
The paper does not present enough evidence that this model is better at jumpy/consistent predictions than other approaches. It is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be OK, but it seems to be a fairly straightforward extension of existing models. ","The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('fairly well written', 'reasonable design'), the overall tone is critical. The reviewer lists more cons than pros and expresses significant concerns about the paper's limitations, particularly the use of toy datasets and lack of evidence for the model's superiority. The politeness score is 50 because the reviewer uses professional and respectful language throughout, balancing criticism with acknowledgment of positive aspects. They use phrases like 'The paper should consider' and 'This is confusing' rather than more harsh or direct criticisms. The reviewer also starts with pros before moving to cons, which is a polite approach in academic reviews.",-40.0,50.0
"The paper uses a number of deep learning approaches to analyse sets of Traffic data. However, as these sets of traffic data are never explained it is difficult to follow or understand what is going on here.

Some major comments:
1) Many of the key concepts in the paper are not discussed. The primary one would be that of what the two data sets contain. Without knowledge of this it is difficult to ascertain what is going on. 

2) Many of the processes used are not described in enough detail to either understand what is going on or to re-produce the work. Without this it is difficult to make headway wit the work.

3) It is not clearly articulated what the experiments performed are doing. For example, how have you applied the other techniques to this data?

4) Key terms are not defined. Such as Traffic Flow.

5) The English structure of the paper is poor with many mistakes. A thorough proof-reading is essential.

Some more specific points:
- ""with the larger road network, the difficulty of flow forecasting grows."" - This seems to be a consequence of the other ones not a challenge in it's own right.

- What is ""superiority""?

- ""Spatiotemporal traffic flow forecasting task is currently under a heated discussion and has attracted a large research population."" - evidence to back up this statement.

- Your contributions aren't contributions, but rather a list of what you have done.

- How does your related work relate to what you have done?

- Hard to parse ""To extract temporal relationships within the history traffic flows, we model this process as a layering structure with autoencoder as cell""

- Appendices B and C should be in the main paper.

- What is in x^{(1)}?

- ""When take the sparsity constrains into consideration"" - what are the sparsity constraints?

- How do you obtain the weights?

- Figure 2 should come much sooner as it relates a lot of the concepts together.

- ""On both datasets, we slice traffic flow information into 15 minutes windows, where 70% of data is for training, 10% for validation and remaining 20% for testing."" - Is that each 15 mins is split 70:10:20?

- Proof by example is not a proof.
","The sentiment score is -60 because the review is predominantly critical, pointing out numerous issues with the paper. The reviewer states that it's 'difficult to follow or understand what is going on' and lists several major concerns about lack of explanation, insufficient detail, and poor English structure. However, it's not entirely negative as the reviewer does provide constructive feedback for improvement. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'Some major comments:' and 'Some more specific points:' to organize their feedback, and provide specific suggestions for improvement rather than just criticism. The language is not overtly polite, but it's not rude either, maintaining a neutral to slightly polite professional tone.",-60.0,20.0
"This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE).  By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy.

Pros:
- The paper carefully constructs a method to estimate the mutual information between high dimensional variables and address the infinite mutual information issue by adding noise to the output. This is novel and theoretical sounding. 
- The paper connects the IB theory of DNN with weight decay, which is a novel founding.

Cons:
- The paper claims no literature has been doing IB functional on a layer-by-layer objective, however, see [1, 2] for the total correlation explanation work which is closely related to IB functional and they have also verified the effectiveness of layer-by-layer objective. 
- The scope of the paper is unclear. It seems that the paper is trying to convince two things to the readers: 1) The compression phase in DNN does exist 2) Layer-wise training helps to improve the accuracy. Although these two things are close related to each other (because they all requires to estimate the IB functional), it seems that neither these two conclusions are convincing. First, the compression phase is achieved only through weight decay; without weight decay, as shown in the paper, the compression phase is gone. Does that verify the incorrectness of IB theory of deep nets? Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. But if we really want to show the `effectiveness` of `layer-wise` training, one should compare the `layer-wise` training with `end-to-end` training while keeping the objective itself fixed. Otherwise, it is really difficult to draw conclusions about why the accuracy is improving, it is because of the objective changes or because of the `layer-wise` training.
- How does the beta (in IB objective) selected in the experiments for comparison? Do you use a validation dataset, and what is the final beta? If the paper fine-tune beta on the validation dateset, then the comparison of ""IB functional, only the first term"" and ""IB functional"" is unfair. 

[1]  Ver Steeg et al. Maximally Informative Hierarchical Representations of High-Dimensional Data. AISTATS 2015
[2]  Gao et al. Auto-Encoding Total Correlation Explanation. Arxiv 1802.05822.

[update] After carefully reading the response (also from other reviewers), I decide not to change my rating.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also list several significant concerns ('Cons') that outweigh the positives. The reviewer points out issues with the paper's scope, methodology, and comparisons, and ultimately decides not to change their rating after reading responses. This suggests an overall negative sentiment, though not extremely so. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They acknowledge the paper's strengths before presenting criticisms, and phrase their concerns as observations rather than attacks. The use of phrases like 'carefully constructs' and 'novel founding' in the pros section demonstrates politeness. Even in the cons section, the language remains constructive and academic, without personal attacks or harsh language.",-20.0,50.0
"This paper proposes a joint embedding model that aligns video sequences with sentences describing the context (caption) in a shared embedding space. With the space, various tasks such as zero-shot activity recognition and unseen video captioning can be performed. The problem tackled in this paper is interesting. However, the approach proposed is limited in novelty and there are some serious flaws in the experimental settings. So overall, this paper is not yet ready for publication. 

Pros:

•	The overall bidirectional encoder-decoder architecture for learning a shared embedding space is sensible. It is also interesting that adversarial training is introduced so that unlabelled data can be utilized. 
•	Additional annotations are provided to two activity recognition datasets, creating new benchmarks.
Cons
•	My first concern is the limited novelty of the work. Although I am not aware of a joint embedding learning model that has exactly the same architecture and formulation, the model is closely related to many existing ones both in zero-shot learning and beyond. More specifically,
o	The overall framework is similar to “correlational neural networks”, Neural Computation, 2016 by Chandar et al. This should be acknowledged.
o	The connections to CyclyGan and its variants for image-to-image style transfer is obvious, as pointed out by the authors.
o	More importantly, there are quite a few closely related zero-shot learning (ZSL) papers published recently. Although they focus on static images and class name, rather that image sequences and sentences, I don’t see any reason why these models cannot be applied to solve the same problem tackled in this paper. In particular, the autoencoder architecture was first used in ZSL in E. Kodirov, T. Xiang and S. Gong, ""Semantic Autoencoder for Zero-Shot Learning"", in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, Hawaii, July 2017. This work is further extended in Chen et al, “Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network”, cvpr18, now with adversarial learning. Similarly, variational autoencoder is adopted in Wang et al, Zero-Shot Learning via Class-Conditioned Deep Generative Models, AAAI 2018.  Note that the joint embedding spaces in these studies are the semantic spaces – attribute or word vector spaces representing the classes. In contrast, since the semantic modality is a variable-length word sequences, this is not possible, so a third space (other than the visual feature space or semantic space) is used as the embedding space. Beyond these autoencoder based models, there are also a number of recent ZSL works that use a conditional generative model with adversarial loss. Instead of learning a joint embedding space where the visual and text modalities are aligned and compared for recognition, these works use the text modality as condition to the generative model to synthesize visual features for the unseen classes followed by a conventional supervised classifier. The representative one of this line of work is Xian et al, “Feature Generating Networks for Zero-Shot Learning, cvpr18”.
o	In summary, there are too many existing works that are similar to the proposed one in one or more aspects. The authors failed to acknowledge most of them; moreover, it is not argued theoretically or demonstrated empirically, why combining different approaches together is necessary/making fundamental differences.

•	My second main concern is the experiment setting. This paper adopts a conventional ZSL setting in two aspects: (1) the visual features are obtained by a video CNN, I3D, which is pretrained on the large (400 or 600 classes depending on which version is used) Kinetics dataset. This dataset have classes overlapping with those in ActivityNet, HMDB and UCF101. So if these overlapped classes are used in the unseen class partition, then the ZSL assumption (the target classes are ‘unseen’) is violated. (2): The test data only contains unseen class samples. In practice, one will face a test set composed of a mix of seen and unseen classes. Under this more realistic setting (termed generalized ZSL in the ZSL community), a ZSL must avoid the bias towards the seen classes which provide the only visual data available during training. The two problems have been identified in the ZSl community when static images are considered. As a result, the conventional setting has been largely abandoned in the last two years and the ‘pure’ and ‘generalized’ settings become the norm; that is, there is no overlapping classes between the test classes and classes used to pretrain the visual feature extraction network; and both seen and unseen class samples are used for testing. The ZSL evaluation is only meaningful under this more rigorous and realistic setting. In summary, the experimental results presented in this paper are obtained under the wrong setting and the proposed model is not compared with a number of closely related ZSL models listed above, so it is not possible to judge how effective the proposed model is. 
","The sentiment score is -60 because the review is overall negative, stating the paper is 'not yet ready for publication' and highlighting 'serious flaws'. However, it does mention some positive aspects ('pros'), preventing an extremely negative score. The politeness score is 20 because while the reviewer uses professional language and provides detailed feedback, which is courteous, the tone is quite direct and critical. The reviewer acknowledges positive aspects and uses phrases like 'interesting' and 'sensible', but also uses strong negative language like 'serious flaws' and 'limited novelty'. The detailed explanations and suggestions for improvement indicate a level of respect and constructiveness, despite the overall negative assessment.",-60.0,20.0
"This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling.  The goal is to reduce the memory access and make efficient computation, in the meantime, to sustain good ASR performance.  Overall, the paper is well motivated and well written.  However, I have following concerns.

1. It is not clear from the paper whether both the training and inference are conducted on embedded devices or only the inference?  I assume it is the latter but can't find it explicitly mentioned in the paper.  

2. The exploration carried out in the paper is more on the system level and the novelty is not overwhelmingly significant.

3. My major concern is that the reported WERs on WSJ and phoneme classification accuracy are quite off.  20%-30% WERs for WSJ  do not seem to be usable in real applications.  Honestly, I don't even think this performance is better than well-trained GMM-HMM acoustic models using a Viterbi decoder.  Furthermore, there is no clear winners across the investigated architectures  in terms of performance.  One question is if one wants to deploy such an on-device system, which architecture shall be chosen?  

4. A more general comment on the work explored  in the paper.  First of all, the on-device memory issue puts a heavy constraint on the capacity of acoustic models, which will significantly hurt the modeling capability for the DNN-based acoustic models.  Deep learning acoustic models can outperform GMM-HMM because they can use large model capacity with very deep and complex architectures when a large amount of training data is available.  Second, for CTC, when the training data is limited,  its performance is far worse than the hybrid DNN-HMM model, let alone a pure end-to-end fashion without using external LM and dictionary.  If WFST-based decoders (composition of WFSTs of LM, dictionary and deblank/repetition) are used, then the memory issue will surface again. 
","The sentiment score is -30 because while the reviewer starts with some positive comments ('well motivated and well written'), they express several significant concerns about the paper's methodology, results, and overall contribution. The concerns outweigh the initial positive remarks, leading to a slightly negative overall sentiment. The politeness score is 50 because the reviewer uses polite and professional language throughout, even when expressing concerns. They use phrases like 'I have following concerns' and 'My major concern is' rather than using harsh or dismissive language. The reviewer also offers constructive feedback and asks questions for clarification, which contributes to the polite tone.",-30.0,50.0
"The paper introduces a relatively simple method for imitation learning that seems to be successful despite its simplicity. The method, SQIL, assigns a constant positive reward (r) to the demonstrations and zero reward to generated trajectories. While I like the connections between SQIL and SQL and the simplicity of the idea, I think there are several issues which connections with GAIL that are not discussed; some ""standard"" environments (such as Mujoco) that SQIL has not compared against the baselines. I believe the paper would have a bigger impact after some addressing some of the issues.

(
Update: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.

The Pong case is also quite interesting, although it seems slightly ""unfair"" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards.
)

1. The first issue is the similarity with GAIL in the ""realistic"" setting. Since we cannot have infinite expert demonstrations, there would always be some large enough network that could perfectly distinguish the demonstrations (assign reward to 1) and the generated policies (assign reward to 0). Therefore, it would seem to me that from this perspective SQIL is an instance of GAIL where the discriminator is powerful and expert demos are finite (and disjoint from generated trajectories, which is almost always the case for continuous control). In the finite capacity case, I am unsure whether the V and Q networks in SQIL does a similar job as the discriminator in GAIL / AIRL type algorithms, since both seem to extrapolate between demonstrations and generations?

2. Moreover, I don't think SQIL would always recover the expert policy even with infinite demonstrations. For example, lets think about the Reacher environment, where the agent controls a robotic arm to reach a target location. The expert demonstration is the fastest way of reaching the target (move minimum distance between joints). If we consider the MDP to have possibly very large / infinite horizon (only stops when it reaches the target), I could construct a hack policy that produces larger episodic reward compared to the expert. The policy would simply move back and forth between two expert demonstrated states, where it would receive 1 reward in the states for odd time and 0 reward for the states for even time. The reward would be something like 1 / (1 - \gamma^2) compared to the experts' reward which is \sum_{i=0..T} \gamma^{i} = (1 - \gamma^{T+1}) / (1 - \gamma). 

Some fix would be to set the reward for generated policies to be negative, or introduce some absorbing state where the expert will still receive the positive reward even after reaching the target (but that is not included in demonstrations). Nevertheless, a suitable reward prior seems to be crucial to the success of this SQIL, as with GAIL requiring reward augmentation.

3. Despite the above issues, I think this could be a very practical method due to its (perhaps surprising) simplicity compared to GAIL. However, the experiments only considered two environments that are not typically considered by GAIL; I believe SQIL would make a bigger impact if it is compared with GAIL in Mujoco environments -- seems not very implementation heavy because your code is based on OpenAI baselines anyway. Mujoco with image inputs would also be relevant (see ACKTR paper).

Minor points:
- What is the underlying RL algorithm for GAIL? It would seem weird if you use Q-learning for SQIL and TRPO for GAIL, which makes it impossible to identify whether Q-learning or SQIL contributed more to the performance. While GAIL used TRPO in the original paper, it would be relatively straightforward to come up with some version of GAIL that uses Q-learning. 
- Some more details in background for MaxEnt RL to make the paper more self contained.
- More details about the hyperparameters of SQIL in experiments -- e.g. what is \lambda?
- Did you pretrain the SQIL / GAIL policies? Either case, it would be important to mention that and be fair in terms of the comparison.
- Why does SQIL-11 perform worse than SQIL even though it is a tighter bound?
- wrt. math -- I think the anonymous comment addressed some of my concerns, but I have not read the updated version so cannot be sure.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('I like the connections between SQIL and SQL and the simplicity of the idea'), they also point out several issues and areas for improvement. The reviewer suggests that the paper would have a bigger impact after addressing these issues, indicating that the current version is not entirely satisfactory. The update section shows some improvement in the reviewer's opinion, but still maintains some reservations.

The politeness score is moderately positive (60) because the reviewer uses respectful and constructive language throughout. They acknowledge positive aspects, use phrases like 'I believe' and 'I think' to soften criticisms, and provide specific suggestions for improvement rather than just pointing out flaws. The tone is professional and aimed at helping the authors improve their work, rather than being dismissive or harsh.",-20.0,60.0
"TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.

* Summary

The manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\lambda\in (0,1)$ (sampled from a $\mathrm{Beta}(\alpha,\alpha)$ distribution).

A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.

A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.

I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.

* Major remarks

- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.
- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.
- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.

* Minor issues

- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.
- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.","The sentiment score is 80 (positive) because the reviewer expresses a very favorable opinion of the manuscript, stating 'In my opinion, a clear accept' and praising several aspects of the work. They found 'no flaw in the (two) proofs' and described the work as having a 'salient asset'. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive feedback and suggestions rather than harsh criticism. They use phrases like 'I would suggest' and 'I would like to read the authors' opinion', which are polite ways of offering recommendations. The reviewer also acknowledges the authors' work positively, which contributes to the polite tone. However, it's not extremely polite (100) as it maintains a professional, slightly formal tone rather than being overly deferential.",80.0,70.0
"This paper introduces a ""behavior module"" (BM) which is a small network that encodes preferences over actions and runs parallel to the fully connected layers of a policy. The paper shows this architecture working in Atari games, where the same policy can be used to achieve different action preferences over a game while still playing well. It also includes a thorough recap of past modular approaches. 

The motivation for the BM is that we may want deep networks to be able to decompose ""strategy"" and ""behavior"", where behavior may influence decisions without affecting the performance. In this framework, the BM is trained on a reward of correctness + personalized “satisfaction”.

The experiments model behavior as preferences over how many actions to play simultaneously. The trained BMs can be transferred to new tasks without finetuning. The ideas here also have some similarity to the few shot learning literature.

Comments on the experiments:
1. The Table 2  do not show a smooth interpolation between reward scaling and AMSR vs BD. This is surprising because the performance on the game should be highest went it is weighted to the most. This indicates to me that the results are actually high variance, the 0.8 vs 0.88 in stage 2 of 0.25r vs 0.5r means that is probably at least +/- 0.08 standard deviation. Adding standard deviations to these numbers is important for scientific interpretability.
2. I expect some BMs should perform much better than others (as they have been defined by number of actions to play at once). I would like to see (maybe in the appendix) a table similar to table 2 for for individual BMs. I currently assume the numbers are averaged over all BMs.
3. Similarly, I would like to see the BD for BM0 (e.g., if a policy is not optimized for any behavior, how close does it get to the other behaviors on average). This is an important lower bound that we can compare the other BD to. 
4. An obvious baseline missing is to directly weight the Q values of the action outputs  (instead of having an additional network)  by the designed behavior rewards. There is an optimal way to do this because of experimental choices. 

Questions:
1.For BM2, you write "" Up and Down (or Right and Left)"" did you mean ""Up and Right""? How can Up and Down be played at the same time?

Overall, this paper uses neuroscience to motivate a behavior module. However, the particular application and problem settings falls short of these abstract ""behaviors"". Currently, the results are just showing that RL optimizes whatever reward function is provided, and that architectural decomposition allows for transfer, which was already showed in (Devin 2017). An experiment which would better highlight the behavior part of the BM architecture is the following:
1. Collect datasets of demonstrations (e.g. on atari) from different humans.
2. Train a policy to accomplish the task (with RL)
3. Train BMs on each human to accomplish the task in the style of each human.
This would show that the BMs can capture actual behavior. 

The dialog examples discussed in the abstract would also be very exciting.

In conclusion, I find the idea interesting, but the experiments do not show that this architecture can do anything new. The abstract and introduction discuss applications that would be much more convincing. I hope to see experiments with a more complex definition of ""behavior"" that cannot be handcoded into the Q function.","The sentiment score is slightly negative (-20) because while the reviewer finds the idea interesting, they express significant concerns about the experiments and results. They state that 'the experiments do not show that this architecture can do anything new' and suggest that the paper falls short of its abstract goals. However, it's not entirely negative as they do acknowledge the interesting concept and potential for future work.

The politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They offer specific suggestions for improvement, ask clarifying questions, and provide detailed feedback without using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper, such as the 'thorough recap of past modular approaches' and the potential of the idea, which contributes to the polite tone.

Overall, the review balances critique with constructive feedback in a respectful manner, resulting in a slightly negative sentiment but a notably polite tone.",-20.0,60.0
"The paper aims to address issues with Domain Adaptation by using RL approaches. Domain Adaptation is an actively studied area in NLP research and so this paper is relevant and timely. This paper proposes and algorithm that is in line with work that aims at selecting data smartly when performing Domain Adaptation. The proposed algorithm learns representations for text in the source and target domains jointly. The proposed algorithm has two components i) a selection distribution generator (SDG) and ii) a task specific prediction for tasks being POS tagging, Dependency parsing and Sentiment Analysis.

While the proposed algorithm is interesting from a RL perspective and make sense, there is no explanation provided as to why this algorithm should do better over non RL based approaches for tasks such as Sentiment Analysis.

Domain Adaptation is widely studied for Sentiment Analysis and a lot of current research focuses on the various aspects of domain data, such as word and sentence level semantics, when developing algorithms. For example the following papers all (saving the third) address the problem of Domain Adaptation for Sentiment Analysis through various approaches fairly similar to the authors' algorithm, that provide similar if not better results than those provided in the paper,

[1]Barnes, Jeremy, Roman Klinger, and Sabine Schulte im Walde. ""Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains."" arXiv preprint arXiv:1806.04381 (2018).
[2] Ziser, Yftah, and Roi Reichart. ""Pivot Based Language Modeling for Improved Neural Domain Adaptation."" In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), vol. 1, pp. 1241-1251. 2018.
[3]An, Jisun, Haewoon Kwak, and Yong-Yeol Ahn. ""SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment."" arXiv preprint arXiv:1806.05521 (2018).

Particularly the second paper is a clear improvement over SCL (the earlier pivot based approach), a baseline that is considered by the authors in this work. There are no comparisons against this work in this paper, yet the authors compare against SCL alone.

Due to lack of comparisons against state-of-the-art in Sentiment Analysis/Domain Adaptation for Sentiment Analysis it is hard to accept the claims made by the authors on the superiority of their algorithm. Had their paper aimed at improving over other RL based approaches for Domain Adaptation for Sentiment Analysis, some experiments could be over looked. 

But, when making a claim that addresses the problem of Sentiment Analysis, comparisons against the state-of-the-art non RL based approaches is extremely important. Particularly, given the size of the data sets used, one could use lexical/dictionary based approaches [3] and improve upon the classification accuracies without having to train such an involved algorithm.

Furthermore there is no qualitative analysis provided to gain insights into the behavior of the embeddings spaces of the target and source domains that are learned jointly via the proposed algorithm. At least such an analysis would have provided some insight into why the authors' RL based solution is better than a non RL based solution.

The lack of reference or comparisons against relevant literature is future highlighted by the seemingly relevant, yet largely dated related works section.
","The sentiment score is -50 because while the reviewer acknowledges the relevance and timeliness of the paper's topic, they express significant concerns about the lack of comparisons with state-of-the-art methods, insufficient explanations, and missing qualitative analysis. The review is more negative than positive, but not entirely dismissive. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'interesting' and 'make sense' when discussing the algorithm. However, they also directly point out shortcomings without much softening language. The reviewer provides constructive criticism and suggests improvements, which contributes to the slightly positive politeness score.",-50.0,20.0
"Edited: I raised the score by 1 point after the authors revised the paper significantly.

--------------------------------------------

This paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.

The approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.

1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.

2. In Section 2.2 the authors write ""... framework is shown in Figure X"" without even showing the figure.

3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.

4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: ""vertices which satisfies"", ""makes W be affected"", ""the some strong baseline methods"", ""a set research papers"", and ""in align with"". The authors are suggested to do a thorough proofreading.

","The sentiment score is -50 because while the reviewer acknowledges the approach as 'reasonable', they state that 'more work is needed before it reaches an acceptable level' and list several significant issues with the paper. The reviewer's tone is critical overall, pointing out dubious theorems, missing figures, and numerous grammatical errors. However, they do offer some positive feedback initially. The politeness score is 20 because the reviewer uses relatively polite language like 'the authors are suggested to' and 'may be strengthened', avoiding harsh direct criticism. They also acknowledge the paper's potential and offer specific recommendations for improvement. However, some phrases like 'dubious' and 'handwavy' slightly reduce the politeness score.",-50.0,20.0
"This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning: 
1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states. 
2) the policy network which takes in the state and the predicted value function of the opponent. 
The main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes. 

Technical detail and concerns: 
The paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns. 

The authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent. 
My expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA. 

Furthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant. 

While the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.


","The sentiment score is -70 because the review is predominantly negative. The reviewer expresses significant concerns about the paper's technical details, missing baselines, and clarity issues. They conclude that the paper is 'unsuitable for publication in its current form', which is a strong negative sentiment. However, they do acknowledge that 'the idea itself is interesting', preventing the score from being even lower. The politeness score is -20 because while the reviewer isn't overtly rude, their language is quite direct and critical. They use phrases like 'The authors entirely miss this and fail to address possible concerns' and 'the above concerns render the paper unsuitable for publication', which come across as somewhat harsh. The reviewer doesn't use many polite phrases or soften their criticisms, leading to a slightly negative politeness score.",-70.0,-20.0
"This paper is about using ""neural stethoscopes"", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process. The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks. Moreover it can promote or suppress the correlation of such features to the tasks related to the main one. The framework is applied to the task of visual stability prediction of block towers. The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes. Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task. Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.

Strengths:
+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.
+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.
+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.

Weaknesses:
- The novelty is limited related to multitask learning, thus it is an incremental paper.
","The sentiment score is 70 (positive) because the review begins with a detailed and appreciative summary of the paper's content, followed by a list of strengths that outweigh the weaknesses. The reviewer describes the paper as 'very nice', 'well written', and 'easy to read', with 'convincing' experiments. The only weakness mentioned is the limited novelty, which is described in a neutral tone. The politeness score is 80 (quite polite) due to the respectful and constructive language used throughout. The reviewer provides positive feedback and constructive criticism without using harsh or dismissive language. The tone is professional and supportive, highlighting the paper's merits while also noting areas for improvement in a respectful manner.",70.0,80.0
"The paper explores unsupervised deep learning model for extractive telegraphic summaries, which extracts text fragments (e.g., fragments of a sentence) as summaries. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.

First, in term of methodology (as described in Section 3), the paper has little novelty. There has been intensive study using various deep learning models on summarization. The models described in the paper contain little novelty compared with previous work using autoencoder and LSTM for both extractive and abstractive summarization. 

Second, the paper claims contributions on using deep learning models on telegraphic summarization, but the advantage is not well demonstrated. For example, the advantage of the resulting summary is not compared with state-of-the-art sentence compression models with intrinsic evaluation or (probably better) with extrinsic evaluation. (By the way, it is interesting that the paper argues the advantage of using telegraphic summaries for fictional stories but actually gives an example which looks also very typical in news articles (the “earthquake Tokyo 12 dead” example).)

Third, there has been much work on speech summarization that summarizes with the “telegraphic” style (this is natural, considering speech transcripts are often non-grammatical, and “telegraphic” style summaries focusing on choosing informative fragments actually result in usable summaries.) The author(s) may consider discussing such work and compare the proposed methods to it.
","The sentiment score is -50 because while the reviewer acknowledges that the paper is well-structured and easy to follow, they express significant concerns about the paper's novelty, methodology, and lack of comparative analysis. The reviewer states that the submission 'does not have enough content to be accepted to the conference,' which is a clear negative sentiment. However, the criticism is balanced with some positive remarks, preventing the score from being extremely negative. The politeness score is 50 because the reviewer uses respectful and professional language throughout. They offer constructive criticism and suggestions for improvement without using harsh or dismissive language. The reviewer also acknowledges positive aspects of the paper before presenting their concerns, which is a polite approach to criticism. The tone is direct but not rude, maintaining a courteous academic discourse.",-50.0,50.0
"This paper provides a theoretical study of GANs in the following setting:

- The target distribution has a locally smooth density on a compact set [0, 1]^D. It might be supported only on M disjoint components, each of which has a smooth boundary, within that compact set.
- The latent noise dimension (inputs to the generator) is of the same dimension as the data.
- An IPM loss (1) is used.
- The discriminator functions of the IPM and the generator networks are both ReLU networks of at most L layers and at most S total nonzero weights, with all weights having magnitude at most B.
- The discriminator functions are Lipschitz continuous; this is implied by the previous assumption, but the bound is tighter if we have a tighter constraint here.
- We obtain the generator which exactly minimizes the IPM between the empirical distributions of m samples from the model and n samples from the target.
- Maybe: there is some g such that P* is produced by g. It's not clear in your statement of Theorem 1 whether this is necessary, or exactly what it means when you say that in Lemma 3, and I haven't fully checked what's used yet; it would help to explicitly say this is not assumed if it's not.

Pointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest.

The full-dimensional support assumption is not ideal, but of course just because the analysis doesn't apply to most practical GAN settings doesn't mean it's not an important step towards one that does.

Also note that the assumption about the structure of the networks eliminates the MMD GANs that you use in experiments -- which have a kernel function at the top of the critic network -- though it does allow for most GAN variants. Maybe the most interesting algorithm for this setting is the Coulomb GAN (ICLR 2018, https://arxiv.org/abs/1708.08819 ), which uses a neural network critic of the kind you study but estimates a distance which (unlike the Wasserstein and most other GAN objective functions) has good statistical convergence properties (kappa=2 as you mention).

My biggest concern by far, though, is Proposition 1. You present it as if it's a lower bound: establishing that there is some class of distributions for which Theorem 1 shows that GANs can account for local smoothness and standard methods are shown not to be able to. This isn't what you do; instead you exhibit a class of distributions for which Theorem 1 shows that GANs can account for local smoothness, and previous analyses of standard methods do not show that they are able to take advantage of it. This is not the same thing at all! Although the previous upper bounds have matching lower bounds, you don't demonstrate (and it is likely not the case) that the distribution you show fits into the class of distributions used by the previous lower bounds, and so it remains very possible that other methods are able to take advantage of local smoothness as well as GANs do.

Given that Proposition 1 is hence a very weak statement, your main contribution in terms of disconnected support becomes ""we can show that GANs can adapt to disconnected supports (under these other assumptions) that has not previously been shown for other methods."" Not only is this a weaker result, but the degree to which you show GANs can take advantage of local smoothness is somewhat limited: at least with the parameter choices in Corollary 1, the smoothness only improves m dependence, not n dependence.

But in the GAN setting, m is essentially a question of how long you optimize for (and the relative rates between generator and discriminator updates, and various other questions like that out of scope for this paper), not any kind of externally fixed limitation like n. It's perhaps not too surprising that you don't show that optimizing a WGAN is statistically easier than estimating Wasserstein the distance, but given that estimating Wasserstein is so hard in high dimensions, it's a little disappointing. (Maybe it's easier, so that kappa is smaller, with locally smooth densities as here, though I don't know of any results like that offhand.)

(It's interesting, then, that in Corollary 1 the generator complexity depends only on m, basically the amount of optimization you're willing to do, while the discriminator complexity depends only on the available number of target samples n.)

Another concern is that to me, it is not very clear exactly what the statements of the assumptions mean. For example, does Theorem 1 apply only if I search over all generators in the class \mathcal G = \Xi(S_g, B_g, L_g)? In particular, does this mean that I have to consider all possible architectures matching those constraints, including allowing for all possible depths up to L_g, and all possible ways of allocating widths of the various interior layers / which weight entries are fixed at zero? It seems so, but this could be more explicit in the statements, even just by replacing ""an existing \mathcal G"" with ""\mathcal G = \Xi(S_g, B_g, L_g)"" in the statement of (e.g.) Theorem 1.

In your numerical experiments: you don't make it at all clear enough that you're plotting *different loss functions* for the GANs and the other methods! (You say this, but only in the text where it should definitely also be in the figure caption.) What happens if you plot the L2 difference for all methods, and the MMD/Wasserstein for all methods? Looking at Figure 6, it's not obvious that the GAN would do so substantially better. (It does seem to perhaps have the overall scale of the two components better than the other methods, but it doesn't look like as enormous a difference as it seems from Figure 5.)

Overall: Theorem 1 is of interest, but the results and especially the comparison to classic methods are not as resounding as they're presented here.

(Note that I have not (yet) verified or even really read most of the proofs; I might come back and do that later.)

Smaller points:

- I don't think that f-GAN actually fits in the framework (1) as you claim, since it needs to use the conjugate of f on the samples from Q. Also, the original GAN does fit into (1) but not your assumptions about the network form of f, since it needs a log in its activations.

- Another class of generative models where disconnected supports are really important is normalizing flows, which often build ""bridges"" between separated modes because (like your Lemma 2) their generators are constrained to be smooth and invertible. See e.g. Figure 2 of https://arxiv.org/abs/1810.01367 (who propose a new normalizing flow less susceptible to these problems). 

- Remark 1 seems so obvious that it need not even be stated, since beta-smooth implies beta'-smooth for beta' < beta. It would only be interesting if you could actually take advantage of the smoother components somehow.

- Many papers in your bibliography are cited only as arXiv preprints when they were actually published in various places. For example, the first four papers were published at ICLR 2017, ICML 2017, ICML 2017, and ICLR 2018, respectively.

- There are many small typos and grammatical errors in the draft, including some that would be caught by a spell-checker (""methdos"" on page 8), and an undefined LaTeX reference at the top of page 2. It would benefit from a thorough proofread.
","The sentiment score is -30 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'Pointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest'), they express significant concerns about key aspects of the work. The reviewer's biggest concern is with Proposition 1, which they argue is much weaker than presented. They also point out limitations in the comparison to classic methods and issues with the numerical experiments. These criticisms outweigh the positive elements, resulting in a somewhat negative overall sentiment. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'My biggest concern' and 'It would benefit from' rather than harsh language. They also acknowledge positive aspects of the work alongside criticisms. However, the review doesn't go out of its way to be excessively polite or complimentary, keeping it from scoring higher on the politeness scale.",-30.0,50.0
"Overview:
The authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set.  
Instead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding 'accuracy vs. randomization‘ curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. 
I have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect 'accuracy over randomization‘ curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a ’steep decrease’?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature.  
Overall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested.


Detailed remarks:

General:
A proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. 

You mention complexity of data and model several times in the paper but never define what you mean by that.


Detailed:
Page 3, last paragraph: Why did you not use bias terms in your model?

Page 4, Assumption. 
- What do you mean by the data being independent? Independent and identically distributed?  
- ""As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn.“ What do you mean by ""easier to learn""? Better generalization? Better training error? 
- I don’t understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later?
- What does ""similar scale“ mean? 

Page 4, Monotony. 
- You state two assumptions or claims, 'the accuracy curve is strictly monotonically decreasing for increasing randomness‘ and 'we also expect that accuracy drops if the regularization of the model is increased’, and then state that 'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization.‘ Although you didn’t show anything but only state assumptions or claims (which may be reasonable but are not backed up here). 
I actually don’t understand the purpose of this paragraph.

- Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didn’t show yet.

My main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as 'above the diagonal line‘ and underfitting as ‚below the line‘, which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of ’sharp drops’ and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). 

Criterion 2 (b) is not clear.  
- I neither understand ""As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)"" 
nor do I understand ""accuracy over regularization curve (plotted in log-log space) is constant""?
Does that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit?

Due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit.




","The sentiment score is -70 because the reviewer expresses several significant issues with the paper and concludes that it is 'not fit for publication'. The reviewer criticizes the vagueness of the criteria, lack of thorough testing, and limited applicability. However, it's not entirely negative as the reviewer acknowledges some potentially reasonable assumptions and provides detailed feedback for improvement. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I have several issues' and 'I think' rather than making blunt statements. The reviewer also provides detailed explanations for their concerns, which is a courteous way to give feedback. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.",-70.0,20.0
"This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario.

Pros:
-Paper was easy to follow
-Using orthogonal encodings to decorrelate gradients is an interesting idea
-Benchmark results appear promising compared to prior works

Cons:
-This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme?
-Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? 
-Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations.
-It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet

Overall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.","The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as it being easy to follow and having an interesting idea. They also mention promising benchmark results. However, the reviewer expresses several concerns and is not fully convinced by the method, which prevents a higher positive score. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, presenting their concerns as questions or suggestions rather than harsh criticisms. They acknowledge the paper's strengths before discussing potential issues, which is a polite approach. The reviewer also uses phrases like 'I believe' and 'I think' to soften their critiques, maintaining a professional and courteous tone.",20.0,60.0
"This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.

I like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.

But, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and I’m not happy with lack of baselines in the experimental comparison (again, see below).

The math problems
1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesn’t depend on r and the right-hand side does.
2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).
3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesn’t form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.
4) The function f() minimized in (5) is not defined (it should be!), but if it doesn’t have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I don’t get how the automatic rank reduction can be done).
5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I don’t even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).
6) I don’t get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.
7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).
8) Also, I don’t get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).
9) Finally, I don’t get the proposition 1 nor it’s proof: how can it be that rounding to a fixed r_max won’t change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.
10) I didn’t get this line: “From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = ∇f(x) and Exp−1 x (y) = −η∇xf(x), and thus Theorem 3 can be derived.” What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?
11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?
12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?
13) What is E in Theorem 3? Expected value w.r.t. something? Since I don’t understand the statement, I was not able to check the proof.

The experimental problems:
1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).
2) The convergence plots are shown only in iteration (not in wall clock time) and it’s not-obvious how much overhead the Riemannian machinery impose.
3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.

And finally some typos / minor concerns:
1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesn’t allow for Riemannian optimization and can be harder to work with numerically).
2) It’s very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.
3) In Def 2 “different function” should probably be “differentiable function”.
4) How is W_c represented in eq (25), as TT or not? It doesn’t follow the notation of the rest of the paper. How is a_t used?
5) What is “score” in eq (27)?
6) Do you include bias parameters into the total number of parameters in figures?
7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).
8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).


Please, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns!
","The sentiment score is -50 because while the reviewer expresses initial interest in the topic ('I like the topic chosen by the authors'), they quickly follow with significant criticisms. The review lists numerous mathematical errors, experimental problems, and other concerns, indicating that the paper needs substantial improvement. The overall tone suggests the paper is not ready for publication in its current state.

The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'Please, do take this as a constructive criticism' and 'I would be happy to see you resubmitting the paper after fixing the raised concerns!' which demonstrate a polite and encouraging attitude. The reviewer also frames their criticisms as areas for improvement rather than outright dismissals. However, the score is not higher because the review is quite direct in pointing out numerous flaws, which could be perceived as somewhat harsh despite the polite framing.",-50.0,50.0
"
This paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation. To summarize my understanding of the proposed objective, in the bi-modal case, it combines both objectives of TELBO [1] and JMVAE-kl [2] with some hyperparameters to learn the uni-modal encoders. The terms of Eqns 7,8, and 9 are equivalent to TELBO and Eqns 9 and 10 are JMVAE-kl. It would be very beneficial for the readers if you could more clearly contrast your objective with the related work given how similar they are. 

Given these similarities between objectives, its unclear why JMVAE-Zero was chosen over JMVAE-kl as a baseline. Furthermore, the reasoning for the improvement of the ELBO of M^2VAE over the baselines in Section 5.3 is unclear, given the similarities between the objectives. 

The qualitative figures throughout the paper are hard to interpret. By looking at Fig 4., I cannot tell which latent space is best. 
“one can see from Fig. 4 that the most coherent latent space distribution was learned by the proposed M^2VAE” 
What is meant by ‘coherent latent space’? 

This paper was hard to follow and there are a number of typos throughout the paper. For instance, the labels within Fig 4 and the caption contradict themselves. If the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.  

[1] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative Models of Visually GroundedImagination. ArXiv e-prints, May 2017.
[2] M. Suzuki, K. Nakayama, and Y. Matsuo. Improving Bi-directional Generation betweenDifferent Modalities with Variational Autoencoders. ArXiv e-prints, January 2018

","The sentiment score is -40 because the review is generally critical, pointing out several issues with the paper such as lack of clarity, hard-to-interpret figures, and typos. However, it's not entirely negative as it suggests ways to improve and acknowledges potential contributions. The politeness score is 20 because while the reviewer is direct in their criticism, they use respectful language and offer constructive feedback. Phrases like 'It would be very beneficial' and 'If the clarity and quality of the writing could be improved' indicate a polite tone. The reviewer also asks questions for clarification rather than making outright negative statements.",-40.0,20.0
"This paper proposes a new meta-learner for few-shot learning that conditions the parameters of the model on the given query image. The authors argue that this allows the model to focus on features particular to the query, thereby facilitating classification. The paper introduces a kernel generator as a meta-learner and report performance on two standard benchmarks, Omniglot and miniImagenet.

Several methods propose meta-learners that adapt the learner’s parameters to the task or each class in the task. This paper adapts to the query itself, which may provide other benefits, and provides a useful complement to prior work on parameter adaptation in few-shot classification.

While the core idea itself is clearly articulated, the reading is dense and many of the finer points are vaguely presented. This makes the paper hard to read and its contribution unclear. In particular, the meta objective itself is not defined, the second loss function contains an undefined (learnable?) functions whose role is not entirely clear. In the experimental section, the authors mention that they use Prototypical Networks (Snell et al., 2017) on top of their kernel generator. This puts their contribution in a different light, now as an extension of Snell et al., (2017). I’m also unclear about the novelty of kernel generator the authors supposedly introduce. The kernel generator appears identical to that of Han et at. (2018), in which case the contribution is its application to few-shot learning, not the kernel generator itself.

Since the main contribution of this paper is to condition the learner’s parameters on the query, as opposed to the task or the classes in the task, the relevant comparison is with respect to such alternative methods. Several such benchmarks are missing (below), and when considered, the reported results are relatively weak. 

For an up-to-date collection of benchmarks on miniImagenet, see Rusu et. al., (2018, https://arxiv.org/abs/1807.05960).

===

[1] Gidari and Komodakis. Dynamic few-shot visual learning without forgetting. 2018.
[2] Oreshkin et al.. TADAM: Task dependent adaptive metric for improved
few-shot learning. 2018.
[3] Qiao et al.. Few-shot image recognition by predicting
parameters from activations. 2017.
[4] Bauer et al.. Discriminative k-shot learning using probabilistic models. 2017.","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects of the paper ('core idea itself is clearly articulated', 'provides a useful complement to prior work'), they express several significant concerns. These include the paper being dense and vague in parts, unclear contribution, missing comparisons, and relatively weak results. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, avoiding harsh criticism. They offer constructive feedback and suggestions for improvement rather than outright dismissal. The reviewer also acknowledges the potential merits of the work alongside the criticisms.",-30.0,50.0
"This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:

1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.

2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).

3. It demonstrates the value of the proposed approach through an extensive experimental setup.

At the same time, I see two major limitations to the paper:

1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.

2. The authors claim that: ""To our best knowledge, this work is the first to propose
an unsupervised CLTL framework without depending on any cross-lingual resource""

This is, unfortunately, not true. I refer the authors to the paper:

Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.

In their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.","The sentiment score is 50 (slightly positive) because the reviewer begins by highlighting several positive aspects of the paper, including its well-written nature and multiple contributions. However, the reviewer also points out two major limitations, which balances out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The reviewer also uses phrases like 'IMO' (in my opinion) to soften subjective statements. The critique is presented constructively, providing specific examples and references to support the points made.",50.0,75.0
"This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling's original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work.

The paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline.  The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed.

The experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper's thread.

- Please explain why tuning the HMC algo by maximizing eq 16 should work. I don't think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this?

- The authors write ""Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution π as L increases.""

This is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don't care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me.

In practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> ""we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate"". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)","The sentiment score is -50 because the reviewer expresses significant criticisms of the paper, including that it is unnecessarily complicated, lacks clear demonstration of improvement over previous methods, and makes claims that may not be justified. However, they do acknowledge some potential value in the proposed method, preventing an extremely negative score. The politeness score is 20 because while the reviewer is direct in their criticisms, they maintain a professional tone throughout, use phrases like 'Please explain' when requesting clarifications, and acknowledge potential merits of the work. The language is not overtly rude, but also not excessively polite, leaning slightly towards politeness in academic discourse.",-50.0,20.0
"The paper present interactive parallel exploration (IPE), a reinforcement learning method based on an ensemble of policies and a shared experience pool. Periodically, the highest-return achieving policy is selected, towards which the other policies are updated in a sense of some distance metric. IPE is applicable to any off-policy reinforcement learning algorithm. The experiments demonstrate some improvement over TD3 on four MuJoCo benchmark tasks.

The method is motivated heuristically, and and it provides some benefits in terms of sample efficiency and lower variance between training trials. However, it is hard to justify the increased algorithmic complexity and additional hyperparameters just based on the presented results. The paper motivates IPE as an add-on that can increase the performance of any off-policy RL algorithm. As such, I would like to see IPE being applied to other algorithms (e.g., SAC or DQN) as a proof of generalizability, and compared to other similar ensemble based algorithms (e.g., bootstrapped DQN).

While the improvement in the sample complexity is quite marginal, what I find the most interesting is how IPE-TD3 reduces variance between training trials compared to vanilla TD3. Convergence to bad local optimum can be a big problem, and IPE could help mitigate it. I would suggest including environments where local optima can be a big problem, for example HumanoidStandup, or any sparse reward task. Also the paper does not include ablations, which, given the heuristic nature of the proposed method, seems important.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some benefits of the proposed method, they express significant reservations. The reviewer notes that the improvements are 'quite marginal' and questions whether the 'increased algorithmic complexity and additional hyperparameters' are justified by the results. They also request additional experiments and comparisons, suggesting the current work is incomplete. However, the score is not deeply negative as the reviewer does find some aspects 'interesting' and offers constructive suggestions for improvement. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They offer critiques in a constructive manner, using phrases like 'I would like to see' and 'I would suggest' rather than making demands. The tone is analytical and objective, without personal attacks or overly harsh language.",-20.0,50.0
"This paper attempts to speed up convergence of deep neural networks by intelligently selecting batches. The experiments show this method works moderately well.

This paper appears quite similar to the recent work ""Active Bias"" [1].
The motivation for the technique and setting appear very similar, while the details of the techniques are different. Unfortunately, this is not mentioned in the related work, or even cited.

When introducing a new method, it is important that design choices are principled, have theoretical guidance, or are experimentally verified against similar design choices. Without one of these, the methods become arbitrary and it is unclear what causes better performance. Unfortunately, this paper makes several choices, about an uncertainty function, the probability distribution, the discretization, and the algorithm (when to update) that appear rather arbitrary. For instance, the uncertainty function is a signed standard deviation of the softmax output. While there are a variety of uncertainty functions, such as entropy and margin, a new seemingly arbitrary uncertainty function is introduced.

The experiments are good but could be designed a bit better. For instance, it is unclear if the gains are because of lower asymptotic error or because of faster convergence. The learning curves are stopped too early, while the test error is still dropping quickly.

In summary, it is not clear if this paper adds any insight beyond ""Active Bias"".

[1] Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. 2017. Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum.","The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper such as similarity to existing work, arbitrary design choices, and experiments that could be improved. However, it's not entirely negative as it acknowledges some positive aspects like 'good' experiments. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism, avoiding harsh or rude phrasing. They use phrases like 'appears quite similar' and 'it is unclear' rather than more accusatory language. The reviewer also provides specific suggestions for improvement, which is a polite approach to criticism.",-50.0,20.0
"This paper proposes a measure (“effective path”) of which units and weights were most important for classification of a particular input or input class. Using the effective path, the authors analyze the overlap between paths across classes for CNNs and between adversarially modified and unmodified images. Finally, the paper proposes an adversarial defense method based on effective path which detects adversarially manipulated images with high accuracy and generality to a variety of settings. 

Overall, this paper is interesting and provides several novel observations. The clarity of the exposition is generally good, but can be improved in several places (mentioned below). As for significance, effective path is likely to inform future analyses of neural networks, and the adversarial defense may prove impactful, though ultimately, its impact will depend on if and when the defense is broken. 

However, there are several important controls missing from the analysis, several claims which are unsubstantiated, and experimental details are lacking in a few places. As such, in its current form, I can only weakly recommend this paper for acceptance. If in the revision the controls requested below are included, additional evidence is provided for the unsubstantiated claims (or if those claims are toned down), and exposition of missing experimental details is included, I’d be happy to raise my score. 

Major points:

1) While the observation regarding path specialization is very interesting, one cannot gauge whether or not the degree of overlap observed between class-specific paths signals path specialization or simply high input-to-input path variance (which is similar both within and across classes). In order to distinguish between these possibilities, a measure of intra-class path similarity is necessary. In addition, an experiment similar to that in Figure 2 with CIFAR-10 would be quite helpful in evaluating whether this phenomenon exists in more natural datasets (the ImageNet results are difficult to interpret due to the large number of classes).

2) Several claims in the path specialization section are unsubstantiated. 

2a) In particular, the claim that ‘1’ has the highest degree of specialization “because of its unique shape” is made without evidence as is the similarity between ‘5’ and ‘8’. ‘6’ is also similar to ‘8’ and yet does not show the same similarity in the path specialization. These differences may very well simply be due to chance.

2b) The claim that the path specialization in ImageNet matches the class hierarchy is made only based on the rough non-linearity of Figure 3. Please either measure the overlap within and across class categories or soften this claim.

3) The similarity analysis for adversarial images is also very interesting, but a comparison between unmodified and randomly perturbed images with matched norms to the adversarially perturbed images is necessary to establish whether this effect is due to noise generally or adversarial noise.
It’s unclear how the effective path is calculated when negative weights are involved. Further exposition of this aspect would be helpful.

Minor points/typos: 

1) There are several places where confusing concepts are introduced in one paragraph but explained several paragraphs later. In particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though they’re introduced on page 3.

2) Typos: 

2a) Section 1, fourth paragraph: “...and adversarial images, we uncover...” should be “...and adversarial images, and we uncover...”

2b) Section 1, fourth paragraph: “...by small perturbation, the network…” should be “...by small perturbations, the network…”

2c) Section 2, first paragraph: “...the black-boxed neural…” should be “...the black-box neural…”

2d) Section 2, first paragraph: “In the high level…” should be “At a high level…”

2e) Section 4, first paragraph: “...as it does no modify…” should be “...as it does not modify…”

2f) Title, should be ""Neural Network""? 
","The sentiment score is 20 (slightly positive) because the reviewer finds the paper 'interesting' and 'novel', but also points out several important issues and can only 'weakly recommend' it in its current form. The overall tone is constructive, suggesting improvements that could raise the score. The politeness score is 70 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'I'd be happy to raise my score' and the use of 'please' in recommendations contribute to the polite tone. The reviewer also balances critique with praise, which is a hallmark of polite academic discourse.",20.0,70.0
"This paper uses pruning and model distillation iteratively to reduce the model sizes. The pruning step is based on Molchanov et al. (2017). This is followed by a hints step that minimizes the feature map difference between student and teacher. Finally, a reconstruction step is used to restore original weights. Results are shown on CIFAR-10, Imagenet and COCO datasets for classification and pose estimation tasks where PWH reduces model costs with a small loss in accuracy.

The paper is interesting that it proposes a unique combination of existing methods iteratively to improve the compression rates in modern CNNs. However, given that these methods already exist, the novelty aspect of this paper is low. Furthermore, it is also hard to rebut these methods, since they have been published and extensively evaluated in the respective papers. Nevertheless, it is interesting to note that both methods assist one another in your set of experiments.

In order to improve the paper and for the reviewers to judge the papers more favorably, the authors can compute the time for reconstruction/hints and demonstrate that it is clearly superior to fine-tuning/re-training and offers a clear advantage. This should be emphasized and articulated in related work and introduction and will help the paper.

How did you arrive at the magic number of pruning just 256 channels in every step?","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper as 'interesting' and notes that the proposed methods assist one another. However, they also point out low novelty and difficulty in rebutting existing methods, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive feedback, and suggests ways to improve the paper. They avoid harsh criticism and frame their concerns politely, using phrases like 'it is interesting to note' and 'in order to improve the paper'. The reviewer also asks a question at the end, showing engagement with the work rather than dismissing it outright.",20.0,60.0
"The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The cost function of the minimax GAN optimization problem is changed accordingly. Experimental results suggest that this approach leads to improved results, both visually and w.r.t. FID metric.

Improving on the well known problem of mode collapse in GAN training scenarios is without a doubt an important endeavor. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission.

With respect to the proposed method, I am not completely clear on how it can increase sample variety, if it does. In understand the arguments brought forward in Appendix B, but:
Consider a minimax game involving one generator G and one discriminator D, where each batch of generated samples from G(Z), is split up into two parts, A and B, via selection without replacement using uniform sampling. Z is a matrix of noise inputs, where each column corresponds to one item of the batch. D is now tasked to differentiate whether a sample came from A or B. It seems intuitive to say that in this case, D can neither win, nor provide any useful signal to G, since the sets A and B were split randomly, and there is no influence on G during training. The variety of samples in A as well as in B will be identical to the variety in the set (A and B).

Yet this random microbatch splitting is what seems to be happening here, if I understood Section 3 correctly; just with an ensemble of discriminators, and not just with one.
While it is thus not completely clear to me *why* the proposed additional term seems to bring increased variety, experiments strongly suggest that it does.
As described in Section 3.1, choice of the weighting parameter alpha seems crucial, and additionally alpha needs to depend on the iteration index. Different schedules are demonstrated, but optimality of either is not guaranteed. This makes the actual influence of the additional loss term even harder to judge and evaluate.

Section 4.1 seems to confirm the increase in variety via the self-defined ""Intra FID"" measure. I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5.
In Table 1, both min and mean FID are given over 50k iterations. Instead of reporting the minimum, it might be fairer to compare FIDs after a fixed number of iterations (i.e. 50k in this case).

The method comparison in Section 5 is generally appreciated, but I think some of its flaws are:
- Datasets, including ImageNet, are all downsampled to 32x32 pixels. We have seen generators in recent work that produce interesting high-resolution output in even megapixel size; the tiny size seems like a pessimization of overall approaches.
- The proposed method is compared to other methods using only 2 discriminators, although Section 4.3 suggest a larger number is better.
- MicroGAN does not compare favorably to many of the compared to methods in Table 2. This may not necessarily by a flaw of the MicroGAN contributions, but is rather a problem of an apples-to-oranges comparison, as the authors readily admit (""the use of more powerful architectures [...] plays a big role""). I question the value of such a comparison, if not only the method differ, but also implementation details such as network architectures.

Overall the submission is quite interesting, but not without the above-mentioned flaws.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the topic and the interesting aspects of the submission, they express several concerns and point out flaws in the methodology and comparisons. The reviewer questions the mechanism behind the proposed method's effectiveness and highlights issues with the experimental setup and comparisons.

The politeness score is moderately positive (60) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I would have liked to see' and 'I am not completely clear' rather than making blunt criticisms. The reviewer also acknowledges the positive aspects of the work, such as the 'interesting' nature of the submission and the importance of the research topic. However, they don't use overly effusive language or go out of their way to soften their criticisms, keeping the tone balanced and professional rather than extremely polite.",-20.0,60.0
"Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\alpha$ weights (see page 4).These weights give ""how much"" this word belongs to a certain topic. For every topic we compute the $\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   

Questions and remarks:
     1) How sensitive is the method to a change in the number of topics (k)?
    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful
 
Points in favor: 
   1) Good results and thorough tests 
    2) Paper is easy to read and follow 

Points against:
A very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. 


The novelty of the paper is not enough to justify its acceptance at the conference.","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('Good results and thorough tests', 'Paper is easy to read and follow'), the overall tone is critical. The reviewer points out that the method is very similar to existing work and concludes that 'The novelty of the paper is not enough to justify its acceptance at the conference', which is a clear negative sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, offering both positive and negative points in a balanced manner. They ask questions and make suggestions for improvement rather than using harsh criticism. The phrasing is consistently polite, such as 'Please provide' and 'Points in favor/against', which maintains a courteous tone even while delivering negative feedback.",-50.0,50.0
"This paper presents a view of sentence-level prediction tasks as statistical relation learning problems. In particular the paper argues that composition functions used in recent SRL techniques developed for entity-to-entity relationship detection can be applied to sentence-level relation prediction tasks. 

Suppose there is a prediction training task defined over pairs of sentences (x1, x2). This task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to 
make the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between R(x1, x2).  This paper aims to show that with a better 'f' we can hope for a better result in transfer tasks (in addition to doing better on the training task). 

The paper argues that this setting, at a high level, is similar to the composition function used in entity-entity relation prediction. There have been many such methods in the recent past (e.g., TransE, ComplEx, RESCAL). This paper asks whether these composition functions can work well for sentence-level tasks.

The paper then presents experiments which compare the performance of different composition functions against a basic composition function used in InferSent.

Strengths of the paper:

1. I like the main question of what can we learn from SRL. This seeks to bridge some independent research threads.
2. The evaluation considers a range of composition functions used in SRL and applies them to the sentence tasks. 
3. Points out that some of the composition functions used in existing models are not particularly strong.

Issues:

I like the starting point for this paper very much and agree that the existing composition functions for sentence relations are rather weak. However, I am struggling to see if there is (i) a convincing conceptual argument for why SRL view of compositions is necessarily the answer for sentence level tasks, or (ii) a convincing empirical case for the same.  Some details on these points:

1) The parallels between entity-entity relations and sentence-sentence relations seems a bit of a stretch to me. There is always some level of abstraction at which two problems might look similar, which can be advantageous for repurposing solutions. However, in this case I think the SRL view of the world hides the complexities in sentence-sentence relation tasks (e.g. aligning relevant pieces of information, requiring more complex composition functions to derive meaning etc.). 

2) I am not sure what knowledge we are getting from an SRL view of the problem that is not already known already to the communities that work on sentence embedding. The minimum requirements laid out can be met easily by existing methods for sentence representations. For instance that we need to allow for asymmetric relations (entailment order) is very well known. As the authors themselves point out there are solutions for this problem.  

3) The empirical results don't appear convincing. The average gain for any particular method over InferSent is 0.3 in macro average. There is no single SRL based composition method that works consistently clear gains across most tasks. 

Here are some suggestions that I think will improve the paper (or at least help me buy the motivation): 

1. One question that might be useful to make a conceptual argument is how much work should be done in 'f' and should it change for the different type of target tasks.

If the idea is to transfer h for single sentence target task, then a powerful 'f' can render h1 and h2 to be simple enough, such that bulk of the work in extracting task related information might be done by 'f' itself. Therefore, transferred h may not be as powerful as it could have been with a less powerful 'f'.

If the idea is to transfer f(h1, h2) for sentence-pair target tasks, then a powerful 'f' might be a good thing. 

2) Another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes. 

The paper wants to go from a simple 'f' (i.e. concat(h1,h2), h1-h2) to some other choices for 'f' that are known functions from SRL. 

There are several sentence-level representation functions such as ESIM [Chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences. A similar representation is computed in BiDAF [Seo et al., 2017] in the context of matching question representation with sentence representations. 

To summarize, I really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.
","The sentiment score is slightly positive (20) because while the reviewer expresses interest in the paper's starting point and main question, they also raise several concerns and issues. The reviewer acknowledges strengths but also points out significant weaknesses, suggesting a mixed but slightly favorable overall sentiment. The politeness score is high (80) as the reviewer uses respectful language throughout, offers constructive criticism, and frames their concerns as suggestions for improvement rather than outright criticisms. They use phrases like 'I like,' 'I agree,' and 'I really like the basic starting point' which contribute to a polite tone. The reviewer also provides detailed explanations for their concerns and offers specific suggestions to improve the paper, demonstrating a helpful and considerate approach.",20.0,80.0
"This paper studied learning unsupervised inductive node embeddings with an attention mechanism. For each positive edge, multiple different sets of neighborhoods are sampled for both the source and target nodes, and the similarity between the neighborhood are used as the attention functions. Experimental results prove the effectiveness of the proposed approach over GraphSAGE on a few networks. 

Strength:
- learning unsupervised inductive node embeddings is an important problem
- the proposed method seems to work

Weakness:
- the novelty of the proposed method seems to be very marginal
- the experiments are quite weak
- the complexity of the algorithm seems to be very high

Details:
- the complexity of the algorithm seems to be very high seem for each pair of nodes, multiple sets of neighborhoods must be sampled for each node.
- there are also other approaches for inductive unsupervised node embeddings, for example, the varitional graph autoencoder method (Kipf et al. 2017).
- I am wondering how the proposed method performs comparing with the methods of only selecting the nodes which form triangles with the given positive edges. 
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper (e.g., 'learning unsupervised inductive node embeddings is an important problem' and 'the proposed method seems to work'), they also point out several weaknesses. The reviewer mentions that the novelty is 'very marginal', the experiments are 'quite weak', and the complexity of the algorithm seems 'very high'. These criticisms outweigh the positive aspects, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) because the reviewer uses neutral and professional language throughout. They present both strengths and weaknesses in a balanced manner, and their criticisms are constructive rather than harsh. The reviewer also uses phrases like 'I am wondering' which softens the tone of their critique. While not overly effusive, the language is respectful and appropriate for a peer review.",-20.0,50.0
"The paper studies the use of embedding techniques in recommender systems, and shows that item2vec (an item vectorization method) can be replaced by user2vec, as users and items are interchangeable.

This is a reasonable enough idea, though not sufficient for publication in ICLR. I'd suggest the authors address the following details:
-- The methodological contribution is too small, and fairly obvious. Not sufficient for this conference.
-- Only evaluated on one dataset, so unclear whether the results are really representative
-- Comparisons against a very limited set of similar methods, which are probably not state-of-the-art for this dataset
-- The results don't seem significant, all methods compared perform almost equally","The sentiment score is -60 because the review is predominantly negative. The reviewer states that the idea is 'not sufficient for publication in ICLR' and lists several criticisms, including a small methodological contribution, limited evaluation, and insignificant results. The only positive comment is that it's a 'reasonable enough idea,' which prevents the score from being even lower. The politeness score is 20 because while the reviewer is direct in their criticism, they use relatively neutral language like 'I'd suggest' and 'reasonable enough idea,' avoiding harsh or rude phrasing. The reviewer also provides specific recommendations for improvement, which is a constructive approach. However, the overall tone is still more critical than overtly polite, hence the modest positive score.",-60.0,20.0
"This paper investigates the use of techniques for improving neural network training (regularization, normalization of covariance, sparsity) in terms of their generalization properties, empirically and analytically. The claim is that most of these tools do not help improve performance, with the exception of mutual information.

Pros: It's interesting to investigate and compare these different ""regularization"" techniques and compare them on different tasks empirically.

Cons:
Many of the points made in the paper are not properly capturing the nuance in the ""conventional wisdom"", and although it's good to be reminded and the empirical results are interesting to look at, in fact these are not really new discoveries, and sometimes the conclusions are very misleading. 

1) There is test loss, and there is generalization loss, and it isn't exactly the same thing. For a hypothesis class H, we have

test loss <= train loss + generalization loss 

where train loss measures how well we've fit a particular sample, and generalization loss measures how well a model that is trained on one sample can fit a new sample. Note that if I apply L2 loss to my model and have a regularization parameter --> infty, my train loss is huge but my generalization loss is 0. In other words, for a large enough regularization parameter, most of the methods experimented here WILL limit effective capacity and minimize generalization loss; it just will not give you the best test error performance. So the distinction here should be made much clearer--conventional wisdom for regularization limiting generalization error is not wrong.

2) The point that is trying to be made in section 3.1 is somewhat well-known in the general optimization literature. Let's consider a much simpler example: linear regression 

min_x ||Ax-b||_2^2 + gamma ||x||_2^2

Let's consider first no regularization, gamma = 0. Assume that A has a huge nullspace. Then technically there are an infinite number of globally optimal solutions x, although if we solve this problem using SGD starting with x = 0, it is known that the minimum norm solution is always picked. You can also think of this as whitening, since large lambda smooths the spectrum of the Hessian. Now add in regularization. Now the solution is unique, even if A is ill-conditioned. It's true that it isn't super necessary to add this regularization, since SGD can get you a good solution, but now we can GUARANTEE that the generalization error is 0. In practice, also, regularization adds stability to the numerics. 

In deep learning, the hidden layer z also acts as a coefficient matrix for determining y. I assume that is why people pursue low correlation, since it affects the conditioning of z. 

3) Comments like  ""for scaling and permutation, their influences are rather insignificant"" seem a bit careless to me. In fact it is well known that scaling can affect training performance significantly. But of course, if I know the global solution for z which feeds into a softmax, then any scaling on z does not affect the output of the softmax. That, however, does not mean I don't care about the scaling of z when training. 

4) Sparsity and rank BOTH limit the degrees of freedom. In fact, sparsity makes more sense when optimizing a nonlinear objective, which is always the case in deep learning. The reason to limit rank is when you wish to ""learn your codewords"", e.g. the eigenvectors, whereas in sparsity, the ""codewords"" are already learned, and you just learn the weighting. But if the codewords span the space, they both have equal representability. 

4) It is not clear to me what the task is in 4.3. What is the ""accuracy"" in a data generation task? Is this the normal classification task? If so, is the accuracy reported train or test accuracy? How exactly is generalization error being measured? 


5)I am not clear as to what conclusion is being drawn in section 5, with the last sentence ""obviously, many of the statistical characteristics become meaningless for such a scalar representation, and it is high time to reconsider the so-called conventional wisdom on representation characteristics."" why is this conclusion drawn based on the observation that, if a scalar z perfectly correlates with y, in fact this is the most generalization neural network? 

6) Table 4: how did you choose your hyperparameters? (regularization performance is extremely sensitive to parameter choice.)

7) A major concern is that basically very little training is done in these comparisons, except in the very last section. As I previously mentioned, many of these regularization / normalization techniques are also meant to better condition the optimization itself, and thus this advantage should not be discarded. 


minor comments:
 - page 5 last sentence ""characteristics"" should be singular
 - page 8 first sentence ""to [a] deep network's performance""","The sentiment score is -50 because the review is predominantly critical. While it acknowledges some positive aspects ('It's interesting to investigate...'), the majority of the review lists numerous concerns and criticisms about the paper's methodology, conclusions, and presentation. The reviewer points out several 'cons' and areas where the paper's arguments are seen as misleading or lacking nuance. However, the score isn't lower because the reviewer does offer some constructive feedback and acknowledges some merits of the work. The politeness score is 20 because while the reviewer is direct in their criticisms, they generally maintain a professional tone. They use phrases like 'It's interesting...', 'it's good to be reminded...', and offer detailed explanations for their concerns rather than dismissive comments. The reviewer also provides suggestions for improvement and asks clarifying questions, which is a polite way to address issues. However, there are a few instances where the language becomes slightly more pointed (e.g., 'seem a bit careless to me'), which prevents a higher politeness score.",-50.0,20.0
"The authors present a novel method for generating images from sounds using a two parts model composed by a fusion network, aka. multi-modal layers, for learning sound and visual features in a common semantic space, and two conditional GANs for converting sound features into visual features and those into images. To validate their approach they created an ad-hoc dataset, based on Flickr-SoundNet dataset, which contains 104K pairs of sounds and images with matching scene content. Their model was trained as two separate models, the fusion network was trained to classify both images and sounds minimizing their cross-entropy and their L1 distance, while the two conditional GANs were trained until convergence penalizing the discriminator to prevent fast convergence.

Although the idea of generating images from sounds with the aid of Generative Adversarial Networks is quite novel and interesting, the paper exhibits several problems starting with the lack of clarity explaining the purpose of the proposed method and the contributions of the work itself. Overall, the idea is good but not well developed. Introduction should present more clearly the problem and framework.

In the related work section the authors omitted some relevant recent prior works such as “Look, Listen and Learn” paper by Arandjelović and Zisserman presented on ICCV’17, “Objects that Sound” by Arandjelović and Zisserman presented on ECCV’18, “Audio-Visual Scene Analysis with Self-Supervised Multisensory Features” by Owens and Efros presented on ECCV’18, and “Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input” by Harwath et al. also presented presented on ECCV’18. These works propose different methods for aligning visual and sound features.

There are also several concerns on the validity of the results: 1) none of the results achieved by training their multi-modal layers were validated against a baseline, e.g. evaluating the quality of the learned visual features against VGG or a simple GAN instead of two stacked conditional GANs, 2) it is not clear why they learned features minimizing L1 loss + Cross-Entropy while using L2 distance to address the quality of their learned features, a simple way of doing so would be evaluating their retrieval capabilities using any standard measure from the retrieval community, e.g. the normalized discriminative cumulative gain (nDCG) or the classical mean-average precision (mAP) as proposed in “Objects that Sound”, 3) the authors assume that using a conditional GAN is suitable for generating images from visual features, but they don’t provide any quantitative results supporting this claim, they only provide a few successful qualitative results and elaborate their model from there. 4) Ablation is completely missing: it would be interesting to prove the effective contribution for i) the multi-modal fusion ii) the two-steps of image generation iii) the L_ losses for the two GANs.

There are many missing citations throughout the paper, in particular: 1) the concatenation of visual and sound features followed by a fusion network for learning features in a common semantic space was already proposed on “Look, Listen and Learn”, 2) when the authors describe their strategy for sound features extraction in section four, they never mentioned that the idea of using pool5 layer features was already introduced by SoundNet authors, and 3) in section 5.3 when they mention that using a conditional GAN to convert between two different feature domains it might be that the discriminator may converge too rapidly while the generator does not learn sufficiently.

Finally although using an ad-hoc extremely simplified dataset with pairs of images and sounds matching scene content, the complete model is able to generate images which achieve only a 8,9% matching rate for the top 3 predicted classes. Given that the dataset was created with 100% matching on the top 3 scores for sound and images, the results are definitely  poor.
","The sentiment score is -60 because the review is predominantly critical, pointing out several major issues with the paper such as lack of clarity, omission of relevant prior work, concerns about the validity of results, missing citations, and poor performance. However, it does acknowledge the novelty and interest of the idea, preventing an extremely negative score. The politeness score is 20 because while the reviewer maintains a professional tone and uses polite language like 'the authors present' and 'although the idea is...interesting', they are also quite direct in their criticisms. The reviewer provides constructive feedback and specific recommendations for improvement, which adds to the politeness, but the overall critical nature of the review prevents a higher score.",-60.0,20.0
"Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.

The current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.

 Strengths:

 This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.

 Minor comments:

 * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.
 * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.","The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper. They describe the experimental results as 'encouraging' and state that the paper 'clearly reveals' important findings. The reviewer also notes that the paper is 'mostly well written and overall is easy to follow'. The score is not higher because there are some minor criticisms and suggestions for improvement. The politeness score is 80 (polite) because the reviewer uses respectful and constructive language throughout. They begin by highlighting the strengths of the paper and frame their criticisms as 'minor comments'. The tone is professional and supportive, offering suggestions for improvement rather than harsh criticism. The use of phrases like 'perhaps a bit more detail should be given' indicates a polite and considerate approach to feedback.",70.0,80.0
"Disclosure: I reviewed this paper for a different conference but have read the new manuscript and noted the changes.

Summary:
The paper considers a very novel (but important) RL context where the agent has a constrained amount of information for representing a policy.  The authors use techniques from rate-distortion theory to generate a clever Bellman loss function that can be used (1) in a context where V*(s) is already known, and more importantly (2) with an actor-critic architecture (CL-AC) where the value function is being learned online.  CL-AC is shown to actually achieve higher converged and cumulative rewards than AC in many grid world domains and is shown to be advantageous in a transfer learning setting as well.

Review:

The ideas in the paper are very well described and laid out.  The experiments are on grid worlds but for such a novel problem like this I think they are at the right level because they allow the reader to understand the results.  The empirical results are compelling, but I have a strong technical concern about the convergence issue noted by the authors (which was also communicated to the authors in a previous conference’s review session).  

My main concern is, as the authors noted, the required state occupation probability p(s) for RDT is approximated in a way that could lead to bad behavior in the RL algorithm.  What we’re seeing here is the application of an RDT procedure that was designed for a static distribution being applied to a dynamic distribution of states (that can change based on the policy).  In RL, there is no guarantee that the previous occupation probabilities have anything to do with the current policy’s induced distribution.  In a hallway world with a decent reward down the left and a bigger reward to the right, an algorithm might start off by going down the left side several times, making the probabilities of states on the right 0.  If I am reading the algorithm right, the states on the right are going to be essentially dismissed as unlikely, and the “go right” action (which is optimal) will likely be compressed out, since the states it should be used in are considered unlikely.  More succinctly, early trajectories will bias p(s) and cause the algorithm to essentially want to optimize the policy for that distribution, likely causing it to stay in that distribution.  Even more dangerously, there may be cases where this could cause the algorithm to thrash between policies as p(s) oscillates between different parts of the state space.  

In order to improve this paper and make it suitable for publication, the authors should at least empirically demonstrate how different state occupation probability approximations affect the algorithm.  A good example is the trace-decay probabilities mentioned (but not implemented) in the paper.  If the paper compared that approach to the current approach, and showed an environment where one or both approaches failed to act correctly, that would complete the scientific result. Right now, only one approximation is demonstrated, and as detailed above, its behavior is suspect. 

While most of the empirical results are well explained, the behavior in Figure 2B, where CL-AC is outperforming standard AC remain unclear. I understand that in 2A (avg. cumulative reward), CL-AC may be inducing a more efficient exploration policy and therefore the rewards during learning will be better.  But in 2B, we are just looking at the final policy.  Was standard AC not able to find the optimal policy after 100 episodes?  

The results in the transfer learning context (Figure 3) are well done and produce a very interesting curve.  

Reference 9 appears to only be available as an arXiv pre-print.  Papers that have not been properly vetted by peer review should not be cited in an ICLR paper unless they are extremely necessary, which this does not appear to be.


Typo: Page 6 – sate -> state

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty and importance of the work, they express a 'strong technical concern' about convergence issues and request significant additional empirical demonstrations to make the paper suitable for publication. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions for improvement. They use phrases like 'The ideas in the paper are very well described' and 'In order to improve this paper' which maintain a collegial tone. The reviewer also discloses their previous involvement with the paper, demonstrating transparency and professionalism.",-20.0,60.0
"This article compares different methods to train a two-layer spiking neural network (SNN) in a bio-plausible way on the MNIST dataset, showing that fixed localized random connections that form the hidden layer, in combination with a supervised local learning rule on the output layer can achieve close to state-of-the-art accuracy compared to other SNN architectures. The authors investigate three methods to train the first layer in an unsupervised way: principal component analysis (PCA) on the rates, sparse coding of activations, and fixed random local receptive fields.  Each of the methods is evaluated on the one hand in a time-stepped simulator, using LIF neurons and on the other hand using a rate-approximated model which allows for faster simulations. Results are compared between each other and as reference with  standard backpropagation and feedback alignment.  The main finding is that localized random projections outperform other unsupervised ways of computing first layer features, and with many hidden neurons approaches backpropagation results. These results are summarized in Table 8, which compares results of the paper and other state-of-the-art and bio-plausible SNNs. PCA and sparse coding work worse on MNIST than local random projections, regardless if the network is rate-based, spike-based or a regular ANN trained with the delta rule. Feedback Alignment, although only meant for comparison, performs best of the algorithms investigated in this paper.

In general the question how to train multi-layer spiking neural networks in a bio-plausible way is very relevant for computational neuroscience, and has attracted some attention from the machine learning community in recent years (e.g. Bengio et al. 2015, Scellier & Bengio 2016, Sacramento et al. 2018). It is therefore a suitable topic for ICLR. Of course the good performance of single-layer random projections is not surprising, because it is essentially the idea of the Extreme Learning Machine, and this concept has been well studied also for neuromorphic approaches (e.g. Yao & Basu, 2017), and versions with local receptive fields exist as well (Huang et al. 2015 ""Local Receptive Fields Based Extreme Learning Machine""). While the comparison of different unsupervised methods on MNIST is somehow interesting, it fails to show any deeper insights because MNIST is a particularly simple task, and already the CIFAR 10 results are far away from the state-of-the-art (which is >96% using CNNs). Another interesting comparison that is missing is with clustering weights, which has shown good performance for CNNs e.g. in (Coates & Ng, 2012) or (Dundar et al. 2015), and is also unsupervised.

The motivation is not 100% clear because the first experiment uses spikes, and shows a non-negligible difference to rate models (the authors claim it's almost the same, but for MNIST differences of 0.5% are significant). All later results are purely about rate models. The authors apparently did not explore e.g. conversion techniques as in (Diehl et al. 2015) to make the spiking results match the rate versions better e.g. by weight normalization.

I would rate the significance to the SNN community as average, and to the entire ICLR community as low. The significance would be higher if it was shown that this method scales to deeper networks or at least can be utilized in deeper architectures. Scrutinizing the possibilitites with random projections on the other hand could lead to more interesting results. But the best results here are obtained with 5000 neurons with 10x10 receptive fields on images of size 28x28, thus the representation is more than overcomplete, and of higher complexity than a convolution layer with 3x3 kernels and many input maps.

Because the results provide only limited insights beyond MNIST I can therefore not support acceptance at ICLR.

Pros:
+ interesting comparison of unsupervised feature learning techniques
+ interesting topic of bio-plausible deep learning

Cons:
- only MNIST, no indications if method will scale
- results are not better than state-of-the-art


Minor comments:

The paper is generally well-written and structured, although some of the design choices could have been explained in more detail. Generally, it is not discussed if random connections have any advantage over other spiking models in terms of accuracy, efficiency or speed, besides the obvious fact that one does not have to train this layer. 

The title is a bit confusing. While it's not wrong, I had to read it multiple times to understand what was meant.

The first sentence in the caption for Fig. 2 is also confusing, mixing the descriptions of panel A and B. Also, in A membrane potentials are shown, but the post-membrane potential seems to integrate a constant current instead of individual spikes. Is this already the rate approximation of Eq. 2? Or is it because of the statement in the caption that they both receive very high external inputs. In general, the figures in panel A and B do not make the dynamics of the network or the supervised STDP much clearer. 

Principal Component Analysis and Sparse Coding are done algorithmically instead of using a sort of nonlinear Hebbian Learning as in Lillicrap 2016. It would have been interesting to see if this changes the comparatively bad results for PCA and SC.

In Fig. 3, the curve in the random projections case is not saturated, maybe it would have been interesting to go above n_h = 5000. As there are 784 input neurons, a convolutional neural network with 7 filter banks already would have around 5000 neurons, but in this case each filter would be convolved over the whole image, while with random projections the filter only exists locally. 

In Eq. 1, the notation is a bit ambigous: The first delta-function seems to be the Dirac-delta for continuous t, while the second delta is a Kronecker-delta with discrete t.

In A.1 and A.4.2 it is stated that the output of a layer is u_{t+1} = W u_t + b but I think in both cases it should be W a_t + b where a_t = phi(u_t). Otherwise, you just have a linear model and no activations. 

In Table 3, a typo: ""eq. equation""   ","The sentiment score is -40 because while the reviewer acknowledges some positive aspects ('interesting comparison', 'interesting topic'), they ultimately do not support acceptance and point out several limitations ('only MNIST', 'results are not better than state-of-the-art'). The overall tone is critical but not entirely negative. The politeness score is 60 because the reviewer uses professional and respectful language throughout, offering both pros and cons, and providing constructive feedback. They avoid harsh criticism and use phrases like 'I would rate' and 'I can therefore not support' which soften the negative feedback. The reviewer also provides detailed explanations for their critiques and offers suggestions for improvement, which is a polite approach to peer review.",-40.0,60.0
"The paper addresses the problem of increasing and decreasing the number of hidden nodes (aka, dimensionality) in the network such that the optimization will not enter the plateaus of saddle points. The opening or closing of tunnels (filters) guarantee the existence of “new escape directions” and faster convergence. 

Strengths:
+ provide a new perspective of designing the shape/dimensionality of a network in a dynamic manner. 
+ provide theoretic proof of CNNs and FCs on the contribution to the gradient after cloning. 

Weakness:
- Experiments are very weak to verify the theory.

Detailed comments:

- Eqn. (6) seems to provide a unified evaluation on the contribution of two units to the gradients. How does it relate to the experiments? It gives me a sense that the manuscript is isolated between theory (Section 2 and 3) and verification (experiments).
- Why does the blue curve get stuck in a flat area? A better staring learning rate could alleviate the plateau bottleneck. 
- The experiment settings are a little bit simple, even for the most complicated one in Section 4.4, where there are five conv layers and the tunnel opening only involves one single filter. Do authors conduct more filters opening in more layers? How about the closing case? There is no result/analysis in the experiments. 
- Why authors claim the blue curve in the left figure 2, a “flat area”? It seems working as the orange one (loss decreases normally). 
- Another big concern is that the proposed method is supposed to prevent network from saddle points and faster convergence, which is verified. And yet, the ultimate goal is to improve the performance. I am surprised that there is no such result at all in the manuscript (for example, error rate goes down on cifar/mnist/etc). 

In summary, I do recognize the theoretical effort the paper has provided; however, the experiments seem not to verify the proposed method in a professional manner. 

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the experimental validation. The review starts positively by mentioning the paper's strengths, but then lists several weaknesses and detailed criticisms, concluding that the experiments do not verify the proposed method adequately. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They acknowledge the theoretical effort and use phrases like 'I do recognize' and 'I am surprised' rather than using harsh or dismissive language. The reviewer also provides constructive feedback and specific suggestions for improvement, which contributes to the polite tone.",-20.0,50.0
"This work builds on the ICML paper from Saxe et al (2017) in which the compositionality property of LMDPs was exploited to solve multi-task hierarchies. The paper extends this work by proposing a method that learns incrementally such hierarchies instead of pre-defining them by design. Some experimental results illustrate the method on two toy problems, a 1D corridor and a corridor of rooms.

The paper deals with an interesting and hard problem. Learning hierarchies while solving an MDP is a much harder problem than solving the flat MDP or solving the hierarchical MDP. The authors leverage the compositionality of optimal controls of the LMDP framework to learn incrementally the hierarchies. Surprisingly, the proposed method not only learns those hierarchies, but also is more effective in terms of exploration.

On the positive side, the main idea is very interesting and has a lot of potential. The authors combine many techniques under the powerful framework of LMDPs such as hierarchical RL, low-rank factorization, and count-based exploration. The authors do a good job describing their approach (at a higher level).

On the negative side, the paper looks a bit incremental, given the prior existing work. I also found the paper unclear in many aspects lacking some relevant technical details (see below). The narrative is sometimes superficial or focused mainly on intuitions and analogies. Overall, it is difficult to assess the significance of this work and the results give the impression of limited applicability, beyond the described toy problem.

1- First, in order to combine optimal controls, the LMDPs need to be solved for each different boundary state, i.e., do you require to solve as many LMDPs as possible states? If that is the case, I don't think it makes sens to talk about exploration/exploration trade-off, since you really need to visit all states a priori.

2- I cannot understand what is learned and what is required a priori. the authors state that if ""the multi-scale structure of the domain is known a priori, the decomposition (...) explicitly specified"". What does exactly that mean? If what the method does is an incremental version of the low-rank factorization proposed in Earle at al (2018), I think the presentation can be better described in those terms.

2- Regarding exploration/exploitation tradeoff. From section 3, it seems that authors focus on a particular spatial problem and define already some exploration choices. But this means that the choice about when a state is integrated in the current MDP is already done, so no real trade-off exists?

3- The narrative in Section 3.2 is not very rigorous. The authors just mention the computational problem to keep consistency between layers and then just argue that ""in practice"" using count-based exploration everything works. I think a more principle approach is necessary.

4- Experiments I: what do the authors means by ""exploration""? Is it just Boltzmann exploration? I can think of an exploration strategy that would choose an unseen state with probability 1 and would bring the agent to the goal in one shot.

5- Experiments II: I like the benchmark but, how does the result depend on the structure of the problem? What happens if I the rooms have very different sizes?

6- I miss some references that are very relevant to this work:

- the ICAPS paper ""Hierarchical Linearly-Solvable Markov Decision Problems"" by Jonson et al. seems to be the first proposing a hierarchical embedding of LMDPs.

- other factorization techniques exist, e.g., ""Incremental Stochastic Factorization for Online Reinforcement Learning"", Barreto et al (AAAI' 16), to uncover an MDP structure.

There are also some minor grammar mistakes:

""passive dynamics then become"" -> ""passive dynamics then becomes""
""reward function r"" -> ""reward function R""
""Howver"" -> ""However""
""the room contains"" -> ""the room that contains""
""have simply add"" -> ""have simply added""
""spacial and temporal"" -> ""spatial and temporal""
...","The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting and potentially impactful nature of the work, praising the main idea and the combination of techniques. However, they also express significant concerns about the paper's clarity, incrementality, and limited applicability, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer maintains a professional and respectful tone throughout, balancing criticism with praise and using phrases like 'On the positive side' and 'I like the benchmark'. They offer constructive feedback and suggestions for improvement without using harsh language. The reviewer also provides helpful specific recommendations and points out minor grammar mistakes in a matter-of-fact way, which contributes to the overall polite tone while maintaining academic rigor.",20.0,60.0
"The paper proposes contextual role representation which is an interesting point. 
The writing is clear and the idea is original.
Even with the interesting point, however, the performance improvement seems not enough compared to the baseline. The baseline might be carefully tuned as the authors said, but the proposed representation is supposed to improve the performance on top of the baseline.
The interpretation of the role representation is pros of the proposed model. However, it is somehow arguable, since it is subjective. 

- minor issues: 
There are typos in the notations right before Eq. (8). 
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting points, clear writing, and original idea, they express concerns about the performance improvement and the subjectivity of the interpretation. The phrase 'Even with the interesting point, however, the performance improvement seems not enough' indicates disappointment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'interesting point' and 'clear writing' to soften their critique. The reviewer also frames their concerns as observations rather than direct attacks, maintaining a professional tone. The minor issues are presented neutrally, without harsh language.",-20.0,50.0
"The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. 

There are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There's a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It's very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they ""successfully complete the task"" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?

The primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn't feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's NLU/NLG system is quite weak and maybe there's a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.

As I understand the paper right now:
1. Humans talking to one another do very well on the task and achieve success very quickly. 
2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).
3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.

There are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn't seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what's going wrong.  The results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. 

I haven't touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.

I found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  

Minor: URLs intermittently anonymized page 12 vs 19","The sentiment score is -60 because the reviewer expresses significant confusion and skepticism about the paper's methodology, results, and claims. They point out several issues and inconsistencies, stating that the paper is 'very confusing to read' and 'fails to clearly explain its contributions or justify its claims.' The reviewer also questions the validity of some of the paper's main conclusions. However, the score is not at the extreme negative end because the reviewer does acknowledge some positive aspects, such as the dataset contribution.

The politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively polite tone throughout. They use phrases like 'I found hard to follow/justify' and 'Additional justification and analysis would be appreciated' rather than using harsh or rude language. The reviewer also offers constructive feedback and suggestions for improvement. However, the score is only slightly positive as the review is quite direct in its criticism and doesn't use many overtly polite phrases or softening language.",-60.0,20.0
"This paper suggests an exploration driven by uncertainty in the reward space.
In this way, the agent receives a bonus based on its squared error in reward estimation.
The resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.

There are several things to like about this paper:
- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).
- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.
- The algorithm does appear to outperform the basic DQN baseline on their experiments.

Unfortunately, there are several places where the paper falls down:
- The authors wrongly present prior work on efficient exploration as ""exploration in state space"" ... rather than ""reward space""... in fact prior work on provably-efficient exploration is dominated by ""exploration in value space""... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to ""counts"" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853

- So this algorithm falls into a pretty common trope of algorithms of ""exploration bonus"" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)

- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around ""randomized value functions""

- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.

Overall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.
It could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests ""exploration by uncertainty in value space""... e.g. ""deep exploration via randomized value functions""","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('There are several things to like about this paper'), they also point out significant shortcomings ('Unfortunately, there are several places where the paper falls down'). The reviewer expresses that the core idea is good but not fully developed, and that there are issues with the presentation of prior work and the experimental results. The overall tone suggests that the paper needs substantial improvements.

The politeness score is moderately positive (60) because the reviewer maintains a professional and constructive tone throughout. They begin by highlighting the positive aspects of the paper before moving on to criticisms. The language used is respectful and focuses on the content rather than making personal comments. Phrases like 'There are several things to like about this paper' and 'I do like the idea behind this paper' soften the criticism. Even when pointing out flaws, the reviewer uses phrases like 'It could be that I am wrong' and offers suggestions for improvement, which contributes to the polite tone.",-20.0,60.0
"-- Paper summary --

The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration.

-- General Commentary --

The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn’t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time?

I have plenty of concerns with the submission itself, listed below:

- First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn’t properly check the paper before submission.

- While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what’s being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least.

- The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (‘uncalibrated’, ‘temp-scal' and ‘BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot.

- As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include ‘Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks’ (Bradshaw et al, 2017), and ‘Calibrating Deep Convolutional Gaussian Processes’ (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models. 

- With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion. 

- I can’t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the ‘alternative hypothesis’ being mentioned, and the investigation is entirely limited to the offline setting, so I’m not entirely sure what distinction the authors are trying to make here. 

- In the same section, the authors then remark that ‘Our results open new perspectives to improve the variational approximation…’ and ‘we believe our results might foster further research in…’, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can’t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers. 

- The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper’s lack of identity and focus.

- There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient. 

- There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by ‘conf’ in Equation 4. 

- Referring to ‘datasets’ as ‘databases’ in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by ‘uses BNNs’? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission.

- In their discussion of the results, the authors state that ‘We cannot conclude that BNNs are calibrating at the cost of losing accuracy’, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score.

-- Recommendation --

Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference’s standards.

Pros/Cons summary:

+  The proposal yields good results in the provided experiments
-   Minor contributions that are not convincing enough
-   Muddled presentation of ideas
-   Dubious or weakly motivated design choices
-   Poorly written with plenty of typos
-   Difficult to follow","The sentiment is highly negative (-80) based on several factors: The reviewer states the paper's goal is 'modest' and 'limited in scope', criticizes the writing as full of typos and grammatical errors, describes the presentation as 'muddled' and 'sloppy', expresses disappointment with the lack of scope, and ultimately recommends rejection, stating it's 'well below the conference's standards'. The politeness score is slightly negative (-20) because while the reviewer maintains a professional tone overall, there are instances of harsh criticism ('sheer negligence', 'sloppiness') and dismissive language ('spurious comments', 'overly sunny view') that go beyond neutral academic critique.",-80.0,-20.0
"This paper proposes a neural network architecture PCnet for the prediction of intensive and extensive chemical properties of molecules and materials. The authors claim that the use of prior chemical knowledge such as Mulliken electronegativity, bond strength and orbital information improves prediction accuracy. While the idea of incorporating chemical domain knowledge in the interactions of an atomistic neural network is interesting in principle, this paper has severe issues ranging from presentation over the proposed approach to the results.

First and foremost, I would like to point out that the results in Table 1 are cherry-picked since the authors fail to cite neural network architectures that outperform their approach, e.g. for U0: 0.45 kcal/mol [MPNN, Gilmer et al 2017, ICML], 0.31 kcal/mol [SchNet, Schütt et al, NIPS 30, 2017], 0.26 kcal/mol [HIP-NN, Lubbers et al., JCP 148, 2018]. This is especially apparent since Figure 4 is obviously inspired by Fig. 1 in [SchNet, Schütt et al, JCP 148, 2018]. 

The authors use a variety of heuristics and approximations such as a ""charge transfer ability"", bond strength, exponential decay of distances and overlaps of atomic orbitals which are multiplied ""to mix all things up"", to arrive at the PixelChem representation which is then fed into an atomistic neural network (PCnet). Combining these chemical features in such a way is neither well-motivated, nor does it lead to an improvement in accuracy compared to state-of-the-art networks.
Even for the intensive properties (gap, HOMO, LUMO), where PCnet is supposed to have an advantage due to its use of orbital information, MPNN, SchNet and even GC and GG-NN in Table 1 outperform the proposed approach. Parameterization of the chemical features and training the PCnet end-to-end might have improved results and seems like a missed opportunity.

Further issues:
- The manuscript is riddled with typos, grammatical errors as well as confusing sentences.
- The authors claim that PCnet is applicable to periodic structures, however, this is never demonstrated. Beyond that their definition of periodic PixelChem does only include adjacent cells, while for a unique representation more cells might be required.
- The ""benefits"" listed in Section 2.3 compare selectively to previous work. E.g., invariances, uniqueness, asymmetric interactions are also fullfiled by the neural networks listed above. A comparison of the PixelChem representation to the Coulomb matrix is not sufficient here.
- The PCnet architecture uses PReLU nonlinearities. While this is fine for equilibrium predictions, for other configurations this prohibits the prediction of a smooth PES.

Overall, I believe that it is important to incorporate chemical knowledge into neural networks. However, neither the approach nor the results convince me that this has been achieved here.","The sentiment score is -80 because the review is highly critical of the paper, pointing out 'severe issues' and stating that neither the approach nor the results are convincing. The reviewer also accuses the authors of cherry-picking results and failing to cite better-performing architectures. The politeness score is -20 because while the reviewer maintains some level of professional language, there are instances of blunt criticism and accusatory language, such as 'cherry-picked' and 'neither the approach nor the results convince me'. The reviewer does not use overtly rude language, but the tone is decidedly negative and critical throughout, with little attempt to soften the criticism or acknowledge positive aspects of the work.",-80.0,-20.0
"

The authors describe an anomaly/novelty detection method based on handcrafted features + VGG based features. 

I think the paper is out of the scope of the conference (the only part dealing with learned representations uses VG), plus it addresses a problem whose relevance is not correctly motivated. Finally, the method is quite basic, and is not compared to any state of the art method for novelty detectiobn.  

In ""... the detection of visual anomalies perceived by human observer is an open challenge… "" can you provide references of people working in this particular problem?

The review of related work seems obsolete, can you provide more recent references (in addition to ""historical"" ones). More importantly, please provide references of anomaly detection from textures
","The sentiment score is -70 because the reviewer expresses several significant criticisms: the paper is out of scope, the problem's relevance is not well-motivated, the method is basic, and there's a lack of comparison to state-of-the-art methods. These are major issues that suggest the reviewer views the paper negatively. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the feedback or offer encouragement. The reviewer uses phrases like 'out of scope' and 'quite basic' without hedging, and the requests for additional references are phrased as commands rather than suggestions. This directness, while not impolite per se, lacks the courtesy often seen in more tactful reviews.",-70.0,-20.0
"The authors present a large synthetic dataset for 3D scenes with templated descriptions.  They then use the model of Eslami 2018 to this new domain.  The previous work appears to already introduce all the necessary mechanisms for 3D generalization from multiple viewpoints, though this work embeds language instead of a scene in the process.  Minor note: A bit more discussion on this distinction would be appreciated.  Also, it appears that the previous work includes many of the rendered scenes also present here, so the primary focus of this paper is on the use of a language encoder (not necessarily a trivial extension).

The model appears to perform well with synthetic data though very poorly with natural sentences.  This may be in part due to the very small dataset size.  It would be helpful to know how much of the performance gap is due to scaling issues (10M vs 5.6K) versus OOVs, new syntactic constructions, etc.  In particular, the results have ~two deltas of interest (NL vs SYN) and the gap in the upper bound from 0.66 to 0.91.  What do new models need to be able to handle to close these gaps?

Regarding the upper bound, there is some discussion that annotators might have had a strict definition of a perfect match.  Were annotators asked about this? The current examples (B.2), as the authors note, are more indicative of failings with the synthetic language than the human annotators.  This may again be motivation for collecting more natural language which would resolve some of the ambiguity and pragmatics of the synthetic dataset.

It would also be helpful to have some ablations included in this work. The most obvious being the role of $n$ (number of scene perspectives).  How crucial is it that the model has access to 9 of 10 perspectives?  One would hope that given the limited set of objects and colors, the model would perform well with far fewer examples per scene, learning to generalize across examples.

Since the primary contributions of the paper are a language dataset and a language encoder for the existing model of Eslami 2018, those should be discussed and ablated in the paper rather than relegated to the appendix.

Minor note:  the related work mentions grounding graphs which are core to work from Tellex and Roy, but omits existing fully neural end-to-end models in grounding (e.g. referring expressions work).
","Sentiment score: The review starts with a neutral tone, acknowledging the authors' work. However, it then points out several limitations and areas for improvement, suggesting a slightly negative sentiment. The reviewer notes poor performance with natural sentences, calls for more ablations, and suggests that the main contributions should be discussed more thoroughly in the main paper rather than the appendix. These critiques outweigh the initial neutral tone, resulting in a slightly negative score of -20.

Politeness score: The language used is generally polite and professional. The reviewer uses phrases like 'it would be helpful' and 'one would hope' rather than making direct demands. They also acknowledge the authors' work and use constructive language throughout. However, the review is not overly effusive in its praise, maintaining a professional distance. This results in a moderately positive politeness score of 50.

Both scores take into account the overall tone, specific language choices, and the balance between positive acknowledgments and constructive criticism throughout the review.",-20.0,50.0
"The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set. This scenario is well-motivated by the ""researcher example"" given throughout the paper. The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain. Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).

The key idea is to learn a mapping from the source domain to the target domain. This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain. In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.  Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.

Pros:
- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.
- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.
- The paper reads well and is easy to follow.

Cons:
- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"". Further, there a number of experimental details that need to be further elaborated upon. e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this). It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?
- Some assumptions are not explicitly stated. In particular, it is unclear what the assumption on the size of the unlabelled test set is. This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.
- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further? Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?
- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.

Overall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research. Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches). Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task. It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.
","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's importance, novelty, and readability while also pointing out significant concerns. The pros mentioned are substantial, but the cons are equally weighty, particularly regarding reproducibility and methodological issues. The overall tone suggests cautious approval, recommending the paper for the venue if certain improvements are made. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, frames criticisms constructively, and offers suggestions for improvement. The reviewer acknowledges the paper's strengths before discussing its weaknesses and uses phrases like 'Can the authors comment on this further?' which shows respect and invites dialogue rather than making blunt criticisms.",50.0,80.0
"The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. 
Pros:
The paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.
Cons:
i) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?
iii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. 

This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also express significant concerns ('Cons') and criticize the paper's main argument as having 'insufficient' evidence. The overall tone suggests the reviewer is not fully convinced by the paper's conclusions. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, presenting both pros and cons objectively without harsh criticism. They offer constructive feedback and suggestions for improvement, maintaining a courteous tone even when expressing doubts about the paper's conclusions.",-20.0,50.0
"
This paper focuses on the reduction of training time by various mechanisms. By introducing a time gate during training, it controls when a neuron (weights?) can be updated during training. By introducing and additional budget term in the loss function, training costs (number of computations) are reduced by one order of magnitude. 
A major advantage of the newly introduced Gaussian-gated LSTM (g-LSTM; I suggest using a capital G for Gauss, e.g., GgLSTM).

Experiments are carried out on the adding-problem from 1997; the sequential MNIST and the sequential CIFAR-10 problem. In all experiments, g-LSTM converges faster. A few things would be of interest:
- clearly state the stopping criterium for training. Especially, I would still be interested to see, how Fig. 3d continues; it seems that the network begins to collapse (also a and be are interesting to see).
The ""This work"" in Table 2 is confusing; I would expect it to appear behind g-LSTM; 
It appears that in the budgeted g-LSTM some units are not used at all (Figure 5b); Please comment on that.

In general, the paper makes the impression that it is overselling the contribution a bit too much. It would be nice to question the outcomes more and investigate the g-LSTM for the existence of possible problems which might be introduced by the omission of computations.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and advantages of the new method, but also points out areas for improvement and suggests the paper may be overselling its contribution. The politeness score is 70 (fairly polite) as the reviewer uses respectful language, makes suggestions rather than demands, and phrases criticisms constructively. The reviewer uses phrases like 'I suggest,' 'It would be nice,' and 'Please comment on that,' which contribute to a polite tone. The review provides specific, constructive feedback without harsh criticism, maintaining a professional and courteous approach throughout.",50.0,70.0
"This paper presents ODD, a method that rejects incorrectly labeled / noisy examples from training on the fly. The motivation is sound, that with the capacity of modern neural networks, it's easy to memorize the mislabeled data and thus hurt generalization. If we could reject such mislabeled data, we may be able to get a more generalizable model. The authors made an observation that when training with large learning rate, examples with correct labeling and incorrect labeling exhibits different loss distributions. The authors further noticed that the loss distribution of incorrectly labeled examples can be simulated using eq.(1). Therefore, by setting a threshold that corresponds to a percentile of the incorrectly labeled loss distribution, the authors are able to reject incorrect examples.

Some comments:
1. Eq.(1) basically assumes all the noise is uniformly distributed among classes. What if only 2 classes are easily mislabeled while others are fine?
2. Section 4.1.3 and Section 4.4 Sensitivity to Noise are confusing. Please clarify the importance and rationale for such analysis.
3. Cosine schedule is used in the experiments. However, since the method does not work well with small learning rate, why not using a fixed large learning rate and decrease it after noise rejection? Also, in section 4.4 Sensitivity to E, the analysis of the sensitivity to the number of epochs is coupled with a changing learning rate. It would be better to see an experiment with the two decoupled.
4. The loss of an example is averaged over h epochs. It will better to clarify how the simulated distribution generated in such case since the distribution is dependent on fc(.), which is changed between two epochs.
5. Except for the first experiment, all other experiments are only compared with ERM, the vanilla algorithm. It would be better to show a comparison with other methods.
6. Please show a precision/recall of the examples that are marked as ""noise"" by the method.
7. I assume this method will remove a lot of hard examples. How does this affect training? Does this make the network more error-prone to harder instances?","The sentiment score is slightly positive (20) because the reviewer acknowledges the sound motivation and potential benefits of the proposed method. They provide constructive feedback and suggestions for improvement, indicating a generally positive view of the paper. However, they also raise several questions and concerns, which prevents the score from being higher. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as suggestions or questions, and maintains a professional tone. They begin with positive comments and use phrases like 'Please clarify' and 'It would be better to' when suggesting improvements, which contributes to the polite tone. The score is not higher because the review is primarily focused on technical aspects rather than using explicitly polite language.",20.0,60.0
"Summary: The paper considers a variational inference strategy for learning neural networks with binary weights. In particular, the paper proposes using a structured recognition model to parameterise the variational distribution, which couples the weights in different layers/filters in a non-trivial way. The gradient of the expected likelihood term in the variational lower bound is estimated using the REINFORCE estimator. This paper adjusts this estimator to use the gradient of the log-likelihood wrt the samples. Experiments on several image classification tasks are provided.

evaluation:

pros:
- the idea of the proposed approach is interesting: using variational inference for binary weight neural networks. While recent work on VI for discrete variables only focused on discrete latent variable models, this work shows how VI can be used for binary neural networks.
 
cons:
- the writing, in my opinion, needs to be improved [see my comments below]. The VI presentation is cluttered and the justification of using the pseudo-reward for reinforce is not clear.
- the experimental results are mixed and it's not clear to me how to interpret them/compare to the baselines -- what is the goal here: computational efficiency, compression or accuracy?

Some specific questions/comments:

+ What is the input of the policy/recognition network? It's not clear from the paper whether this includes the inputs of the current batch or outputs or both? If so, how are variable batch sizes handled? What is the input to this network at test time? In contrast to generative models/VAEs, the weights here are global parameters and it's not clear to me these should be varied for different data batches.

+ related to the question above: how is prediction handled at test time? Say the parameters of the variational distribution over weights are generated using the recognition network, then 100 weights are sampled given these parameters which then give 100 predictions -- should these be then averaged out to get the final prediction? I'm not quite sure I understand why the paper chose to *pick the best one* out of 100 predictions and the justification/criterion for this procedure.  

+ The writing is not very clear at places, and it does not help that the references being merged with the text. I'm also not sure about some of the technical jargons/terms used in the papers:
- reinforcement learning: is this really a reinforcement learning problem? If you tackle this problem from a pure variational perspective, reinforce is used to obtain the gradient of the expected log-likelihood wrt the variational parameters. But instead of using the log likelihood, a learning signal that depends on the gradient of the log-likelihood is used.
- concrete weights -- what are these? I assume they are just binary weights sampled from the variational approximation.
- middle of page 3: p(w|X, Y) = p_\theta(w): this is not precise as p_\theta(w) is only an approximation to the exact posterior, which then allows us to lower bound the log marginal likelihood. ""common practice in modern variational approximation"": This is the standard way of deriving the lower bound and has been used for many years.

+ the reinforce estimator tends to have high variances since it does not make use of the gradient of the function in the expectation. This paper adjusts the vanilla estimator with a learning signal that involves the gradient. Could you comment on the bias/variance trade-off of the resulting estimator? Much of recent literature on learning discrete variables, as far as I understand, propose ways to not to have to use the vanilla reinforce, for example Concrete, Relax or rebar, albeit the focus on latent variable models.

+ model selection and uncertainty measure: the paper mentions these potential advantages of the proposed approach over deterministic binarisation schemes, but does not fully explore and test these.

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('the idea of the proposed approach is interesting'), they also list several significant concerns ('cons') and raise multiple questions about the methodology and clarity of the paper. The overall tone suggests more criticism than praise. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They use polite language such as 'in my opinion' and phrase criticisms as questions or suggestions rather than harsh statements. The reviewer also balances negative points with positive ones, showing a fair approach. However, the score is not higher as the review is primarily focused on critiquing rather than praising the work.",-20.0,50.0
"This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input.

My biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage).

It would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet's depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here.

That said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio).

The fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process.

I think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model.

Overall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used.



Miscellany:

- In the abstract: ""is substantially better in quality"", compared to what?

- In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don't tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective.

- Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3.

- ""Conditioning Deep Generative Raw Audio Models for Structured Automatic Music"" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference.

- In the contributions of the paper, it is stated that ""the generated audio is practically identical to ground truth as can be seen in Figure 4"" but the CQTs in this figure are visibly different.

- I don't think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio).

- At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued.

- Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning.

- Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities.

- The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without).

- In Section 4.3, specify the unit, i.e. ""Delta < 1 second"".

- For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks.

- In Section 4.3 under ""global conditioning"", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used?","The sentiment score is -40 because the reviewer expresses significant concerns about the paper's fundamental approach, particularly the use of synthetic data, which they feel 'defeats the purpose' of the work. They state that the paper has 'fundamental flaws' and 'overstates its contributions'. However, the score is not extremely negative as the reviewer also acknowledges some positive aspects, such as the 'impressive' training on limited data and the potential value of the RMSE-CQT error measure. The politeness score is 60 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I think' and 'I feel' to soften criticisms. They also balance negative points with positive observations and provide constructive suggestions for improvement. The language is not overly formal or deferential, but it avoids harsh or dismissive phrasing, maintaining a collegial tone appropriate for academic discourse.",-40.0,60.0
"Summary:
This paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \theta) \log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.

Critique:
The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih & Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  

Conclusion: Due to lack of novelty, I recommend rejection.


Miscellaneous points:
“...there exist no simple solutions to circumvent this problem.”  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. 

“...after training q(z|x) is a very good approximation to the true posterior p(z|x).”  That’s not necessarily true.  

Equation #2 should be just equal to Equation #1.

“Kingma & Welling (2013) proposed to minimize L(\theta) using stochastic gradient descent on a training set...”. First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma & Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.

Before Equation #11, the reference to Equation #4 should be to Equation #5.

“...the weighting...depends only on \theta_D and not on \theta_E” (p 4). D and E should be switched.","The sentiment score is -80 because the review is predominantly negative, recommending rejection due to lack of novelty. The reviewer points out that the proposed method is equivalent to an existing technique (REINFORCE) and has been previously used in similar contexts. The conclusion explicitly states 'Due to lack of novelty, I recommend rejection.' The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use neutral language like 'The gradient estimator the paper proposes is...' and 'Miscellaneous points:' rather than using harsh or dismissive language. The reviewer also acknowledges potential for novelty if the authors had chosen a different approach, showing some consideration. However, the overall tone is more matter-of-fact than overtly polite, hence the relatively low positive score.",-80.0,20.0
"This paper aims for open-domain question answering with distant supervision. First, the authors proposed an aggregation-based openQA model with sentence discriminator and sentence reader. Second, they use a semantic labeler to handle distant supervision problem by utilizing other span supervision tasks, and propose two different denoising methods. They run experiments on 3 open-domain QA datasets and achieve SOTA.


Strengths

1) Their semantic labeler and exploration of two different denoising methods are interesting and meaningful.
2) They conducted experiments on 3 widely-used open-domain datasets, and the performance gain is impressive.


Weakness

Although there is an impressive performance gain, the contribution of the paper seems to be marginal.
1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot. Also, the architecture of the models in this paper are almost identical to Chen et al (ACL 2017) and Lin et al (ACL 2018).
2) Thus, the contribution is more on semantic labeler and denoising method. However, this contribution is marginal as well since its role is almost the same as sentence discriminator plus pretraining methods which have widely used already.


Questions

1) What exactly is the difference between semantic labeler and sentence discriminator? For me, it seems like both of them label each sentence `yes` or `no`. My thought is sentence discriminator is only trained on the target dataset (distant supervision dataset) while semantic labeler is also trained (either jointly or separately) trained on the source dataset (span supervision dataset). (If my thought is wrong, please let me know, I would like to update my score.)
2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. Similarly, Min et al (ACL 2018) have pretrained both QA model and sentence selector on SQuAD. While I think pretraining sentence selector on SQuAD is almost identical to sentence labeler with SSL method, could you give exact comparison of these different methods? For example, remove sentence labeler, and pretrain both sentence discriminator and reader on SQuAD, or jointly train them on SQuAD & target dataset.


Marginal comments

1) At the beginning of Section 2.4.1, it says the semantic labeler is able to transfer knowledge from the span supervised data — however, the authors should be careful since people usually refers to `knowledge` as an external knowledge. This method is more like better learning of accurate sentence selection, not transferring knowledge.
2) Please mention the TriviaQA data you used is Wikipedia domain, since there are two different domains (Wikipedia and Web).
3) In References section, the conference venues in many papers are omitted.


Overall comments

The paper explored several different methods to deal with distant supervision via sentence labeling, and I really appreciate their efforts. While the result is impressive, the idea in the paper is similar to the methods that have widely used already.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths and impressive results, they express significant concerns about the novelty and contribution of the work. The reviewer states the contribution seems 'marginal' multiple times and points out similarities to existing methods. However, they do appreciate the authors' efforts, which prevents the score from being more negative. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the authors' efforts, and frames criticisms as questions or suggestions rather than harsh statements. They also balance negative points with positive ones. The reviewer maintains a professional tone while still clearly conveying their concerns.",-20.0,60.0
"This paper points out a important issue in current continual learning literature: Due to the different settings and different evaluation protocols of each method, comparison between methods are usually not fair, and lead to distinct conclusions.
The paper is in general easy to understand except a few drawbacks listed in the cons.

Pros:
1. This paper investigates an important problem, aka, how does the methods compare to each other with the same evaluation protocol.
2. Experiments are performed on the previous methods, which could be used as a baseline for future works in this field.
3. Proposes to combine discriminative model with generative model to save computation when using generative model to store rehearsal examples.

Cons:
1. Details of each experiments are missing. 
Different methods are evaluated under the ""incremental task learning"", ""incremental domain learning"",  ""incremental class learning"" settings. However, to my knowledge, some of the methods will not work under all of the three settings, as the author also suggest that XdG only works with task id. However, I think there are a few more. For example, the LwF methods has multiple sets of output neurons, which implicitly assumes the task id is known. It is not described in the paper how to evaluate it under ""incremental domain learning"", aka, how to decide which set of output to use if task id is not available during testing. Another example, the results in table 3 and 4 indicates that EWC with task id is better than without. However, original EWC does not take task id during testing, it is not described how to introduce dependency on the task id for EWC.
2. Using the term feedback connection is misleading to the reader since the described method is just using an encoder/decoder structure. In my opinion this is different from feedback connection in which higher layer is an input for lower layers. Autoencoder or encoder/decoder structure is more appropriate.
3. There is some contribution in the RtF part, namely the saved computation compared to DGR. However, subjectively, I think this contribution is not very significant. The same thing can be achieved with DGR by sharing the network between the discriminative model and the discriminator in GAN. In my opinion this is more a design bonus in using generative replay than a major methodology innovation.

Conclusion:
The first part that compares different methods is worth publishing given more details are provided. I'm more than happy to give a higher score if the authors are able to provide more details and the details are reasonable.","The sentiment score is slightly positive (20) because the reviewer acknowledges the importance of the paper's topic and its contributions, but also lists several significant cons. The overall tone suggests the paper has merit but needs improvements. The politeness score is moderately high (60) as the reviewer uses respectful language, acknowledges the paper's strengths, and offers constructive criticism without harsh language. The reviewer also expresses willingness to give a higher score if improvements are made, which is a polite way to encourage the authors to address the concerns.",20.0,60.0
"This paper sets out to build good bilingual word alignments from the information in an NMT system (both Transformer and RNN), where the goal is to match human-generated word-alignments as measured by AER. At least that’s how it starts. They contribute two aligners: one supervised aligner that uses NMT source and target representations as features and is trained on silver data generated by FastAlign, and one interpretability-based aligner that scores the affinity of a source-target word-pair by deleting the source word (replacing its embedding with a 0-vector) and measuring the impact on the probability of the target word. These are both shown to outperform directly extracting alignments from attention matrices by large margins. Despite the supervised aligner getting better AER, the authors proceed to quickly discard it as they dive deep on the interpretability approach, applying it also to target-target word pairs, and drawing somewhat interesting conclusions about two classes of target words: those that depend most of source context and those that depend most on target context.

Ultimately, this paper’s main contribution is its subtraction-based method for doing model interpretation. Its secondary contributions are the idea of evaluating this interpretation method empirically using human-aligned sentence pairs, and the idea of using the subtraction method on target-target pairs. The conclusion does a good job of emphasizing these contributions, but the abstract and front-matter do not. Much of the rest of the paper feels like a distraction. Overall, I believe the contributions listed above are valuable, novel and worth publishing. I can imagine using this paper’s techniques and ideas in my own research.

Specific concerns:

The front-matter mentions ‘multiple attention layers’. It would probably be a good idea to define this term carefully, as there are lots of things that could fit: multiple decoder layers with distinct attentions, multi-headed attention, etc.

In contrast to what is said in the introduction, GNMT as described in the Wu et al. 2016 paper only calculates attention once, based on the top encoder layer and the bottom decoder layer, so it doesn’t fit any definition of multiple attention layers.

Equation (1) and the following text use the variable L without defining it.

‘dominative’ -> ‘dominant’

Is there any way to generate a null alignment with Equation 3? That is, a target word that has no aligned source words? If not, that is a major advantage for FastAlign.

Similarly, what exactly are you evaluating when you evaluate FastAlign? Are you doing the standard tricks from the phrase-based days, and generating source->target and target->source models, and combining their alignments with grow-diag-final? If so, you could apply the same tricks to the NMT system to help even the playing field. Maybe this isn’t that important since the paper didn’t win up being about how to build the best possible word aligner from NMT (which I think is for the best).

I found Equations (7) and (8) to be confusing and distracting. I understand that you were inspired by Zintgraf’s method, but the subtraction-based method you landed on doesn’t seem to have much to do with the original Zintgraf et al. approach (and your method is much easier to the understand in the context of NMT than theirs). Likewise, I do not understand why you state, “we take the uniform distribution as P(x) regarding equation 8 for simplicity” - equation 9 completely redefines the LHS of equation 8, with no sum over x and no uniform distribution in sight.

The Data section of 4.1 never describes the NIST 2005 hand-aligned dataset.

The conclusions drawn at the end of 4.4 based on ‘translation recall’ are too strong. What we see is that the Transformer outperforms Moses by 2.8 onCFS, and by 3.7 on CFT. This hardly seems to support a claim that CFT words are the reason why Transformer yields better translation.

4.5 paragraph 1: there is no way to sample 12000 datasets without replacement from NIST 2005 and have the samples be the same size as NIST 2005. You must mean “with replacement”?","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's valuable and novel contributions, stating they could imagine using the techniques in their own research. However, they also point out several areas for improvement and clarification. The politeness score is 80 (quite polite) as the reviewer uses respectful language throughout, offers constructive criticism, and frames their concerns as suggestions rather than demands. They use phrases like 'It would probably be a good idea' and 'I found... to be confusing' instead of more direct or harsh criticism. The reviewer also balances critique with positive feedback, demonstrating a considerate approach to their review.",50.0,80.0
"This paper unifies both classification and regression task based on the polar prototype network. For classification, the prototypes for all classes are chosen in advance based on a max-margin principle, while the embedding of all instances is then optimized to have small cosine distance to assigned prototypes. For the regression, the output value is interpolated between the two prototypes. Experiments on classification, regression, and combined tasks show the method can achieve good results.

The idea of using the prototype and the polar system is interesting, and the whole paper is well-written. However, there are still some problems and questions about this paper.
1. There are two problems with using the max-margin prototypes. First, to maximize the smallest distance between two prototypes, the authors use MC or evolutionary algorithms to do the optimization, which may be time-consuming, and it may be extremely difficult when the prototype space is high dimension. Second, the previous approach indeed obtains discriminative prototypes, but we lose the **class correlation**. In the extreme case, it is equal distance between all prototypes, but some similar classes will have a smaller prototype distance than others. For example, the prototype distance between ""cat"" and ""dog"" should not be the same as that between ""car1"" and ""car2"". The semantic consideration in the paper can solve this problem to some extent, but there needs more evidence.

Using the pre-defined prototype is also considered in the paper ""M. Perrot et al. Regressive Virtual Metric Learning. NIPS15"". 

2. For the unified output space
One main contribution is that based on the polar system, the method unifies both classification and regression tasks in the same space. We can also do this in basic embedding algorithms. In the embedding space, a method can do both classification and regression with the nearest neighbor rule (based on majority voting and average respectively). The authors should compare with such kinds of methods in the experiments.

3. Experiments
From the experiments, using semantic cannot improve a lot for the classification task. The authors can try more datasets to validate is this the common scenario. The reviewer strongly suggests the authors should compare with more methods. For example, in some papers the prototypes are learned simultaneously (Snell et al. Prototypical networks for few-shot learning. NIPS17; Wen et al. A discriminative feature learning approach
for deep face recognition. ECCV16); while in other cases, there are no prototypes as we optimize the triplet/contrastive loss directly. First, the authors can compare classification performance with these approaches; besides, some visualization results can also show the used prototypes or embeddings. 
The main advantage of the method is not stressed clearly in the experiments part. The authors can clarify it in later versions.

The final rating depends on the authors' response.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper is 'interesting' and 'well-written', they raise several significant concerns and problems with the approach. The reviewer states there are 'still some problems and questions' and provides detailed critiques on multiple aspects of the work. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positives before raising concerns, and phrases criticisms constructively as suggestions (e.g. 'The authors can try...', 'The reviewer strongly suggests...'). The tone remains professional and courteous even when pointing out issues.",-20.0,60.0
"# overview
In this work, Nesterov Accelerated gradient based updates are applied in a distributed fashion to scale SGD based training to multiple nodes without the introduction of further hyperparameters or having to adapt the learning rate schedule from that of single node training on the same data.

Evaluation is carried out on image classification workloads using ResNet model variants across CIFAR10,100, and ImageNet datasets, utilizing from 8-32 nodes. In contrasting test error relative to single node performance, the authors find their method degrades less than other synchronous and asynchronous SGD based approaches as node count increases.

Overall, this work is presented in a fairly clear and logical manner, and the writing is easy to follow.  However the approach described appears to be contingent on very specific worker communication patterns and timing which seem unrealistic for real-world settings (namely that each worker sends exactly one update per N sized block received).  Extrapolating from the curvature of the results shown it doesn't appear that DANA would continue to outperform other methods like ASGD once the worker count scales beyond the 32 node limit evaluated.

# pros
* no additional hyperparameter tuning required
* should be easy to drop into existing asynchronous SGD implementations, just need to modify the worker side.
* does appear to scale slightly better from an accuracy perspective in 16-32 node counts

# cons
* Biggest criticism is the assumption of block random or round-robin worker update scheduling. Presuming each worker will update master exactly once to determine future parameter position is far from realistic on real hardware (varying capacity, performance, system loads, dealing with stragglers) and should probably be considered a synchronous not asynchronous update.
* only evaluated on image classification tasks on cifar10, cifar100, imagenet on resnet-20 and resnet-50. Would have been better to evaluate on a more varied set of tasks/models/datasets

# other comments
* Figure 2 baseline performance reported is a bit misleading/confusing since it was only evaluated on a single worker. Would suggest restricting to a single point rather than some extrapolated line that seems to indicate being run on multiple-workers.
* Figure 3 should should also show multi-node speedups for the other methods compared for completeness. 
* Section 5.2 should report on percentage scaling efficiency rather than using speedup as it doesn't normalize for worker count.  For instance 16x could be interpreted as good or poor if it was achieved using 16 vs 160 nodes.
* Section 5.2 there's a small typo: GPUs -> GPU
* Consider https://arxiv.org/abs/1705.07176 in related work?","The sentiment score is slightly positive (20) because the reviewer acknowledges some strengths of the work, such as its clear presentation and potential benefits. However, they also raise significant concerns about the approach's assumptions and limitations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'fairly clear and logical manner' and 'easy to follow' while also providing specific suggestions for improvement without harsh language.",20.0,60.0
"This paper claims to propose a new iRDA method. Essentially, it is just dual averaging with \ell_1 penalty and an \ell_2 proximal term. The O(1/\sqrt{t}) rate is standard in literature. This is a clear rejection.","The sentiment score is -90 because the review is highly negative. The reviewer states that the paper's claim of proposing a new method is essentially just a combination of existing techniques, and the results are described as 'standard in literature'. The review concludes with 'This is a clear rejection', indicating a strong negative sentiment towards the paper. The politeness score is -50 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't offer any constructive feedback or suggestions for improvement, and the tone is curt and somewhat condescending, especially in the final sentence. The review lacks the typical courtesies or softening language often found in more polite academic discourse.",-90.0,-50.0
"In the paper, the authors proposed a new saliency map method, based on some empirical observations about the cause of noisy gradients.
Specifically, through experiments, the authors clarified that the noisy gradients are due to irrelevant information propagated in the forward pass in DNN. Because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.
To avoid noisy gradients, the authors proposed a new backpropagation named Rectified Gradient (RectGrad). In RectGrad, the backward pass is filtered out if the product of the forward signal and the backward signal are smaller than a threshold. The authors claim that, with this modification in backpropagation, the gradients get less noisy.
In some experiments, the authors presented that RectGrad can produce clear saliency maps.

I liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. The experiments are designed well to support the claim.
Here, I would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. Each filter in convolution layer is trained to respond to certain patterns. Because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. I think this does not happen if the network is densely connected without convolutional structures. The trained dense connection will be optimized to remove the effects of occluded parts. Hence, for such networks, the gradient will be zeros for occluded parts.

The second half of the paper (Sec.4 and 5) are not very much convincing to me.
Below, I raise several concerns.

1. There is no justification on the definition of RectGrad: Why Rl = I(al * Rl > t) R(l+1)?
The authors presented Rl = I(al * Rl > t) R(l+1) as RectGrad, that can filter out irrelevant passes. However, there is no clear derivation of this formula: the definition suddenly appears. If the irrelevant forward passes are causes of noisy gradients, the modification Rl = I(al > t) R(l+1) seems to be more natural to me. It is also a natural extension to the ReLU backward pass Rl = I(al > 0) R(l+1). Why we need to filter out negative signals in backward pass?

2. The experimental results are less convincing: Is RectGrad truly good?
In Sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely. However, it is not clear that those ""nicely looking"" saliency map are truly good ones. I expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those ""nicely looking"" saliency maps are truly good ones.
Sec.5.3 presents some quantitative comparisons, however, the reported Sensitivity and ROAR/KAR on RectGrad are not significant. The authors mentioned that this may be because of the sparsity of RectGrad. However, if the sparsity is the harm, the underlying observations of RectGrad may have some errors. I think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of RectGrad.

[Minor Concern]
In Sec.5, the authors frequently refer to the figures in appendix. I think the main body of the paper should be self-contatined. I therefore think that some of the figures related to main results should appear in the main part.

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper in the first half, they express significant concerns and skepticism about the second half. The reviewer states that the second half is 'not very much convincing' and raises several major concerns about the methodology and results. However, the score is not deeply negative because the reviewer does appreciate some aspects of the work.

The politeness score is moderately positive (60) because the reviewer uses respectful and professional language throughout. They begin by acknowledging what they liked about the paper and use phrases like 'I would like to point out' and 'I expect' rather than making harsh demands. The reviewer also offers constructive criticism and suggestions for improvement rather than outright dismissal. However, the score is not extremely high as the review is still quite critical in its content.",-20.0,60.0
"This paper presents a new method for fully- and semi-supervised few-shot classification that is based on learning a general embedding as usual, and then learning a sub-space of it for each class. A query point is then classified as the class whose sub-space is closest to it.

Pros: This is a neat idea and achieves competitive results. Learning a sub-space per class makes intuitive sense to me since it’s plausible that there is a lower-dimensional subspace of the overall embedding space that captures the properties that are common to only examples of a certain class. If this is indeed the case, it seems that indeed classifying query examples into classes based on their distances from the corresponding sub-spaces would lead to good discrimination. 

Cons: First, an inherent limitation is that this approach is not applicable to one-shot learning, and I have doubts in its merit for very low shot learning (explained below). Second, I’m missing the justification behind a key point used to motivate the approach, which requires clarification (explained below). Third, I feel that certain aspects of the approach were unclear (details to follow). Finally, I feel more analysis is needed to better understand the differences of this method from previous work (concrete suggestions follow). For semi-supervised learning, the novelty regarding how the unlabeled examples are incorporated is limited, as the approach used is previously-introduced in Ren et al, 2018.

Overall, even though I like the idea and the results are good, there are a few points, mentioned in the above section that I feel require additional work before I can strongly recommend acceptance. Most importantly, relating to getting more intuition about why and when this works best, and tying it in better with previous approaches. 

A key point requiring clarification.
There is a key fact that the authors used to motivate this approach which remains unclear to me: why is it the case that this approach is less sensitive to outliers than previous approaches? In Figure 1, an outlier is pictured in each of subfigures (a) and (b) corresponding to Matching and Prototypical Networks, but not in subfigure (c) which corresponds to PSN. No explanation is provided to justify this conjecture, other than empirical evaluation that is based on the overall accuracy only. In particular, since SVD is used to obtain the sub-spaces, instead of an end-to-end learned projector that directly optimizes the query set accuracy, it’s not clear why if a support point is an outlier it would not affect the sub-space creation. If I’m missing something, please clarify!

(A) Comments on the approach.
(1) Why define X_k as the support set examples minus the class prototype instead of just the support examples themselves? The latter seems simpler, and should have all the required information for shaping the class’ subspace.
(2) Note that if X_k is defined as [x_{k,1}, \dots, x_{k,K}] as proposed in the above point (ie. without subtracting the class mean from each support point) then this method would have been applicable to 1-shot too. How would it then compare to a 1-shot Prototypical Network? Notice that in this case the mean of the class is equal to this one example.
(3) In general, the truncated SVD decomposition for a class can be written using the matrices U, \Sigma and V^T with dimensions [D, n], [n, n] and [n, K] respectively, where D is the embedding dimensionality and K is the number of support points belonging to the given class. The middle matrix \Sigma in the non-truncated version would have dimensions [D, K]. Does this mean that when truncating, n is enforced to be smaller than each of D and K? This would mean that the dimensionality n of the sub-space is limited by the number of the support examples, which in some cases may be very small in few-shot learning. Can you comment on this?
(4) How to set n (the dimensionality of each subspace) is not obvious. What values were explored? Is there a sweet spot in the trade-off between the observed complexity and the final accuracy?

(B) Comparison with Prototypical Networks.
(1) In what situations do we expect learning a sub-space per class to do better than learning a  prototype per class? For example, Figure 4 shows the test-time performance as a function of the test ‘way’. A perhaps more interesting analysis would be to compare the models’ performance as a function of the test *shot*: if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space? 
(2) Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.

(C) Clarifications regarding the semi-supervised setup.
(1) Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.
(2) Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? In Ren et al, 2018, this was prevented by creating an additional labeled/unlabeled split even for the training examples. Therefore they use strictly less overall information at meta-training time than if that split weren’t used. To be comparable with them, it’s important to apply this same setup.

(D) Additional minor comments.
(1) “To work at the presence of distractors, we propose to use a fake class with zero mean”. Note that this was already proposed in Ren et al, 2018. They used a zero-mean, high-variance additional cluster whose aim was to ‘soak up’ the distractor examples to prevent them for polluting legitimate clusters (this was the second model they proposed).
(2) In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or ‘ways’ at test time than what was used at training time.
(3) Gidaris and Komodakis (2018) is described in the related work as using a more complicated pipeline. Note however that their pipeline is in place for solving a more challenging problem than standard few-shot classification: they study how a model can maintain the ability to remember training classes while rapidly learning about new ‘test’ classes.
(4) In the last line of section 5.3, use N-way instead of K-way since in the rest of the paper K was used to refer to the shot, not the way.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('neat idea', 'achieves competitive results'), they express several concerns and state that 'additional work' is needed before they can 'strongly recommend acceptance'. The overall tone suggests more work is required, which leans towards a negative sentiment.

The politeness score is relatively high (70) because the reviewer uses respectful and constructive language throughout. They balance criticism with praise, use phrases like 'please clarify' and 'Can you comment on this?', and offer specific suggestions for improvement. The tone is professional and courteous, avoiding harsh or dismissive language.",-20.0,70.0
"In the paper, author investigate the use of copy mechanisms for the question generation task. It evaluates on the SQuAD dataset. The model is a popular seq2seq/encoder-decoder model with copy mechanisms using pointer networks. 

Pros:
It is well motivated. For the question generation task, a word to be predicted can be from either a global vocabulary list or copied from the given documents (location vocabulary).  There are some overlap between these two vocabulary lists.  This paper mainly investigates this issue.

It is well written and easy to follow.

Interesting analysis of human/automatic metrics.

Cons:
The tricks here are a bit of ad hoc. It is better to have a systemic study.

Baseline results are too low. E.g., officially QANet results (from the paper) on SQuAD v1 is around 82.7 (my implementation obtains 83.1). However in the paper, its best result is 72.6 in terms of F1 score. 

The authors only evaluated on one dataset. It is hard to convincing.

It is lack of comparison results of question generation in literature. 
","The sentiment score is slightly positive (20) because the review lists several pros including that the paper is well motivated, well written, and contains interesting analysis. However, it also lists several significant cons that temper the positive sentiment, such as ad hoc tricks, low baseline results, lack of dataset variety, and missing comparisons. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout and frames criticisms constructively as 'Cons' rather than using harsh or dismissive language. The reviewer also acknowledges positive aspects before moving to criticisms, which is a polite approach. However, the language doesn't go out of its way to be overtly polite or complimentary either, maintaining a fairly neutral professional tone overall.",20.0,50.0
"This paper presents a new adversarial defense method.  I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.  The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value.  The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all).  They use both partially converged perceptron models as well as fully converged perceptron models. 

Pros:
	1. The defense technique does not require knowledge of the attack method

Cons:
	1. The paper is incredibly difficult to understand due to the writing.
	2. The performed experiments are insufficient to determine  whether or not their technique works because:
		a. They don't compare against any other defense techniques.  In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect.
		b. They do not show the results of an attack by an adversary that is aware of their technique.  (i.e. F(PR(A(F,PR,x))))  Many alternative defense techniques will work much better if we assume the adversary does not know about the technique.
		c. Their comparison against random noise is not an apples-to-apples comparison.  Instead of perturbing uniformly within a range, they perturb according to a normal distribution.  The random noise perturbations also have a much larger L2 distance than the perturbations from their technique.  To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method.
		d. They only show results for one value of epsilon and one value of ""# of colors"" for their technique.  Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large ""# of colors""/small range per color and the attacker chooses a large epsilon).
	3. Their use of machine learning models is quite ad-hoc.  In particular they use a perceptron trained on algorithmically generated data.  And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model.
	4. The authors partly deanonymize the paper through a github link
	5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length.
","The sentiment score is -70 because the review is predominantly negative. While it acknowledges one pro (defense technique not requiring knowledge of attack method), it lists numerous cons including difficulty in understanding, insufficient experiments, ad-hoc use of machine learning models, and unnecessary length. The reviewer expresses significant doubts about the paper's methodology and results. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and direct. Phrases like 'incredibly difficult to understand' and 'quality of the paper does not justify the additional length' come across as somewhat harsh. The reviewer doesn't soften criticisms with polite language or positive reinforcement, leading to a slightly impolite overall tone.",-70.0,-20.0
"This paper tries to build a neural net to learn Nash equilibrium of games, even though it has been proved that no uncoupled algorithm can do that, except on specific games, as the ones considered in the example (0-sum, potentiel, solvable by iterated elimination of dominated strategies, etc.).

The algorithm proposed is a classical neural net without any insight (I believe its behavio must more or less be similar to regret matching).

In table 10, I do not think that the underline case is the NE. 

In table 12, the algorithm si conveniently initiated at the NE.","The sentiment score is -70 because the reviewer is highly critical of the paper's approach and findings. They point out fundamental flaws in the methodology, stating that it's been proven that the proposed approach can't work except in specific cases. The reviewer also criticizes the lack of insight in the algorithm and questions the accuracy of some results. The politeness score is -30 because while the reviewer doesn't use explicitly rude language, the tone is quite dismissive and blunt. Phrases like 'without any insight' and 'conveniently initiated' suggest a lack of respect for the authors' work. The reviewer doesn't offer any positive feedback or constructive suggestions for improvement, which contributes to the overall negative and impolite tone.",-70.0,-30.0
" This paper proposed a so-called ISS-GAN framework for data hiding in images, which  integrates steganography and steganalysis processes in GAN. The discriminative model simulate the steganalysis process, and the steganography generative model is to generate stego image, and confuse steganalysis discriminative model. 

Overall the application seems interesting. My concern is its use in real secure information transmission systems: it can fool human eyes but what is its capacity against decoding algorithms; if the intent is to transmit some hidden information, how the receiver is supposed to decode it; is there something similar to the public key in encryption systems? These basic questions/concepts should be made clear to the reader to avoid confusion. 

The evaluation protocol should be clarified and especially on how the PSNR is calculated (i.e., using the reconstructed secret image and real one?) ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the application as 'interesting', they express several concerns and questions about the paper's content and methodology. The reviewer points out missing information and unclear aspects, which suggests the paper needs improvement. However, the criticism is not harsh, hence the score is only mildly negative. The politeness score is moderately positive (50) as the reviewer uses neutral language and phrases their concerns as questions rather than direct criticisms. They use phrases like 'My concern is' and 'should be clarified' which are polite ways of pointing out issues. The reviewer also acknowledges positive aspects ('the application seems interesting') before presenting concerns, which is a polite approach to criticism.",-20.0,50.0
"Contributions:

The main contribution of this paper is the proposed DelibGAN for text generation. The framework introduces a coarse-to-fine generator, which contains a first-pass decoder and a second-pass decoder. Instead of using a binary classifier, the discriminator is a multiple instance discriminator. Two different variants of DelibGAN are proposed, with experiments showing that DelibGAN-II performs the best.

Strengths:

(1) Novelty: I think this paper contains some novelty inside. Using a coarse-to-fine generator is an interesting idea. However, as I noted below, the paper is not well-executed.

Weaknesses:

(1) Presentation: This paper is easy to follow, but poorly written.

First, the paper is too repetitive. For example, the two-pass decoding process has been repeatedly mentioned too many times in the paper, such as the paragraph above Eqn. (8). Please be concise. 

Second, when citing a paper, there should be a space between the word and the cited paper. For example, in the first paragraph of the introduction section, instead of ""(RNN)(Mikolov et al., 2011)"", it should be ""(RNN) (Mikolov et al., 2011)"". This should be corrected for the whole paper.

Third, in the first paragraph of the introduction section, I am not sure why (Wang & Wan, 2018a) is cited here. This is not the first paper that points out the problem. One should refer to [a], which is also not cited in the paper. 

Missing reference: I also encourage the authors citing [b] since it is directly related to this work, which is about using GAN for text generation.  

[a] Sequence Level Training with Recurrent Neural Networks, ICLR 2016
[b] Adversarial Feature Matching for Text Generation, ICML 2017

(2) Evaluation: My main concern lies in the experimental evaluation, with detailed comments listed below.  

Questions:

(1) In Algorithm 1, there exists the pretraining process of G_1 & G_2. However, it is not clear to me how this this pretraining is implemented, since the output of G_1 is not observed, but is hidden and imagined by the model. So, what is the training signal for pretraining G_1? Can the authors provide more details? Please clarify it. 

(2) In experiments, why the authors only compare with SeqGAN, SentiGAN & MaskGAN? One would naturally ask how the proposed model compare with RankGAN, TextGAN and LeakGAN. For example, what are the corresponding results of RankGAN, TextGAN & LeakGAN in Table 1 to 5? This should not be difficult to compare with, based on the availability of Texygen.

(3) Besides using the perplexity, the authors also use the novelty and diversity terms defined in (11) & (12) for evaluation. This is good. However, why not using some commonly used metrics in the literature, such as BLEU and self-BLEU? I agree these metrics are also not perfect, but it will be helpful to also report these results for benchmark purposes. 

(4) Instead of using datasets like HappyDB & SSTB, it would be helpful to also report results on some benchmark datasets such as COCO captions and EMNLP News as used in the LeakGAN paper.  What are the results looking like on these two datasets?

(5) The results in Table 1 & Figure 2 is misleading. They do not necessarily mean the proposed model is better, as the NLL value only measures how the generated sentence look like a real sentence, but it does not measure the diversity of generated sentences. For example, a model that only repeatedly produces one very realistic sentence would also achieve a very high NLL score.  

(6) Table 3 & 5 shows the human evaluation results. However, how this is performed is not clear at all. For example, is the human evaluation performed using AMT? How many samples are used for human evaluation? Also, how many workers has one sentence been evaluated by? Without all these details, how reliable this result is is questionable.

Minor issues:

(1) In Eqn. (3), since only two classes exist, there is no need to say ""softmax"", use ""sigmoid"" function is enough for illustration. 

(2) In the line below Eqn. (5), ""bias"" => ""baseline""

(3) In Eqn. (3), there is "")"" missing. 

(4) In Figure 2, there are two methods ""SentiGAN"" and ""DelibMLE"" with the same green color. Which one is which is hard to see. 

(5) In the first paragraph of related work, MaskMLE => MaskGAN.  
","The sentiment score is -30 because while the reviewer acknowledges some novelty in the paper, they express significant concerns about the execution, presentation, and evaluation. The review lists more weaknesses than strengths and raises several critical questions about the methodology and results. However, it's not entirely negative as it does recognize some positive aspects.

The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I think' and 'I encourage' which soften criticism. They also provide constructive feedback and specific suggestions for improvement. However, some statements are quite direct (e.g., 'poorly written', 'misleading'), which prevents a higher politeness score. The reviewer balances critique with acknowledgment of the paper's strengths, maintaining a generally respectful tone while delivering substantial criticism.",-30.0,20.0
"The authors of this paper compare the robustness of CNN and CapsNet to global translation on the MNIST dataset. Both models were trained on the standard training set of MNIST, and then tested on a set with digits shifting from the upper left corner to the lower right corner. The results of both models were poor. To improve it, the authors add some shifted digits to the training set, and the performances of both models were significantly enhanced. Moreover, the performance of CNN was better than that of CapsNet in the experiments. Generally speaking, the work presented in this paper is clear and straightforward. However, the work is not significant enough to publish as a ICLR paper. Below is my major comments.

1. There are lots of typos and grammatical errors everywhere in the paper. Thus, the manuscript was not well prepared.

2. It is unclear which CapsNet and what settings were used in the experiment.  

3. It is well-known that convolutional networks are good at capturing local patterns from the images, while capsule networks enhance it to consider global configurations of the local patterns, and robust to affine transformation. Obviously, the experiments presented in this manuscript is too simple. Lots of work should be done in the investigation. For example,o on the training set and shifted test set, the authors can enlarge the background and keep the digits in the original size to make it as a local pattern in the image. Will it be detected by CNNs with larger receptive fields for the images? How is it compared with CapsNets? 

4. How are both models compared on other (perhaps more complicated and larger) datasets?

In summary, the work presented here is interesting, but lots of work should be done in order to make it publishable.

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the work is 'clear and straightforward' and 'interesting', they also state that it's 'not significant enough to publish' and that 'lots of work should be done' to make it publishable. The overall tone suggests that the paper needs substantial improvement. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'Generally speaking' and 'It is unclear' which maintain a respectful tone. However, the mention of 'lots of typos and grammatical errors' slightly reduces the politeness score.",-20.0,50.0
"This paper presents a GAN approach adapted for multi-modal distributions of single class data. The generator is trained to generate samples in the low density areas of the data distribution. The discriminator is training to distinguish between generated and real samples and hence is able to discriminate between normal data (=real) and anomalous data (=generated, in low density areas of the normal data). 

To force the model to map the different modes of the data, a categorical latent variable is used that represents the potential distribution modes. Both a one-hot code and a Gaussian mixture model are explored. This is not a novel approach, however, no citations are provided.

To force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used. The authors cite OCAN [zheng18], which in turns cites [Dai, NIPS17]. This approach has the advantage that no threshold needs to be fine-tuned since the discriminator can directly be used for anomaly detection.

Constraints derived from both these goals are included in the loss function, which, in addition, includes terms to encourage diversity and similarity of the generated samples.

The model is tested on a proprietary dataset of real manufacturing product. The dimension of the data is 280 (after proprietary feature extraction). The authors compare their approach to 9 other anomaly detection methods. The reported performance is the highest. The OCAN method has similar performance. The authors specify that fine-tuning is need for all other methods (except OCAN). Fine-tuning is performed on the same data used for testing, hence providing a marked advantage. However, I do not understand why OCAN is listed in table 1 with both fine-tuning and no fine-tuning (raw). This is not explained and should be clarified. In any case, the combination of Complementary-GAN and the multi-modal latent variable seem to be very effective on this dataset. To understand whether this approach is really superior, other benchmark datasets should be tested.

The article is technically sound. The citations are generally ok, except for the missing citation related to the use of latent categorical codes for push the model into mapping multiple modes of the data. The math is reasonable, although some notations are a bit hard to follow. The English needs to be improved. There are many grammatical errors and the paper needs to be proof-read. Some errors make it hard to understand the text. In particular the adjective modal is used throughout the paper as a noun instead of 'mode'. There are also several LaTex formatting errors which lead to some gibberish and some of the figures are too small making them unusable when the paper is printed.

Overall, I think the paper is incremental, as it combines two previously published methods. It also lacks generality as only one (proprietary) dataset is used. English needs to be proof-read and formatting errors fixed.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'technically sound', 'very effective on this dataset'), they also point out several limitations and areas for improvement. The review concludes by calling the paper 'incremental' and noting it 'lacks generality', which contributes to the overall negative sentiment. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, offering constructive criticism and specific suggestions for improvement without using harsh language. They acknowledge the paper's strengths while also pointing out areas that need work, which is a balanced and respectful approach to peer review.",-20.0,50.0
"In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.

1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?

2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.

3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?

4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?

5) is the Original GAN exactly the Ian's original GAN or WGAN?

6) Have you tried this approach using small sized generator (having  small number of parameters)?



","The sentiment score is slightly positive (20) because the reviewer acknowledges the reasonableness of the authors' approach and provides constructive feedback. The review doesn't express strong enthusiasm but offers suggestions for improvement, indicating a generally positive but reserved sentiment. The politeness score is high (80) as the reviewer uses respectful language throughout, phrasing criticisms as questions or suggestions rather than direct criticisms. The reviewer consistently uses phrases like 'could you', 'it would be better', and 'have you tried', which are polite ways of offering feedback. The review maintains a professional and courteous tone throughout, avoiding any harsh or rude language.",20.0,80.0
"This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model's state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.

Pros:
-Paper was easy to understand
-Detailed analysis of model behavior. The breakdown analysis between ""what"" and ""where"" was particularly interesting.
-Attention results appear interpretable as claimed

Cons:
-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.
-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?
-I would also recommend including a citation for :
Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. ""End-to-end memory networks."" NIPS 2015.


General questions:
-Was there an effect of attention grid coarseness on performance?
-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?","The sentiment score is 60 (moderately positive) because the reviewer lists several pros and finds the paper easy to understand and interesting, while also providing constructive criticism and suggestions for improvement. The tone is generally positive, but not overwhelmingly so. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, frames criticisms as suggestions or questions rather than direct attacks, and acknowledges the paper's strengths. The reviewer maintains a professional tone without being overly formal or deferential.",60.0,70.0
"The authors argue that they propose a method to find adversarial examples, when the data lie on a Riemannian manifold. In particular, they derive a perturbation, which is argued to be the worst perturbation for generating an adversarial example, compared to the classical Euclidean derivation.

I strongly disagree with the proposed (Riemannian) geometric analysis, because there are several technical mistakes, a lot of arbitrary considerations, and flawed assumptions. In particular, my understanding is that the proposed method is not related at all with Riemannian geometry. For justification I will comment some parts:

#1) In Section 1 paragraph 4, and in Section 2.3 after Eq. 14, the sentences about the gradient of a function that is defined on a manifold are strange and unclear. In general, the gradient of a function defined on a manifold, points to the ascent direction. Thus, if I understood correctly the sentences in the paper support that the gradient of such a function is meaningless, so I think that they are wrong.

#2) How the $\ell_2$-ball on a manifold is defined? Usually, we consider a ball on the tangent space, since this is the only Euclidean space related to the manifold. Here, my understanding is that the authors consider the ball directly on the manifold. This is clearly wrong and undefined.

#3) To find the geodesic you have to solve a system of 2nd order non-linear ODEs, and there are additional details which I will not include  here, but can be easily found in the Riemannian geometry literature. Also, I think that the Lemma 2.2. is wrong, since the correct quantity of Eq. 3 is $ds^2 = g_ij(t)d\theta^i d\theta^j dt^2$, where $dt\rightarrow 0$ based on the included proof. This is clearly not a sensible geodesic, it is just the infinitesimal length of a line segment when $t\rightarrow 0$, which means that the two points are infinitesimally close.

#4) If $x, y$ is on a Riemannian manifold then the $x+y$ operator does not make sense, so Eq. 7 is wrong. In particular, for operations on Riemannian manifolds you need to use the exponential and the logarithmic map.

#5) Continuing from #4). Even if we consider the perturbation to be sufficiently small, still the $x+\epsilon$ is not defined. In addition, the constraint in Eq. 8 is wrong, because the inner product related to the Riemannian metric has to be between tangent vectors. Here the $\epsilon$ is an arbitrary quantity, since it is not defined where it actually lies. In general, the derivation here is particularly confusing and not clear at all. In my understanding the constraint of Eq. 8 is a purely linear term, and $d$ is not the geodesic distance on a manifold. It just represents the Mahanalobis distance between the points $x$ and $x+\epsilon$, for a matrix $G$ defined for each $x$, so it is a linear quantity. So Eq. 9 just utilizes a precondiner matrix for the classical linear gradient.

#6) The Eq. 12 is very flawed, since it equalizes a distance with the Taylor approximation error. I think that this is an unrealistic assumption, since these terms measure totally different quantities. Especially, if $d$ is the geodesic distance.

#7) The upper bound in inequality Eq. 13 comes from Eq. 12, and it is basically an assumption for the largest absolute value of the Hessian's eigenvalues. However, this is not discussed in the text, which are the implications?

#8) I find the paper poorly written, and in general, it lacks of clarity. In addition, the technical inconsistencies makes the paper really hard to follow and to be understood. I mentioned above only some of them. Also, there are several places where the sentences do not make sense (see #1), and the assumptions made are really arbitrary (see #6). The algorithms are not easy to follow. Minor comments, you can reduce the white spaces by putting inline Eq. 3, 4, 5, 6, 9, 10, 14, and Figure 2. The notation is very inconsistent, since it is very unclear in which domain/space each quantity/variable lies. Also, in Section 2.5. the authors even change their notation.

In my opinion, the geometrical analysis and the interpretation as a Riemannian manifold is obviously misleading. Apart from the previously mentioned mistakes/comments, I think that the proposed approach is purely linear. Since actually the Eq. 14 implies that the linear gradient of the loss function, is just preconditioned with the Hessian matrix of the loss function with respect to the input $x$. Of course, if this function is convex around $x$, then this quantity is the steepest ascent direction of the loss function, simply on the Euclidean space where $x$ lies. However, when this function is not convex, I am not sure what is the behavior when all the eigenvalues of the Hessian are set to their absolute value. Also, the (arbitrary) constraint in Eq. 13 implicitly sets a bound to the eigenvalues of the Hessian, which in some sense regularizes the curvature of the loss function. To put it simple, I think that the proposed method, is just a way to find a preconditioner matrix for the linear gradient of the loss function, which points to the steepest direction. This preconditioner is based on the Hessian of the loss function, where the absolute values of the eigenvalues are used, and also, are constrained to be bounded from above based on a given value.

Generally, in my opinion the authors should definitely avoid the Riemannian manifold consideration. I believe that they should change their perspective, and consider the method simply as a local preconditioner, which is based on the Hessian and a bound to its (absolute) eigenvalues. They should also discuss what is the implication by setting the eigenvalues to their absolute values. However, since I am not an expert in the field of adversarial examples, I am not sure how novel is this approach.


","The sentiment score is -80 because the reviewer strongly disagrees with the proposed method, pointing out numerous technical mistakes, arbitrary considerations, and flawed assumptions. The review is overwhelmingly negative, with phrases like 'strongly disagree,' 'clearly wrong,' and 'obviously misleading' used throughout. The reviewer suggests that the entire perspective of the paper should be changed, indicating a very negative sentiment towards the current work. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism without much softening language. Phrases like 'clearly wrong,' 'very flawed,' and 'poorly written' are used without much mitigation. However, the reviewer does provide detailed explanations for their criticisms and offers some constructive suggestions, which prevents the score from being even lower.",-80.0,-20.0
"1. This papers leverages the concept of wavelet transform within a deep architecture to solve the classic problem (especially for wavelet analysis) of change point detection. The authors do a reasonably comprehensive job of demonstrating the efficacy of the proposed framework using various synthetic and real data sets with both gradual and abrupt changes

2. The concept of pyramid network idea is not really new, in the context of CNN it has been established quite well. The paper should highlight this fact by citing papers such as ""Lin, Tsung-Yi, et al. ""Feature Pyramid Networks for Object Detection."" CVPR. Vol. 1. No. 2. 2017."" 

3. Involving wavelet transforms in deep nets have been done before. This paper attempts to learn wavelet transform parameters by involving them as trainable layers. But even this kind of idea is also emerging in the community. Papers such as ""Fujieda, Shin, Kohei Takayama, and Toshiya Hachisuka. ""Wavelet Convolutional Neural Networks."" arXiv preprint arXiv:1805.08620 (2018)"" need to be discussed in this context. 

4. The biggest issue in my mind is that I feel ""Chung et al 2016"" is still a very similar framework as the proposed one. While authors argue that it uses more like CNN architecture and the proposed method may pick up the multi-scale features better, comparison with this seems to be most appropriate. This will also clearly identify the benefits of the wavelet structure to the filters and multi-resolution analysis approaches.

5. RCNN term has been used for CNN+RNN architecture. This may not be a good terminology to use since RCNN is a very popular term referring to Region based CNN for detection and localization purposes.

6. AUC metric, I believe is the - area under ROC curve, this needs to be spelled out, how it is computed? at least in the Appendix

xxxxxxxxxxxxxxxxxxx

Appreciate the authors' rebuttal, updated my score.","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's comprehensive demonstration of the framework's efficacy and the reasonably good job done by the authors. However, they also point out several issues and areas for improvement, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and providing constructive feedback. They use phrases like 'reasonably comprehensive job' and 'appreciate the authors' rebuttal', which contribute to the polite tone. The reviewer also offers specific suggestions for improvement without using harsh or critical language.",20.0,60.0
"Authors extends stacked hourglass network with inception-resnet-A mudules and a multi-scale approach for human pose estimation in still RGB images. Given a RGB image, a pre-processing module generates feature maps in different scales which are fed into a set of serial stack hourglass modules each responsible for a different scale. Authors propose an incremental adaptive weighting formulation for each stack-scale-joint. They evaluate proposed architecture on LSP and MPII datasets.

positive:
- Having an adaptive weight strategy is a necessary procedure in multi-loss functions where cross-validation or manual tuning of fixed weights are expensive. While the weights are functions of the loss, it is not analyzed and evaluated thoroughly, e.g. evolution of weights for each joint-stack-scale. Even it is not given in the section 5.2.1. So it is hard to judge effectiveness of the proposed loss. 

negative:
- In general experiments section is the most weakness of the paper. I comment some points in the following:
a) Multi-scale image processing is not a novel idea in computer vision and specifically in human pose estimation. The authors have not compared their methods with most recent papers in the literature and I believe the results are not state-of-the-art (see [1] for instance which is a multi-scale approach).
b) What is the effect of each scale in the results and for each joint? This must be analyzed and shown visually or numerically.

- The number citations is not enough.

- The writing must be improved.

overall:
Regarding mentioned comments, I believe the paper needs a huge extra effort (both analytically and numerically) to be adequate for publication. Therefore, I recommend rejection.


[1] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning feature pyramids for human pose estimation. In: ICCV. (2017)","The sentiment score is -70 because the review is predominantly negative. The reviewer recommends rejection and points out several weaknesses, including insufficient experiments, lack of comparison with recent literature, and inadequate writing. The few positive comments are outweighed by the critical ones. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone and provide specific, constructive feedback. They use phrases like 'I believe' and 'must be analyzed' rather than harsh or rude language. However, the overall negative recommendation and critique prevent a higher politeness score.",-70.0,20.0
"This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1)  the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where  the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. 

This paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.

In the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to  analyze the reason. 

In addition, as claimed from the introduction, the contribution of this paper was  to reduce the overhead of expensive quantization. However, no experimental results on computation time  and parameter size have been shown. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper is clearly presented, they point out several areas for improvement and limitations. The reviewer notes that the paper's method performs worse than some competitors and lacks analysis on computation time and parameter size, which were claimed as contributions. However, the tone is not entirely negative, as the reviewer also provides constructive feedback. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, offering suggestions for improvement rather than harsh criticism. Phrases like 'it can be improved by' and 'it is better to' indicate a polite and constructive approach to feedback. The reviewer also begins with a positive comment about the paper's clarity before moving on to areas of improvement, which is a polite way to structure feedback.",-20.0,50.0
"The paper presents an extension of the MXFusion language that allows the use of probabilistic modules. These modules are defined as a set of random variables and a specific probabilistic distribution. The modules also contain dedicated inference methods. Using these modules, one can use probabilistic distributions with inference methods tailored to the distribution, which are usually more efficient than generic inference systems.
The paper presents several examples using Gaussian process models, evaluated by comparison with GPy and the standard spare gaussian process method implemented in MXFusion.

Overall, the paper is well written and clear, and all claims are justified. The idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules. The related work section provides a good positioning of the approach.
I have not found any specific problems in the paper, the quality is rather high. However, the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.
Moreover, the results of the experimental test do not seem to me to be able to support the main objective of the extension, which is to give the possibility to exploit more specific probabilistic model and inference methods to achieve better results than an approach using general methods.
As far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?

As for minor issues that I can point out, one concerns the definition of shape in the Variable of m.sigma2 (figures 1, 2, 3). I do not know the used in MXFusion, thus this might not be an error, but it seems that in the shape definition something is missing. It is written that shape=(1,), is it correct or is there an error? In case of absence of error, what does the empty argument mean?

The power benchmark is not described.

In references, Thomas V. Wiecki is mentioned with and without the first letter of middle name. I suggest to uniform the references.

Typos:
- Abstract: ""... but also sophisticated probabilistic model*s* such as ...""
- Sec. 1, first row of page 2: The sentence ""this would bring the a lot of benefits ..."". The ""the"" word should be deleted.
- Sec. 1 refers to a section after 4 which does not exist in the paper.
- Page 5: remove the full stop before the colon in the 4th row.
- Page 5: ""The log_pdf method of the SGPR module compute*s* the above variational lower bound""
- Sec. 6: the sentence ""MXFusion aims at closing the gap between having specialized, highly performant algorithms and generic, easily maintained generic algorithms by introducing probabilistic modules."" should be corrected.


Pros
- The extension allows the use of modules that define specific probabilistic distribution/inference methods
- It seems easy to extend the system with other modules
- Its a really useful extension...

Cons
- The performance presented in the paper is not entirely convincing
- ... but it is just an extension of an existing system","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper is well-written and clear, they also express some reservations about the novelty and impact of the work. The reviewer notes that the idea isn't entirely new and that the experimental results don't fully support the main objective. However, they do highlight several pros and the overall tone is more positive than negative. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms constructively. They use phrases like 'Overall, the paper is well written and clear' and offer specific, helpful suggestions for improvement. The reviewer maintains a professional tone, even when pointing out issues or typos.",20.0,60.0
"Overall, this is a interesting paper on an important topic: knowledge extraction from Neural Networks.
Even though the authors seem propose a novel approach to knowledge extraction, the paper would 
dramatically benefit from two additions:
- an empirical evaluation on at least two more datasets (as is, the paper uses a single dataset)
- an illustrative-but-realistic example of how at least one rule is extracted from each layer of the neural network 

Other comments:
- on page 4 (1st paragraph in 3.3), the authors talk about a ""test set"" that, it turns out, it is extracted from the actual training set (1st paragraph of 4.1); the authors should use a more careful terminology
- from the paper, it seems that the authors tried a single randomly chosen set 1000 random inputs in 4.1; they should most definitely try several such sets
- Figure 1 should have a legend in the image, rather than as a 2-line caption
 ","The sentiment score is slightly positive (20) because the reviewer describes the paper as 'interesting' and on an 'important topic', indicating some positive aspects. However, they also suggest significant improvements, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, such as 'the paper would dramatically benefit' and 'the authors should', rather than using harsh or demanding language. They also start with a positive note before moving to suggestions. The review is constructive and professional in tone, offering specific recommendations without being overly critical or rude.",20.0,60.0
"Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.","The sentiment score is highly negative (-90) because the reviewer expresses strong criticism of the paper, stating that the presentation is 'critically weak' and they 'failed to understand' the paper's objective and contributions. The reviewer also mentions 'many editing problems' and 'problematic' English, indicating a very poor overall impression. The politeness score is somewhat negative (-30) because while the reviewer doesn't use explicitly rude language, the criticism is direct and unmitigated. Phrases like 'critically weak' and 'failed to understand' are quite harsh and could be perceived as impolite in academic discourse. The reviewer doesn't attempt to soften the criticism or provide any positive feedback, which contributes to the negative politeness score.",-90.0,-30.0
"*Update after discussion period*
I remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks.


Summary:
The authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network's ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network's generalization performance. 

Strengths:
+ Very straightforward and easy to follow 
+ Technically sound

Weaknesses:
- Feels trivial
- The claims seem to be too general

Conclusion:
I'm torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I'm not really sure what we learn from the paper.


Specific comments:

- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?

- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.

- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (""orientation selectivity [plays] a causally important role in object recognition"" – abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.

- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?
","The sentiment score is -60 because the reviewer expresses significant skepticism and criticism throughout the review. They state they are 'unconvinced' and 'torn on the paper', pointing out several weaknesses such as the results feeling 'trivial' and claims being 'too general'. The reviewer also mentions mixed results and questions the generalizability of the findings. However, they do acknowledge some strengths, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I'm not really sure' and 'How do the authors explain this result?' rather than using harsh or rude language. They also acknowledge some positive aspects ('potentially interesting observations', 'technically sound') which contributes to a more polite overall tone. However, the criticism is still quite direct, preventing a higher politeness score.",-60.0,20.0
"After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. 

Pros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. 

Cons: In spite of its theoretical contributions, this paper has a few major issues. 

Q1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. 

Q2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices?

Q3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? 

","The sentiment score is 20 (slightly positive) because the review starts with a positive note that the authors have addressed the reviewer's comments, and the reviewer has increased their rating. The review also mentions some pros of the paper, acknowledging its theoretical contributions. However, it also lists several cons and major issues, which balances out the positive aspects. The politeness score is 50 (moderately polite) because the reviewer uses respectful language throughout, such as 'nicely addressed' and 'this paper makes theoretical contributions.' The critique is presented as questions rather than direct criticisms, which is a polite approach. However, the reviewer doesn't use overly formal or deferential language, maintaining a professional tone.",20.0,50.0
"This paper introduces a new way to have more compressed (lower rank) representation of the data in a supervised fashion. The authors motivates their work by saying that such representation are more useful for transfer learning and are more robust to adversarial examples! In order to achieve this, the authors introduce a virtual LR layer and utilize Nystrom technique to make the process more efficient.  The idea introduced in this paper is interesting but the paper is poorly written and organized which makes its through evaluation difficult. Below, I provide more detailed comments.

I don't understand at what frequency the low-rank optimization as the  subproblem to Equation (OPT) is being done. Given that the DNN training  is being done using batched of examples, where do you put the low-rank  optimization, in the end of each epoch? It seems it is in the end of each epoch but it should be clearly stated.

I don't understand motivation for L_N(A). The authors justify it by  saying that a trivial solution for the optimization problem would be setting A+b=0 and they introduce this term to avoid it. However, this is not correct. Note that there are n examples in A and we can only make one of them  zero at a time. (Note that talking about A+b is not also accurate  because they are not of the same size).

In order to use Nystrom method for low-rank approximation, W needs to be  a symmetric postive semi-definite matrix. I am not sure if the heuristic  procedure introduced in Page 4 is well-justifed. and whether we are  still optimizing the objective function introduced in Page 3. Do we  still have a stable training?

What is ResNet N-LR in experiment? The authors introduced ResNet 1-LR and ResNet 2-LR but not ResNet N-LR! I found the description N-LR later but the naming is rather confusing. I would use LR instead of N-LR because it seems it has N LR layer. I would also explain this next to other methods. 

Not sure if I understand Bottle-LR. The description in the text is not clear and I don’t understand the motivation for this baseline! Again, this should be described next to other methods.

The authors do not mention what is their setting for r in the experiments (Table 1).

Page 5, Paragraph after Table 1: First CIFAR-100 should be CIFAR-10.

One way the authors defend their framework is to have a representation that can be used in transfer learning. Nonetheless, the results in Table 1.b shows that their framework is not doing good for transfer learning.

Not sure if I understand Figure 1. How do you change the number of singular values? I understand this is a hyper parameter for your framework but I am confused how it is being set for N-LR method. Similarly, I don’t understand Table 2 and how you change the embedding dimension. 


Unlike the claim made by the authors, it seems that VGG19 N-LR does better compared to VGG19 2-LR in Figure 2. ","The sentiment score is -30 because while the reviewer acknowledges the paper's interesting idea, they express significant concerns about the paper's writing, organization, and clarity. The reviewer points out several issues and misunderstandings, indicating a generally negative sentiment. However, it's not extremely negative as they do recognize some positive aspects. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I don't understand' and 'Not sure if I understand' rather than making accusatory statements. They also offer specific suggestions for improvement, which is constructive. The language is not overly formal or polite, but it avoids rudeness and maintains a respectful tone.",-30.0,20.0
"In convolutional neural networks, a convolutional filter conducts a series of multiply-accumulate (MAC) operations, which is computationally heavy. To save computational cost, this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy.

Significance wise, the results look promising, though it is not always the best method to preserve accuracy while doing the pruning. There are a few experiments where CP is better at preserving accuracy. In addition, it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation. In this manuscript, the best results are roughly 50% MAC savings (which is not equal to the saving of the whole networks) with some accuracy drop. It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned.

Quality wise, there seems to be some room to improve from a technical point of view. The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers. This sounds about right, but lacks data support. In addition, even this intuition is correct, it might not be the optimal setting for pruning. We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal. As a counterexample, if the shallow layer is the most computationally heavy, saving 10% MAC ops might save more computational cost than saving 5% in the center layer while preserving the same accuracy level. 

The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1.

The method is quite original, and the manuscript is very well written and easy to follow.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the promising results and the originality of the method, while also pointing out areas for improvement. The review begins with positive aspects ('results look promising', 'method is quite original', 'manuscript is very well written') but also includes constructive criticism and suggestions for enhancement. The politeness score is 80 (quite polite) due to the reviewer's use of respectful language throughout. They offer criticism in a constructive manner, using phrases like 'it would be great if' and 'there seems to be some room to improve' rather than harsh or dismissive language. The reviewer also balances critique with praise, acknowledging the strengths of the work alongside areas for improvement.",50.0,80.0
"To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network.

The paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness.

However, my main concern about the paper is still about its significance. 
1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, “lower value of \sigma^\ell(s) are indicative of better separation between classes”, what is the reason of not directly penalizing this value, rather than requesting a “stability” property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper.
2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of “Proposed + Parseval”. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings.

Other comments:
1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. 
2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some improvements and interesting ideas, they express significant concerns about the paper's significance and experimental results. The reviewer states that the main concern 'is still about its significance' and that the results are 'not convincing', indicating overall dissatisfaction despite some positive aspects. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging improvements and using phrases like 'It would be very helpful' when making suggestions. They also frame criticisms as personal opinions ('I am not very convincing') rather than absolute statements. However, the review doesn't go out of its way to be overly polite, maintaining a professional tone.",-20.0,60.0
"# Summary

This work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.

# Quality

The proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)

A second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).

Some smaller comments:

* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.
* Input sequences in NLP are not always sentences given as sequences of words.
* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).
* The authors say that operations of the same ""type"" can be batched together, but don't specify what ""type"" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?
* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1', y2']'), reducing the number of memory accesses in the output buffer.
* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.

# Clarity

The writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.

The text contains grammatical errors (""days even weeks"", ""The parallel computing helps""), tautological definitions (""batching [...] means organizing the same operations of computation graphs into batches"", ""padding, which is to pad the input sequences""), unclear use of language (""cooperating with the existing strategies""), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.

# Originality and significance

The central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.

# Summary

Pros:

  * Useful dynamic batching trick that can lead to speedups
  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime

Cons:

  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage
  * Some questionable statements and assumptions
  * Lack of formalization and clear definitions
  * Paper reads long-drawn-out, subpar writing hurts readability","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('useful dynamic batching trick that can lead to speedups'), they list more cons than pros and express significant criticisms about the paper's methodology, clarity, and assumptions. The overall tone suggests the paper needs substantial improvement. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'should have been made' and 'needs significant improvement' rather than harsh language. They also acknowledge positive aspects before diving into criticisms. However, the review is not overly polite or effusive, maintaining a direct and critical stance appropriate for peer review.",-30.0,20.0
"The authors proposed an unsupervised learning framework to learn multisensory binding, using visual and auditory domain from animal videos as example. First, the visual and auditory inputs are autoencoded, and these latent codes are binding using a recurrent self-organizing network (Gamma-GWR). Furthermore, the authors proposed the expectation learning idea, which is inspire by psychology literature. In short, after the first pass of training using the real data. The authors fine tuned the model to bind the real data from one domain and the reconstructed data from another domain. This could be a good idea, as the authors pointed out, human usually bind all kinds of yellow bird to a same mental 'chirping' sounds. So, this expectation learning could potentially group the representation to a canonical one. Also, the authors showed in Table 1 that with the expectation learning, the model's recognition accuracy is improved a bit. I think it would be interesting to show the reconstruction output example (as in Fig. 3) for both model with and without expectation learning. To see if it is as the authors claim, that the model with expectation learning is reconstructing the missing modality with more canonical images/sounds. (This may not be the goal in other practice, though I'm convinced it is a potentially good psychological model as it explain well the multisensory imagery effect (Spence & Deroy, 2013). 

I found this manuscript quite hard to follow though. The description seems sometime not flowing very smoothly. And there are some clear typos and mess up of math notations make the reading unpleasant. I have noted down several points below, and hope the authors could improve in the next iteration.

1. The description of variational autoencoder is not well written. The citation (Chen, 2016) is not the standard VAE paper people usually cite (unless the author is adopting something specific from the Chen's paper.). For example, the authors wrote ""the KL divergence between the encoded representation and a sample from the Gaussian distribution"" which sounds incorrect to me.

2. Why a Variational autoencoder is necessary for visual domain, but a regular autoencoder is used in auditory domain?

Typos:
1. page 2, 2nd line: a online --> an online
2. Use subscript I-1 to mean the winner neuron at t-1, I think this is not quite clear. I suggest to follow the notation in (Parisi & Wermter 2017), use I(t-1), which is easier to follow.
3. page 7, 2nd line: more than 17% for audio.  -> for vision.
4. page 8, 3rd line: not on the original high-abstraction data. Do the authors mean highly specific data? That seems make more sense.
5. Several notation mismatch here and there. for example, in formula 6 it is w_j^s, but in the text below it become w_{j,s}.
","Sentiment Score (20): The review begins with a positive tone, acknowledging the authors' proposed framework and its potential benefits. The reviewer finds the idea interesting and sees value in the expectation learning concept. However, they also express some criticism about the manuscript being hard to follow and containing typos, which slightly lowers the overall sentiment. The balance of positive comments and constructive criticism results in a mildly positive score.

Politeness Score (60): The reviewer maintains a professional and respectful tone throughout. They use phrases like 'I think it would be interesting' and 'I hope the authors could improve' which are polite ways of offering suggestions. The criticism is presented constructively, focusing on specific areas for improvement rather than harsh judgments. The reviewer also acknowledges the potential merits of the work. However, some direct statements about the manuscript being hard to follow and containing typos prevent it from reaching a higher politeness score.",20.0,60.0
"The paper addresses the problem of computational inefficiency in video surveillance understanding approaches. It suggests an approach called Dynamic Convolution consists of Frame differencing, Prediction, and Dyn-Convolution steps. The idea is to reuse some of the convolutional feature maps, and frame features particularly when there is a significant similarity among the frames. The paper evaluates the results on 4 public datasets. However, it just compares the approach to a baseline, which is indeed applying convnet on all frames. 

- State of the art is not well-studied in the paper. Video understanding approaches usually are not just applying convnet on all frames. Many of the approaches on video analysis, select a random set of frames (or just a single frame) [5], and extract the features for them. There is another set of work on attention, that try to extracts the most important spatio-temporal [1-4] information to solve a certain task. These approaches are usually computationally less expensive than applying convnet on all video frames. I suggest the authors compare their model with these approaches. 

[1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al. 
[2] Recurrent Models of Visual Attention, Mnih et al.
 [3] Action recognition using visual attention, Sharma et al.
 [4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al.
 [5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al. 

- In addition, car and pedestrian detection performance is part of the evaluation process. In this case, the approach should be also compared to the state-of-the-art tracking approaches (that are cheaper to acquire) in terms of computational efficiency and performance. 
- The writing of the paper should also improve to make the paper more understandable and easier to follow. Some examples: 1. Unnecessary information can be summarized. For example, many details on the computational costs in abstract and the introduction can just simply be replaced by stating that “these approaches are computationally costly”.  2. Using present tense for the SoTA approaches is more common.“ShuffleNet (Zhang et al. (2017)) proposed two new strategies”.  3. Long sentences are difficult to follow: “In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”
  + The problem of large-scale video understanding is an important and interesting problem to tackle.  ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper addresses an important problem, they point out several significant shortcomings, including lack of comparison to state-of-the-art methods, need for improved writing, and suggestions for additional comparisons. The overall tone suggests the paper needs substantial revisions. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement. They also acknowledge the importance of the problem being addressed. The reviewer maintains a professional tone without harsh or rude language, even when pointing out weaknesses.",-20.0,50.0
"This paper proposes new heuristics to prune and compactify neural networks. The heuristics try to consider 1) filter weight and gradient normalization by their size, 2) saliency normalization across layers, 3) saliency accumulation across batch. The author claims that these can address problems previous studies had and experimental results show that the proposed method achieve higher compression ration with less loss of accuracy.

This paper discusses how to determine the importance of filters. As cited in the paper, there have been various attempts to tackle the same problem and the paper contributes to the series of efforts. The paper introduces a new way to compute such importance values based on their observations. The method is tested on a few dataset and a various models and compared with some previous studies. I like the simple but yet effective method, however, I think it is not good enough for ICLR. 

1. What is effective is not very clear.

The paper pointed out issues of previous studies and proposed the new method based on the observations. However, only the final method is compared with other work and it did not examine which part of the method was essential. The paper needs more detailed analyses on the proposed method. For example, the readers would want to know if the normalization in Eq. (2) is really important or not. The readers would be also interested in a visualization like Fig. 2 without saliency normalization. 

2. The numbers of previous studies come only from their papers.

It is very difficult to know if the proposed method is actually better than the previous methods if the numbers just come from their papers. We want to compare the ideas, but not numbers. The essential ideas of other papers need to be abstracted and tested in the paper by itself. It relates to the first item above. ""Baseline"" should be a baseline method but not models without pruning.

Numbers from other papers are still useful to show that the numbers in the paper are good in an absolute manner.

3. Weak theoretical reasoning

Eq. (1) in the paper is not actually used for optimization while some previous methods do. If the proposed method is better than other methods which directly optimizes the loss, should we think that the formulation itself is bad? 

The paper discusses imbalanced pruned pruning results. It needs to show that it is actually bad.

* minor things

** Table 1: Should the first row of ""Wen et al. (2016)"" have ""5-19"" and ""1-5"" or ""4-19"" and ""1-4"" for ""Filters"" and ""Channels"", respectively?

** I'd recommend another proofreading.","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('I like the simple but yet effective method'), they ultimately conclude that 'it is not good enough for ICLR' and list several significant criticisms. The reviewer points out issues with clarity, comparison methodology, and theoretical reasoning, indicating an overall negative sentiment towards the paper's current state. The politeness score is slightly positive (20) as the reviewer uses generally respectful language, offers constructive criticism, and includes positive comments. They use phrases like 'I like' and 'I'd recommend' which maintain a polite tone. However, the criticism is direct and doesn't use overly softening language, keeping the score from being higher.",-30.0,20.0
"This paper introduces an approach to pruning while training a network. This is interesting and experiments show interesting results in several datasets including ResNet18

Here are a few comments:

 - Pruning or regularization for compression is not new. Alvarez and Wen have used group lasso types as suggested in the paper and some others such as Alvarez and Salzmann (Compression aware training NIPS 2017) and Wen (Coordinating filters ICCV2017) have used low-rank type of while training. How is this different to those? They also do not need any sort of fine tuning and more importantly, they show this can scale to large networks and datasets. 

- These last two works I mentioned promote redundancy, similarly to what is suggested in the paper. Would be good to get them cited and compared. Important from those is the training methodology to avoid relevant overheads. How is that happening in the current approach


- While I like the approach, would be nice to see how this scale.  All for methods above (and others related) do work on full imagenet to show performance.  For ResNet, cleaning the network is not really trivial (near the block), is that a limitation?
- Why limiting experiments to small networks and datasets? Time wise, how does this impact the training time?
- Why limiting the experiments to at most 4 layers? 
- I am certainly not impressed by results on fully connected layers in MNIST. While the experiment is interesting does not seem to be of value as most networks do not have those layers anymore.

- Main properties of this approach are selecting right filters while training without compromising accuracy or needing fine tuning. While that is of interest, i do not see the difference with other related works (such as those I cited above)

- As there is enough space, I would like to see top-1 results for comprehensive comparison. 

- I think tables need better captions for being self-contained. I do not really understand what i see in table 5 for instance. 
- Droping 13% of top5 accuracy does not seem negligible, what is the purpose there? Would also be interesting to compare then with any other network with that performance. 
- What about flops and forward time? Does this pruning strategy help there?

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as 'interesting' and notes 'interesting results', they raise several critical points and express that they are 'not impressed' by some aspects. The overall tone suggests more concerns than praise. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, framing criticisms as suggestions ('would be good to', 'would be nice to see') and asking questions rather than making blunt statements. They also begin with a positive note about the paper being interesting. However, the review doesn't go out of its way to be excessively polite or complimentary, maintaining a professional tone.",-20.0,50.0
"The paper proposes a new loss function which can be used in the reconstruction term of various auto-encoder architectures. The pixel-wise cost function \ell(X, X') = f(X - X'; a) is defined for pairs of two input images X and X' and has one positive real-valued hyperparameter a. For small values of t the function f(t; a) behaves like a quadratic function, while for large t it behaves like |t|. As a consequence, it is smooth, everywhere differentiable (like L2) while not penalizing outliers too hard (like L1). The authors present several experiments conducted on MNIST and Celeba datasets, demonstrating that a simple change of a conventional pixel-wise squared L2 distance with the proposed log-cosh cost function improves the FID scores of generated samples as well as the visual quality of reconstructions (including ""the sharpness""). 

I would say this is clearly an empirical study (even though the authors claim they provide ""theoretical justifications"", they are rather hand wavy), which is not a bad thing in this case. The message of the paper is very clear and I think the authors did a good job in selling their point. The main (and, perhaps, the only) contribution is the proposal to use the log-cosh function as the reconstruction cost. And this proposal is well justified by the set of experiments. 

However, there are several major issues:
(1.1) The objective functions reported in appendix A.1 corresponding to WAE have in fact nothing to do with WAE. In WAE the regularizer penalizes the divergence between the prior distribution p(a) and *the aggregated posterior* distribution \int_x q(z|x) p(x) dx. In other words, D_MMD(q(z|x) || p(z)) in Eq. 8 should be replaced with D_MMD(\int_x q(z|x) p(x) dx || p(z)) in order to result in the WAE model. In summary, if the authors indeed used objectives reported in Eq. 8 of Appendix A, they were actually not using WAE but rather some other sort of regularized auto-encoders, which in a way are quite similar to VAEs. 
(1.2)  I am surprised to see the reported FID scores for the Celeba data set. Having worked with this data set myself in combination with VAEs and WAEs, I am impressed with the extremely low FID scores: 46 for the vanilla L2 VAE and 30 for the L2 WAE. Note that while in the appendix the authors say they follow the architectural choices provided in [1] while performing the ""L2 WAE Celeba"" experiment, the authors arrive at FID=30 compared to FID=55 reported in the ""Wasserstein Autoencoders"" paper. Also, based on my experience, achieving FID=46 on CelebA with a vanilla VAE is very impressive. Note that the authors use 10^4 of samples to evaluate the FID scores, which is exactly the same as in [1]. This size is known to be large enough to reduce the variance of FID, so the difference (55 - 30) can not be explained by the fluctuations of FID. Therefore, I ask the authors to (anonymously) share the code and/or checkpoints of the 2 particular trained models: L2 VAE and L2 WAE trained on Celeba. 

Other comments:
(2.1) Note that the reconstruction cost function in VAE should be normalized for every value of the code Z, as it corresponds to the logarithm of the likelihood (density) function -log p(X|Z). L2 and L1 costs both correspond to the well known likelihood (decoder) models (Gaussian and Laplace). However, it is hard to say what decoder model (what type of conditional distribution p(X|Z) ) would give rise to the proposed log-cosh function. In particular, the normalizing constant is not known and may depend on Z. In other words, by exchanging the L2 cost with the log-cosh loss in the VAE one looses the theoretical guarantees supporting VAE, including the fact that the objective is the lower bound on the marginal log likelihood. While this is not necessarily a problem (unless one uses the value of the objective as the bound on the marginal log likelihood, which is not the case in this paper), I would suggest mentioning it. Notice that, for instance, in WAE this problem does not appear, as the reconstruction term there does not involve any likelihood functions and thus does not need to be normalized.
(2.2) In Figure 2 I don't see why the authors did not highlight bad samples in the second row corresponding to their proposed method? I see many badly looking images there. Say, (4, 9) in VAE (MLP) and (8, 9) in VAE (Conv) and (6, 1) in WAE (MLP) and (2, 10) in WAE (Conv), where (i, j) means i-th row, j-th column, indexing starting from 1. 
(2.3) How would the Huber loss perform and how does it compare to the proposed loss?

[1] Wasserstein Autoencoders. Tolstikhin et al., ICLR, 2018.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('The message of the paper is very clear and I think the authors did a good job in selling their point'), they also point out 'several major issues' and express skepticism about some results. The overall tone suggests more concerns than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, even when critiquing. They use phrases like 'I would suggest' and 'I ask the authors to' rather than making demands. The reviewer also acknowledges positive aspects before diving into criticisms, which is a polite approach. However, it's not extremely high as the review is still quite direct in its criticisms.",-20.0,60.0
"The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.

Contributions
-----------------
A simple method for improving structural diversity.

The use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.

Experiments suggest that the method is rather effective (albeit only improving translation quality marginally)

I like the proposed discrepancy score based on pairwise BLEU scores.

Issues
---------
It is not clear if teacher forcing was used in the ""tag planning"" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the ""tag planning"" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).

It is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.

No details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.

Having a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.

Minor issues
-----------------
Citation for the Xavier method is missing.

Notation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.

There are many grammatical errors. Please make sure to proofread!

""Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap."" What does this mean?","The sentiment score is slightly positive (20) because the reviewer acknowledges several contributions and positive aspects of the paper, such as the 'simple method for improving structural diversity' and the 'rather effective' experiments. However, they also point out several issues, which tempers the overall positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I like' and 'I would urge the authors to consider' which maintain a polite tone. The reviewer also balances positive feedback with areas for improvement, demonstrating a considerate approach to the review process.",20.0,60.0
"This paper reads like the first thoughts and experiments of a physicist or mathematician who has decided to look at word representations and hyponymy. I mean that in both the positive and negative ways that this remark could be construed. On the positive side, it provides an interesting read, with a fresh perspective, willing to poke holes in rather than accepting setups that several previous researchers have used.  On the negative side, though, this paper can have an undue, work-from-scratch mathiness that doesn't really contribute insight or understanding, and the current state of the work is too preliminary. I think another researcher interested in this area could benefit from reading this paper and hearing the perspective it presents. Nevertheless, there just isn't sufficient in the way of strong, non-trivial results in the current paper to justify conference acceptance. 

Quality:

 - Pro
   o Everything is presented in a precise formalized fashion. The paper has interesting remarks and perspectives. I appreciate that the authors not only did find most existing work on modeling hyponymy but provide a detailed and quite insightful discussion of it.  (A related paper from overlapping authors to papers you do cite that maybe should have been included is Chang et al. https://arxiv.org/abs/1710.00880 – which is a bit different in trying to learn hyponyms from text not WordNet, but still clearly related.)
  -Con
   o There just isn't enough here in the way of theoretical or experimental results. In the end, two ""methods"" of hyponymy modeling are presented: one is a simple logistic regression, which is estimated separately PER WORD for words with 10 or more hyponyms. This performs worse than the methods of several recent papers that the author cites. The other is a construction that shows that any tree can be embedded by representing nodes as ranges of the real line. This is true, but trivial. Why don't ML/NLP researchers do this? It's because they want a representation that doesn't only represent the ISA hierarchy but also other aspects of word meaning such as meaning similarity and dimensions of relatedness. Furthermore, in general they would like to learn these representations from data rather than hand-constructing it from an existing source like WordNet. For instance, simply doing that gives no clear way to add other words not in wordnet into the taxonomy. This representation mapping doesn't really give any clear advantage beyond just looking up hyponymy relationships in wordnet when you need them.

Clarity:
 - Pro
   o The paper is in most respects clearly written and enjoyable to read.
 - Con
   o The mathematical style and precision has it's uses, but sometime it just seemed to make things harder to follow. Referring to things throughout as ""Property k"" – even though some of those properties were given names when first introduced – left me repeatedly flicking up and down through the PDF to refresh myself on what claim was being referred to without any apparent need....

Originality:
 - Pro
   o There is certainly originality of perspective. The authors make some cogent observations on how other prior work has been naive about adopted assumptions and as to what it has achieved (e.g., in the discussion at the start of section 5.1).
 - Con
   o There is not really significant originality of method. The logistic regression model is nothing but straightforward. (It is also highly problematic in learning a separate model for each word with a bunch of hyponyms. This both doesn't give a model that would generalize to novel words or ones with few hyponyms.) Mapping a tree to an interval is fairly trivial, and besides this is just a mapping of representations, it isn't learning a good representation as ML people (or ICLR people) would like. The idea that you can improve recall by using a co-product (disjunction) of intervals is cute, though, I admit. Nice.

Significance 
 - Con
   o I think this work would clearly need more development, and more cognizance of the goals of generalizable representation learning before it would make a significant contribution to the literature. 

Other:
 - p.1: Saying about WordNet etc., ""these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models"" seems misplaced when there is now a lot of work on producing neural graph embeddings (including node2vec, skip-graphs, deepwalk, etc.). Fundamentally, it is just a bad argument: It is no different to saying that words have a fundamentally symbolic representation which cannot be readily used as input to neural NLP models, but the premise of the whole paper is already that we know how to do that and it isn't hard through the use of word embeddings.
 - p.2: The idea of words and phrases living in subset (and disjointness etc.) relationships according to denotation is the central idea of Natural Logic approaches, and these might be cited here. There are various works, some more philosophical. A good place to start might be: https://nlp.stanford.edu/pubs/natlog-iwcs09.pdf
 - p.2: The notions of Property 1 and 2 are just ""precision"" and ""recall"", terms the paper also uses. Do we gain from introducing the names ""Property 1"" and ""Property 2"" for them? I also felt that I wouldn't have lost anything if Property 3 was just the idea that hyponymy is represented as vector subspace inclusion.
 - p.2: fn.2: True, but it seems fair to more note that cosine similarity is very standard as a word/document similarity measure, not for modeling hyponymy, for this reason.
 - p.4: Below the equation, shouldn't it be Q(w', w) [not both w'] and then Q(w', w) and not the reverse? If not, I'm misunderstanding.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting read', 'fresh perspective'), they ultimately conclude that the paper is too preliminary and lacks strong results to justify acceptance. The overall tone suggests more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, balancing criticism with positive remarks, and provides constructive feedback. They use phrases like 'I appreciate' and 'interesting remarks' to soften critiques, and offer detailed explanations for their points, which is considerate to the authors.",-20.0,60.0
"Summary. The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples. They also study the influence of the smoothness. From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.

Pros.  
* Robustness of neural nets is a challenging problem of interest for ICLR
* The paper is well written
* The experimental study is convincing
* The experimental results for the smoothed gradient attacks are promising

Cons.
* The results of the experimental study are somehow expected
* the idea of smoothing gradients is not new

Evaluation.
The experimental study of the transferability of adversarial examples is well designed. Experimental protocol is convincing. The smoothed gradient attacks improve many previously proposed attacks. Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.

Some details.
Typos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4; 
* the choice \sigma = 15 in Section 6.2 should be justified by the following study
* \sigma is not given in Figure 3(a)","The sentiment score is 60 (moderately positive) because the reviewer expresses a 'rather positive' opinion overall, praising the paper's writing, experimental design, and promising results. However, they also mention some cons and express uncertainty about the novelty, preventing a higher score. The politeness score is 80 (quite polite) due to the reviewer's constructive tone, balanced feedback, and use of phrases like 'well written' and 'convincing'. They provide both pros and cons professionally, and offer specific suggestions for improvement without harsh language. The presence of a few typo corrections slightly reduces the politeness score, as it could be seen as nitpicking, but they are presented matter-of-factly rather than critically.",60.0,80.0
"Summary:

The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.


Comments:

The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. However, the motivation of using RL is missing from the technical contribution. Considering a deterministic policy, using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. 

The motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. 

The literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein).

Empirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. 


Questions:

-How are cold-start situations encountered if items are one-hot encoded?
-Why is there a strong focus on quick adaptation to user sessions? Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests.


Minor:

-Proofreading is necessary
-Table 1 and 2 would be more readable if they were figures
-Figure 3 seems to be taken from Tensorflow runtime convergence plots, which could be dropped given the limited space","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths ('interesting inspiration', 'empirical part exceptionally strong'), they also point out several significant weaknesses and missing elements in the paper. The critique is substantial, questioning the motivation, literature review, and some methodological choices. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, framing criticisms as suggestions or questions rather than direct attacks. They use phrases like 'The proposed architecture is an interesting inspiration' and 'Empirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong.' Even when pointing out flaws, the tone remains professional and constructive.",-20.0,50.0
"The authors introduce a novel distance function between point sets, based on the ""permutation invariance"" of the zeros of a polynomial, calling it ""holographic"" distance, as it essentially depends on all the points of the sets being compared. They also consider two other permutation invariant distances, and apply these in an end-to-end object detection task. These distance functions have time-complexity O(N^2) unlike the previously proposed ""Hungarian distance"" based on the Hungarian algorithm which is O(N^3) in general. Moreover, they authors show that in two dimensions all local minima of the holographic loss are global minima.

Pros: The paper is well written, the ideas are clearly and succinctly presented. Exploiting the connection between 2D point sets and zeros of polynomials is an interesting idea.

Cons: The experimental section could be better. For example, the authors could do simple experiments to show how an optimization algorithm would explore the holographic loss surface (in terms of hitting global/local minima) in dimensions greater than two. Also, in the object detection example, no comparison is given with the Hungarian loss based algorithm of Stewart et al. (2016) (at the very least, the authors could train their neural nets using the Hungarian loss, choosing one optimal permutation at the ""transitioning points"") .","The sentiment score is 60 (positive) because the reviewer starts with a neutral summary of the paper's content, followed by a 'Pros' section that praises the paper as 'well written' with 'clearly and succinctly presented' ideas. The reviewer also describes the main idea as 'interesting'. However, the score is not higher due to the 'Cons' section, which suggests improvements for the experimental section. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively, and frames criticisms constructively as suggestions for improvement rather than harsh critiques. The reviewer maintains a professional tone, balancing praise with constructive feedback.",60.0,80.0
"The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \= p(y).   Figure 1 is used to illustrate this issue. 

If the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \= p(y|x_T)  ), the optimal invariance can lead the same generalization problem.   Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability.  It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. 

It is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. 

Reference:
[1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014.
[2]Conditional Adversarial Domain Adaptation, NIPS 2018","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contribution, they point out significant oversights and suggest major additions. The reviewer notes that 'a lot of works has been done' on related problems that weren't addressed, and requests additional analysis and comparisons. This indicates some dissatisfaction with the current state of the paper. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, framing criticisms as suggestions ('It is desirable to discuss...', 'It is also suggested to...') rather than direct criticisms. The tone is professional and constructive, avoiding harsh or dismissive language.",-20.0,50.0
"This paper proposes to jointly train a classifier with a domain and label-aware word embedding model and a variational Bayes model for sentiment domain adaptation. The model is evaluated on a standard multi-domain sentiment analysis dataset where it achieves convincing results against strong baselines. Extensive ablations are conducted.

Pros:
- The paper is clearly written. I particularly appreciated Figure 1 as it might otherwise be difficult to see the relation of the different components of the model.
- The model achieves convincing results and ablations and analyses are extensive.

Cons:
- None of the presented ideas are entirely novel. The model rather combines many existing ideas successfully.
- The framework consisting of many components (particularly the joint training with CBOW as indicated in the appendix) seems rather brittle and very task-specific. I am concerned if this framework will be able to work on other tasks. I would love to see an evaluation on another dataset. 
- The joint variational Bayes approach seems to be the most interesting aspect of the paper. Despite the ablations, it's not entirely clear to me, though, how useful this component is and if it can be applied beyond this particular model. I would like to see one of the baselines models or another model augmented with this component. 

Questions:
- Do you alternate updates between each of the components or use a more sophisticated multi-task learning strategy when training the word embedding model and the other components jointly? Did you try fine-tuning the trained word embeddings with the classifier?
- Why do you use this particular affine transformation for learning sentiment-specific word embeddings? Did you try, for instance, an MLP as used in [1]?
- You say that you use a classifier q_φ(z |D, c) in order to benefit from more freedom of design for Bayesian inference. Could you elaborate why you use this classifier in addition to the label classifier q_φ(y |D, c)? In Table 6, it seems that it doesn't help much. The use of the term ""classifier"" is confusing at times, as it seems to be both used to refer to the latent variable and the label classifier.

[1] Tang, D., Wei, F., Yang, N., Zhou, M., Liu, T., & Qin, B. (2014). Learning Sentiment-Specific Word Embedding. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 1, 1555–1565.","The sentiment score is 50 (slightly positive) because the reviewer begins by highlighting the paper's strengths, including clear writing, convincing results, and extensive ablations. However, they also point out several cons and areas for improvement, balancing the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's merits, and frames criticisms constructively as questions or suggestions for improvement. They use phrases like 'I particularly appreciated' and 'I would love to see,' which maintain a collegial tone. The reviewer also provides specific, actionable feedback without using harsh or dismissive language.",50.0,80.0
"The paper proposes to use the technique in VAT to generate adversarial complementary examples in the (K+1)-class semi-supervised learning framework described by the Bad GAN paper. This leads to a formulation that combines the VAT loss and the (K+1)-class classification loss. The paper also provides analysis regarding why VAT is useful for semi-supervised learning.

Pros
1. It is interesting to bridge two state-of-the-art semi-supervise learning methods in a meaningful.
2. Some positive results have been presented in Table 1 and Figure 4.

Cons and questions
1. I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here?
2. Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy.
3. If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method?
4. The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak.
5. There is not ablation study of different terms in the objective function.
6. In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting', 'positive results'), there are more cons than pros listed, and the overall tone suggests the paper has significant shortcomings. The reviewer points out several issues with the methodology, theoretical justification, and empirical results. The politeness score is moderately positive (60) because the reviewer uses neutral, professional language throughout. They frame criticisms as questions or suggestions rather than direct attacks, and use phrases like 'I don't understand' instead of more accusatory language. The reviewer also acknowledges the positive aspects of the work before delving into criticisms, which is a polite approach in academic reviews.",-20.0,60.0
"This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values.

weakness:
1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.

2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.

3. There are no titles in the figures and tables in this paper

4. the authors don't confirm the superiority of the proposed method to others.

minor comments
1. what's the meaning of Target mAP in the table?
2. It seems that Some figures are cropped from TensorBoard, with some extra shadows.","The sentiment score is -90 because the review is overwhelmingly negative. The reviewer states that the paper has 'no potential values' and 'doesn't give any new contributions'. They list several weaknesses and no strengths, indicating a very negative sentiment. The politeness score is -20 because while the language isn't overtly rude, it's quite blunt and dismissive. Phrases like 'the paper is written badly' and 'I even can't find any contribution' are quite harsh. The reviewer doesn't use any softening language or acknowledge any positive aspects, which would be more polite. However, they do use some neutral language in parts, preventing the score from being even lower.",-90.0,-20.0
"\clarity & quality
The paper is easy to follow and self-contained. 
However, the motivation for minimizing the upper bound is not so clear for me. 
As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. 
In the introduction, the author just mentioned that ""the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.""
For me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.

\originality & significance
Although the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.

However, it is hard to evaluate the usefulness of the proposed method from the current experiments.
It seems that there are two merits about the proposed method as above.
The only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.
About the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.
So I cannot tell whether the proposed objective is really useful to learn the deep generative models.
I think further experimental results are needed to validate the proposed method.

\Question
In page 4,  the variance of the p(y|x) and p_\theta(y|z) are set to be the same. What is the intuition behind this trick? 
Since this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('easy to follow', 'new and interesting'), they express several concerns and criticisms. They state that the motivation is not clear, the experimental evidence is weak, and further validation is needed. The overall tone suggests the paper needs significant improvements.

The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use phrases like 'As far as I understood' and 'I think', which soften their criticisms. The reviewer also balances negative points with positive ones, acknowledging the paper's strengths before discussing its weaknesses. However, the score is not higher as the language, while polite, is not overly formal or deferential.",-20.0,50.0
"This paper provide a modification on the classical LSTM structure. Specifically, it reformulate the forget gate with a monotonically decreasing manner, using sinusoidal function as the activation function. 

However, both the motivation and experimental results on such modification are not convincing enough. 

1. While there are many heuristic guesses in sec3, important supports of these guesses are missed. For example, Figure 2 is designed to provide supports for the claim that we need controlled forget gates.  However, all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*, which provides limited insight on what will happen in the real cases. While the reformulation in sec4 is based on the observations in Figure 2, it is important to plot the real cell propagation after the reformulation, and see whether the real observation meets the conceptual observations in Figure 2.
BTW, Plots in Figure 2 only account for LSTMs' propagation within 3 steps, but in real cases there are way more steps. 

2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed. But it isn't straightforward to get the relations between abrupt shrinkage and sudden growth on forget gates and the expressive power of the vanilla-LSTM. Also, it's hard to say the monotonic propagation is more interpretable because we don't know what's the meaning of such propagation on the behaviors of LSTMs in applications. 

3. The reformulation in sec 4, especially the formula for the forget-polar input p_k, looks heavily hand-crafted, without experimental supports but statements such as ""we ran numerous simulations"", which is not convincing enough. 

4. Experiments are applied on MNIST and Fashion-MNIST. While both datasets are not designed in nature for sequential models like LSTMs. There are better datasets and tasks for testing the proposed reformulation.   e.g. sentence classification, text generation, etc.  No explanation on the choice of datasets.  In addition, the difference between vanilla-LSTM and DecayNet-LSTM is small and it's hard to say it isn't marginal. Maybe larger-scale datasets are needed. 

5. Lacking of explanation on specific experimental settings. E.g. training all methods for *only one epoch*, which is very different from the standard practice.  

6. More qualitative interpretations for real cell states in both vanilla LSTM  and DecayNet-LSTM are needed. Only conceptual demonstration is included in Figure 2. ","The sentiment score is -60 because the review is predominantly critical. While it acknowledges the paper's contribution in the first paragraph, the rest of the review lists several significant concerns about the methodology, experimental design, and lack of convincing evidence. The reviewer uses phrases like 'not convincing enough', 'limited insight', and 'heavily hand-crafted', indicating a negative sentiment towards the paper's quality and rigor. The politeness score is 20 because while the reviewer is critical, they maintain a professional and objective tone throughout. They use neutral language like 'However' and 'It's hard to say' rather than harsh or dismissive phrasing. The reviewer also offers constructive suggestions for improvement, which contributes to the politeness of the review.",-60.0,20.0
"This paper describes a two-stage encoder-decoder model for semantic parsing. The model first decodes a cross-domain schema (CDS) representation from the input utterance, then decodes the final logial form from both the utterance and CDS. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset, but is still behind the traditional slot-filling models (Goo et al., 2018).

My main concern is that it is unclear to me how CDS (cross-domain schema) can be generalized to the other semantic parsing datasets, e.g., the Overnight dataset (Wang et al., 2015), which also contains multiple domains. 

I think it would be nice to have some details about the CDS in the paper. For example, I’m wondering 1) how is this CDS designed? 2) how are the CDS annotations derived from the target output? 

There are other details missing regarding the comparisons and the evaluation metrics. In 4.2, the authors mentioned “We use accuracy as the evaluation metric’’, does “accuracy” mean full logical form accuracy or accuracy on execution results?

* More minor comments:
In the first paragraph of Section 3, “irrelevant to domain’’ -> “domain-general’’ or “domain-agnostic’’?

It will be nice to write something more specific than “explore more ways to make it work better” in the future work.

This paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).

* Missing references:
Neural semantic parsing over multiple knowledge-bases, Herzig and Berant, ACL 2017 <- This paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.

(Concurrent) Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing, Herzig and Berant, EMNLP 2018 
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper, they express several concerns and point out missing details and errors. The overall tone suggests the paper needs significant improvements. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, offering constructive criticism and suggestions rather than harsh criticism. They use phrases like 'I think it would be nice' and 'It will be nice to', which contribute to a respectful tone. The reviewer also provides specific, helpful feedback and additional references, which is considerate and helpful to the authors.",-20.0,50.0
"This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. 

The hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs.

In a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize ""shape"" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images).

Additional analysis involving visualization methods to further explain why shape features were ignored would have been a plus– with bonus points for providing a heuristic to determine the ""shapelessness"" of a convolution kernel.","The sentiment score is slightly positive (20) because while the reviewer acknowledges the value of the work and its clever experiments, they also point out that it's not groundbreaking and offers a null result. The positive aspects (adding to literature, clever experiments) are balanced against the limitations (lack of novelty, absence of proposed solutions). The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's contributions while offering constructive criticism. They use phrases like 'cleverly answer these questions' and 'there is value in the work presented', which maintain a polite tone even when discussing limitations. The reviewer also offers suggestions for improvement without being demanding or harsh.",20.0,60.0
"This paper presents a method to train NNs as black box estimators of the commitor function for a physical, statistical mechanical, distribution. This training is performed using samples from the distribution. As the committor function is used to understand transitions between modes of the distribution, it is important that the training samples include points between modes, which are often extremely low probability. To address this concern, this paper draws MCMC samples at a high temperature, and then uses importance weights when training the committor function using these samples. Overall -- this seemed like a good application paper. It applies largely off-the-shelf machine learning techniques to a problem in physics. I don't have enough background to judge the quality of the experimental results.

I had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in high-d. The variance of importance sampling estimates typically increases exponentially in the dimensionality of the problem, so I suspect this technique as presented would fall apart quickly if pushed to higher dimensions. All experiments are on problems with either 9 or 10 (effective) degrees of freedom, which from the ML perspective at least is quite low dimensional, and which is consistent with this exponentially poor scaling. There are likely fixes to this problem -- e.g. the authors might want to look into annealed importance sampling*.

more specific comments:

""and dislocation dynamics"" -> ""dislocation dynamics""

""One can easily check"" -> ""One can check"" :P

eq 5 -- this is very sudden deep water! Especially for an ML audience. You should either give more context for the Kolmogorov backward equation, or just drop it. (The Kolmogorov formulation of the problem is not used later, and for an ML audience describing the task in terms of it will confuse rather than clarify.)
what is \Delta q? Does that indicate the Laplacian? Not standard ML notation -- define.

similarly, define what is intended by \partial A and \partial B (boundary of the respective regions?)

eq. 9 -- nit -- recommend using a symbol other than rho for regularization coefficient. visually resembles p, and is rarely used this way. lambda is very common.

eqs 10/11 -- include some text motivation for why the definition of chi explicitly excludes the regions inside A and B.

eq 14: cleverly formulated!

eq 14 / eq 20:
factor of 1000 is very fast! corresponds to an epsilon of O(1e-3). You need to make sure that training samples are generated in the epsilon width border around A and B, otherwise the effect of chi will be invisible when training q_theta. So it seems like epsilon should be chosen significantly larger than this. Might want to include some discussion of how to choose epsilon.

* Totally incidental to the current context, but fascinatingly, annealed importance sampling turns out to be equivalent to the Jarzynski equality in nonequilibrium physics.","The sentiment score is 50 (slightly positive) because the reviewer starts by describing the paper as 'a good application paper' and praises its approach. However, they also express a 'major concern' about the method's scalability to higher dimensions, which tempers the overall positivity. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offers constructive criticism, and even provides helpful suggestions (e.g., 'the authors might want to look into annealed importance sampling'). The use of phrases like 'I don't have enough background to judge...' shows humility. The reviewer also uses polite hedging language like 'I suspect' when expressing concerns. The only slightly impolite moment is the ':P' emoticon, which is informal but not rude. Overall, the review maintains a professional and courteous tone while providing both praise and criticism.",50.0,70.0
"The paper is extremely difficult to read. There are too many concepts introduced at once, casual comments mixed with semi-formal statements. The theorems sound interesting, the implications are grand and of interest to ICLR, but the proofs are impossible to follow. As such, I am not in a position to make a recommendation. 

I strongly recommend the authors to split the paper into multiple parts with clear-cut statements in each, with clear and detailed proofs, and submit to appropriate journals / conferences. 
","The sentiment score is -60 because the reviewer expresses significant difficulty in reading and understanding the paper, stating it's 'extremely difficult to read' and the proofs are 'impossible to follow'. However, they do mention that the theorems sound interesting and have implications of interest to ICLR, which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they use relatively polite language. They 'strongly recommend' rather than demand changes, and they acknowledge the potential value of the work. The reviewer also explains their inability to make a recommendation, showing consideration for the authors. The language is professional and constructive, albeit direct about the paper's shortcomings.",-60.0,20.0
"This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by  angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca.

This paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference).

In addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's 'impressive theme' and that it 'shows many insights', they also state that 'the contribution of this paper is limited' and that it 'lacks the discussion of how much the layer-level training speed is important'. The reviewer also strongly recommends improvements to the figures, indicating significant issues with the presentation. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positive aspects before providing criticism, and offers constructive suggestions for improvement. The use of phrases like 'It is better to show' and 'I strongly recommend' are polite ways of offering criticism and advice.",-20.0,50.0
"This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough. The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection. On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups. Also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.). The contributions of the method could also be underlined more clearly in the abstract and introduction. The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments. 
The idea of selective sampling for self-training is promising and the investigated questions are interesting. As far as I understand, the main contribution of this paper is the use of separate ""selection network"" to estimate the confidence of predictions by ""classification network"". However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information. For example, imagine the case of binary classification. If the selection network predicts 0 in come cases, it can be used to improve the result of ""classification network"" by flipping the corresponding label. How can you interpret such a thought experiment? One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds. Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of ""classification network"" is greater than some threshold? Finally, could you show a plot of top-1 prediction of ""classification network"" vs score of ""selection network"" and elaborate on that?
Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes. Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled. Ablation study shows that the use of the ""selection network"" strategy does not improve the results without these heuristics. It would be interesting to see how these heuristics would do without ""selection network"", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG. In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.
It is very valuable that the experimental results include many recently proposed methods. Besides, the settings are described in details that could help for the reproducibility of the results. However, I have a few concerns about the results. First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3). Besides, as the base classifier is different for various baselines, it is hard to compare the methods. Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2). How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice? Another important parameters is the number of iterations of the algorithm. How was it chosen? Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes? What would happen if you use random class splits or split animal classes (like in a more realistic scenario)? 
To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.

Some questions and comments:
- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?
- In the training procedure of ""selection network"" of Sections 3.1, do you use the same datapoints to train a ""classification network"" and ""selection network""? If it is the case, how do you insure that the ""classification network"" does not learn to fit the data perfectly and thus all labels s_i are 1?
- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?
- What do you mean in section 3.3 by ""if one class dominates the dataset, the model tends to overfit""?
- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.
- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?
- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?
- Can you explain the sentence ""To prevent data being added suddenly, no data was added until 5 iterations""?
- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?","The sentiment score is slightly negative (-30) because while the reviewer finds the topic interesting and acknowledges some positive aspects, they express several significant concerns about the methodology, experimental evaluation, and clarity of the paper. The reviewer states that the paper is difficult to follow, the writing needs improvement, and they are not convinced by the experimental results or the necessity of the proposed approach. However, they do note some positive elements like the interesting problem and reasonable intuitions.

The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They phrase criticisms as suggestions for improvement or questions rather than harsh judgments. The reviewer also acknowledges positive aspects and the potential value of the work alongside the critiques. The language used is respectful and focused on the content rather than making personal comments about the authors.",-30.0,50.0
"This paper formulates feature attribution from a feature selection perspective, and compares EFS (Exclusive Feature Selection) and IFS (Exclusive Feature Selection), which shows IFS is a better fit for feature attribution.

[+] The paper is well-structured and the proposed approach is clearly presented.
[-] It would helpful if the author could discuss the time complexity of proposed methods and compare the running time with baseline methods in evaluation.
[-] My major concern on this paper is the significance, as the contribution of the paper seems to be very limited.
    1) Formalizing the feature attribution problem as a feature selection problem is straightforward. IFS and EFS are just Forward and Backward stepwise feature selection, which are classic feature selection schemes. Applying them to feature attribution/saliency map does not seem to have much technical contribution.
    2) One claimed contribution of this paper is that existing feature attribution methods can be viewed as approximation of IFS and EFS. However, this contribution also seems to be minor. As many feature selection methods are known to be approximation of backward or forward stepwise feature selection, it is straightforward to show the connection between other feature attribution methods and IFS/EFS.

In conclusion, I would recommend to reject this paper due to the limited novelty and technical contribution.
","The sentiment score is -70 because the review is predominantly negative. While it starts with a positive comment about the paper's structure, the majority of the review focuses on significant concerns and limitations, ultimately recommending rejection due to 'limited novelty and technical contribution'. The politeness score is 20 because the reviewer uses relatively neutral language and provides specific feedback, avoiding harsh or rude phrasing. They acknowledge positive aspects before presenting criticisms, which is a polite approach. However, the overall tone remains professional rather than overtly polite, hence the modest positive score.",-70.0,20.0
"The paper proposed a RNN with skip-connection (external memory) to past hidden states, this is a slightly different version of the TARDIS network. The authors experimented on PTB and a temporal action detection method.

Novelty:

I dont see a lot of novelty to the method. The authors proposed a method very similar to TARDIS, the difference seems to be that MMARNN does not use extra usage vectors for reading from previous memory, but this is not a fundamental difference between MMARNN and Tardis.

Shortcomings of the paper:

1. The experiments seem rather weak. The authors experimented on PTB and temporal action detection method. It is not clear why authors experimented with PTB, this is not a task with long-term dependencies, I do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used

2. The model uses a single past hidden state, it is not clear to me why this is better than using  a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past. The authors should cite ""Sparse attentive backtracking"" (https://arxiv.org/abs/1809.03702) at NIPS 2018. SAB is very related in that it also propagate gradients to a few hidden states in the memory. The difference is that SAB used a few hidden states from the past/ memory instead of one; another difference is that it propagates gradients locally to the selected hidden states/ memory slots.

3. The paper only demonstrated experimental results on PTB and temporal action prediction. I think it would make the paper a lot stronger if the authors experimented with a variety of  different tasks. Tasks that requires long term dependencies can really demonstrate the strength of the model (copy and adding tasks).

4. If the authors could run the model on copy and adding tasks, I would be curious to see if the model is picking the ""correct"" timestep in the memory / past.

post rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes.
","The sentiment score is -30 because the reviewer expresses several criticisms and shortcomings of the paper, indicating a generally negative view. However, it's not extremely negative as they acknowledge some positive aspects and mention that the authors addressed some concerns in the rebuttal. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or rude expressions. They offer constructive criticism and suggestions for improvement rather than outright dismissal. The slightly positive politeness score reflects the reviewer's effort to maintain a courteous tone while still providing honest feedback.",-30.0,20.0
"This paper suggests a continuous-time framework consisting of two coupled processes in order to perform derivative-free optimization. The first process optimizes a surrogate function, while the second process updates the surrogate function. This continuous-time process is then discretized in order to be run on various machine learning datasets. Overall, I think this is an interesting idea as competing methods do have high computational complexity costs. However, I’m not satisfied with the current state of the paper that does not properly discuss notions of complexity of their own method compared to existing methods.

1) “The computational and storage complexity for (convex) surrogates is extremely high.” The discussion in this paragraph is too superficial and not precise enough.
a) First of all, the authors only discuss quadratic models but one can of course use linear models as well, see two references below (including work by Powell referenced there):
Chapter 9 in Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.
Conn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.
I think this discussion should also be more precise, the authors claim the cost is extremely high but I would really expect a discussion comparing the complexity of this method with the complexity of their own approach. As discussed in Nocedal (reference above) the cost of each iteration with a linear model is O(n^3) instead of O(n^4) where n is the number of interpolation points. Perhaps this can also be improved with more recent developments, the authors should do a more thorough literature review.
b) What is the complexity of the methods cited in the paper that rely on Gaussian processes?
(including (Wu et al., 2017) and mini-batch (Lyu et al., 2018)).


2) “The convergence of trust region methods cannot be guaranteed for high-dimensional nonconvex DFO”
Two remarks: a) This statement is incorrect as there are global convergence guarantees for derivative-free trust-region algorithms, see e.g.
Conn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.
In chapter 10, you will find global convergence guarantees for both first-order and second-order critical points.
b) The authors seem to emphasize high-dimensional problems although the convergence guarantees above still apply. For high-order models, the dimension does have an effect, please elaborate on what specific comment you would like to make. Finally, can you comment on whether the lower bounds derived by Jamieson mentioned depend on the dimension.

3) Quadratic loss function
The method developed by the authors rely on the use of a quadratic loss function. Can you comment on generalizing the results derived in the paper to more general loss functions? It seems that the computational complexity wouldn’t increase as much as existing DFO methods. Again, I think it would be interesting to give a more in-depth discussion of the complexity of your approach.

4) Convergence rate
The authors used a perturbed variant of the second-order ODE defined in Su et al. 2014. The noise added to the ODE implies that the analysis derived in Su et al. 2014 does not apply as is. In order to deal with the noise the authors show that unbiased noise does not affect the asymptotic convergence. I think the authors could get strong non-asymptotic convergence results. In a nutshell, one could use tools from Ito calculus in order to bound the effect of the noise in the derivative of the Hamiltonian used in Lemma 1. See following references:
Li, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.
Krichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous
and discrete time. In Advances in neural information processing systems, pages 2845–2853.
Of course, the above works rely on the use of derivatives but as mentioned earlier, one should be able to rely on existing DFO results to prove convergence. If you check Chapter 2 in the book of Conn et al. (see reference above), you will see that linear interpolation schemes already offer some simple bounds on the distance between the true gradient of the gradient of the model (assuming Lipschitz continuity and differentiability).

5) Noise
“The noise would help the system escape from an unstable stationary point in even shorter time”
Please add a relevant citation. For isotropic noise, see
Ge, R., Huang, F., Jin, C., and Yuan, Y. Escaping from saddle points-online stochastic gradient for tensor decomposition.
Jin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456,

6) Figure 2
Instead of having 2 separate plots for iteration numbers and time per iteration, why don’t you combine them to show the loss vs time. This would make it easier for the reader to see the combined effect.

7) Empirical evaluation
a) There are not enough details provided to be able to reproduce the experiments. Reporting the range of the hyperparameters (Table 2 in the appendix) is not enough. How did you select the hyperparameters for each method? Especially step-size and batch-size which are critical for the performance of most algorithms. 
b) I have to admit that I am not extremely familiar with common experimental evaluations used for derivative-free methods but the datasets used in the paper seem to be rather small. Can you please justify the choice of these datasets, perhaps citing other recent papers that use similar datasets?

8) Connection to existing solutions
The text is quite unclear but the authors seem to claim they establish a rigorous connection between their approach and particle swarm (“In terms of contribution, our research made as yet an rigorous analysis for Particle Swarm”). This however is not **rigorously** established and needs further explanation. The reference cited in the text (Kennedy 2011) does not appear to make any connection between particle swarm and accelerated gradient descent. Please elaborate.

9) SGD results
Why are the results for SGD only reported in Table 1 and not in the figure? Some results for SGD are better than for P-SHE2 so why are you bolding the numbers for P-SHE2?
It also seem surprising that SGD would achieve better results than the accelerated SGD method. What are the possible explanations?

10) Minor comments
- Corollaries 1 and 2 should probably be named as theorems. They are not derived from any other theorem in the paper. They are also not Corollaries in Su et al. 2014.
- Corollary 2 uses both X and Z.
- Equation 5, the last equation with \dot{V}(t): there is a dot missing on top of the first X(t)
“SHE2 should enjoy the same convergence rate Ω(1/T) without addressing any further assumptions” => What do you mean by “should”?
- There are **many** typos in the text!! e.g. “the the”, “is to used”, “convergeable”,... please have someone else proofread your submission.
","The sentiment score is -30 because while the reviewer acknowledges the paper as 'interesting', they express significant dissatisfaction with the current state of the paper, particularly regarding the lack of proper discussion on complexity. The review lists numerous critiques and areas for improvement, indicating an overall negative sentiment. However, it's not extremely negative as the reviewer sees potential in the idea. The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I think' and 'Can you please' which are polite ways of offering criticism. The reviewer also provides helpful suggestions and references to improve the paper. However, the score isn't higher because the critique is quite direct and extensive, without much softening language or praise.",-30.0,50.0
"The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks. The probabilities are approximated with variational autoencoders. During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.

This paper focuses on the second part, with a different model. Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification. This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.  This pre-trained model is then incorporated into the neural net for MNIST classification.  The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model. This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)

The authors compare to the work of Schott for one type of attack. It would be nice to see more detailed experiments as done in Schott.

Questions:
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.
4- Could you please add the found J_h's to the appendix. This architecture reminds me of the good old MRFs for image denoising. Could it be that what we are seeing is the attack being denoised?

I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott. 

Thanks in advance. I will re-adjust the review rating following your reply.



","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the work and its comparison to previous research, they express puzzlement and request several clarifications and additional experiments. The phrase 'I don't yet understand' indicates some skepticism about the approach. However, the tone is not overtly critical, hence the score is only mildly negative. The politeness score is positive (60) as the reviewer uses respectful language throughout, asks questions politely ('Could you please...', 'Why not...'), and expresses gratitude ('Thanks in advance'). The reviewer also indicates willingness to reconsider their rating based on the authors' response, which is a courteous approach. The language is professional and constructive, avoiding any rudeness or harsh criticism.",-20.0,60.0
"The main difficulty of neuroevolution---requiring a huge number of simulations for high dimensional problem---is addressed in this paper by introducing VAE to reduce the state space dimensionality and using a rather shallow controller network. This idea itself is very promising, however, it has been introduced in (Ha and Schmidhuber, 2018).  Still, there seems to be differences in how to gather histories and how to use them. Nevertheless, the differences are not well described in the text. The effect of the modification is not evaluated on experiments.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the promise of the idea, they point out that it's not entirely novel and that the paper lacks sufficient description of differences from previous work and experimental evaluation. The politeness score is moderately positive (50) as the reviewer uses neutral language and acknowledges the potential of the idea, but doesn't use overtly polite phrases. They critique the work constructively without harsh language.",-20.0,50.0
"The paper introduces a novel architecture for low-resource named entity tagging: a dual adversarial transfer network, in which fusion between high- and low-resource, or high- and low-noise data is achieved via also novel resource-adversarial discriminator.

The model is interesting, novel, clearly exposed in sufficient detail, and warrants publication as such. The idea to unify representation differences and data imbalance under one model is noteworthy.

I find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.

This unfair account of related work would not trouble me as much if it weren't coupled with an experiment in ""low-resource"" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.

This experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is poor scholarship in my submission, regardless of the clever model.

I do let the clever model upvote my review, but not beyond borderline.

Minor:
- the use of ""lingual"" as noun is rather off-putting, at least to me","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's novelty and interesting aspects, they express significant concerns about the experimental setup and representation of related work. The reviewer states the paper 'warrants publication' but also calls it 'poor scholarship' in some respects, leading to a 'borderline' overall assessment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positives before presenting criticisms, and uses phrases like 'I find that' and 'This troubles me' rather than making blunt accusations. However, some criticisms are quite direct, preventing a higher politeness score.",-20.0,50.0
"The paper investigates different machine learning approaches to model and
predict the return on property investments, in particular with respect to eager
and lazy learning techniques. The authors evaluate those different techniques on
a dataset of properties in Virginia. They conclude that lazy techniques provide
better performance than eager ones.

The paper is a purely empirical study that does not introduce any novel
machine learning or evaluation techniques. The authors use the off-the-shelf
WEKA toolbox. The results are not clear, given that only a single data set was
used to evaluate the different approaches, and general recommendations cannot be
made.

The paper is not well written and the descriptions do not convey what the
authors have done very well. An example of this is Figure 3, which purports to
show the average rent (or rent distribution?) for different housing types. There
are multiple categories in there that are not valid housing types (""Make Me
Move"") and the rents shown are incorrect (e.g. more than a million for a town
house). It is also unclear why the average rent for a single family house is
approximately 4 times as much as for a town house.

The problems with Figure 3 are exemplary of the paper; the other issues are too
numerous to list.

In summary, this paper should be rejected.","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper should be rejected, criticizes the methodology, writing quality, and results presentation. They mention numerous issues and provide no positive feedback. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and dismissive. They use phrases like 'purely empirical study that does not introduce any novel techniques' and 'the problems... are too numerous to list,' which come across as somewhat impolite. The reviewer doesn't offer constructive feedback or soften their criticisms, leading to a slightly negative politeness score.",-80.0,-20.0
"This paper describes a meta-RL algorithm through imitation on RL policies. While the paper builds nicely up to the core part, I find essential details missing about the imitation setup. By glancing at previous BC papers (some of which are cited), the quantity for supervised imitations, etc., were clearly defined. 

It will be useful for this reviewer if the authors can provide more clarity in explaining the BC task involved in their algorithm.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper 'builds nicely up to the core part', they also express concerns about 'essential details missing' and request more clarity. This indicates a mixed but slightly critical view. The politeness score is moderately positive (50) as the reviewer uses respectful language, framing their critique as a request for clarification ('It will be useful if the authors can provide more clarity') rather than a direct criticism. The tone is professional and constructive, avoiding harsh or rude language.",-20.0,50.0
"This paper proposes to learn a manifold of deep generative models using a pre-trained VAE. To generate samples with desired features, this paper proposes to learn an embedding of each feature in the hidden space using VAE. Then the learned hidden space is used to train a GAN.

However, the method in this paper and main contributions are not clearly represented. I can hardly understand the motivation of this paper. In the introduction part, this paper mentions “large scale of latent space” lots of times, but does not make it clear that why a large latent space hinders the deep generative models. In Fig.1, it demonstrates that for some manifold, L2 distance cannot be applied directly. However, for most DGMs, the hidden space is defined in Euclid Space, and L2 distance is a valid distance for them. 

In Sec. 3, the method is not presented clearly and the notation is confusing. In Sec. 3.2.1, Eqn (8) is not an objective function and it is confusing how to optimize the generator using it. In Sec. 3.2.2, the notation is really confusing and I can hardly understand the proof the Theorem 2.

The experimental results are not solid where no well-known metrics, such as Inception Score, FID, are used to evaluate the generated samples. For compression rate, the size of bottleneck has not been mentioned above, and the experimental setting of each baseline is not ignored which makes the experimental results incomparable.

Overall, this paper is not a qualified paper for ICLR.
","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that they 'can hardly understand the motivation', finds the method and contributions 'not clearly represented', and concludes that 'this paper is not a qualified paper for ICLR'. There are no positive comments to balance these criticisms. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite blunt and dismissive. Phrases like 'I can hardly understand' and 'the notation is really confusing' could be phrased more diplomatically. However, the reviewer does maintain some level of professional language, preventing the score from being lower.",-80.0,-20.0
"The authors propose a method to generate predictions under fairness constraints. The main idea is to take linear fairness constraints, and replace them with weak squared penalties plugged into the objectives, which supposed to help in cases where the loss function is not convex. The penalty coefficients are chosen by cross-validation, and the effectiveness of this approach is demonstrated empirically.

In Sec. 3.1, the authors point out several shortcomings of using linear penalties (using Lagrange multipliers) for non-convex losses. These seem valid. Sec. 3.2, however, is not clear on why exactly replacing the linear penalties with quadratic penalties solves these issues. I'm hoping the authors can clarify the following points:

1) The authors note that, for quadratic penalties, \lambda->0 means no constraints, and \lambda->\infty means hard constraints. Isn't this also true for linear constraints?

2a) Why do linear penalties have unique \lambda_k for each constraint k, but the quadratic objective has only a single \lambda for all constraints?

2b) Why can CV over \lambda be used for quadratic constraints - what is the justification? And, more importantly, why *can't* it be used with linear constraints? If it can, then this should be one of the baselines compared to in the experiments.

3) What is the criterion optimized for by CV - accuracy or the constraints? Different parts of the paper give different answers to this question. For example, ""... may be easily determined via standard hyperparameter optimization methods"" vs. ""tuning \lambda to directly satisfy the desired fairness metrics"". Or even more unclear - ""choose \lambda ... so that the final solution gives the desired fairness-accuracy trade-off"". How is the desired trade-off defined?

4) If there is a trade-off between fairness and accuracy, and no clear-cut criterion for evaluation is pre-defined, then the evaluation procedure should compare methods across this trade-off (similarly to precision-recall analysis).

5) The authors differentiate between cases where the loss is either convex or non-convex. This is confusing - most losses are convex, and non-linearity appears when they are composed with non-linear predictors. Is this the case here? If so, the fairness constraints are no longer linear, and they're quadratic counterpart is no longer quadratic. It would be helpful if the authors specify where the non-linearity comes from, and what they assume about the loss and predictors.

6) Why is it important to show that the quadratic constraints can be written as an expectation? Isn't the square of an expectation always an expectation of pairs? How does the double summation/integral effect runtime?

7) It would be helpful if the authors differentiate between loss/constraints over the entire distribution vs. over a given sample set.


","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the authors' proposed method and its potential benefits, they express several concerns and request numerous clarifications. This indicates that the reviewer is not fully convinced by the paper in its current form. The review begins with a neutral summary but quickly moves into a series of critical questions and points that need addressing.

The politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I'm hoping the authors can clarify' and frame their criticisms as questions or requests for clarification rather than direct criticisms. The reviewer also acknowledges the valid points made by the authors. However, the score is not higher because the review is quite direct in pointing out areas of confusion or potential weaknesses, without much softening language or praise.",-20.0,60.0
"This paper introduces two methods of adjusting the overconfidence error for predictions on novel data. The ensemble distillation approach is to penalize the distillation loss on a potentially unlabeled general dataset.  The second approach (NCR) detects the novelty first and reweigh the prediction based on the familiarity to training data. 

*stationarity*
From the statistical perspective, the overconfidence of extrapolation can kick in from two sources: a)the epistemic uncertainty.  The point estimation of softmax ignores the uncertainty of prediction at all.  A full Bayesian approach will remedy this though computationally impractical.  b) the generative distribution p(y|x) might not be identical on training data and test data.  To see the difference, if the training sample size goes to infinity, the uncertainty in a) will go to zero, but b) may still exist.  Section 3 assumes the invariant p(y|x) in novel data.  But theoretically, both methods do not require such invariance?

Slightly related here, there can be novel data for classification, and in principle, there can also be novel data for novelty detection?  That will make NCR fail.

*why the distillation helps uncertainty adjustment*
I am not convinced how the g-distillation works for this task.  In the extreme case if the ensemble model itself is totally wrong for novel data and the unlabeled general data used in training, how can I learn any extra uncertainty information from that noise? To be fair, when the temperature goes high enough, the ensemble will make uniform prediction and then the distillation loss is merely a loss function that enforces uniformity.  If I replace the ensemble softmax by a uniform prior for unlabeled general data, do I achieve the same effect?  That is essentially the same regularization as method 2, except g-distillation is on logit scale.  

*robustness-accuracy tradeoff*
The experiments do not reveal too much robustness-efficiency conflict, as the new methods still perform good enough on familiar dataset. Indeed they can be even better than the baseline in E99 loss. Does it suggest the over-confidence is even a concern for familiar data/ iid data?

In general, the paper is well-written and well-motivated. It would be more interesting to make some theoretical explanation why/when this simple approach works.   I would recommend a weak accept at this point.
","The sentiment score is slightly positive (20) because while the reviewer points out some concerns and areas for improvement, they ultimately recommend a 'weak accept' and acknowledge that the paper is 'well-written and well-motivated'. The overall tone is constructive rather than critical. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions rather than direct attacks, and concludes with positive remarks. The reviewer maintains a professional and courteous tone, even when expressing doubts or requesting clarifications.",20.0,60.0
"The authors proposed a generalized adaptive moment estimation method(Game). Compared to the existing methods AMSGrad and PAdam, the new method Game tracks only two parameters in iteration and hence saves memory. Besides, they introduced a additional tuning parameter $q$ to track the q-th moment of the gradient and allow more flexibility. The authors also provided the theoretical convergence analysis of Game for convex optimization and smooth nonconvex optimization. Their experiment shows Game may produce better performance than AMSGrad and PAdam with a little bit sacrifice of convergence speed. Game is a promising alternative method for training large-scale neural network.","The sentiment score is 80 (positive) because the reviewer expresses a favorable view of the proposed method (Game). They highlight its advantages over existing methods, such as memory efficiency and flexibility. The reviewer also notes that Game shows better performance in experiments and describes it as a 'promising alternative method'. The politeness score is 50 (somewhat polite) because the language used is professional and respectful, without any negative or critical remarks. However, it doesn't contain explicitly polite phrases or compliments, maintaining a neutral, objective tone typical of scientific reviews.",80.0,50.0
"The papers studies estimators of gradients taken from expectations with respect to the distribution parameters. The paper has studied two main types of estimators, Finite Difference and Continuous Relaxation. The paper made several improvements to existing estimators. 

My rating of the paper in different aspects (quality 6, clarity 8, originality 6, significance 4). 

Pros: 
1. The paper has made a nice introduction of FD and CR estimators. The improvements over previous estimators are concrete -- it is generally clear to see the benefit of these improvements. 

2. The first method reduces the running time of the RAM estimator. The second method (IGM) reduces the bias of GM estimator. The first improvement avoids many function evaluations when the probability is extreme. The second improvement helps to correct bias introduced by continuous approximation of \zeta_i itself. 

Cons: 
1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper. 

2. All these improvements are not very significant considering a few previous papers on this topic. Some arguments are not rigorous. (see details below)

3. A few important papers are not well discussed and omitted from the experiment section. 

Detailed comments

1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.  

2. In the equation 7 and above: what does 4 mean? When beta \neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)

3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]? 

4. In the PWL development, the paper argues that we can choose alpha_i \approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.

","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), there are more 'Cons' listed, and the overall tone suggests room for improvement. The reviewer mentions that the improvements are 'not very significant' and some arguments are 'not rigorous'. The politeness score is moderately positive (60) as the reviewer maintains a professional tone throughout, using neutral language like 'The paper has studied...' and 'My understanding is...'. They also balance criticism with positive feedback, which is a polite approach. The reviewer offers specific, constructive feedback without using harsh or dismissive language, contributing to the politeness of the review.",-20.0,60.0
"Given an MDP <S, A, T, R>, the paper suggests to learn both the optimal Q function of that MDP (denoted Q^+), but also that of the MDP <S, A, T, -R> (denoted Q^-). The basic idea is that min_a Q^-(s, a) could be a good action for the initial MDP. Based on this idea, the authors propose to combine Q^+ and Q^- with a linear combination in order to obtain what they call a hybrid policy. 

The proposed idea is indeed interesting and I find the experimental results surprising. It is not clear to me why the policy obtained from Q^- does better than Q^+. Theoretically, this should not happen: if we have the exact optimal Q function, Q^-, for <S, A, T, -R>, the policy defined by argmin_a Q^-(s, a) in every state s may be suboptimal in <S, A, T, R>. Is there a good conjecture/explanation for why the policy induced by Q^- works so well in 2(a) and (b)?

The authors chose to report the results using off-line training, which seem to favor their proposition. What are the results for on-line training?

In the experimental part, I think the authors should also report the results of the method that consists in learning two Q^+ and combining them with an average. This baseline would help understand if the good performance of hybrid policies really comes from learning Q^-.

Obtaining hybrid policies faces one important issue, which is the need to perform two actions in the environment in a given state, one for Q^+ and the other for Q^-. Therefore, the proposition seems to be doable only when one has access to a simulator.

The writing is generally clear, but the paper should be checked for typos.","The sentiment score is slightly positive (20) because the reviewer finds the idea interesting and the experimental results surprising. They acknowledge the potential of the proposed method but also raise several questions and concerns. The reviewer suggests additional experiments and baselines, which indicates a constructive approach rather than outright criticism. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, phrases criticisms as questions or suggestions, and acknowledges the interesting aspects of the work. They use phrases like 'I find' and 'I think' to soften their statements, and there's no harsh or dismissive language. The review maintains a professional and constructive tone throughout.",20.0,60.0
" This paper proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN). And then the landscape state is utilized to training the neural network by using the standard quantum amplitude amplification method.   

Although this idea is interesting, I trend to reject this submission as I think its presentation is unclear and the technical detail is a little difficult to follow. So, the correctness and soundness of this work is difficult to verify. I urge the authors to revise their draft to provide more and clearer technical details.

Detailed comments and questions:

Could the authors further point out that what the scope of the binary features are, {0, 1} or {-1, +1}? To my understanding, it should be {-1, +1}, or the corresponding variables are always +1. In addition, the construction of the “multiplying values by binary weights” module implies that the value should take -1 or +1, rather than 0 or 1. However, at the bottom of page 5, the authors claim that the binary values take +1 and 0. Could the authors clearly explain the term “parameter” and “value”?
 
Could the author further explain how to construct the majority activation function?
In the part of “calculating accuracy”, the authors mention that “running the QBNN with the weights in superposition for each point in the training set separately” and “there are N qubits containing the prediction of the QBNN”. To my understanding, there are 3 qubits representing the 8 weights, several qubits representing the input values, and N qubits representing the predictions. But how to construct the final landscape state to be optimized with these qubits?
 
Could the author explain intuitively the main idea of the amplitude amplification method? Specifically, what is the relation between the qubits representing parameters and the qubits presenting prediction results?
 
During the amplitude amplification process, the probabilities change periodically. How to select the best number of steps k in advanced if we do not known the best parameter? Or how to judge if the training is success?
 
Overall speaking, I think this paper is interesting. However, the presentation is unclear and I suggest the authors to revise their draft by providing more technical details.","The sentiment score is -30 because while the reviewer finds the idea interesting, they 'trend to reject this submission' due to unclear presentation and difficulty in verifying the work's correctness. This indicates a generally negative sentiment, but not extremely so due to the acknowledgment of the interesting concept. The politeness score is 60 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. They use phrases like 'Could the authors further explain...' and 'I suggest the authors to revise...', which are polite ways of requesting clarification and improvements. The reviewer also acknowledges the interesting aspects of the paper, which adds to the politeness. However, it's not extremely high on the politeness scale as it's a professional review maintaining some critical distance.",-30.0,60.0
"This paper proposes a novel parameter-efficient generative modeling approach that is based on the Monge-Ampere equation. In the proposal, a feed-forward neural network is trained as an ODE integrator which solves (2) and (3) for a fixed time interval $[0,T]$, so that the distribution $p(x,t)$ at time 0 is a simple base distribution such as a Gaussian, and that at time $T$ mimics the target distribution.

[pros]
- The proposal provides a parameter-efficient approach to generative modeling, via parameter sharing in the depth direction.
- I think that the idea itself is quite interesting and that it is worth pursuing this direction further.

[cons]
- The Applications section is not convincing enough to demonstrate usefulness of the proposal as an approach to generative modeling.
- How the gradient-based learning in the proposal behaves is not discussed in this paper.

[quality]
How the gradient-based learning in the proposal behaves is not discussed. I understand that the non-convex nature of the loss function poses problems already in the conventional back-propagation learning of a multilayer neural network. On the other hand, in the proposal, the loss function (e.g., (4)) is further indirectly parameterized via $\varphi$. It would be nice if the parameterization of the loss in terms of $\varphi$ is regular in some sence.

[clarity]
Description of this paper is basically clear. In the author-date citation style employed in this paper, both the author names and publication year are enclosed in parentheses, with exception being the author names incorporated in the text. This paper does not follow the above standard convention for citation and thus poses strong resistance to the reader. For example, in the first line of the Introduction section, ""Goodfellow et al. (2016)"" should read ""(Goodfellow et al., 2016)"".

[originality]
The idea of considering the Monge-Ampere equation in its linearized form to formulate generative modeling seems original.

[significance]
In the experiment described in Section 4.1, it is not clear at all from the description here whether the learned system is capable of successfully generating MNIST-like fake images, which would question the significance of the proposal as a framework for generative modeling. It is well known that the KL divergence $D(P\|Q)$ tends to put more penalty when $P$ is large and $Q$ is small than the opposite. One can then expect in this experiment that it tolerates the model, appearing as $Q$ in $D(P\|Q)$, to put weights on regions where the data are scarce, which might result in generation of low-quality fake images. It would be nice if the authors provide figures showing samples generated via mapping of Gaussian samples with the learned system.
Also, in the experiment described in Section 4.2, I do not see its significance. It is nice to observe in Figure 4 that the loss function approaches the true free energy as well as that the snapshots generated by the model seem more or less realistic. My main concern however is regarding what the potential utilities of the proposal are in elucidating statistical-physical properties of a system. For example, it would be nice if the proposal could estimate the phase-transition point more easily and/or more accurately compared with alternative conventional approaches, but there is no such comparison presented in this paper, making the significance of this paper obscure.

Minor points:

The reference entitled ""A proposal on machine learning via dynamical systems"" would be better cited not as ""E (2017)"" but rather as ""Weinan (2017)"".

Page 6, line 10: the likelihoods of these sample(s)

----Updated after author feedback----
Upon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.
I still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('quite interesting', 'worth pursuing'), there are significant criticisms and concerns raised. The reviewer points out that the applications section is not convincing, the gradient-based learning behavior is not discussed, and the significance of the experiments is questioned. The downgrade of the rating in the updated section further confirms the overall negative sentiment. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positives before presenting criticisms, and using phrases like 'It would be nice if' when suggesting improvements. The tone is professional and constructive, even when expressing concerns.",-20.0,60.0
"This paper proposes a new approach to sequential learning by introducing an adversarial memory unit for each new task and uses EWC as a regularizer for training other parts of the network on the new task.
The memory units are trained with Fast Gradient Sign Method to increase the loss, and they are connected to the next layer with weights trained to decrease the loss. 
It shows superior performance than EWC and the plain gradient descent baseline on disjoint MNIST/CIFAR10 and EMNIST. The authors also share their experience with EWC, which provides useful feedbacks to the community.

The proposed adversarial memory unit is novel to the best of my knowledge. However, its motivation is not quite intuitive to me, and the authors fail to provide persuading explanations. My major concern is whether it is better to take the adversarial direction rather than the direction that decrease the loss for the memory units.

To support their ideas, the authors mentioned the paper ""Adversarial Reprogramming of Neural Networks"" and said this paper's ""adversarial program"" is formed by choosing the ""intersection of adversarial subspaces"" as in their paper. However, they (Elsayed et al. 2018) are actually finding such adversarial programs in the direction of decreasing the loss, which is contrary to finding the ""intersection of adversarial subspaces"". 
The authors also want to support the pros of adversarial memory units by comparing against ""Gradient"" memory units that are trained to decrease the loss with the experiment shown in Figure 2. However, Figure 2a seems problematic to me, so I am not sure whether the authors are doing their experiments correctly. I think the experimental conditions for FGSD and Gradient are different, which makes the comparison meaningless. We can see that the network's accuracy with Adversarial memory unit on task 1 is a constant when the network is trained on task 2 and 3, because the network's weights (except memory units and their weights for task 2 and 3) and task 1's memory units are fixed, as described in the experimental setting for ""AD"". The accuracy on task 1 with Gradient memory units is changing when the network is trained on task 2 and 3, which means either the network's weights are changing or the memory unit is changing. 

As a result, I don't think this paper will be accepted until the authors provide further explanations and results to support the adversarial memory unit, or clarify my misunderstandings in the comments above.","The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (novel approach, superior performance, useful feedback), they express significant concerns about the paper's motivation, explanations, and experimental results. The reviewer concludes that the paper will not be accepted without major revisions, indicating an overall negative sentiment. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms as concerns or requests for clarification rather than direct attacks. They use phrases like 'to the best of my knowledge' and 'I am not sure whether' which maintain a polite tone while expressing doubts.",-50.0,50.0
"The paper presents two variational inference frameworks for generative models of knowledge graphs. Such models are based respectively on latent fact model and latent information model.
The authors argue that with the presented framework the underlying probabilistic semantics can be discovered. Experiments show performances comparable with state-of-art approaches.

Unfortunately, the paper seems to me not clear and rather incomplete in its actual form.
Overall, the proposal is novel. I cannot decide about significance because results do not outperform those of other approaches. To this extent, the authors should better discuss the results, explaining in more detail why this approach should be used instead of others (scales better, is faster, etc.).

In the abstract, it is asserted that one can discover underlying probabilistic semantics, but in the corpus of the paper this aspect is not described or mentioned in detail.
Similar problem for the reference to von-Mises distribution. This distribution is just named, it is said that the framework can handle such a distribution, but a reference to a paper and/or a short paragraph to explain the sentence are missing. This statement now results to be just information disconnected by the rest of the paper.

In a similar way, many other points suffer from a poor organization in my opinion.
When describing LIM an error is introduced here that is then copied and pasted throughout the paper: in the productory on p, p is in R not in E. This is a simple typo, but the fact that it is repeated so many times, also in the proof, gives me the feeling that the paper was written at the last moment.
Figures 1 and 2 are never referred.

Formula 6 must be better explained. If I have not lost something, n is the number of labeled triples, s_c is undefined, b_c is the probability of s_c to be equal to 1, the index i is never used. The paper lacks information here.

As regards the experimental part, some results are shown in subsection 4.3 called link prediction, others in section 5 called link prediction analysis. This organization does not seem to me to be really optimal. I would suggest creating an experimental section.
Moreover, the tests should be better explained, the tables are shown without specifying how they are built and how the values are collected. Information is provided in the appendix but could be included in the paper as the maximum limit is of 10 pages (8 suggested but I think an extra half page can be used).
The knowledge bases used should be at least cited, I know that freebase and wordnet are well-known but somewhere, in the description of the test, the name should be included. Also to specify the characteristics of the versions (WN18 vs WN18RR). Moreover, what does the value -257 in column 1, row 4 means?
Then, it is said that Table 1 shows improvements for ComplEx, but such improvements are rather low, is there a way to prove their significance? Otherwise, I would say that the performance is the same for WN18.
Tables 1 and 2 contain cells with '-' value, what does it mean?

Discussion about table 3 is incomplete in my opinion. First of all, the ""proportion"" column should be described. Also, on one hand, it is true that the _member_meronym is the least accurate and prominent but the most problem may come from _hypernym, which is the most prominent and the accuracy is also low. This fact is highlighted for table 4 but not for table 3.

Minor issues
- sec 3: references to Miao et al. must be enclosed by brackets
- sec 4.3: ""We believe this *is* due to ...""
- sec 5.2: what is Model A? Also, the sentence seems incomplete.

Pros:
- Novel approach

Cons
- Test results are not convincing
- The paper is not mathematically sound
- The paper needs to be re-organized","The sentiment score is -60 because the review is predominantly negative. The reviewer states the paper is 'not clear and rather incomplete', points out numerous issues with organization, clarity, and mathematical soundness, and concludes that the test results are not convincing. The only positive aspect mentioned is that the approach is novel. The politeness score is 20 because while the reviewer is critical, they use relatively polite language such as 'Unfortunately', 'I would suggest', and 'in my opinion'. They also acknowledge positive aspects and provide detailed feedback for improvement, which is constructive. However, some phrases like 'poor organization' and 'the paper lacks information' are more direct criticisms, preventing a higher politeness score.",-60.0,20.0
"This paper centers around efficient estimation of the kernel function for the Hawkes process and relaxation of the “linearity” assumption in the original Hawkes process. They rely on a classical sparse generalized linear model using the wavelet basis set and Hawkes loss function to estimate a shallow kernel function. This approach is opposite to the deep function estimation approach which does not rely on a predefined basis set [e.g. see [Du et al, 2016]]. However, it can have an advantage that the learned functions are interpretable, thought the authors never demonstrate it in the paper.

Given this view of WRNs (an unfortunate coincidence with WideResNets), we understand how LSTM2 outperforms LSTM1 in the results. However, the results tables do have peculiar numbers too. For example, why the Goodman-Kruskal gammas for H. Poisson are exactly -1? Why is it always pointing in the wrong direction? There are other observations in the results table that the authors have listed without much explanation. For example, in Section 5, what is the reason for “The WRN-PPL method excelled particularly in tasks with many target occurrences”?

Another example is the arguments in the discussion section about the use-case of rate functions. For example, the authors state: “ For example, the rate prediction for the individual denoted in green in Figure 5 (right) suggests that individual may have skipped, missed, or rescheduled 5 to 6 appointments over the last decade.” How did the authors conclude this claim? What is the clinical significance of missing or rescheduling 5-6 appointments in the context of A1c prediction?

Writing can be seriously improved (basically the paper is not ready in the current state). For example, only in Section 6, the authors have introduced the full name of WRN-PPL after using it many times before. 

The motivation for this paper is misleading. There have been several works on “Deep Cox” and “Deep Hawkes” models. I don’t see the novelty in the authors’ contribution in defining the clinical risk. Especially Fig. 3 (left) is already known and does not add much value.

Overall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models. Unfortunately, the authors do not clearly state how many training data points they have. They have a vague statement: “798,818 timestamped events in a study population of 4,732 individuals”, but it does not say exactly how many training examples they have.","The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper, such as peculiar numbers in results tables, unexplained observations, and misleading motivation. However, it's not entirely negative as it acknowledges some positive aspects, like the potential advantage of interpretable functions. The politeness score is 20 because while the reviewer is direct in their criticism, they use professional language and avoid personal attacks. They use phrases like 'can be seriously improved' rather than harsh language. The reviewer also balances criticism with some positive remarks, which contributes to a slightly polite tone overall.",-50.0,20.0
"This paper proposes an alternative search procedure for A* sampling that, in contrast to the original optimistic search, doesn't rely on (possibly difficult-to-find) bounds for the log-probability function.

The first major issue with this paper is clarity. The preliminary section describing the Gumbel process and A* sampling is very difficult to understand (despite my being quite familiar with A* sampling). The authors use undefined notation frequently throughout the introduction and refer to it in abstract terms. There are also numerous errors -- for example, when describing the ""bottom up"" approach to generating a Gumbel process, the authors suggest perturbing all points in the input space by independent Gumbel noise (which would result in Gp(S) = infty almost surely when omega is uncountable). 

The description of the main contributions in section 4 is equally unclear. This section starts by suggesting that sampling the next node to investigate according to the probability that it contains the maximum is reasonable, and then presents a lemma about regret in a bandit setting where the sampler never splits the subset. This lemma does not apply to the actual sampler proposed in the paper, so it is not clear why it is included. Section 4.2 is also very unclear -- I am not certain how both w and script Y are defined, nor why we need an ""unbiased estimator of Y"" (a random variable?) when we can simply sample from Y directly. As the definition of w is unclear, the purpose of 4.3 is unclear as well.

The other major issue is more fundamental -- I am not convinced the sampler is correct. The algorithm simply terminates after some finite horizon (rather than having a conclusive proof of termination via branch and bound as in the original A*). There is no proof or argument included in the paper regarding this. Any proposed sampling algorithm must be correct to be acceptable in ICLR.
","The sentiment score is -70 because the review is predominantly negative. The reviewer points out major issues with clarity, numerous errors, and fundamental problems with the proposed algorithm. They express strong doubts about the correctness of the sampler and state that the paper is unacceptable in its current form. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and direct. They use phrases like 'very difficult to understand', 'numerous errors', and 'I am not convinced', which, while not impolite, do not soften the criticism. The reviewer also doesn't offer any positive comments or encouragement, which contributes to the slightly negative politeness score.",-70.0,-20.0
"The underlying motivation for the paper is really interesting and cuts straight to the heart of Deep Learning and strives to unravel the key understanding that we are still to a large extent missing.

When it comes to clarity and organization I find the paper a bit ""messy"" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions. Especially given the very specific nature of the topic I miss a strong and clear path through the paper. Unfortunately the paper leaves me with the distinct feeling that there are still a lot of work needed to be able to tell the story about the problem under study. Having said that the paper does contain several individual findings. Having said that I find the ideas leading up to what the authors refers to as ""gradient starvation"" to be really interesting and that would be a great clear idea to focus on.

A few concrete questions/comments:
Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.

Given the very specific nature of the topic treated in the paper I find the title of the paper largely misleading. The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.

In Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?

The final sentence in Section 2 is highly speculative and I find this hard to believe without solid backing. The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.

Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.","The sentiment score is slightly negative (-30) because while the reviewer acknowledges the interesting motivation and some individual findings, they express concerns about the paper's clarity, organization, and the limited scope of the results compared to the broad claims. The reviewer uses phrases like 'a bit messy,' 'missing a strong and clear path,' and 'misleading title,' which contribute to the negative sentiment. However, they also mention 'really interesting' ideas, balancing the criticism somewhat. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, using phrases like 'I find,' 'Can you explain,' and 'I am missing' to express concerns politely. They also acknowledge positive aspects and provide constructive feedback, which adds to the politeness. The reviewer avoids harsh language or direct criticism, instead framing their comments as suggestions or questions.",-30.0,50.0
"The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The loss of the bandit is binary: zero if the validation loss decreases and 1 otherwise. In the experiments, the Exp3 bandit algorithm is run with Adam and Adagrad on MNIST, CIFAR-10, and CIFAR-100. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.

The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know.

The paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.

It is not clear how results scale with the number and choice of the grid values.

I would have liked to see a more thorough investigation of the impact of the bandit loss on the experiments. It is true that as far as the theory is concerned, any bounded loss is OK. But I practice I would expect that a graduated loss (e.g., signed percentage of change in validation loss), would be more informative.","The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty of the paper's approach and its potential benefits, stating that it 'allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size.' However, the reviewer also points out some limitations and areas for improvement, which prevents a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout and offers constructive criticism without harsh or dismissive comments. The reviewer suggests improvements and expresses personal preferences ('I would have liked to see...') in a respectful manner, maintaining a collegial tone throughout the review.",20.0,50.0
"This work proposes an approach for explicitly placing information in a subset of the latent variables. The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information. 

Experiments focus on learning global information. The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.

# Approach seems limited.
 - This approach seems very limited, as there must exist a known transformation that removes the desired information. Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)
 - Can this approach learn multiple factors as opposed to just two? 
 - What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)

# More ablations or experiments with comparable settings would be desirable.
 - What is the choice of beta in the beta-VAE training objective? Apart from 1.2, this isn't mentioned. My concern here is that beta might be affecting the result more than the proposed training algorithm. Can the proposed approach perform just as well without a modified objective? Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper. (e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)
 - Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?

# Related work.
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:
- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.
- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2]. I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder. Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?

[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. ""Transforming auto-encoders."" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.
[2] Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" Advances in neural information processing systems. 2015.

---- Update since rebuttal ----

I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters. I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement. More experiments based on different transformations that the authors have mentioned would make this a stronger contribution. The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation. ","The sentiment score is -30 because the review is generally critical, pointing out limitations and suggesting more experiments. However, it's not entirely negative as it acknowledges the work's potential and offers constructive feedback. The politeness score is 50 because the reviewer uses respectful language, offers suggestions rather than demands, and thanks the authors for their rebuttal. The tone is professional and constructive, even when critical. The reviewer uses phrases like 'Can the authors provide...', 'would be desirable', and 'I thank the authors for clarifying', which contribute to the polite tone. However, it's not extremely polite, as it maintains a direct and critical stance throughout.",-30.0,50.0
"The paper claims to propose a computationally efficient algorithm for training deep CNNs by making assumptions about the distribution of data. The authors argue that (i) they don't make very simplistic assumptions about the data generating distribution as some other papers do, and (ii) their algorithm resembles the actual methods that are used for training deep models and shows some surprising properties of SGD.

Throughout the paper, the authors make a number of assumptions which seem arbitrary at times; not much justifications are provided. The authors claim that their assumptions are not as simplistic as assuming e.g., the inputs are sampled from Gaussian distribution. Unfortunately this is highly unclear: while the ""assumptions"" themselves are complex, the combination of those assumptions may make the problem solution trivial. While proving a lower bound to address this issue may be hard, at least the authors should try to illuminate more why the solution is not trivial (e.g., why a linear classifier doesn't work, etc.)

Despite the claims, I find the proposed algorithm very far from the usual SGD-based training methods; this is not a problem per se but I don't think that the result illuminates on the effectiveness of SGD (as the authors suggest). The proposed algorithm is a greedy layer-wise method that in each level does a clustering and also trains a ""linear"" CNN with SGD. So the hardness of end-to-end training of a deep network does not show up. Furthermore, it is not clear for training a linear CNN the SGD is even needed.

I suggest that the authors name each of the assumptions and clearly say which ones are assumed for which result. Here are some of the assumptions that the authors talk about.

1_ The data is generated by the following recursive procedure: First a small ""high-level image"" is generated from a distribution, G_0. The ""pixels"" of this high-level image are supposed to encode semantic classes, e.g., sky or ground. In the next step, each of these high-level pixels are turned into a small (lower-level) image. Therefore, we will have a more refined image after the second step. (each semantic class (e.g., sky) has a corresponding distribution that generates the smaller lower-level image (e.g., uniform over 4 possible types of skies)). This procedure continues recursively until we have the final image.

2_ G_0 is ""linearly separable"".

3_ Semantic classes defined in the model are different enough from each other

4_ {F_c} corresponding to semantic classes are linearly independent 

5_ Patch Orthonormality (apparently not assumed everywhere) 


it appears that if one assumes all of 1-5, then the problem becomes trivial (linearly separable). The authors then say that we don't want to make assumption 5 for this reason; still, the problem solution may be trivial (authors should at least intuitively justify why it isn't )

Here are some more uses of the word ""assumption"".

6_ ""For simplicity of analysis, we assume only the first layer of the network is trained"".

7_ ""We assume the algorithm [KMEANS++] returns a mapping [...] such that [...]"" 

The experiments do not seem conclusive. Only a few experiments have been done. I think the acquired results for CIFAR-10 are below the usual ones using CNNs, and the effects of various hyper-parameters may have interfered.

--
After reading the authors' response, I still think the way that the contributions are depicted (e.g., a justifying the effectiveness of SGD) are inaccurate/unsupported. 

Furthermore, although the authors' suggest that they have tested a linear classifier and observed that the data is not linearly separable, more explanations/intuitions are needed about the assumptions that are made throughout the paper.","The sentiment score is -60 because the review is predominantly critical. The reviewer expresses skepticism about the paper's claims, points out several issues with the methodology and assumptions, and states that the experiments are not conclusive. The reviewer also mentions that the results are below usual standards. However, it's not entirely negative as the reviewer acknowledges some potential in the approach, hence not scoring at the extreme negative end.

The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'I suggest' and 'I find' rather than making blunt statements. The reviewer also provides specific recommendations for improvement, which is constructive. However, the score is only slightly positive because the review doesn't go out of its way to be particularly polite or encouraging, maintaining a mostly neutral, academic tone.",-60.0,20.0
"
UPDATE:

I have read the authors’ response and the other reviews.  While the authors have made some improvements, my core criticism remains – the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.

The authors also seemed to have missed the point of the Kearns & Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.  


Summary:

The paper attempts to link the known mismatch between infinite horizon MDP values and finite trajectory sums to the problem of exploration.  Trajectories in environments requiring exploration (mountain car and a number-line walk) are shown and the effects of changing trajectory lengths and initial values are discussed.  Potential solutions to the problem are proposed though the authors did not deem any of the solutions satisfactory.


Review:

The paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.  Specifically, several issues seem to be studied at once here (including finite-horizon MDPs, function approximation, and exploration), relevant work from the exploration and RL community is not cited, the early experiments do not reach a formal theoretical claim, and the proposed solutions do not appear to adequately address the problem).  These issues are detailed below.

First, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories.  While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior.  While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.

The paper also fails to cite much of the relevant work on these topics.  For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with  epsilon derived from the discount factor (see “Near-Optimal Reinforcement Learning in Polynomial Time”).  In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehl’s thesis “Probably Approximate Correct Exploration in Reinforcement Learning” ).  Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.

In addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point.  No theorem or proof is given that compactly describes which exact problem the authors have uncovered.  Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.

Finally, several of the references are only ArXiv pre-prints.  Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers).
","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper 'fails to tackle' the issues 'in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.' They also mention multiple shortcomings such as not citing relevant work, lack of formal theoretical claims, and inadequate proposed solutions. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'The paper brings up a number of important issues' before critiquing, and provide detailed explanations for their criticisms rather than making blunt negative statements. However, the language is not overtly polite or complimentary, hence the relatively low positive score.",-70.0,20.0
"Authors present a new LSTM architecture, i.e. reduced gate convolutional LSTM. Authors use only one trainable gate, i.e. the forget gate which leads to less trainable parameters. They demonstrate the superiority of ti in two datasets, the moving MNIST and KITTI. Their results show that their architecture performs better than others in more accurately predicting next-frame. The paper is clearly written and the evaluation is based on other proposed similar architectures. For the moving MNIST they compared their model against the vanilla convLSTM with three gates and no peephole connections, based on previous work which has shown that it exceeds the accuracy performance of other LSTM based approaches. 
The results show that the training time is reduced and the standard error alike. The only limitations is that more databases could have been used to demonstrate the enhancement in performance.","The sentiment score is 80 (positive) because the reviewer praises the paper's clarity, the authors' demonstration of their architecture's superiority, and the improved performance in terms of training time and standard error. The only criticism is minor, suggesting more databases could have been used. The politeness score is 50 (slightly polite) as the reviewer uses neutral, professional language without overly formal or polite phrases, but also avoids any rudeness or harsh criticism. The review focuses on the paper's merits and provides a balanced, constructive assessment.",80.0,50.0
"The authors propose to study what they call weak contraction map. The idea may have its merits, but in the present form, it is not acceptable.

Notably, the key definition of the paper, that is that of the weak contraction mapping (starting with ""Then a mapping T : X → X is called weak"") is incomplete, because it uses a \mathcal{R}, which is never defined. This makes it hard to evaluate any of the results. 

Further, there is no clear separation between text and theorems. Worse, the theorems are not self-contained. E.g. what could perhaps be Theorem 2 (Starting with ""x* is a fixed-point of T in X0."") does not define x*.

While I cannot be certain because of the reasons stated above, the authors seem to have had in mind something like the Kakutani theorem:
https://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem
which they don't cite. Their assumptions on the map are weaker than that of Kakutani (upper hemicontinuity), which makes me a bit doubtful as to whether the statements could be proven, even if made precise. ","The sentiment score is -70 because the review is predominantly negative. The reviewer states that the paper is 'not acceptable' in its current form and points out several significant issues, including incomplete definitions, lack of clarity, and potential lack of originality. The politeness score is -20 because while the reviewer isn't overtly rude, the language is quite direct and critical without much attempt to soften the criticism. Phrases like 'not acceptable' and 'Worse, the theorems are not self-contained' come across as somewhat harsh. The reviewer does acknowledge that the idea 'may have its merits', which prevents the scores from being even lower. The reasoning behind the scores is based on the overall negative tone, the direct criticism, and the lack of positive or encouraging comments.",-70.0,-20.0
"Strength: 

The proposed approach is architecture-independent: the attack is constructed only from the dataset.

Weaknesses: 

Paper is not sufficiently well written for a venue like ICLR. 
Attack has very low success rate.
To the exception of Figures 4 and 5, many experiments are conducted on MNIST.

Feedback: 

Experimental results show that the attack is able to degrade a classifier’s performance by inserting perturbations that are computed on the data only. However, there are no baselines included to compare adversarial evasion rates achieved here to prior efforts. This makes it difficult to justify the fairly low success rate. In your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?

Furthermore, strong conclusions are made from the results despite the lack of supporting evidence. For instance, on P10, the attack is said to “also explains why adversarial examples can be universal.”. However, not only does the attack achieve less success than universal adversarial examples would (so it cannot explain all of them) but also does it not share any characteristics with the way universal adversarial examples are crafted. Drawing such a strong conclusion thus most likely needs a lot more supporting evidence. 

Several directions would improve the content of the paper: 

* Complete existing experimental results by being more systematic. For instance, in Section 3.1, measurements are only performed on one pair of MNIST images. Without studying a significant portion of the test set of two datasets, it is very difficult to draw any conclusions from the limited evidence.
* Perform a human study to have all perturbed images labeled again. Indeed, because of the ways images are perturbed here, it is unclear how much perturbation can be added without changing the label of the resulting input. 
* Study how existing adversarial example techniques modify internal representations. This would help support conclusions made (e.g., about universal perturbations---see above). 
* Rewrite the related work section to scope it better: for instance, Sabour et al. in Adversarial Manipulation of Deep Representations and Wicker et al. in Feature-Guided Black-Box Safety Testing of Deep Neural Networks explore adversaries operating in the feature space. This will also help build better baselines for the evaluation.

Additional details: 

TLDR: typo in the first word
P1: The following definition of adversarial examples is a bit restrictive, because they are not necessarily limited to vision applications (e.g., they could be found for text or speech as well). “Adversarial examples are modified samples that preserve original image structures” 
P1: The following statement is a bit vague (what is obvious impact referring to?): “Experiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks.”
P1: References do not typeset properly (the parentheses are missing: perhaps, the \citep{} command was not used?)
P2: What is the motivation for including references to prior work in the realm of image segmentation and more generally-speaking multi-camera settings in the related work section? 
P2: Typo in “ linear vibration”
P2: It remains difficult to make a conclusion about humans being robust to the perturbations introduced by adversarial examples. For instance, Elsayed et al. at NIPS 2018 found that time-constrained humans were also misled by adversarial examples crafted to evade ML classifiers: see Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.
P2: Prior work suggests that the following conclusion is not entirely true: “Most of these kinds of examples are generated by carefully designed algorithms and procedures. This complexity to some extent shows that adversarial examples may only occupy a small percentage for total image space we can imagine with pixel representations.” For instance, Tramer et al. in ICLR 2018 found that adversarial subspaces were often large: “Ensemble Adversarial Training: Attacks and Defenses”.
P2: Others have looked at internal representations of adversarial examples so the following statement would be best softened: “To the best of our knowledge, this paper should be the first one that discusses adversarial examples from the internal knowledge representation point of view.”. See for instance, Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning by Papernot and McDaniel.
P3: Could you add pointers to support the description of human abstraction and sparsity? It reads a bit speculative as is, and adding some pointers would help relate the arguments made to relevant pointers for readers that are less familiar with this topic. 
P3: What is the motivation for including the discussion of computations performed by a neural network layer-by-layer in Section 2?
P4: Given that saliency maps can be manipulated easily and are only applicable locally, it appears that Figure 1 is too limited to serve as sufficient evidence for the following conclusion: “This, in some way, proves the point that the knowledge storage and representation of current neural networks are not exactly sparse prototype based.”
P5: The error rate reported on MNIST is quite low (45%). Even using the Fast Gradient Method, one should be able to have the error rate be as high as 90% on a standard CNN.
P7: Would you be able to provide references to backup the following statement? “This is a network structure that is very common.”
P10: How does the discussion in Section 4.2 relate to the attack described in the submission?
","I assigned a sentiment score of -50 because the review is predominantly critical, pointing out several weaknesses such as low success rate of the attack, insufficient writing quality, and lack of baselines. However, it's not entirely negative as it acknowledges some strengths and provides constructive feedback for improvement. The politeness score of 50 reflects the reviewer's professional and respectful tone throughout. They use phrases like 'could you clarify' and 'would improve' rather than making harsh demands. The reviewer also balances criticism with positive remarks and offers specific suggestions for improvement, which is a polite approach to peer review.",-50.0,50.0
"Summary:
This paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as “ultrafine-grained” class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of “class embedding” for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.

Comments:
Arguably, the primary contribution of this paper is the introduction of a new “ultrafine-grained” classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it’s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It’s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.

The model introduced is interesting, but its novelty is limited. It’s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an “embedding” is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.

Other Comments:
-Since you ran a cross-validation, you should add confidence intervals to your reported numbers
-One additional dataset detail I was hoping to see that you didn’t provide is the mean/standard deviation of the number of instances per class,
-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well
-The title of your paper is somewhat misleading - it’s hard to argue that the form of class embedding you use is a “deep” class embedding since it’s just a matrix of parameters that are learned during training.

Overall, I’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.

[1] Ma, Xuezhe, and Eduard Hovy. ""End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.""


REVISION:
The other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (interesting dataset, outperforming baselines), they express significant reservations about the novelty and broader impact of the work. The review concludes by stating the paper is not ready for acceptance, indicating an overall negative sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive feedback, and frames criticisms as suggestions (e.g. 'I would have liked to see...'). The reviewer also acknowledges the interesting aspects of the work alongside the criticisms. However, the tone remains professional rather than overtly friendly, hence the score is not higher.",-20.0,50.0
"# Summary of the paper

This paper proposes an embedding of directed graphs based on the SVD of a normalized adjacency matrix. This embedding is shown to be equivalent to the spectral embedding of a co-citation graph, which is more complex to calculate. Interestingly, the proposed approach does not require the *explicit* representation of this graph. Moreover, the paper also shows that distances of the embedded vectors are proportional to mean commute times of a forward--backward random walk in the original graph. A suite of experiments is run on graphs from KONECT.

# Review

This is a well-written paper, which I enjoyed reading. The extension of embeddings to the case of directed graphs is significant and warrants a detailed exploration.

The principal issues I see with this paper are as follows:

- The originality or scope of the contribution is not clear
- The experimental section is uncompelling
- Several relevant works appear to have been ignored

Overall, I like the way the paper treats the subject. In particular, I appreciate the fact that proofs are explained well; additionally, code is provided, which will increase reproducibility. This is uncommon and praiseworthy!

As for the originality of the paper, I find it hard to judge the scope of the contribution. The paper is extremely well written and employs a very pedagogical treatment of the subject, which I appreciate. Yet, it is hard for me to judge the utility and novelty of the proposed method in light of Section 8, where the paper shows that a spectral embedding of the undirected variant of the graph leads to essentially the *same* eigenvectors (up to renormalization and permutations). To prove that a new method is more effective, this point should be emphasized more:

1. In a sense, I would see the results from Section 8 as the equal to what 'Laplacian Eigenmaps' (LE) yields. This needs to be stressed, and analysed in an experimental section.

2. I understand that the order of the singular vectors is different, so embeddings that use only parts of them will be different. However, a convincing experiment should assess the differences. For example, in which regime for $d$ (number of used vectors) will the new method be surpassed by the old one? Is there such a regime? Ideally, this will be answered in the form of an asymptotic theorem; it could also be a larger experiment, though (to simulate the conditions in practice).

3. I understand that the new approach has a lower run-time, because the SVD is more efficient than eigendecomposition. However, what about a simple baseline algorithm that uses SVD for the *undirected* graph of the input data? This should be simple to accomplish, and would be a way to ascertain the benefit of using edge directions.

  To my understanding, LE should be this embedding, but from the table, I can see that its runtime is a lot worse than the novel method. What causes this? The fact that eigendecomposition is used instead of SVD?  If so, an additional SVD-based approach should be implemented.

This brings me to the experimental section. Here, the paper demonstrates the superiority of the new embedding based on evaluating modularity of a set of different clusterings of larger graphs, obtained using $k$-means. I have several concerns about this:

1. Modularity has problems with larger networks because only a small part of the network will be used in its configuration.

2. Since the embeddings cannot be easily compared due to missing ground truth information, other metrics should be employed. Here are a few, which are often used  by the community. See 'Is there a best quality metric for graph clusters?' by Almeida et al. for more details and a description of their shortcomings:
    - Silhouette coefficient
    - Coverage
    - Conductance

  Different ones should be evaluated here in order to show the behaviour of the new embedding. Do the embeddings differ if the modularities are similar?

3. How do the results change for different values of $d$? I find it hard to disentangle such a discussion from instabilities in $k$-means, but to my understanding of the method, tuning $d$ means that more or less information is used from the singular vectors.

   This could also be quantified in a proof (about asymptotic behaviour) but an experiment would be equally fine.

Concerning the bibliography, or the treatment of prior works, there are some issues:

- There appear to be some missing references of earlier works that used SVD or variants in order to cluster graphs or embed them:

  - Drineas et al.: 'Clustering Large Graphs via the Singular Value Decomposition'
  - Malliaros and Vazirgiannis: 'Clustering and Community Detection in Directed Networks: A Survey'

- Likewise, the use of pseudo-inverse Laplacians has a lot more papers attached to it (these are only a few that are relevant):

  - Ho and Dooren: 'On the pseudo-inverse of the Laplacian of a bipartite graph'
  - Gutman and Xia: 'Generalized inverse of the Laplacian matrix and some applications'

# Suggestions for improvement

- In some sense, this work can be seen as an extension of Laplacian eigenmaps to the directed case. The paper needs to be more clear about these extensions with respect to prior work. In Section 2, it is claimed that 'our main contribution is a proper normalization'. This strikes me as a rather small contribution in light of the experimental section, as outlined above.

- I am also hesitant to speak about a better interpretability of the mean commute time. I agree that it is nice to know that the distance permits such an interpretation in terms of random walks, but what is the impact of knowing the MCT? It is not only used in the embedding insofar as one obtains a vector representation.

- Section 5 is then the standard way of defining random walks based on a Laplacian matrix, and the correspondence to the pseudo-inverse of the Laplacian is shown. This is mathematically interesting, but appears to me to be in line with previous research.

- The section about co-citation graphs should make it more clear that 'successors' are to be taken in terms of the original graph and the directionality of edges. Since this is a standard definition in the domain of network analysis I would suggest citing a textbook here.

- In Section 6, the paper could give more details about random walk concepts such as 'stochastic', 'stationary distribution' etc., as it would make the paper more accessible (I am familiar with these concepts but since the writing of the paper is of high quality in the other sections, I am convinced this would improve its impact, and attract more readers).

Typos & grammar issues:

- 'in terms of random walk' --> 'in terms of random walks'
- 'equivalent to build' --> 'equivalent to building'
- 'with corresponding unitary matrix' --> 'with a corresponding unitary
  matrix'
- 'square Euclidean distance' --> 'squared Euclidean distance'
- 'equivalent to consider' --> 'equivalent to considering'
- 'irreductible' --> 'irreducible'
- 'and provide generally' --> 'and provides generally'
- 'in low dimension' --> 'in a lower dimension'

Furthermore, the bibliography should employ consistent capitalization and journal names for articles.","The sentiment score is slightly negative (-20) because while the reviewer expresses some positive aspects ('well-written paper', 'enjoyed reading', 'pedagogical treatment'), they also raise several significant concerns about the paper's originality, experimental section, and missing references. The overall tone suggests that major revisions are needed. The politeness score is relatively high (70) as the reviewer uses respectful language throughout, offers constructive criticism, and balances negative points with positive observations. They use phrases like 'I appreciate', 'praiseworthy', and provide detailed suggestions for improvement, which contributes to the polite tone. The reviewer also acknowledges their own limitations in judging certain aspects, which adds to the politeness.",-20.0,70.0
"This paper proposes an autoencoder architecture and training procedure for producing high-quality reconstructions and realistic interpolations. A ""generator"" autoencoder is trained to fool a ""discriminator"" autoencoder. The generator tries to minimize its own reconstruction error and minimize the reconstruction error of the discriminator when fed with interpolated latent vectors of real datapoints. The discriminator autoencoder has three losses, corresponding to minimizing reconstruction error on real datapoints and maximizing reconstruction error on the generator's output on both real datapoints and interpolated outputs. The authors also propose a loss which encourages the distances between real datapoints and their corresponding latent vectors to be similar, as well as a heuristic procedure for stabilizing GAN training. Qualitative results are shown on CelebA.

While the results look nice, the paper is not fit for publication in its current form. At a high level, the issues include a lack of convincing experimental verification of the method, a generally contradictory and confusing description of the methods, and frequent factual errors or mischaracterizations. Here I will try to describe many of the issues I found while reading the paper:
- Experimental results are only given on CelebA which is a dataset with a very strong and easy-to-model structure. The experimental results are completely qualitative. No effort is made to provide a quantitative proof of claims such as ""the reconstructions are less blurry"" or ""the interpolations are higher quality""; only a few examples are shown. The experiments are not even described in the text, and many of the figures are unreferenced. No ablation studies are done to determine the importance of different loss terms, such as L_dist. No mention is given to how hyperparameters like alpha should be chosen (and in fact, the value given for it ""1^{-4}/2"" is nonsense; 1^{-4} is just 1). No results for a baseline autoencoder (i.e., just optimizing reconstruction loss) are given.
- At a higher level, no effort is given to argue why interpolation is a useful characteristic to try to encourage. There are no downstream applications proposed or tested. Earlier models, such as VAEGAN, also give reasonable reconstructions and good interpolations. Why is GAIA better? On what problem would I use GAIA and achieve better results apart from making nice-looking interpolations of people's faces?
- Definitions are often unclear or contradictory. For example, the generator autoencoder is alternatingly treating as taking input X and taking input Z. I believe what is meant is that the generator consists of two networks which compute Z = encoder(X) and X = decoder(Z). Instead, the paper just switches between G(Z) and G(X) wherever convenient. Similarly, the equation for \delta_Disc is different in Algorithm 1 and in the equation in 2.2. Interpolation, arguably one of the core parts of the model, is described as ""interpolations are Euclidean interpolations between pairs of points in Z, sampled from a Gaussian distribution around the midpoint between Zg1en and Zg2en."" I assume the mean of this Gaussian is the midpoint; what is its covariance? Etc.
- All autoencoders are not generative models, and in particular GAIA is not a generative model. There is no generative process. It does not estimate a data distribution. A VAE is a generative model which an autoencoder-like structure, but this does not make all autoencoders generative models.
- GAIA is described as encouraging ""convex latent distributions"" and a convex set is defined in the text as ""A convex set of points is defined as a set in which the line connecting any pair of points will fall within the rest of the set."" A convex set is not defined in terms of lines; it's defined in terms of convex combinations of points within the set. In the paper, only lines between points are considered. Claiming that the latent space is ""convex"" in the sense of purple blobs in B is not done - you would need to take a convex combination of multiple latent vectors and decode the results.

This is an incomplete list of the issues with this paper. The paper would need significant changes before publication.","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'not fit for publication in its current form' and lists numerous significant issues with the paper, including lack of experimental verification, confusing descriptions, and factual errors. The politeness score is -20 because while the reviewer maintains a professional tone overall, the language used is quite direct and critical. Phrases like 'the paper is not fit for publication' and 'This is an incomplete list of the issues with this paper' are blunt and could be perceived as somewhat impolite. However, the reviewer does attempt to provide constructive feedback and explains their reasoning, which prevents the score from being even lower.",-80.0,-20.0
"The paper addresses the problem of pixel-wise segmentation of lanes from images taken from a vehicle-mounted camera. The proposed method uses multiple passes through encoders decoders convnets, thereby allowing extract global features to inform better local features, and vice versa. Only qualitative baseline comparisons are presented by manually comparing the output of the network to reported results of other methods in [Pan et al.2017].
It is unclear to me if the proposed multiple encoder-decoder network is a novel architecture, or a known architecture applied to a novel use case. In case of the former, more details should be given on the design of the network, how it is trained, etc. for reproducibility. The biggest problem however is the subjective manual comparison to existing methods, which the authors do in favor of a quantitative comparison using well-understand objective metrics. While they point out problems with evaluating segmentation with conventional accuracy metrics, no attempt is made to make a better objective measure. We are left to judge the results on only a few selected example frames. 
It is also unclear how the method and evaluation strategy compares to methods which predict lanes as splines or other parameterized functions. E.g. see surveys on existing approaches, and discussion of different evaluation strategies, e.g. ""Recent progress in road and lane detection: a survey"" [Hillel et al.2014] and ""Visual lane analysis and higher-order tasks: a concise review"" [Shin, 2014].
Throughout the paper, various fuzzy and unclear statements are made (see detailed comments below). The paper would be in a better shape if more time is spend to improve the writing, provide more details on the method, and extend the experiments.

Pros:
+ multiple encoder-decoder stages could be beneficial for lane segmentation

Cons:
- lacking evaluation and comparison to baseline methods
- missing details on proposed network architecture, making it hard to reproduce
- unclear what colors in figures for qualitative evaluation represent: are individual lanes also distinguished?

Below are more detailed comments and questions:
* Abstract
	* ""the capability has not been fully embodied for"" → Fuzzy statement, I don't understand what this means.
	* ""In especial"" → check grammar
* Sec 1.: Introduction
	* ""the local information of a lane such as sharp, edges, texture and color, can not provides distinctive features for lane detection"" local edges are not distinctive for lanes? Possibly local edges alone are not sufficient, but various lanes detection approaches rely on edge extraction as features. This statement therefore seems too strong.
	* ""End-to-end CNNs always give better results than systems relying on hand-crafted features."". It is not possible to say that one type of classifier categorically better than another. The 'best' classifier depends on the problem at hand, valid assumptions that can be made, and the amount of training data avaiable, among others. For instance, ""How Far are We from Solving Pedestrian Detection?"" [Zhang,CVPR16] demonstrates that CNNs do not always give better results than hand-crafted features for some tasks and datasets. The paper should be more careful with such strong statements.
	* ""Highly hand-craft features based methods can only deal with harsh scenarios."". I don't understand, is this statement intended as an argument against hand-crafted features? Isn't it good to deal especially with harsh scenarios?
	* ""but less explored on Semantic Image Segmentation due to strong prior information is needed."" CNNs are extensively used for semantic image segmentation, e.g. see the well-known Cityscapes benchmark.
	* ""recent methods have replaced the feature-based methods with model-based methods."". Not sure why the paper call CNNs ""model-driven methods"", but refer to the earlier classical methods with highly designed representations (Kalman filter, B-snakes, ...) as ""feature-based methods"". This seems diferent from what I typically see, where CNNs are referred to as 'data-driven methods', and the classical methods as ""model-driven"".
* Sec 1.2: Contributions
	* ""First, reduced localization accuracy due to the weak performance of combining the local information and global information effectively and efficiently"". Instead of presenting a first contribution, the paper presents a problem. Do the authors mean that they ""tackle the problem of reduced localization accuracy ..."" ? That would still not make this contribution very concrete though ...
	* ""We make our attempts to rethink these IoU based methods."" → Please argue in favor of your new method. An in-depth comparison of evaluation methods, and why some metrics fail or could be redesigned would be good. However, the paper currently fails to present a new metric, and convince that it tackles shortcomings of established metrics.

* Sec 2.: Multiple Encoder-Decoder Nets
	* Figure 2: Is this the first paper to propose this multiple encoder-decoder net? Or is the idea taken from other work, and is the novelty to apply it to this problem? If this general architecture was already proposed (for semantic segmentation?), please add citations and discuss it as related work. If this network design is completely novel, I would expect more details on how the network is constructed (e.g. dimensions of each layer, non-linear activation function used, batch normalization, strides, etc.). 
	* ""the following loss function:"". Since it is a binary classification problem, and not a regression problem, why not use a (binary) cross entropy loss instead of a mean squared error?

* Sec 3: Experiments
	* Figure 3: What is the ""Baseline"" method ? Where are the references to the other works, or is the reader required to read [Pan'2017] to understand your figures?
	* Figure 3: How are the colors in these figures determined? Is this also an instance segmentation problem? From your methodology section I though only binary classification was considered. Do you do some post-processing to separate individual lanes? I find this confusing, as I thought that the task was limited to binary segmentation.
	* ""Recent works evaluated ..."" please cite the works you refer to.
	* ""we have compared more than 500 probmaps of each level nets manually and count the accuracy of these probmaps as shown in figure 7."" So if I understand correctly, instead of using an objective evaluation metric, you have reverted to manual labor to visually judge lane detection quality. This is not really a metric, and not really a solution that 'rethinks IoU based methods.' Problems of your approach is that it is unclear on what criteria results are judged, your evaluation is not objectively reproducible by others, and does not scale well for novel future evaluations. Why is this even needed? E.g. why not use some chamfer distance or Gaussian smoothing of the edge map if you want to evaluate near coverage instead of hard boundaries? Or, fit a function through the boundary, and evaluate distance (in meters) to true lane. I find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking.
	* Figure 6: What are the Ground Truth images of each row ? E.g. in the fourth row from the top, should the right-most yellow lane be present or not? As it stands, I can't interpret the columns and see which x times is visually 'better'.
* Sec 3.4:
	* ""To improve ability of the network, we propose a small quantity of channel to reduce overfitting by considering inter-dependencies among channels."" To improve relative to what? Where are the results comparing large amounts vs small amount of channels? Note that Figure 8 is not referred to in the text, and confusingly compares ""18 layers"" to ""1 layers"". Do you mean channels instead of layers? And, how many channels were to obtained the results in the preceding sections?
","The sentiment score is -60 because the review is predominantly critical, pointing out several major flaws in the paper such as lack of proper evaluation, missing details, and unclear statements. However, it's not entirely negative as it acknowledges some potential benefits of the proposed method. The politeness score is 20 because while the reviewer maintains a professional tone and offers constructive criticism, there are instances of direct criticism without much softening language. The reviewer uses phrases like 'The biggest problem however is...' and 'It is unclear...' which are direct but not impolite. The reviewer also provides a balanced view by listing both pros and cons, and offers detailed suggestions for improvement, which contributes to the slightly positive politeness score.",-60.0,20.0
"The author proposed a novel classification procedure of discerning, maximum response, and multiple check (DMM) to improve accuracy of mediocre network and enhance feedforward network. The author used a toy model with randomly scattered points around four centres to demonstrate the dynamics of decision boundary construction and show how and why the filter training had worked when a multiclass problem is reduced to a pseudo binary classification. The literature review is rather short, some discussion on how the proposed work advances the latest deep neural networks should be added. The experiments are carried out with a simple CNN, it will be better if the author demonstrate its integration to more state-of-art network and make a comparison to their performance.  ","The sentiment score is slightly positive (20) because the reviewer acknowledges the novel aspects of the author's work and its potential, but also points out several areas for improvement. The review begins by highlighting the author's proposed novel classification procedure and its potential benefits. However, it then suggests areas where the work could be expanded or improved, such as a more comprehensive literature review and testing with more advanced networks. This mix of positive recognition and constructive criticism results in a mildly positive sentiment. The politeness score is moderately positive (50) because the reviewer uses neutral, professional language throughout. They offer suggestions for improvement in a constructive manner, using phrases like 'it will be better if' rather than using more critical or harsh language. The reviewer maintains a respectful tone while providing feedback, which is typical of polite academic discourse.",20.0,50.0
"
In this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed “generalized” BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. 

I have the following three main comments about the paper. 
1)	Only deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. 
2)	Because the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. 
3)	Only one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that “the technique presented can be extended to more layers with additional notation”. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. 
4)	The authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. 
5)	Assumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. 
6)	Follow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. 
","The sentiment score is -50 because while the reviewer acknowledges the authors' work, they express several significant concerns and doubts about the paper's methodology, practical value, and theoretical foundations. The review lists six main comments, most of which are critical, indicating a generally negative sentiment. However, it's not entirely negative as the reviewer does recognize the authors' efforts and proposes suggestions for improvement. The politeness score is 20 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I have the following comments' and 'Please specify' rather than using harsh or dismissive language. The reviewer also offers constructive feedback and suggestions for improvement, which is a polite approach. However, the score is not higher as the review is quite direct in its criticisms without much softening language.",-50.0,20.0
"In this paper, the authors present two methods, Sequential and Parallel-FEED for learning student networks that share architectures with their teacher.

Firstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.

The paper could do with some further grammar/spell checks.

It isn't clear to me where the novelty lies in this work. Sequential-FEED appears to be identical to BANs (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. Parallel-FEED is just an ensemble of teachers; please correct me if I'm wrong.

The experimental results aren't convincing. There aren't any fair comparisons. For instance, in table 6 a WRN-28-10(sFEED) after 5 whole training iterations is compared to a WRN-28-1(BAN) after 1. It would be good to run BAN for as many iterations. A comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the ImageNet experiments. Furthermore, if one isn't interested in compression, then Table 4 indicates that an ensemble is largely preferable.

This work would benefit from a CIFAR-10 experiment as it's so widely used (interestingly, BANs perform poorly on CIFAR-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.

In summary I believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.

Pros:
- Simple method
- Largely written with clarity

Cons:
- Method is not very novel
- No compared thoroughly enough to other work","The sentiment score is -70 because the reviewer recommends rejection and points out several significant issues with the paper, including lack of novelty and unconvincing experimental results. The overall tone is critical, though not entirely negative as the reviewer acknowledges some positive aspects ('Pros'). The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone throughout. They use phrases like 'It would be a good idea to...' and 'please correct me if I'm wrong', which add a degree of politeness. However, the review is not overly polite, maintaining a neutral to slightly positive level of courtesy while delivering critical feedback.",-70.0,20.0
"This paper presents an analysis of the inverse invariance of ReLU networks. It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments. They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope. They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.

The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together. The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting. The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network. However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.

I have several questions for the authors:
- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability. Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?

In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space. The experiments are not very convincing or illustrative of the theoretical results in my opinion. It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper as 'well-written' and the analysis as 'interesting and original'. However, they also point out some weaknesses, such as disconnected structure and unconvincing experiments, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, poses questions rather than making demands, and offers constructive criticism. They use phrases like 'I hope the author can address' instead of more forceful language. The review maintains a professional and courteous tone while still providing honest feedback.",20.0,60.0
"The reinforcement learning tasks with sparse rewards are very important and challenging. The main idea of this work is to encourage intra-life novelty. The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode. 

However, the results are not enough to be accepted to ICLR having a very high standard. In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C. There are some RL algorithms reported to be better than A2C. For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017. 

=================================================================================================
I've read the rebuttal. I updated my score but still not vote for accept. 

This paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments.
","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the topic and the main idea, they express that the results are not sufficient for acceptance at ICLR. They also request additional comparisons and express uncertainty about judging the paper fairly. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges their own limitations in evaluating the paper, and suggests considering other reviewers' comments. They also mention reading the rebuttal and updating their score, showing engagement with the authors' response. The language is professional and constructive, avoiding harsh criticism while still conveying their concerns.",-20.0,50.0
"Pros:
- The paper generalizes upon past observations by Ott et al. that NMT models might decode ""copies"" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in ""training set predictions."" It's unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel.
- The paper draws a connection between the observed degradation and ""label bias"", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding.
- The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing.
- The paper is very well written. The analysis throughout the paper is easy to follow and convincing.

Cons:
- Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor.

Comments/questions:
- In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$.
- To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row.
- In Section 5, when describing the ""Discrepancy gap"" constraint, you say that you ""modify Eq. 3 to include the constraint"", but I suspect you meant that you modify Eq. 1 to include this constraint.
- In Table 4, why didn't you tune $\mathcal{M}$ and $\mathcal{N}$ separately for each beam width?","The sentiment score is 80 (positive) because the review starts with a list of 'Pros' that are extensive and detailed, highlighting several positive aspects of the paper. The reviewer uses phrases like 'very well written', 'easy to follow', and 'convincing', indicating a strong positive sentiment. There is only one brief 'Con' mentioned, which slightly reduces the overall positive sentiment. The politeness score is 90 (very polite) because the reviewer uses respectful and constructive language throughout. They offer suggestions and questions in a courteous manner, using phrases like 'consider using' and 'you might compress'. Even when pointing out potential errors, the tone remains polite and helpful. The high scores in both categories reflect the overall positive and constructive nature of the review.",80.0,90.0
"The manuscript proposes a method for unsupervised learning with unknown class number k. The problem is classical and important. The proposed method is interesting and novel, but the experiments are not convincing. In detail, it did not compare other methods in the experiments. 
Pros: clear description and novelty of the method
Cons: insufficient experiments. ","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the novelty of the method, they express significant concerns about the insufficient experiments and lack of comparison to other methods. The positive aspects (clear description, novelty) are outweighed by the critical feedback. The politeness score is moderately positive (50) as the reviewer uses neutral language and balances criticism with praise. They objectively list pros and cons without using harsh or judgmental language, maintaining a professional tone throughout the review.",-20.0,50.0
"edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.

I also agree with the introduction of the Chi vector, and with the use of the term of ""photometric BA"", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.

---------------


This paper presents a method for dense Structure-from-Motion using Deep Learning:
The input is a set of images; the output is the camera poses and the depth maps for all the images.
The approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function. This Jacobian is used to update an estimate of the camera poses. As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step. In LM optimization, this lambda evolves with the improvement of the estimate. Here lambda is also predicted using a network based on the feature difference.

If I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.

The method is compared against DeMoN and other baselines with good results.

I like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps. The key was to include a network to predict lambda as well.

However, I have several concerns:

* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct. The image features learned with the proposed method are re-used in an approach using a fixed lambda. If I understand correctly, there are 2 things wrong with that:
- for GN optimization, lambda should be set to 0 - not a constant value. Several constant values should also have been tried.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.
Thus, the advantage of using a LM optimization scheme is not very convincing.

Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.

* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.

Less critical concerns that still should be taken into account if the paper is accepted:

- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.

- the name 'Bundle Adjustment' is actually not adapted to the proposed method.  'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors. Here the objective function is based on image feature differences. I thus find the name misleading. The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error. The proposed method is more about dense alignment for multiple images.


More minor points:

1st paragraph:  Marquet -> Marquardt
title of Section 3: revisitED
1st paragraph of Section 3: audience -> reader
caption of Fig 1: extractS
Eq (2) cannot have Delta Chi on the two sides. Typically, the left side should be \hat{\Delta \Chi}
before Eq (3): the 'photometric ..' -> a 'photometric ..'
1st paragraph of Section 4.3: difficulties -> reason
typo in absolute in caption of Fig 4
Eq (6): Is B the same for all scenes?  It would be interesting to visualize it.
Section 4.5: applies -> apply
","The sentiment score is 50 (slightly positive) because the reviewer starts by acknowledging improvements made by the authors, stating the paper is 'much more convincing now' and increasing their rating to 'clear accept'. However, they still list several concerns and recommendations, balancing the positive opening. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, thanks the authors for making changes, and frames criticisms constructively. They use phrases like 'If I understand correctly' and 'I like the fact that' to soften critiques. The reviewer also provides detailed explanations for their concerns, which is helpful and considerate. The tone remains professional and objective throughout, even when pointing out issues.",50.0,70.0
"This paper proposes Leap, a meta-learning procedure that finds better initialization for new tasks. Leap is based on past training/optimization trajectories and updates the initialization to minimize the total trajectory lengths. Experiments show that Leap outperforms popular alternatives like MAML and Reptile.

Pros
- Novel idea
- Relatively well-written
- Sufficient experiment evidence

Cons
- There exist several gaps between the theory and the algorithm

I have several concerns.
1. The idea is clearly delivered, but there are several practical treatments that are questionable. The first special treatment is that on page 5, when the objective is increased instead of decreased, the sign of the f part is flipped, which is not theoretically sound. It is basically saying that when we move from psi^i to psi^{i+1} with increased objective, we lie to the meta-learner that it is decreasing. The optimization trajectory is what it is. It would be beneficial to see the effect of removing this trick, at least in the experiments. Second, replacing the Jacobian with the identity matrix is also questionable. Suppose we use a very small but constant learning rate alpha for a convex problem. Then J^i=(I-G)^i goes to the zero matrix as i increases (G is small positive). However, instead, the paper uses J^i=I for all i. This means that the contributions for all i are the same, which is unsubstantiated.

2. The proof of Thm1 in Appendix A is not complete. For example, ""By assumption, beta is sufficiently small to satisfy F"", which I do not understand the inequality. Is there a missing i superscript? Isn't this the exact inequality we are trying to prove for i=0? As another example, ""if the right-most term is positive in expectation, we are done"", how so? BTW, the right-most term is a vector so there must be something missing. It would be more understandable if the proof includes a high-level proof roadmap, and frequently reminds the reader where we are in the overall proof now.

3. The set \Theta is not very well-defined, and sometimes misleading. Above Eq.(6), \Theta is mathematically defined as the intersection of points whose final solutions are within a tolerance of the *global* optimum, which is in fact unknown. As a result, finding a good initialization in \Theta for all the tasks as in Eq.(5) is not well-defined.

4. About the experiments. What is the ""Finetuning"" in Table 1? Presumably it is multi-headed but it should be made explicit. What is the standard deviation for Fig.4? The claim that ""Leap learns faster than a random initialization"" for Breakout is not convincing at all.

Minors
- In Eq.(4), f is a scalar so abs should suffice. This also applies to subsequent formulations.
- \mu is introduced above Eq.(8) but never used in the gradient formula.
- On p6, there is a missing norm notation when introducing the Reptile algorithm.","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they express several significant concerns and criticisms about the paper. The review begins with a neutral summary and lists both pros and cons, but the majority of the review focuses on detailed concerns and criticisms, indicating an overall negative sentiment. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They present their criticisms as 'concerns' rather than outright flaws, and use phrases like 'It would be beneficial' and 'It would be more understandable' when suggesting improvements. The reviewer also acknowledges the paper's strengths before delving into criticisms, which is a polite approach. However, the score is not higher because the review is primarily focused on critiquing the paper rather than offering excessive praise or using overtly polite language.",-20.0,50.0
"(Score raised from 8 to 9 after rebuttal)
The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such “winning-tickets” on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets.

The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing “feature” of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version.

1) The main “weakness” of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively “small” and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author’s decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds “in general”. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies.

2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably “break” the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, …? On page 2, second paragraph, the paper states: ”When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch”. I don’t fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough “juice” and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers).

3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow “shipping” each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer).

4) Some things that would be interesting to try:
4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all “behind” some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights’ “trajectories” during training to see if there is a correlation between the “distance weights traveled” and whether or not they end up in the winning ticket.
4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning?

5) Typo (should be through): “we find winning tickets though a principled search process”

6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well)
","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, stating it 'adds a novel and interesting angle' to an important question, praises the experiments as 'clean' and 'well executed', and explicitly recommends accepting the paper for presentation. The reviewer also offers constructive criticism and suggestions for improvement, which slightly tempers the overall positivity. The politeness score is 90 (very polite) due to the reviewer's consistently respectful and constructive tone. They use phrases like 'I acknowledge and support the author's decision', 'The paper would be stronger, if...', and frame criticisms as suggestions or interesting questions to explore. The reviewer also explicitly states that their comments are meant to be constructive and not requirements for acceptance, which is particularly considerate.",80.0,90.0
"The paper proposes a convolutional alternative to self-attention. To achieve this, the number of parameters of a typical convolution operation is first reduced by using a depth-wise approach (i.e. convolving only within each channel), and then further reduced by tying parameters across layers in a round-robin fashion. A softmax is applied to the filter weights, so that the operation computes weighted sums of its (local) input (LightConv).

Because the number of parameters is dramatically reduced now, they can be replaced by the output of an input-dependent linear layer (DynamicConv), which gives the resulting operation a ""local attention"" flavour. The weights depend only on the current position, as opposed to the attention weights in self-attention which depend on all positions. This implies that the operation is linear in the number of positions as opposed to quadratic, which is a significant advantage in terms of scaling and computation time.

In the paper, several NLP benchmarks (machine translation, language modeling) that were previously used to demonstrate the efficacy of self-attention models are tackled with models using LightConv and DynamicConv instead, and they are shown to be competitive across the board (with the number of model parameters kept approximately the same).

This paper is well-written and easy to follow. The proposed approach is explained and motivated well. The experiments are thorough and the results are convincing. I especially appreciated the ablation experiment for which results are shown in Table 3, which provides some useful insights beyond the main point of the paper. The fact that a linear time approach can match the performance of self-attention based models is a very promising and somewhat surprising result.

In section 5.3, I did not understand what ""head band, next band, last band"" refers to. I assume this is described in the anonymous paper that is cited, so I suppose this is an artifact of blind review. Still, even with the reference unmasked it might be useful to add some context here.","The sentiment score is 90 (highly positive) because the reviewer expresses strong approval of the paper, describing it as 'well-written and easy to follow' with 'thorough' experiments and 'convincing' results. They also note that the findings are 'very promising and somewhat surprising'. The only minor criticism is about a lack of clarity in one section, which doesn't significantly impact the overall positive sentiment. The politeness score is 80 (quite polite) due to the consistently respectful and professional tone. The reviewer uses phrases like 'I especially appreciated' and provides constructive feedback without harsh criticism. The language is formal and courteous throughout, maintaining a positive and encouraging tone while still offering a minor suggestion for improvement.",90.0,80.0
"There are several ingredients in this paper that I really liked. For example, (1) the notion that an agent should build a deterministic function of the past which implicitly captures the belief (the uncertainty or probability distribution about the state), by opposition for example to sampling trajectories to capture uncertainty, (2) modelling the world's dynamic in a learned encoded state-space (by opposition to the sensor space), (3) instead of modeling next-step probabilities p(z(t+1)|z(t)), model 'jumpy transitions' p(z(t+delta)|z(t)) to avoid unrolling at the finest time scale.

Now for the weak points:
(a) the justification for the training loss was not completely clear to me, although I can see that it has a variational flavor
(b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to next-step probabilities via the chain rule of probabilities, so we don't have a clear way to compare the TD-VAE models with jumpy predictions against other more traditional models
(c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).

So I believe that the authors are onto a great direction of investigation, but the execution of the paper could be improved.","The sentiment score is slightly positive (20) because the reviewer starts by highlighting several aspects they 'really liked' about the paper, showing appreciation for the authors' work. However, they also point out 'weak points', which balances out the positivity. The overall tone suggests the paper has potential but needs improvement. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the strengths before addressing weaknesses, and concludes with a constructive statement about the 'great direction of investigation'. The reviewer avoids harsh criticism and frames suggestions politely, using phrases like 'could be improved' rather than more negative alternatives.",20.0,60.0
"The paper is well written and flow well. The only thing I would like to see added is an elaboration of 
""run a semantic parsing module to translate a question into an executable program"". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. 

This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. 

In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words?   ","The sentiment score is 60 (positive) because the reviewer starts by praising the paper as 'well written and flow well'. They then provide constructive suggestions for improvement, which indicates a generally positive view of the paper with some areas for enhancement. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, framing their suggestions as requests ('I would like to see') and opportunities rather than demands. They also use phrases like 'This is not a requirement but an opportunity' which shows consideration for the authors. The reviewer's tone is consistently professional and courteous, offering constructive feedback without being critical or harsh.",60.0,80.0
"Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.

------

The paper proposes an end-to-end neural architecture for learning protein structures from sequences. The problem is highly important. The method proposes to use a Langevin simulator to fold the protein ‘in silico’ from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). The system works on internal coordinates, which are conditioned and integrated on the fly. The method seems to perform very well, improving upon their baseline model considerably.

In spite of the paper being an outstanding work, I have two criticisms about the accessibility and impact of the paper on the broader ICLR audience. In its current form and complexity, the paper feels accessible mostly to a narrow audience.

First, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding (for instance, the appendix reveals that MSA is used to generate more data), and even with it the method is difficult to grasp as a whole. This paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. 

Second, the introduction lists numerous competing methods both on the protein modelling side and on the MCMC vs optimisation side. The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general. No bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. Finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. The discussion on the exploding gradients and dampening is excellent however. The only baseline is one with the simulator replaced by an RNN. There does not seem to be any running time analyses. As such, it is hard to interpret the current system, and it feels like a black box.","The sentiment score is 50 (slightly positive) because while the reviewer praises the paper as 'outstanding' and acknowledges its significant contributions, they also express major concerns about its accessibility and lack of comparisons to other methods. The initial paragraph indicates they raised their score after the authors addressed some concerns, showing overall positive sentiment despite criticisms. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'I believe' and 'I wonder' to soften critiques, and describe the work as 'outstanding' and 'staggering'. However, they don't use overly deferential language, maintaining a professional tone while clearly expressing concerns.",50.0,70.0
"This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.
The contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. 
The density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. 
Although the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.
Concerning discussions and analysis:
- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?
In algorithm 1, is \epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator.","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and its ability to be competitive, while also pointing out some limitations and areas for improvement. The review is not overwhelmingly positive, but it does recognize the value of the work. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, asks questions for clarification rather than making accusations, and provides constructive feedback. The reviewer acknowledges the paper's merits while also offering suggestions for improvement in a professional manner.",50.0,70.0
"This paper combines state of the art models for piano transcription, symbolic music synthesis, and waveform generation all using a shared piano-roll representation.  It also introduces a new dataset of 172 hours of aligned MIDI and audio from real performances recorded on Yamaha Disklavier pianos in the context of the piano-e-competition.  

By using this shared representation and this dataset, it is able to expand the amount of time that it can coherently model music from a few seconds to a minute, necessary for truly modeling entire musical pieces.

Training an existing state of the art transcription model on this data improves performance on a standard benchmark by several percentage points (depending on the specific metric used).

Listening test results show that people still prefer the real recordings a plurality of the time, but that the syntheses are selected over them a fair amount.  One thing that is clear from the audio examples is that the different systems produce output with different equalization levels, which may lead to some of the listening results.  If some sort of automatic mastering were done to the outputs this might be avoided.

While the novelty of the individual algorithms is relatively meager, their combination is very synergistic and makes a significant contribution to the field.  Piano music modeling is a long-standing problem that the current paper has made significant progress towards solving.

The paper is very well written, but there are a few minor issues:
* Eq (1) this is really the joint distribution between audio and notes, not the marginal of audio
* Table 4: What do precision, recall, and f1 score mean for notes with velocity?  How close does the system have to be to the velocity to get it right?
* Table 6: NLL presumably stands for Negative Log Likelihood, but this should be made explicity
* Figure 2: Are the error bars the standard deviation of the mean or the standard error of the mean?
","The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, highlighting its significant contributions and progress in the field. They use phrases like 'significant contribution,' 'significant progress,' and 'very synergistic.' The few criticisms are minor and constructive. The politeness score is 70 (polite) because the reviewer maintains a professional and respectful tone throughout. They acknowledge the paper's strengths and offer constructive feedback without harsh language. The use of phrases like 'very well written' and the balanced approach to discussing both strengths and areas for improvement contribute to the polite tone. However, it's not extremely polite, as it maintains a standard professional tone rather than going out of its way to be exceptionally courteous.",80.0,70.0
"This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.

Overall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.

I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.

In equation 4.1 describes the GIN update, which is proposed as “the most powerful GNN”.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isn’t made very clear in the paper.

The training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.

As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR.

--------

Considering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.

In the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.

--------

Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again.","The sentiment score is 70 (positive) because the reviewer expresses strong approval overall, stating they 'quite liked this paper' and would vote to accept it. They praise the paper's approach as 'very interesting' and 'important'. However, they do raise some concerns and suggest clarifications, which prevents a perfect score. The politeness score is 80 (very polite) as the reviewer uses respectful language throughout, offers constructive criticism, and balances positive feedback with suggestions for improvement. They phrase critiques gently, using phrases like 'I do feel however' and 'isn't made very clear' rather than harsh language. The reviewer also acknowledges when the authors address concerns in revisions, showing a collaborative attitude.",70.0,80.0
"General:
The paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images. The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image. The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels. In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN. The obtained results are impressive, the generated images are large and contain realistic details.

In my opinion the paper would be interesting for the ICLR audience.

Pros:
+ The paper is very technical but well-written.
+ The obtained results constitute new state-of-the-art on HQ image datasets.
+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling. The proposed approach is a very interesting step towards this direction.

Cons:
- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.
- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, “Taming VAEs”, 2018).

--REVISION--
I would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2).","The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, highlighting its technical merits, impressive results, and relevance to the field. They explicitly state it would be interesting for the ICLR audience and list several pros. The cons mentioned are relatively minor suggestions for improvement rather than major criticisms. The revision note further increases the positive sentiment, with the reviewer thanking the authors and raising their final score. The politeness score is 90 (very polite) due to the consistently respectful and constructive tone throughout. The reviewer uses phrases like 'I would like to thank the authors', 'I highly appreciate', and 'I am especially thankful', demonstrating a high degree of courtesy. Even when raising concerns, the language remains polite and professional.",80.0,90.0
"This paper addresses questions about the representation of visual information in the retina. The authors create a deep neural network model of the visual system in which a single parameter (bandwidth between the “retina” and “visual cortex” parts) is sufficient to qualitatively reproduce retinal receptive fields observed across animals with different brain sizes, which have been hard to reconcile in the past. 

This work is an innovative application of deep neural networks to a long-standing question in visual neuroscience. While I have some questions about the analyses and conclusions, I think that the paper is interesting and of high quality.

My main concern is that the authors only show single examples, without quantification, for some main results (RF structure). For example, for Fig. 2A and 2B, an orientation selectivity index should be shown for all neurons. A similar population analysis should be devised for Fig 2C, e.g. like Fig 3 in [1]

Minor comments:
1. Page 4: “These results suggest that the key constraint ... might be the dimensionality bottleneck..”: The analyses only show that the bottleneck is *sufficient* to explain the differences, but “the key constraint” also implies *necessity*. Either soften the claim or provide control experiments showing that alternative hypotheses (constraint on firing rate etc.) cannot explain this result in your model.

2. I don’t understand most of the arguments about “cell types” (e.g. Fig. 2F and elsewhere). In neuroscience, “cell types” usually refers to cells with completely different connectivity constraints, e.g. excitatory vs. inhibitory cells or somatostatin vs. parvalbumin cells. But you refer to different CNN channels as different “types”. This seems very different than the neuroscience definition. CNN channels just represent different feature maps, i.e. different receptive field shapes, but not fundamentally different connectivity patterns. Therefore, I also don’t quite understand what you are trying to show with the weight-untying experiments (Fig. 2E/F).

3. It is not clear to me what Fig. 3B and the associated paragraph are trying to show. What are the implications of the nonlinearity being due to the first or second stage? 

4. Comment on Fig 3F: The center-surround RFs probably implement a whitening transform (which is linear). Whitened inputs can probably be represented more efficiently in a network trained with L2-regularization and/or SGD. This might explain why the “quasi-linear” retina improves separability later-on.

[1] Cossell, Lee, Maria Florencia Iacaruso, Dylan R. Muir, Rachael Houlton, Elie N. Sader, Ho Ko, Sonja B. Hofer, and Thomas D. Mrsic-Flogel. “Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex.” Nature 518, no. 7539 (February 19, 2015): 399–403. https://doi.org/10.1038/nature14182.","The sentiment score is 60 (positive) because the reviewer states that the work is 'innovative' and 'interesting and of high quality'. However, they also express some concerns, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions or questions rather than direct attacks. For example, they use phrases like 'I have some questions' and 'My main concern is' rather than more aggressive language. The reviewer also provides constructive feedback and specific recommendations for improvement, which is a polite and professional approach.",60.0,80.0
"Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence ""Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied.""  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase ""in the settings studied"" replaced by ""for an alexnet architecture"".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.

Revision 1: This is a revision of my earlier review.  My overly-excited earlier rating was based on tables 1 and 2 and the claim to have unsupervised features that are competitive with fully-supervised features. (I also am subject to an a-priori bias in favor of mutual information methods.)  I took the authors word for their claim and submitted the review without investigating existing results on CIFAR10.  It seems that tables 1 and 2 are presenting extremely weak fully supervised baselines.  If DIM(L) can indeed produce features that are competitive with state of the art fully supervised features, the result is extremely important.  But this claim seems misrepresented in the paper.

Original review:

There is a lot of material in this paper and I respect this groups
high research-to-publication ratio. However, it might be nice to have
the paper more focused on the subset of ideas that seem to matter.

My biggest comment is that the top level spin seems wrong.
Specifically, the paper focuses on the two bullets on page 3 ---
mutual information and statistical constraints.  Here mutual
information is interpreted as the information between the input and
output of a feature encoder.  Clearly this has a trivial solution
where the input equals the output so the second bullet --- statistical
constraints --- are required.  But the empirical content of the paper
strongly undermines these top level bullets.  Setting the training
objective to be the a balance of MI between input and output under a
statistical consrtraint leads to DIM(G) which, according the results in
the paper, is an empirical disaster.  DIM(L) is the main result and
something else seems to be going on there (more later).  Furthermore,
the empirical results suggest that the second bullet --- statistical
constraints --- is of very little value for DIM(L). The key ablation
study here seems to be missing from the paper.  Appendix A.4 states
that ""a small amount of the [statistical constraint] helps improve
classification results when used with the [local information
objective].  No quantitative ablation number is given.  Other measures
of the statistical constraint seem to simply measure to what extent
the constraint has been successfully enforced.  But the results
suggest that even successfully enforcing the constraint is of little,
if any, value for the ability of the features to be effective in
prediction.  So, it seems to me, the paper to really just about the
local information objective.

The real power house of the paper --- the local information objective
--- seems related to mutual information predictive coding as
formalized in the recent paper from deep mind by van den Oord et al
and also an earlier arxiv paper by McAllester on information-theoretic
co-training.  In these other papers one assumes a signal x_1, ... x_T
and tries to extract low dimensional features F(x_t) such that F(x_1),
..., F(x_t) carries large mutual information with F(x_{t+1}).  The
local objective of this paper takes a signal x1, ..., x_k (nXn
subimages) and extracts local features F(x_1), ... F(x_k) and a global
feature Y(F(x_1), ..., F(x_k)) such that Y carries large mutual
information with each of the features F(x_i).  These seem different
but related.  The first seems more ""on line"" while the second seems
more ""batch"" but both seem to be getting at the same thing, especially
when Y is low dimensional.

Another comment about top level spin involves the Donsker-Varadhan
representation of KL divergence (equation (2) in the paper).  The
paper states that this is not used in the experiments.  This suggests
that it was tried and failed.  If so, it would be good to report this.
Another contribution of the paper seems to be that the mutual
information estimators (4) and (5) dominate (2) in practice.  This
seems important.

","Sentiment score: The review is generally positive, especially in the final paragraph where the reviewer states they are 'upping my score' due to new comparisons. However, there are also some critical comments and suggestions for improvement, which prevents it from being extremely positive. The overall tone suggests a moderate positive sentiment.

Politeness score: The language used is consistently professional and respectful. The reviewer acknowledges the authors' work and provides constructive feedback. They even apologize for a late comment ('I apologize that this is coming after the update deadline'). The tone is polite throughout, with phrases like 'I respect this groups high research-to-publication ratio' and 'The authors should simply consider...'. There are no rude or harsh statements, but the politeness is not overly effusive, hence a score of 75.

Both scores reflect a balanced, professional review that is generally positive and consistently polite, while still providing critical feedback.",50.0,75.0
The paper proposed a novel differentiable neural GLCM network which captures the high reference textural information and discard the lower-frequency semantic information so as to solve the domain generalisation challenge. The author also proposed an approach “HEX” to discard the superficial representations. Two synthetic datasets are created for demonstrating the methods advantages on scenarios where the domain-specific information is correlated with the semantic information. The proposal is well structured and written. The quality of the paper is excellent in terms of novelty and originality. The proposed methods are evaluated thoroughly through experiments with different types of dataset and has shown to achieve good performance. ,"The sentiment score is 90 (highly positive) because the reviewer uses phrases like 'novel', 'excellent in terms of novelty and originality', and 'good performance'. They also mention that the paper is 'well structured and written'. There are no negative comments, indicating a very positive sentiment. The politeness score is 70 (polite) because the language is professional and respectful, using phrases like 'The author proposed' and 'The proposal is well structured'. While not overly effusive, the tone is consistently positive and courteous, avoiding any harsh or critical language.",90.0,70.0
"The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. 

Quality 
The proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. 
The results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).

Clarity
The paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. 

Originality
There is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. 

Significance
- pros clear and seemingly state-of-the-art results, intuitive approach, 
-cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).

One comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. ","The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper. They describe the algorithm as 'well defined' and 'competitive', noting that it 'slightly outperforms' other approaches. The reviewer also praises the paper as 'well written and clearly structured'. While there are some minor criticisms, such as the difficulty in understanding section 5.4 and the results being only 'modestly better' than other methods, the overall tone is approving.

The politeness score is 50 (somewhat polite) because the reviewer maintains a professional and respectful tone throughout. They offer balanced feedback, highlighting both pros and cons, and use neutral language when pointing out areas for improvement. The reviewer also offers a constructive suggestion at the end. However, the review doesn't go out of its way to be exceptionally polite or use particularly warm language, hence the moderate positive score rather than a very high one.",70.0,50.0
"Language is hierarchically structured: smaller units (e.g., noun phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the different units of an LSTM can learn to track information at different time scales, the standard architecture does not impose this sort of strict hierarchy. This paper proposes to add this constraint to the system by ordering the units; a vector of ""master"" input and forget gates ensures that when a given unit is reset all of the units that follow it in the ordering are also reset.

Strengths:
* The paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly.
* The evaluation tasks are very sensible. It's good that the model is shown to obtain good perplexity and slightly improve over an LSTM baseline; it's not the state of the art, but that's not the point of the paper (in fact, I would emphasize that even more than the authors do). The unsupervised parse evaluation (Table 2) is the heart of the paper, in my opinion (and should probably be emphasized more) -- the results from the second layer are quite impressive.
* The (mildly) better performance than LSTMs on long-distance dependencies, and (mildly) worse performance on local dependencies, in the Marvin & Linzen dataset, is interesting (and merits additional analysis).

Weaknesses:
* The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused. I am not sure that neural networks with latent syntactic structures can really address the seemingly very fundamental question mentioned in the first paragraph (whether syntax is related to ""an underlying mechanism of human cognition"") - I would suggest eliminating this part. At the same time, the authors might want to add another motivation for studying architectures that discover latent structure (as opposed to being given that structure) - this setting corresponds more closely to human language acquisition, where children aren't given annotated parse trees.
* The authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the LSTM is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences.
* There is limited analysis of the model. Why does the second layer show better unsupervised parsing performance than the third layer? (Could this be related to syntactic vs. semantic/discourse units I mention in the previous bullet?) Why is the model better at ADJP boundaries than NP boundaries? It would have been more useful to report less experiments but analyze the results of each experiment in greater depth.
* In this vein, I am not sure it's useful to include WSJ10 in Table 2, which is busy as it is. These sentences are clearly too easy, as the right branching baseline shows, and require additional POS tagging.
* I found it difficult to read Figure A.2: could you help us understand what we should take away from it? 
* It is not entirely clear why the model needs both unit-specific forget/input gates and the ""master"" forget/input gates, and there is no discussion of this issue. Have you tried using only the ""master"" gates?

Minor notes:
* RNNGs are described as having an explicit bias to model syntactic structure; this is an arguably confusing use of the word ""bias"", in that the architecture has a hard constraint enforcing syntactic structures (bias implies a soft constraint).
* There are some language issues: agreement errors (e.g. ""have"" in the sentence that starts with ""Developing"" in the introduction), typos (""A order should exist"", ""co-occurance""), determiner issues (""values in [the] master forget gate"", ""when the overlap exists"") - I would suggest going through and copy editing the paper.
* ""cummax"" seems like a better choice of name for cumulative maximum than ""cumax"".
* It may be helpful to remind the reader of the update equation for c_t in a standard LSTM.
* Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?
* It's not clear if the results in Table 2 reflect the best seed out of five (as the title of the column ""max"" indicates) or the average (as the caption says).
","The sentiment score is 50 (mildly positive) because the reviewer begins by highlighting the strengths of the paper, praising its 'elegant' approach and 'sensible' evaluation tasks. They note that the results are 'quite impressive' in some areas. However, they also point out several weaknesses, maintaining a balanced view. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions ('I would suggest...') and asking questions rather than making blunt statements. They also use positive language when possible ('it's good that...', 'interesting'). The reviewer maintains a professional and constructive tone, even when pointing out areas for improvement.",50.0,80.0
"Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. 

In this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.

If the authors address my major concerns, I would increasing my rating 1-2 points.

Major Comments:

The results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.

1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:

  --> ""These experiments provide conclusive behavioural evidence in favour of the texture hypothesis""
  --> ""we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.""

I would prefer to see language such as ""We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs."" or ""We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements"". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.

2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?

Although the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?

3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. ""the convex outline of object segmentation"", etc.).

Minor Comments:

- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.

- Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.

- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.

- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?

- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?

- Please use names of Shape-ResNet, etc. in Table 2.

- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.

- A.2. ""not not used in the experiment"" --> ""not used in the experiment""
","The sentiment score is 50 (moderately positive) because the reviewer expresses enthusiasm for the results, calling them 'compelling' and 'very cool', while also providing constructive criticism and suggestions for improvement. The reviewer states they would increase their rating if concerns are addressed, indicating a generally positive but not overwhelmingly enthusiastic sentiment. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers praise where due, and frames criticisms constructively. They use phrases like 'I would prefer to see' and 'Please provide' when making suggestions, maintaining a collegial tone. The reviewer also acknowledges the authors' work positively, demonstrating professional courtesy.",50.0,80.0
"This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes. 
The key components of the approach are :
- increasing the batch size by a factor 8
- augmenting the width of the networks by 50% 
These first two elements result in an Inception score (IS) boost from 52 to 93.  
- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.
The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.
Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments. The comments that more data helps (internal dataset experiments) is also informative. 
Very nice to have included negative results and detailed parameter sweeps.

This is a very nice work with impressive results, a great progress achievement in the field of image generation. 
Very well written.

Suggestions/questions: 
- it would be nice to also propose unconditioned experiments. 
It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve. 
- I understand that no data augmentation was used during training?    
- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?
- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.
- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.
- It would be nice to display more Nearest neighbors for the dog image.
- It would be nice to add a figure of random generations.
- make the bib uniform: remove unnecessary doi - url - cvpr page numbers
","The sentiment score is 90 (highly positive) because the reviewer describes the paper as 'a very nice work with impressive results' and 'a great progress achievement in the field of image generation'. They also praise it as 'Very well written' and mention 'impressive images generations'. The politeness score is 80 (quite polite) due to the use of positive and respectful language throughout, such as 'Very nice to have included...', 'it would be nice to...', and phrasing suggestions as questions or polite requests. The reviewer provides constructive feedback and suggestions without any harsh criticism, maintaining a professional and courteous tone throughout the review.",90.0,80.0
"This paper introduces a novel feature selection method by utilizing GAN to learn the distributions. The novelty of this paper is to incorporate two recent works, i.e. knockoff for feature selection and W-GAN for generative models. Compared to the latest knockoff work which requires a known multivariate Gaussian distribution for the feature distribution, the proposed work is able to generate knockoffs for any distribution and without any prior knowledge of it.

Pros: This paper is very well written. I enjoyed reading this paper. It is novel and addresses an important problem. The numerical study clearly shows the advantage of the proposed work. 

Cons:

Q1: In the discriminator, instead of training with respect to the full loss, the authors consider to mask some information by using a multivariate Bernoulli random variable $B$ with success probability 0.9. Then the discriminator needs to predict only when $B_i = 0$. Can the authors provide some justification of such choice of the parameters? This choice is a little bit mysterious to me.

Q2: How sensitive are the hyper-parameters $\eta$ (set to 10 in the experiments), $\lambda$, and $\mu$ (set to 1 in the experiments)?

Q3: In the real data example, the feature selection performance is less justified as there is no truth. One suggestion is to evaluate the prediction errors using the selected features and compare with the benchmarks.



","The sentiment score is 80 (positive) because the reviewer expresses clear appreciation for the paper, describing it as 'very well written', 'novel', and addressing 'an important problem'. They also mention enjoying reading it and highlight its advantages. The few questions raised are presented as constructive feedback rather than criticisms. The politeness score is 90 (very polite) due to the reviewer's use of respectful and encouraging language throughout. They begin with positive comments, phrase their questions politely (e.g., 'Can the authors provide...'), and offer suggestions rather than demands. The tone is consistently professional and courteous, showing respect for the authors' work while providing thoughtful feedback.",80.0,90.0
"This paper deals with Memory Augmnted Neural Networks (MANN) and introduces an algorithm which allows full writes to the dense memory to be only exectued every L timesteps. The controller produces a hidden output at most timestps, whih is appended to a cache. Every L steps, soft attention is used to combine this cache of N hidden states to a single one, and then this is used as the input hidden state for the controller, with the outputs performing a write in the full memory M, along with clearing the cache.

The authors first derive ""Uniform Writing"" (UW) which updates the memory at regular intervals instead of every timestep. The derivation is based on the ""contribution"" which is norm of the gradient of some input timestep to some hidden state (potentially at a different timestep). I am not clear on whether this terminology for the quantity is novel, if this is the case maybe the authors should state this more clearly. UW says that if all timesteps are equally important, and only D writes can be made in a sequence of length T, then writes should be done every T/(D+1) steps. I have not checked the proof in detail but this seems reasonable that it would maximise the contribution quantity introduced. I am less clear on whether this is obviously the right thing to do - sometimes this value is referred to in relation to information, but that term does not strictly seem to be being used in the information theory sense (no mention of bits or nats anywhere). Regardless, as the authors point out, in real problems there are obviously timesteps which have less or no useful information, and clearly UW is mostly defined in order to build towards CUW.

CUW expands on UW by adding the cache of different hidden states, and using soft attention over them. This feels like a reasonable step, although I would presume there are times when the L hidden states were collected over timesteps with no information, and so the resulting write is not that useful, and times when all of hte L timesteps contain different useful information. In these circumstances it seems like the problem of getting the *useful* information into the memory is still present, as the single write done with the averaged hidden state will need to contain lots of information, which may be more ideal written with several timesteps.

The experiments are well described and overall the paper seems reproducable. The standard toy datasets of copy / reverse / sinusoid are used. The results are interesting - regular DNC with memory size 50 performs surprisingly badly on clean Sinusoid, my guess would be that with hyperparameter tuning this could be improved upon. I'm not sure that using exactly the same hyperparameters for a wide variety of models is appropriate - even with optimizers like Adam and RMSProp, I would want to see at least some sweeping for the best hyperparams, and then graphs like figure 3 should show error bars averaged across multiple runs with the best per-model hyperparameters. However, The DNC with CUW seems to perform well across all synthetic tasks.

There is no mention of Adaptive Computation Time/ACT (Graves, https://arxiv.org/abs/1603.08983) throughout the paper, which is surprising considering Alex Graves' models form two of the baselines used throughout the paper. ACT aims to execute an RNN a variable number of times, usually to do >1 timestep of processing for a single timestep of input. In the context of this paper, I believe it could be adapted to do either zero or one steps of computation per timestep, and that would yield a very comparable network where the LSTM controller always executes, and writes to the memory only happen sometimes. Given that it allows a learned process to decide whether to write, as opposed to having a fixed L which separates full writes, this should have the potential to outperform CUW, as it could learn that at certain times, writes must happen at every step. In my view ACT is attempting to solve essentially the same problem as this paper, so it should either be included as a baseline, or the manuscript should be updated to explain why this is not an appropriate comparison.


I think this is an interesting paper, trying to make progress on an important problem. The results look good, but I can only give a borderline score due to missing ACT numbers, and a few other unclear points. The addition of ACT experiments, and error bars on certain results, would change my mind here.


Notes:

""No solution has been proposed to help MANNs handle ultra long sequence"" - (Rae et al 2016) is an attempt to do this, by improving the complexity of reads / writes. This allows bigger memory and longer sequences to be processed.

""Current MANNS only support dense writing"" - presumably this means dense as in 'every timestep', but this terminology is overloaded - you could consider NTM / DNC as doing dense writing, and then work of Rae et al 2016 doing sparse writing.

In my experience training these kind of RNNs can have reasonably high variance across seeds - figures 2 & 3 should have error bars, and especially Table 4 as that contains the most important results. Getting 99 percent accuracy when previous SOTA is only 0.1% lower is only really meaningful if the standard deviation across seeds is very small.

Appendix A: the 'by induction' result - I believe there is an error, it should be:

h_t = \sigma_{i=1}^t U_{t-i}W x_i + C

As W is applied to inputs, before the repeated applications of U? I believe the rest of the derivation still holds the same, after the correction.

","The sentiment score is slightly positive (20) because the reviewer finds the paper interesting and acknowledges its potential importance, stating 'I think this is an interesting paper, trying to make progress on an important problem.' However, they also express some reservations and suggest improvements, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and balances positive comments with areas for improvement. They use phrases like 'I am not clear on' and 'I would presume' when expressing doubts, which maintains a polite tone. The reviewer also provides detailed feedback and suggestions for improvement, which is a courteous approach in academic peer review.",20.0,60.0
"Summary:

This work is tackling two difficulties in current VB applied to DNNs (""Bayes by backprop""). First, MC approximations of intractable expectations are replaced by deterministic approximations. While this has been done before, the solution here is new and very interesting. Second, a Gaussian prior with length scales is learned by VB empirical Bayes alongside the normal training, which is also very useful.

The term ""fixing VB"" and some of the intro is not really supported by the rather weak experiments, done on small datasets and networks, where much older work like Barber&Bishop would apply without any problems. While interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.

- Quality: Extremely well written paper, I learned a lot from it. Approximations are
   tested, great figures to explain things. And the major technical novelty, the
   expression for <h_j h_l>, is really interesting and useful.
- Clarity: Excellent writing until it comes to the experiments. Here, important
   details are just missing, for example what q(w) is (fully factorized Gaussian?).
   Very nice literature review, also historical.
- Originality: The idea of matching Gaussian moments along the network graph is
   previously done in PBP (Lobato, Adams), as acknowledged here. Porting this from
   ADF to VB gives dDVI. PBP also has the property that a DL system gives you the
   gradients. Having said that, I think dDVI may be more useful than PBP.
   While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in
   there. Now, what is done here, is more elegant, does not need 1D quadrature.
- Significance: Judging from the existing experiments, the significance may be
   rather small, *if one only looks at test log likelihood*. I'd still give this the
   benefit of the doubt, as in particular dDVI could be really interesting at large
   scale as well. But the authors may tone down their language a bit.
   To increase significance, I recommend to comment beyond just test log
   likelihood scores. For example:
   - Does the optimization become simpler, less tuning required, more automatic?
      Would one not expect so, given you make a big point out of reducing variance?
      Does it converge faster?
   - Can you do something with your posterior that normal DNN methods cannot
      do? Better decisions (bandits, active learning, HPO)? Continual learning?
      In the end, who really cares about test log likelihood?

Experiments:
- What is the q(w) family being used here? Fully factorized Gaussian? I
   suppose so for dDVI. But for DVI? Not said anywhere, in main paper or
   Appendix
- A bit disappointing. Why not evaluate at least dDVI with diagonal q(w) on
   some much larger models and datasets? Why not quote numbers on speed
   and robustness of learning, etc? Show what you really gain by reducing the
   variance.
- Experiments are OK, but on pretty small datasets, and for single hidden
   layer NNs. On such data and models, the Barber&Bishop 98 method could
   be run as well
- Was MCVI run with re-parameterization? This is really important. If not,
   this would be an important missing comparison. Please be clear in the main
   text
- Advantages over MCVI are not very large. At least, dDVI should be faster to
   converge than MCVI.
   Can you say something about robustness of training? Is it easier to train
   dDVI than MCVI?
- Why not show the PBP-1 results, comparing to dDVI, in the main text? Are they
   obtained with the same model? dDVI is doing better.

Other points:
- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98. Yours is
   more elegant and faster (does not need 1D quadrature)
- Relation to PBP: Note that dDVI has an advantage in practice. With PBP, I need
   to compute gradients for every datapoint. In dDVI, I can do mini-batch
   updates.
- I just *love* the header ""Wild approximations"". I tend to refer to this kind of work
   as ""weak analogies"". Why do you not also compare against this, and show it really
   does not work?
","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths, such as its excellent writing, interesting novelties, and potential usefulness. However, they also point out weaknesses in the experiments and suggest improvements. The politeness score is 80 (quite polite) due to the reviewer's constructive tone, use of phrases like 'I learned a lot from it,' and offering specific suggestions for improvement without harsh criticism. The reviewer balances praise with constructive feedback, maintaining a respectful and professional tone throughout.",50.0,80.0
"
Given the growing interest in building trust worthy and privacy protecting AI systems, this paper demonstrates a novel approach to achieve these important goals by allowing a trusted, but slow, computation engine to leverage a fast but untrusted computation engine. For the sake of protecting privacy, this is done by establishing an additive secret share such that evaluation on one part of the share is performed offline and the computation on the other part of the share is performed on the untrusted engine. To verify the correctness of the computation on the untrusted server, a randomized algorithm is used to sample the correctness of the results. Using these techniques, the authors demonstrate an order of magnitude speedup compared to running only on the trusted engine and 3-4 orders of magnitude speedup compared to software-based solutions.

Overall this is a strong paper which presents good ideas that have influence in ML and beyond. I appreciate the fact that the authors are planning to make their code publicly available which makes it more reproducible. Below are a few comments/questions/suggestions 

1.	This papers, and other papers too, propose mechanisms to protect the privacy of the data while outsourcing the computation on a prediction task. However, an alternative approach would be to bring the computation to the data, which means performing the prediction on the client side. In what sense is it better to outsource the computation? Note that outsourcing the computation requires both complexity on the server side and additional computation on the client side (encryption & decryption).
2.	You present the limitations of the trust model of SGX only in the appendix while in the paper you compare to other techniques such as Gazzelle which have a different trust model and assumption. It makes sense to, at least, hint the reader on these differences. 
3.	In section 2.2: “has to be processed with high throughput when available” is it high throughput that is required or low latency?
4.	In Section 4.3: in one of the VGG experiment you computed only the convolution layers which, as you say, are commonly used to generate features. In this case, however, doesn’t it make more sense that the feature generation will take place on the client side while only the upper layers (dense layers) will be outsourced?
5.	In section 4.3 “Private Inference” : do you include in the time reported also the offline preprocessing time? As far as I understand this should take the same amount of time as computing on the TEE.
","The sentiment score is 80 (positive) because the reviewer starts by highlighting the paper's importance and novelty, praising its 'strong' nature and 'good ideas'. They appreciate the authors' plan to make their code publicly available. The overall tone is very supportive, with constructive feedback provided. The politeness score is 90 (very polite) due to the respectful language used throughout. The reviewer uses phrases like 'I appreciate' and frames their comments as 'suggestions' rather than criticisms. They ask questions to prompt further thought rather than making direct criticisms. The review maintains a professional and courteous tone while providing detailed feedback, demonstrating high politeness in academic discourse.",80.0,90.0
"This paper studies auto-encoders under several assumptions: (a) the auto-encoder's layers are fully connected, with random weights, (b) the auto-encoder is weight-tied, (c) the dimensions of the layers go to infinity with fixed ratios. The main contribution of the paper is to point out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations. The idea is original and will probably lead to new directions of research. Already the first applications that the paper suggests are exciting.

The paper does a good job in justifying assumptions (a), (b) and (c) in the introduction. It is convincing in the fact that this point of view may bring practical insights on training initialization for real-world autoencoders. Thus my opinion is that this paper brings original and significant ideas in the field.

One flaw of this paper is that the writing might be clearer. For instance when presenting the technical theorem (Theorem 1), it would be useful to have an intuitive explanation for the theorem and the state-evolution-like equations. However, I believe that there are some easy fixes that would greatly improve the clarity of the exposition. Here is a list of suggestions: 

- In Section 2.1, a large number of notations are introduced. It would help a lot if the authors made a graphical representation of these. For instance, a diagram where every linearity / non-linearity is a box, and the different variables $x_l$, $\hat{x}_l$ appear would help a lot. 

- Section 2.2 is rather technical. The authors could try to give some more intuition of what's happening. For instance, they could spend more time after the theorem explaining what $\tau_l, \gamma_l$ and $\rho_l$ mean. They could also introduce the notation S_sig and S_var early and this section (and not in Section 3), because it helps interpreting the parameters. It would also help if they could write a heuristic derivation of the state-evolution-like equations. From the paper, the only way the reader can understand the intuition behind those complicated equations is to look at the proof of Theorem 1 (which is rather technical). 

- In Section 3.1, I did not understand the difference between interpretations 1 and 2. Could the authors clarify? 

- In Section 3.4, I did not understand the sentence: ""In particular, near the phase transition of \gamma, S_sig/S_var = \Omega(\beta^{1.5}"". If one uses the \Omega notation, it means that some parameter is converging to something. What is the parameter? As a consequence, I did not understand this paragraph. 

- In Section 3.5, the authors should make clear from the beginning why they are running those specific simulations. What hypothesis are they trying to check? I finally concluded that they are running simulations to check if the hypothesis they make in the first paragraph are true. They also want to compare with some other criteria in the literature, named EOC, that also gives insights about the trainability of the network. However, they could explicitly say in the beginning of the second paragraph that this is the goal.

- In a similar spirit, the authors should end Section 3.5 with a clear conclusion on whether or not the framework enables us to predict the trainability of the autoencoder. 



Minor edits / remarks: 

- Typo: last but one paragraph of the introduction: ""whose analysis is typically more straighforwards"" -> ""straightforward"".

- At the end of Section 3.2: what can be proved about the behavior of \gamma / \sqrt{\rho}? It is obviously a central quantity and the authors do not say what happens in the phases where \gamma and \rho go to infinity for instance. Is it because it is hard to analyse?

","The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, calling it 'original' and 'significant', and stating that it will 'probably lead to new directions of research'. The reviewer also mentions that the paper's applications are 'exciting'. However, it's not a perfect score as the reviewer does point out some flaws, particularly in the clarity of writing. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'Could the authors clarify?' and 'It would help a lot if the authors...', which are polite ways of pointing out areas for improvement. The reviewer also balances criticism with praise, acknowledging the paper's strengths before discussing its weaknesses.",70.0,80.0
"Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.

------

The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The paper presents the overall idea beautifully and is very easy to follow. The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical. The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements. Similarly in Flickr and MovieLens the method performs well. This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper. 

An illustrating figure would still be nice to include, also for the convolutions of eq 2. The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.

The paper should clarify that the \prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the “a” in the p(a), should it be ""p(x)” ? 

I have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. The definition of the m(x) is too clever, please clarify the function in more conventional notation.  ","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer states that 'All my concerns were addressed' and calls it a 'great paper' that 'should be accepted'. They describe the paper as 'well-founded, elegant and practical' and predict it will be 'high-impact'. The few criticisms are minor and constructive. The politeness score is 80 because the language is consistently respectful and professional. The reviewer uses phrases like 'would be nice to include' and 'should clarify' rather than making demands. They also balance critiques with praise, showing consideration for the authors' work. The tone is collegial and supportive throughout, though not excessively formal or deferential, hence the score of 80 rather than 100.",90.0,80.0

review,reasoning,sentiment_score,politeness_score
"The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","The reviewer acknowledges the importance of the topic and recognizes the contribution of the study to the community, which indicates a positive sentiment. However, they also mention that the paper misses strong novel methods, which slightly dampens the positivity. The language used is formal and constructive without any harsh or rude words, indicating a high level of politeness.",50,90
"This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.","The review starts by summarizing the paper's approach and acknowledges the strengths of the proposed model, particularly its performance on different datasets. However, it also points out that the work is incremental and lacks innovation, suggesting that a more in-depth qualitative evaluation would be beneficial. The sentiment is somewhat mixed, recognizing positive aspects but also highlighting a significant limitation. The language used is formal and constructive, without any harsh or disrespectful wording, indicating a polite tone.",25,75
"Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.","The reviewer's sentiment appears to be quite positive, as indicated by phrases such as 'a pleasure to read' and 'offers a novel approach,' which suggest that the reviewer appreciates the work. The reviewer also acknowledges the paper's 'clear and interesting conclusions' and describes it as a 'significant step forward.' However, the reviewer does note that the novelty is somewhat diminished by concurrent work, which slightly tempers the positivity. The language used is very polite and academic, with no signs of rudeness or disrespect. The reviewer uses constructive language and phrases their questions and comments in a respectful manner.",80,90
"This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.","The reviewer's sentiment appears to be cautiously positive. They acknowledge the paper's approach to a known challenge and practical benefits, and they find the results compelling, although they admit to not being an expert in the field. The language used is constructive and respectful, with suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'As far as I can tell' and 'I'm happy to support another reviewer,' which indicate a positive but reserved stance. The language is polite and focused on providing constructive feedback, with no signs of rudeness or disrespect.",60,90
"I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the paper's strengths in using multiple attentional vectors and demonstrating consistent gains across tasks. However, the reviewer also expresses a desire for more analysis and transparency, particularly regarding the 2D representations and model sizes. The language used is constructive and focuses on areas for improvement without being dismissive or overly critical. The use of polite language such as 'I'd like to see' and 'I suspect' indicates a respectful tone, and the reviewer provides specific recommendations for improvement. The minor comments about typos and missing citations are also presented in a helpful manner, contributing to the overall politeness of the review.",30,80
"This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: *The capturing the similarities* -- typo in line 2 of intro. *Recently, (Wieting et al.,2016) learned* -- use citet instead of parenthesized citation","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'good' and acknowledge the 'interesting probabilistic motivation' for the model. However, there is a slight negative sentiment when they mention that 'it is sad that for sufficiently large datasets, NB-SVM still works better,' indicating a limitation of the work. The language used is polite and constructive, with suggestions for improvement and a mention of a typo and citation style, but without any harsh or rude comments.",60,80
"The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question*s sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary. This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs. The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.","The reviewer's sentiment appears to be largely positive, as they describe the paper as 'refreshing' and appreciate the clear hypothesis and testing approach. The reviewer also acknowledges the paper's potential influence on future design principles of RNNs. However, there is a slight critique regarding the scope of experiments, suggesting that the sentiment is not entirely positive. The language used is polite and constructive, with no negative or rude words used, and the critique is presented in a respectful manner.",70,90
"This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion.","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'interesting', 'well-written', and 'easy to follow'. They acknowledge the paper's contribution and note that the proposed method is competitive with other methods. The use of 'totally' and 'still suitable to be accepted' also conveys a positive sentiment. However, they do mention a drawback, which slightly moderates the overall positive sentiment. The language used is polite and constructive, with no negative or rude language present. The reviewer provides their opinion in a respectful manner, which is typical for academic discourse.",70,90
"This paper has two main contributions: (1) Applying adversarial training to imagenet, a larger dataset than previously considered (2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution. This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.","The reviewer's sentiment is positive, as indicated by the use of phrases like 'important contribution', 'clear, well written', and 'does a good job'. There is no negative language or criticism in the first paragraph, and the reviewer expresses a belief that the paper is suitable for the venue. The language used is polite and professional, with a focus on the paper's strengths and contributions. The reviewer uses respectful language and provides a positive assessment without any harsh or rude comments.",90,90
"The authors propose to apply virtual adversarial training to semi-supervised classification. It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world. In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP). Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup). Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.","The reviewer's sentiment appears neutral to slightly critical, as they acknowledge the paper reads well and the algorithm works in practice, but also point out significant gaps in the literature review and comparison with existing work. There is no overt praise or enthusiasm, but neither is there harsh criticism. The language used is formal and constructive, without any personal attacks or overly harsh wording, indicating a polite tone.",0,80
This is a parallel work with BiGAN. The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result.,"The sentiment expressed in the review is somewhat positive, as the reviewer describes the approach as 'promising' based on the 'reported result.' However, the brevity and lack of enthusiastic language moderate the positivity. The language used is straightforward and lacks any negative or impolite connotations, suggesting a neutral to slightly formal tone.",50,70
"This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. In particular, experiments are shown in a synthetic denoising task as well as in machine translation. I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different? If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I*d like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline. The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi* with interpolation, penalize the variance, reducing the value of rare actions, etc. Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. It seems that the authors need more work to make the model work without so many heuristics. The authors also mentioned several optimization difficulties (some of which are non-intuitive), 1) why does the critic assign very high value to actions with very low probability according to the actor? 2) why is a lower square error on Q resulting in much worst performance? The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework. Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this? The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. I*ll revise my score if the authors address my questions. In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.","The reviewer expresses a positive sentiment towards the idea of the paper but is critical of the experimental evaluation, use of heuristics, and clarity of the manuscript. The language used is polite and constructive, with suggestions for improvement rather than direct negative criticism. The reviewer's tone is of someone who is engaged and interested in the work but requires more convincing evidence and clarity.",10,70
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise. The original Bell & Sejnowski infomax framework only considered the no noise case. Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods. This seems like an interesting and potentially more general approach to unsupervised learning. However the paper is quite long and it was difficult for me to follow all the twists and turns. For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going. *Hierarchical* is probably not the right terminology here because it*s not like a deep net hierarchy, it*s just decomposing the tuning curve function into different parts. I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document. Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related. They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.","The reviewer's sentiment appears to be mildly positive with some constructive criticism. They describe the approach as 'interesting and potentially more general' which indicates a positive view of the work's potential. However, they also express difficulty in following the paper and suggest improvements, which indicates a mix of positive and negative sentiments. The language used is polite and constructive, with recommendations given in a helpful manner rather than a dismissive or rude tone. The use of phrases like 'I would recommend' and 'the authors should look at' are indicative of a polite and constructive review.",30,80
"In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably. One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates. Pros: The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work. The experiments are good proofs of concept, but do not go beyond that i.m.h.o. Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning). Cons: As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell. The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016). Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task. Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.","The sentiment of the review appears to be mixed, with the reviewer acknowledging the interesting direction and potential of the work, but also pointing out significant limitations and areas that were not addressed. The reviewer provides a balanced view by listing both pros and cons, indicating that while the paper has merits, it also has shortcomings that need to be addressed. The language used is formal and constructive, with no personal attacks or overly harsh criticism, which suggests a polite tone. The reviewer uses phrases like 'regrettably', 'i.m.h.o.', and 'perhaps underwhelming' to express disappointment or disagreement without being offensive.",25,75
"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don*t discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads *log p(topic proportions)* which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn*t the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not *error*; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.","The reviewer's sentiment appears to be cautiously optimistic about the paper's direction and the potential of NVI for LDA, which suggests a positive sentiment. However, the reviewer also expresses concerns about the experimental results, the lack of discussion on hyper-parameter selection, and the clarity of certain sections, which tempers the positivity. The language used is constructive and professional, with suggestions for improvement and clarification, indicating a polite tone.",50,80
"EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score. This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM*s predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers. Comments: - Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it*s not clear at that point that the answer is an entity within the document. - 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?) - 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction? - how does performance of the pattern matching change with different cutoff constant values? - 5.2: are there questions whose answers are not entities? - how could the proposed approach be used when predictions aren*t made at every word? is there any extension for, say, sentence-level sentiment classification?","The reviewer's sentiment appears to be cautiously optimistic and constructive. They acknowledge the thorough revisions and the paper's improved clarity, recommending acceptance and increasing their score. The reviewer appreciates the motivation behind the paper and is surprised by the high performance of the pattern matching. However, they also point out areas that need clarification, indicating a balanced view that is neither overly positive nor negative. The language used is polite and professional, with the reviewer providing clear and specific feedback without any harsh or disrespectful wording. The use of phrases like 'I really like' and 'I recommend' contribute to a positive tone, while the constructive criticism is framed respectfully.",50,90
"This is a solid paper with good results. However, there aren*t many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing. The second paragraph of the introduction is very confusing. It*s clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn*t familiar with similar approaches. The authors keep referring to *previously popular attention paradigms* without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches *summarize each modality into a single vector.* That*s one of the most incorrect descriptions I*ve yet seen for attention mechanisms. First, I don*t know what model works over several modalities in a single attention pass. Maybe the authors don*t know what a *modality* is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper*s supposedly new way of using attention is pretty much exactly the standard way. Both modeling and modelling spellings are in the text. I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a *modeling layer (RNN)*... It*s just an RNN, you don*t need to give an RNN another name, especially one that*s as nondescript as *modeling layer* all layers are part of a model? Typo: *let*s the modeling (RNN) layer to learn* This paragraph is supposed to give an overview of the model but just confuses readers. I would delete it. *Phrase embedding layer* -- terrible word choice as you are not embedding phrases here. It*s a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to *contextual word embedding layer* or something less incorrect. Your phrase layer embeddings only show single words, as expected in Table 2. Section 2: point 4. Second sentence needs citations for *popular* Typo: *from both *of* the context and query word* Typo: *aveaged* It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn*t quite accurate. I*d say you*re changing one module or part of your model. Section 4: attention isn*t countable (no *a* in front of *huge attention*). Also, academic writing usually doesn*t include such adjectives in the first place.","The reviewer's sentiment appears to be mixed, starting with a mild compliment ('solid paper with good results') but quickly moving to criticism about the lack of interesting takeaways and issues with writing and explanations. The language used to describe the issues is direct and could be perceived as harsh, particularly with phrases like 'one of the most incorrect descriptions I've yet seen' and the suggestion to 'delete' a paragraph. However, the reviewer does not use insulting language or personal attacks, maintaining a professional tone throughout. The reviewer's directness in pointing out flaws is balanced by the initial positive comment.",-20,40
"This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data. I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature. Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 *Finally, let*s whose discriminative power*. I*m not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's contribution to the literature while also pointing out practical issues and areas for improvement. The reviewer appreciates the 'elegant mathematical exposition' which indicates a positive sentiment towards the theoretical aspects of the work. However, the concerns about scalability and accuracy, as well as the critique of the writing and title, temper the overall sentiment. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides clear and specific feedback, which is a sign of politeness in academic discourse.",50,80
"This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular. Pros: * This paper addresses an important question I and many others would have liked to know the answer to but didn*t have the computational resources to thoroughly attack it. This is a nice use of Google*s resources to help the community. * The work appears to have been done carefully so that the results can be believed. * The basic answer arrived at (that, in the *typical training environment* LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful. Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper. * The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work. In sum, they*re much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity. * The point about the near-equivalence of capacity at equal numbers of parameters is very useful. * The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures. * The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it*s a practical problem that everyone working with these networks has but often isn*t addressed. * The paper text is very clearly written. Cons: * The work on the UGRNNs and the +RNNs seems a bit preliminary. I don*t think that the authors have clearly shown that the +RNN should be *recommended* with the same generality as the GRU. I*d at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel). In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important. I don*t really mind having them in the paper though. I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don*t know what the ICLR area chairs will think. * The paper gives short shrift to the details of the HP algorithm itself. They do say: *Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs* and give some good references, but I expect that actually trying to replicate this involves a lot of missing details. * I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness. * The neuroscience reference (*4.7 bits per synapse*) seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained. I guess it*s just in the discussion, but it seems gratuitous. Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be *in agreement* here between computational architectures and neuroscience, but perhaps they could say something like -- *We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.*)","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the paper's question, the careful work done, and the practical utility of the findings. The reviewer also appreciates the clarity of the paper and the insights it provides into the trainability and capacity of neural network architectures. However, there are some criticisms, such as the preliminary nature of work on certain architectures and the lack of detail in the hyperparameter (HP) algorithm, which prevent the sentiment from being entirely positive. The language used is polite and constructive, with the reviewer providing specific points of praise and criticism without any harsh or rude language. The reviewer's tone is respectful and aimed at improving the work, which is indicative of a high level of politeness.",70,90
"This paper introduces a continuous relaxation of categorical distribution, namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients. The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models. The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments. The paper is well-written, and I applaud the details provided in the appendix. The main application seems to be semi-supervised situations where you really want categorical variables. - P1: *differentiable sampling mechanism for softmax*. *sampling* => *approximate sampling*, since it*s technically sampling from the Gumbal-softmax. - P3: *backpropagtion* - Section 4.1: Interesting experiments. - It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported. - It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.","The reviewer's sentiment appears to be generally positive, as they describe the main contribution as 'simple yet nontrivial and worth publishing' and acknowledge that the method 'seems effective in experiments.' Additionally, the reviewer compliments the paper as 'well-written' and applauds the details in the appendix. However, there are constructive criticisms and suggestions for improvement, which are typical in peer reviews and do not significantly detract from the overall positive sentiment. The language used is formal and constructive, with no signs of rudeness or impoliteness. The reviewer uses phrases like 'It would be interesting to report...' and 'It would be nice to give more details...' which are polite ways of suggesting improvements.",70,90
"The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class?","The sentiment of the review appears to be cautiously positive. The reviewer acknowledges the simplicity and effectiveness of the proposed method, which suggests a positive sentiment. However, they also point out a significant limitation and ask for further clarification, which indicates a balanced view rather than outright enthusiasm. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer poses questions and provides feedback in a professional manner, which is typical for academic discourse.",50,90
"This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.","The reviewer's sentiment is quite positive, as they describe the paper as 'very nicely written' and state that the theoretical results are 'new and insightful.' They also mention that the paper's contributions will likely be useful in the RL field and that the empirical section is well explained. The minor comment about the stationary distribution does not detract from the overall positive sentiment. The language used is polite and constructive, with phrases like 'This being said' to introduce a mild critique without diminishing the overall positive feedback.",90,90
"Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.","The sentiment of the review is highly positive, as indicated by the use of phrases such as 'great exploration', 'convincingly demonstrates', 'consistent improvements', and 'very simple', which suggests that the reviewer appreciates the work and finds it valuable. The language used is polite and constructive, with no negative or harsh words, and includes positive reinforcement for the authors' work.",90,90
"This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling. Comments: - p. 3 “can be seen has a way” -> “can be seen as a way” (?) - p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts? - p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations. - p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.","The sentiment of the review appears to be mildly positive with some reservations. The reviewer acknowledges that the paper is 'clearly written' and that the techniques 'improve the performance in various tasks,' which indicates a positive sentiment. However, the reviewer also expresses a concern that the method is 'too simple and sounds ad hoc,' suggesting that there is a lack of theoretical justification. This concern tempers the overall positive sentiment. The language used in the review is polite and constructive. The reviewer provides specific recommendations and questions for improvement without using harsh or negative language. The use of 'May main concern' instead of a more direct criticism indicates a polite tone, and the suggestions are framed as questions or recommendations rather than demands.",40,80
"This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets. The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction. While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset. The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question. Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.","The reviewer's sentiment appears to be cautiously positive, acknowledging the interesting approach of the paper but also pointing out its incremental novelty and limited impact. The reviewer praises the work as well-executed and the idea as interesting, which suggests a positive sentiment. However, they also express concerns about the novelty and broader impact, which tempers the positivity. The language used is formal and constructive, with no signs of rudeness or disrespect, indicating a high level of politeness.",30,90
"The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it*s a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB. The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model *deeper*, whereas adding linear bias terms isn*t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). I think that this paper is more proper for NLP conferences.","The reviewer begins with positive remarks about the paper being 'very well written' with 'impressive experimental results and analysis.' This suggests a positive sentiment. However, the reviewer quickly shifts to criticism, stating that the idea is 'hardly novel' and that certain aspects of the paper are 'trivial' or not 'confirmed to work.' This indicates a mix of positive and negative sentiments. The language used is formal and constructive, without any personal attacks or harsh words, indicating a high level of politeness.",20,80
"The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.","The reviewer's sentiment is highly positive, as indicated by the use of superlatives such as 'extremely well written' and 'thoroughly analyses'. The reviewer also compliments the mathematical analysis and the inclusion of numerical experiments, which suggests a high level of appreciation for the paper's content. The language used is formal, respectful, and contains no negative connotations, indicating a high level of politeness.",90,90
"Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.","The sentiment in the review appears to be mildly positive as the reviewer acknowledges the clear benefits of the approach for specific tasks and the implementation of TensorFlow Fold. However, the sentiment is not highly positive because the reviewer also points out a missing comparison which they believe is necessary for a higher score. The language used is polite and constructive, focusing on the content of the work rather than personal remarks, and it provides specific feedback without any harsh or rude words.",50,80
"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning. The framework is nice and appealing. However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning* for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.","The reviewer begins with a positive note, acknowledging the 'nice framework' and describing it as 'appealing,' which suggests a positive sentiment. However, the reviewer quickly transitions to a critical tone, questioning the necessity of deep learning for the simple datasets used and the lack of comparison with existing shallow multi-task learning methods. The critique is constructive but points out significant shortcomings in the paper. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite approach to criticism.",20,80
"An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. *Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).* It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.","The sentiment in the review appears to be constructive and somewhat positive, as the reviewer begins by describing the architecture as 'interesting' and acknowledges the helpfulness of the video generated. However, the reviewer also points out areas where the model underperforms, which introduces a critical tone. The language used is polite and professional, with suggestions and criticisms framed in a respectful manner. There is no use of rude or aggressive language, and the reviewer provides clear feedback without personal attacks.",30,80
"The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF. Presentation: * Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter. * I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example. It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. Experiments: * Why is the run time not reported in table 1? * What are the *difficulties around convergence* encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? * Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. * How difficult is it to get the inference to work (eg HMC) when we don*t have full control over the computational graph structure and sampler? * It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models. * What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example? Minor comments: * Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I*m not sure why alpha=-1 was chosen here. * How do you handle discrete distributions (eg Figure 5)? * x_real is not defined in Figure 7. * I would suggest highlighting M in Figure 8. * Comma instead of period after *rized), In* on page 8. In conclusion I would say that the software developments presented here are quite exciting, and I*m glad the authors are pushing towards practical and accessible *inference for all*. In its current form though I am forced to give the submission itself a score of 5.","The sentiment of the review is generally positive towards the software's potential but critical of the current demonstration of its practicality. The reviewer appreciates the promising aspects of the software and its composability principles but emphasizes the need for empirical demonstrations of its real-world use cases. The language used is constructive and professional, with specific recommendations for improvement, which indicates a polite tone. The reviewer's final score of 5 (on an unspecified scale) suggests a neutral-to-positive sentiment overall, as it implies neither a strong endorsement nor a rejection.",50,80
"This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn*t discuss how priors over the beta*s can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity. My compliments for the amount of detail put in the paper and appendix. The experiments are on toy examples, but show promise. - Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized). - Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve. - Eq 9: “This interpretation implies the factorization of the recognition model:..” The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)","The sentiment of the review appears to be mildly positive. The reviewer describes the paper as 'well-written' and an 'interesting application,' which suggests a positive view. However, they also note that the paper could have been more compact and clear, and they point out specific areas that seem trivial or could be improved. This mix of praise and constructive criticism suggests a sentiment that is positive but with reservations. The language used in the review is polite and professional. The reviewer offers compliments and frames their criticisms in a constructive manner, using phrases like 'missed opportunity' and 'it's probably better to clarify,' which are suggestive of a desire to help improve the work rather than to disparage it.",40,80
"Update: raised the score, because I think the arguments about adversarial examples are compelling. I think that the paper convincingly proves that this method acts as a decent regularizer, but I*m not convinced that it*s a competitive regularizer. For example, I don*t believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc. I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply). ---- Summary: If I understand correctly, this paper proposes to take the *bottleneck* term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective. The argument is that this is an effective regularizer and increases robustness to adversarial attacks. Pros: -The presentation is quite good and the paper is easy to follow. -The idea is reasonable and the relationship to previous work is well described. -The robustness to adversarial examples experiment seems convincing, though I*m not an expert in this area. Is there any way to compare to an external quantitative baseline on robustness to adversarial examples? This would help a lot, since I*m not sure how the method here compares with other regularizers in terms of combatting adversarial examples. For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)? Cons: -MNIST accuracy results don*t seem very strong, unless I*m missing something. The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%. So the 1.13% error rate listed here doesn*t necessarily prove that the method is a competitive regularizer. I also suspect that tuning this method to make it work well is harder than other regularizers like dropout. -There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z. For example, the output could directly follow z, or there could be several layers between z and the output. As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it*s not obvious why this choice was made. Did it work best empirically? Other: -I wonder what would happen if you *trained against* the discovered adversarial examples while also using the method from this paper. Would it learn to have a higher variance p(z | x) when presented with an adversarial example?","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's good presentation and reasonable idea, as well as the convincing experiment on robustness to adversarial examples. However, the reviewer also expresses skepticism about the method being a competitive regularizer compared to others like dropout, and points out potential difficulties in tuning the method and some weaker results. The language used is formal and constructive, with no signs of rudeness or personal attacks, indicating a high level of politeness.",30,90
"This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain! The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code. I see this paper as being above the threshold for acceptance.","The reviewer's language is positive, acknowledging the paper as 'good' and 'well written,' and noting the 'solid gain' from the experiments. The reviewer also expresses optimism about the approach used in the paper, suggesting it is 'a good starting point' and 'a right approach.' Additionally, the reviewer concludes with a positive endorsement, stating the paper is 'above the threshold for acceptance.' The language used is polite and constructive, with no negative or rude words used. The reviewer provides encouragement and constructive criticism without any harsh language.",80,90
"This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks. Pros: The use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users. The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice. The ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited. DeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures. Cons: There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow). This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API. (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.) The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax. This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years -- see a ~15 line definition of AlexNet [1]. Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format. DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework. Some examples: “This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.” This separation is not unique -- it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there). “The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.” There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL. I asked about this statement, and the response didn’t convince me otherwise. The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution. This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2]. It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this. Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise. [1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24 [2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92 ===================== Update: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review. I*ve upgraded my rating from 5 (below threshold) to 7 (good paper, accept).","The sentiment of the review is mostly neutral with a slight lean towards negative due to the critical analysis and the identification of several cons. However, the reviewer acknowledges the value of the work and upgrades their rating after revisions, indicating a positive shift. The language used is formal and constructive, with no personal attacks or overly harsh criticism, reflecting a high level of politeness.",-10,90
"The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would*ve been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.","The reviewer's sentiment appears cautiously optimistic and constructive, indicating a leaning towards acceptance of the paper. The language used is professional and offers constructive criticism without any harsh or negative wording. The reviewer acknowledges the effort and potential impact of the study while also providing clear and specific recommendations for improvement. This balance of constructive feedback with a positive outlook on the paper's potential contributes to a moderately positive sentiment and a high level of politeness.",50,90
"This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. The paper nicely presents this core idea and a way to achieve this - by choosing special *routings* between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration. In summary - the paper is nicely written, results are quite good and the model is interesting - I*m happy to recommend acceptance.","The reviewer's sentiment is quite positive, as indicated by the use of phrases like 'clever way', 'nicely presents', 'quite convincing', 'quite good', and 'happy to recommend acceptance'. There is no negative language or criticism in the text, suggesting a high level of satisfaction with the paper. The language used is formal and respectful, with no signs of rudeness or impoliteness. Phrases like 'I would be very happy to see' and 'In summary - the paper is nicely written' contribute to the overall polite tone.",90,90
"This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.","The reviewer's sentiment appears to be cautiously optimistic. They acknowledge the novelty and promise of the approach, which indicates a positive sentiment towards the work. However, they also point out limitations and express uncertainty about the method's scalability, which tempers the overall sentiment. The language used is formal and constructive, without any harsh or disrespectful wording, indicating a high level of politeness.",50,90
"As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston*16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.","The reviewer's sentiment appears to be generally positive, acknowledging the impressive nature of the system learning from natural feedback and the new quality of results achieved. The reviewer also notes the significance of the contribution and its potential impact. However, there are constructive criticisms and suggestions for improvement, indicating that the review is not entirely positive. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides clear and specific recommendations, which suggests a polite and professional tone.",70,90
"The paper presents an application of deep learning to genomic SNP data with a comparison of possible approaches for dealing with the very high data dimensionality. The approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches. The presentation would benefit from more precise math. Quality: The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable. The experimental evaluation is rather weak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper. Clarity: The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely what was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the *raw end2end* model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used. The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ... Originality: The application and the approach appear quite novel. Significance: There is clearly strong interest for deep learning in the genomics area and the paper seeks to address some of the major bottlenecks here. It is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on. Other comments: I think releasing the code as promised would be a must.","The reviewer's sentiment appears constructive and moderately positive, acknowledging the interesting aspects of the paper while pointing out the limitations in the experimental evaluation. The language used is professional and offers constructive criticism without any harsh or rude remarks, suggesting a polite tone.",40,80
"This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition! The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?) I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.) Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive. Minor: the second paragraph of Section 3 needs a reference to Appendix A.","The reviewer's sentiment appears to be quite positive, as they commend the paper for being well written, thorough, and precise. They express appreciation for the detailed appendices and the care taken in the exposition, which is not always common in the field. The reviewer also acknowledges the strong log-likelihood results of the paper. However, there are concerns raised about the variance of the gradients and the sequential operations needed for inference, suggesting areas for improvement or clarification. Overall, the positive remarks outweigh the concerns, indicating a generally favorable view of the paper. The language used in the review is polite and constructive. The reviewer uses phrases like 'I wish more papers in our field would take this degree of care' and 'I think the response is convincing,' which are respectful and show a positive engagement with the authors' work. The concerns are presented as constructive criticism rather than blunt or harsh feedback.",70,90
"In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD. The main contributions of the paper are: 1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating. 2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers. 3) Empirically illustrate the performance of the method, and show: - Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10) - K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet) - Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet) - Favorable scaling properties of K-FAC with mini-batch size Pros: - Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. - Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN). Cons: - “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point. - Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective. The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. Typos: - “updates that accounts for” — “updates that account for” - “Kronecker product of their inverse” — “Kronecker product of their inverses” - “where P is distribution over” — “where P is the distribution over” - “back-propagated loss derivativesas” — “back-propagated loss derivatives as” - “inverse of the Fisher” — “inverse of the Fisher Information matrix” - “which amounts of several matrix” — “which amounts to several matrix” - “The diagram illustrate the distributed” — “The diagram illustrates the distributed” - “Gradient workers computes” — “Gradient workers compute” - “Stat workers computes” — “Stat workers compute” - “occasionally and uses stale values” — “occasionally and using stale values” - “The factors of rank-1 approximations” — “The factors of the rank-1 approximations” - “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.” - “Psi is captures” — “Psi captures” - “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices” - “which is a nested applications of the reshape” — “which is a nested application of the reshape” - “provides a computational feasible alternative” — “provides a computationally feasible alternative” - “according the geometric mean” — “according to the geometric mean” - “analogous to shrink” — “analogous to shrinking” - “applied to existing model-specification code” — “applied to the existing model-specification code” - “: that the alternative parametrization” — “: the alternative parameterization” Minor Issues: - In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example: 1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) 2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231) 3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1) 4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html) 5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl) - Page 2, equation s = WA, is there a dimension issue in this expression? - x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots. - “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.","The sentiment of the review appears to be generally positive with constructive criticism. The reviewer acknowledges the main contributions and the interesting ideas presented in the paper, as well as the clear writing and well-thought-out experiments. However, the reviewer also points out some potential issues and areas that were not addressed, which indicates a balanced view. The language used is polite and professional, with the reviewer providing specific feedback and suggestions for improvement without any harsh or rude language. The minor typos and grammatical errors are pointed out in a helpful manner, and the reviewer also suggests additional references that could be included. Overall, the review is constructive, with a tone that is aimed at improving the paper.",50,90
"Description. This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs. The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. Strong points. - The experiments are carefully done with thorough selection of hyperparameters. - The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014). - The paper is well and clearly written. Weak points: - CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar? Originality: - This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights. Quality: - The experiments are well done. Clarity: - The paper is well written and clear. Significance: - The results go against some of the conclusions from previous work, so should be published and discussed. Overall: Experimental paper with interesting results. Well written. Solid experiments.","The reviewer's sentiment appears to be generally positive, as they acknowledge the careful execution of experiments, the clarity of the paper, and the significance of the results that challenge previous work. No negative language or criticism is directed at the authors; instead, constructive feedback is provided. The language used is formal, constructive, and respectful, indicating a high level of politeness. The reviewer uses phrases like 'well and clearly written,' 'interesting results,' and 'solid experiments,' which contribute to a positive sentiment. The suggestions for improvement are presented as an opportunity for further research rather than as a flaw.",70,90
"summary The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model. The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights). This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap. Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning. Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally) The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout. The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable. The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced. p6 line 8 typo: expecatation","The review provides a balanced assessment of the paper. It acknowledges the interesting theoretical model and the potential of the method to perform better than standard dropout, which indicates a positive sentiment towards the work. However, it also points out limitations such as the simplicity of the datasets used, the relatively small gains, and the increased computational cost, which tempers the positivity. The language used is formal and constructive, with no rude or disparaging remarks, indicating a high level of politeness.",50,90
"Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model*s performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.","The sentiment of the review is quite positive, as the reviewer acknowledges the novelty and state-of-the-art performance of the model, the clear description, and the strengths of the paper in detail. The reviewer's preliminary evaluation is a 'clear accept,' which indicates strong support for the paper. The language used in the review is formal and constructive, with no negative or rude remarks, indicating a high level of politeness. The reviewer provides specific recommendations and questions for future work, which is typical of a constructive and polite peer review.",85,95
"This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting. That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations. <rant Finally a stylistic comment that the authors can feel free to ignore. I don*t like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name. Especially here when the idea is so simple. Why not use words? English words from the dictionary. Instead of *EPOpt* and *EPOpt-e*, you can write *ensemble training* and *robust ensemble training*. Is that not clearer? />","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's well-written nature and the interesting questions it poses, but also pointing out its limitations regarding the scope of results. The reviewer's anticipation for a journal version with broader experiments indicates a positive outlook. The reviewer's stylistic comment, while critical, is framed as a personal preference and not a harsh criticism. The language used is formal and constructive, with no personal attacks or harsh language, indicating a polite tone.",50,70
"The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call *monotonic chains of linear segments*, which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very *compatible*. In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the *bound for this case is very loose*. The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio*s group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using *the difference between the ground truth distance ... and the distance computed by the network* seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?","The sentiment of the review is critical but constructive. The reviewer acknowledges the novelty of the results but also points out that they are not surprising given the compatibility of the chosen network architecture and data structure. The reviewer raises significant concerns about the relevance of prior work, generalizability, and practicality of the error analysis. Despite the critical nature, the reviewer's tone remains professional and focused on the content, suggesting improvements and further research. Therefore, the sentiment is slightly negative due to the critical content but not entirely dismissive, and the language is polite and academic.",-30,70
"This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3. The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis? While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it*s an interesting algorithm, nicely composed and very efficient, so it*s reasonable to assume that other readers might have use for some of the ideas presented here.","The reviewer's sentiment appears to be cautiously positive. They acknowledge the novelty of the combination of methods and appreciate the detailed analysis in section 3, which suggests a positive sentiment. However, they also express disappointment with the empirical evaluation of the paper, indicating some reservations. The language used is formal and constructive, with no signs of rudeness or strong negative language, which suggests a high level of politeness.",40,80
"This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina. The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim. The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy. Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective. Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim. Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the paper's argument but points out several areas for improvement, questioning the model's effectiveness and suggesting comparisons with other models. The language used is professional and offers a critique without being dismissive or harsh, indicating a neutral to slightly negative sentiment. The politeness is high, as the reviewer's language is respectful and focuses on the content of the paper rather than personal comments about the authors.",-20,80
"This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function. The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages. I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.","The review provides a comparison between the paper in question and a related work, acknowledging the uniqueness of the empirical results and the theoretical soundness. The reviewer's tone is constructive and provides a suggestion to include related work, which indicates a positive sentiment towards the paper. There is no use of negative language or criticism that diminishes the work, hence the sentiment is not neutral or negative. The language used is formal and appropriate for a scientific peer review, suggesting a high level of politeness without any harsh or rude language.",70,90
"Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch. Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It*s not clear to me how to reconcile those two claims. (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD. (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used. (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks. (5) It*s not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It*s a little mysterious in the current version of the paper.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the paper's strengths, such as the well-written presentation and promising mathematical arguments, but also points out significant weaknesses and areas that require further clarification or evidence. The sentiment is not overtly negative as the reviewer seems to encourage improvement rather than dismissing the work, hence a slightly negative score is appropriate. The language used is formal and respectful, with the reviewer providing clear and specific feedback without any personal attacks or harsh language. The use of phrases like 'promising way forward,' 'generally very well written,' and 'helpful and interesting' contribute to the polite tone of the review.",-20,80
"The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem. The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this. There are a few seemingly arbitrary choices that are justified only by *it worked in practice*. For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: *Also we neglected the argmax operation that chooses the actions*. I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as *We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important* and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper? The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(	ilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: *because we neglected that a single u is sampled for an entire episode*; actually, no, you did mention this in the text above and it*s clear from the pseudo-code too. *perturbated* -> *perturbed* --- After response period: No rebuttal entered, therefore review remains unchanged.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the clear contributions and wins of the paper, but also points out areas for improvement and clarification. The language used is constructive and focuses on the content rather than the authors, which suggests a positive sentiment towards the work. The reviewer's suggestions for additional text and clarification indicate a belief in the potential of the paper, which contributes to a positive sentiment. The politeness of the language is high, as the reviewer uses academic and respectful language throughout the review. Phrases like 'it would have been nice to see' and 'leaves the reader wondering why' are polite ways of suggesting improvements without being confrontational.",60,90
"The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance. The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network. Overall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that. Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets. Improving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing. Pros: The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance. The experiments are done using a state-of-the-art system for a practical application. Cons: The proposed method is very similar to earlier work and barely novel. There is no comparison with other pruning methods. The data is private and this prevents others from replicating the results. [1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34. [2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.","The reviewer's sentiment appears to be cautiously positive, acknowledging the substantial computational gains and successful application of the method to RNNs, while also pointing out the lack of novelty and comparisons with other methods. The language used is formal and constructive, providing both praise and criticism without any harsh or rude wording. The reviewer expresses interest in the method's application to large networks and its potential for computational savings, which contributes to a positive sentiment. However, the concerns about the lack of novelty and comparisons, as well as the use of private datasets, temper the overall sentiment, preventing it from being entirely positive.",40,80
"Authors propose a mechanism for selecting the design of filters in convolutional layers. The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events. For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected. Authors argue that square filters are therefore appropriate in images. However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions. Such an operation can thus be thought of computing data-dependent dilated convolutions. Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case). The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize. While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling). Also, the theory is not directly applicable to choosing filters when number of layers are more than 1. I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing. The method of choosing the filter shape can be briefly summarized as: (a) The covariance matrix of the input features is computed. (b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7). A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15). This leads to choice of a single design for all filters in the layer. (c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance. The filter design of the next layer attempts to minimize this residual variance. This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers. Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer. However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement. Authors instead use the method described in (c). A question which comes to my mind is – a single feature design is chosen for each layer. Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters? Regarding baselines: B1. It would be great to see a comparison with randomly chosen filter designs. Two comparisons could be made – (1a) A single random design is chosen for each layer. (1b) The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer). B2. Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers. A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square. B3. Authors mention the use L1 regularization in the baselines. Was the L1 penalty cross-validated? If so, then upto what range? Somethings which are unclear: - “exclude data that represent obvious noise” - DFMax mentioned in the supplementary materials Overall I think this is very interesting idea for filter design. The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3. I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.","The reviewer's sentiment appears to be cautiously optimistic but critical, acknowledging the interesting idea and convincing empirical evidence while also pointing out the lack of rigor in some theoretical arguments and the need for further experiments. The sentiment is not overtly positive or negative, but it leans towards the positive side due to the willingness to reconsider the rating if additional information is provided. The language used is formal, constructive, and respectful, with suggestions for improvement and questions posed in a professional manner. There is no use of rude or aggressive language, and the reviewer maintains a tone of scientific inquiry throughout.",30,80
"The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs. The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated. The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly. It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work. One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly. Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the *base learning rate* in Section 3 is the start or end rate of the annealing schedule; typos: *punning* (p.4), *spares* (p.5).","The reviewer's sentiment appears to be quite positive, as they appreciate the availability of source code and find the second contribution interesting. They also commend the authors for being methodical and thorough in their evaluation. The suggestion for future work is presented as a positive opportunity. The critique provided is constructive, focusing on areas for improvement rather than dismissing the work. The language used is polite and respectful, with no harsh or negative words. The reviewer provides specific, actionable feedback, which is a sign of a constructive and polite review.",80,90
"The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content. Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in *Improving Distributional Similarity with Lessons Learned from Word Embeddings* by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important. While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding*s inequality). The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is *Generating Text with Recurrent Neural Networks* by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as *debagging.* Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.","The reviewer's sentiment appears to be quite positive, as they describe the experiments as 'thorough' and providing 'interesting insights.' They also acknowledge the importance of the research area and recommend the paper for publication, which indicates a positive view of the work. However, they do mention that the evaluations are 'quite simple' and may not correlate with real-world needs, which slightly tempers the positivity. The language used is formal and respectful, with no negative or rude language, indicating a high level of politeness.",80,90
"Looking through the comment section here, I agree to a large degree with the author*s standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don*t think those contributions are groundbreaking, I believe they are significant enough to merit acceptance. The reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments *the experimental evaluation is not convincing, e.g. no improvement on SVHN* or *the effect of drop-path seems to vanish with data augmentation* below. I believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance. Further, I believe the trend to focus excessively on performance is problematic for a number of reasons: - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar. - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true? - How does one even draw a *fair comparison* on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What*s worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline *against which the improvements can be clearly demonstrated by making isolated changes* seems unrealistic to me. - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to *training on the test set*, but also wastes the time of researcher that could be better spent exploring new ideas. - It gives too much power to bad research. In science, there is always a certain background rate of *bad* results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What*s worse, these *bad* results are far more likely to hold the SOTA title at any given time than a *good* result. By requiring new publications to beat SOTA, we give too much power to bad results. - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don*t think that was a coincidence. The same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting *ultra-deep* in the paper title seems exaggerated and I would recommend scaling back the language. However, I don*t think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement. In summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark.","The reviewer's sentiment appears to be cautiously positive, acknowledging the contributions of the paper while also noting that they are not groundbreaking. The reviewer emphasizes the importance of scientific contribution over incremental performance improvements, which shows a supportive stance towards the authors' work. The language used is polite and constructive, with the reviewer providing a detailed critique without any harsh or disrespectful wording. The reviewer's suggestions for improvement, such as adding more details in an appendix and reconsidering the use of 'ultra-deep' in the title, are presented in a respectful manner.",60,90
"This paper focusses on attention for neural language modeling and has two major contributions: 1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well. 2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact. The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task. I am convinced with authors’ responses for my pre-review questions. Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.","The reviewer's sentiment appears to be quite positive, as they describe the paper's contributions as 'interesting' and 'novel', and acknowledge the thorough experimental analysis. They also express being convinced by the authors' responses. No negative language or criticism is present in the excerpt provided. The language used is formal and constructive, with a minor comment being made in a polite suggestion format, indicating a high level of politeness.",80,90
"In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term *labeled MDP* to mean the typical MDP framework where the reward function is unknown. They use the confusing term *unlabeled MDP* to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source *labeled* MDP to a target *labeled* MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an *unlabeled* CMP, and the source is both a *labeled* MDP and an *unlabeled* CMP. The basic approach is to use inverse RL to infer the unknown *labels* and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's attempt to explore an important area, but also pointing out the incoherence and limitations in the analogy used and the scope of the work. The reviewer does not dismiss the work outright but suggests that it does not fully achieve a general notion of semi-supervised RL that would interest the broader RL community. The language used is formal and constructive, with no personal attacks or overly harsh criticism, indicating a polite approach to critique.",30,80
"This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning. Starting from the test power, the authors justified the criterion. Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD. Pros: 1) The criterion is principled which is derived from the test power. 2) The criterion can be used to detect the difference template by incorporating ARD technique. 3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN. Cons: 1) How to train the provided t objective is not clear. 2) The algorithm is only tested on MNIST dataset as model criticism and learning objective. Comprehensive empirical comparison to the state-of-the-art criteria, e.g., log-likelihood, and other learning objectives is missing.","The reviewer's sentiment appears to be cautiously positive, acknowledging the interesting idea and principled criterion derived from the test power, as well as the efficient implementation provided. However, the reviewer also points out clear areas for improvement, such as the lack of clarity in training the proposed objective and the limited testing on the MNIST dataset. The language used is formal and constructive, with no negative or rude language present, indicating a high level of politeness.",50,90
"This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect. I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.","The reviewer's sentiment appears to be cautiously optimistic about the work, recognizing the promise in the approach but suggesting further comparisons to strengthen the paper. The language used is constructive and focuses on the potential of the work rather than any shortcomings. The reviewer's tone is not overly enthusiastic but acknowledges the positive aspects, which indicates a sentiment that is slightly positive. As for politeness, the language is formal and respectful, offering criticism in a constructive manner without any harsh or rude language, which is typical for academic peer reviews.",30,80
"On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It*s difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (*MFS*) yields the highest F-measure of 63.5%. The paper*s result with *2 clusters* (with an average of about 1.9) seems to be close to MFS. So I don*t think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with *5 clusters*) has 14.4%. That isn*t very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that*s a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. *SemEval-2010 task 14: Word sense induction & disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. *Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. *Unsupervised word sense disambiguation rivaling supervised methods.* Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the mathematical interest of the model but points out significant shortcomings in the literature review, experimental persuasiveness, and metric selection. The reviewer also suggests alternative approaches and references. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone despite the critical content.",-30,70
"Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: “have not” -> “have no” Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.","The sentiment of the review appears to be mildly positive, as the reviewer acknowledges the insights and contributions of the paper, such as the new model that outperforms the state-of-art and the experimental verification of design choices. However, the reviewer also expresses skepticism and concerns, particularly about the statistical significance of the performance improvement and the comparison with compact bilinear pooling. The language used is polite and constructive, with suggestions for improvement and clarification rather than harsh criticism.",30,80
"This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN?","The reviewer's sentiment appears positive as they describe the method as 'elegantly formulated' and acknowledge its 'state-of-the-art performance on several benchmarks.' Additionally, the paper is described as 'well written,' which further indicates a positive sentiment. There are no negative comments or criticisms in the provided text. The language used is formal and academic, with no indications of rudeness or impoliteness. The question posed at the end is a constructive suggestion, indicating engagement with the paper's content without any negative connotations.",80,90
"Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets. Pros - the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions. - Figure 3 gave a clear visualization for the iterative unrolling view Cons - Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models. - Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.","The sentiment in the review appears to be generally positive, as the reviewer acknowledges the interesting angle, the simplicity and intuitiveness of the iterative unrolling view, and the clear visualization provided by Figure 3. However, the reviewer also points out the lack of empirical results and suggests improvements for Figures 4 and 5, which indicates some constructive criticism. The language used is polite and professional, with no rude or aggressive terms used. The reviewer provides both pros and cons in a balanced manner, which is typical of academic discourse.",60,90
"Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2): - Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow. - Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset. My current rating is a weak reject, but if the authors address my concerns I would change to an accept.","The reviewer's sentiment appears to be cautiously optimistic about the research direction but critical of the paper's clarity and lack of comparison with non-deep learning methods. The reviewer expresses difficulty in understanding some sections and suggests improvements, indicating a mixed sentiment. The language used is constructive and respectful, with suggestions for improvement rather than outright dismissal, which suggests a polite tone.",25,75
"A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics. For example, recognizing animals will likely require different features than categorizing flowers. Using different parameters for different types of examples has the potential to greatly reduce underfitting. This can be seen in recent results with generative models, where image quality is much better for less diverse datasets. However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network). The hypernetworks paper cleverly proposes to get around this problem by adapting different *parameters* for different time steps in recurrent networks and different. The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different *parameters*. In this paper, the *parameters* are modified for different positions based on the output from a hypernetwork which conditions on the time step. Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch. I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases. Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive. The experiments on convolutional networks are less experimentally impressive. I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters. It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation. It may be the case that for convnets, the cases where hypernetworks help are very specific. For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization. While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity. Applications to time series forecasting and modeling could be an interesting area for future work.","The sentiment of the review appears to be quite positive. The reviewer acknowledges the limitations of deep neural networks and recognizes the paper's proposal as a clever solution, expecting it to become standard for training RNNs. The mention of 'SOTA result' (state-of-the-art) is highly positive. However, there is a slight reservation about the experiments on convolutional networks, which prevents the sentiment from being entirely positive. The language used is formal, constructive, and respectful, indicating a high level of politeness. There are no negative or rude words used, and the tone is academic and encouraging.",80,90
"This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible. Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I*m excited to try it. I*m cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.) I would suggest that the authors remove the (incorrect?) claim that this algorithm is *embarrassingly parallel* as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress. As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I*m optimistic that it should carry over, but it*s not trivial). For future work, the authors might be interested in Hutter et al*s work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs.","The reviewer expresses a positive sentiment towards the paper, acknowledging the clarity of the algorithm, the recognition of the problem, and the strength and plausibility of the results. The use of phrases like 'interesting paper', 'excited to try it', and 'cautiously optimistic' indicate a positive reception, although there is a slight disappointment mentioned regarding the overlap with previously published work. The language used is polite and constructive, with suggestions for improvement and references to further work. The reviewer uses phrases such as 'I would suggest' and 'the authors might be interested in', which are polite ways to offer criticism and advice.",70,90
"This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U in R ? 	imes k","The reviewer expresses a very positive sentiment towards the paper, highlighting the significance of the results and the well-written nature of the work. The language used is formal and appreciative, indicating a high level of politeness. There is no use of negative or harsh language, and the reviewer seems to be constructive and supportive in their feedback.",90,90
"This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN. Extra comment: Please add more discussion with EBGAN in next version.","The reviewer's sentiment is positive, as indicated by phrases like 'well written', 'well presented', and 'neat'. There is no negative language or criticism in the first paragraph. The language used is polite and constructive, with suggestions for improvement rather than harsh criticism. Phrases like 'In my view' and 'Please add more discussion' are courteous and show respect for the authors' work.",80,90
"The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines. The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing. The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks. My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.","The reviewer's sentiment appears to be generally positive, as they acknowledge the efficiency, scalability, and practicality of the method presented, as well as its good improvements on language modeling. They also appreciate the simplicity of the approach and its potential usefulness to the community, despite their criticism regarding its simplicity and incrementality. The language used is polite and constructive, with no negative or rude remarks, and the criticism is framed in a respectful manner.",70,90
"This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning. This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation. Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space. -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue. For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014. I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.","The reviewer's sentiment appears to be cautiously positive, acknowledging the promising results of the paper but also pointing out areas that need clarification or improvement. The reviewer's language is constructive and focuses on the potential of the work while suggesting specific areas for enhancement. The politeness is maintained throughout the review, with the reviewer providing criticism in a professional and respectful manner. The reviewer's initial rating increase after the rebuttal indicates a positive shift in sentiment, although some reservations remain.",30,80
"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. The paper claims that this a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method b) improves performance on texture inpainting tasks compared to the Gatys et al. method c) improves results in season transfer when combined with the style transfer method by Gatys et al. Furthermore the paper shows that d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved. I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient. Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms. With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm. Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance. That said, I could imagine this to be a nice contribution to the workshop track though.","The reviewer's sentiment appears to be mixed with a lean towards the negative side. They acknowledge the paper's claims and agree with one of them (claim a), but they also point out issues such as 'greyish regions' and the high cost of the proposed method. The reviewer compares the work unfavorably to concurrent research, suggesting that it is not as efficient or exciting. The sentiment is not entirely negative, as they do see some merit in the work, suggesting it could be a contribution to a workshop track. Therefore, the sentiment score is slightly negative. The language used in the review is formal and constructive, without any personal attacks or overly harsh criticism. The reviewer provides clear and specific feedback, which is a sign of a polite and professional review. Hence, the politeness score is high.",-30,80
"There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches. Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.","The reviewer's sentiment is quite positive, as indicated by phrases like 'extremely effective,' 'very compelling,' and 'I recommend acceptance.' The reviewer acknowledges the novelty and thoroughness of the work, and the positive results achieved. The language used is polite and constructive, with suggestions for improvement rather than criticism, as seen in the recommendation for a 'second pass for writing style and grammar' and the request for clarification on a specific point. There are no negative or rude words used, and the tone is respectful and encouraging.",90,90
"This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.) In the authors* previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they*ve considered both scenarios. Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential. It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's contribution to the literature and describing it as 'interesting'. However, the reviewer also points out a drawback, which they consider not significant, and suggests improvements for clarity. The language used is formal and constructive, with no negative or rude expressions, indicating a high level of politeness.",60,90
"In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD. Pros: + The organization is generally very clear + Novel meta-learning approach that is different than the previous learning to learn approach Cons: - The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks. - Neither MNIST nor CIFAR experimental section explained the architectural details - Mini-batch size for the experiments were not included in the paper - Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.","The sentiment in the review appears to be slightly positive due to the recognition of the paper's clear organization and the novelty of the approach. However, the reviewer also points out several shortcomings and areas for improvement, which indicates a balanced view rather than outright enthusiasm. The language used is formal and constructive, with no personal attacks or overly harsh criticism, which suggests a high level of politeness.",20,80
"The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator. The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit. It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.","The review provides a balanced assessment of the paper, acknowledging the potential merit of the architectural contributions while also pointing out the limitations of the method. The reviewer's tone is constructive, focusing on the content of the paper rather than making any personal comments. The language used is formal and academic, with no use of slang or colloquialisms, which contributes to the politeness of the review. There is no overt praise or criticism, which suggests a neutral sentiment. Overall, the review is informative and provides clear feedback without being overly positive or negative.",0,70
"This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. Minor points - Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it*s hard to tell if the model is learning phonemes or just most frequent character n-grams. - Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).","The reviewer's sentiment appears to be cautiously optimistic about the paper's approach and potential applications, recognizing the novelty and motivation behind the work. However, the reviewer also expresses a desire for further comparisons and clarifications, suggesting that the paper has room for improvement. The language used is constructive and respectful, with the reviewer offering specific recommendations and posing questions that indicate a genuine interest in the work's implications.",50,80
"The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I*d love to see an experiment that evaluates the relative advantage of this proposed method :) Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that? I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run? Minor comments: Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures. Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?","The reviewer's sentiment appears positive as they describe the proposed approach as 'very promising' and express a personal interest with 'I'd love to see an experiment that evaluates the relative advantage of this proposed method :).' The smiley face emoticon further conveys a friendly tone. The language used is polite and constructive, with suggestions for improvement and questions posed in a respectful manner. The reviewer also provides specific feedback on minor issues such as font size, which indicates a thorough review without any harsh language.",80,90
"This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough *hours* of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness. This is great for the community. The description of the validation of the dataset is interesting, and indicates a careful process was followed. The authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context. One might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used. However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards. (If research groups were making great new datasets available on a regular basis, that would place this in a different context. But so far, that is not the case.) In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.","The reviewer's sentiment appears to be quite positive, as they describe the dataset as 'meticulously annotated, carefully verified and organized' and state that it is 'great for the community.' They acknowledge the careful validation process and the usefulness of the dataset for future research, suggesting that it carries 'substantial weight.' The reviewer does offer a mild critique regarding the experiments and techniques not being in the top 50% of accepted papers, but this is framed within the context of the dataset's value, which they guess to be in the top 15% or better. The language used is formal, constructive, and respectful, indicating a high level of politeness. There are no negative or rude remarks, and the critique is presented in a constructive manner.",80,90
"This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm. One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network). General remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for *f* and *g* (the embedding functions on the two domains), to check that the bad performance of the *no transfer* version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal alpha. The experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no *CCA* nor *direct mapping* results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer? Overall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.","The sentiment of the review appears to be cautiously positive, with the reviewer acknowledging the interesting experiments and the relevance of the study in RL, suggesting that the paper is interesting enough for publication. However, the reviewer also points out several limitations and areas for improvement, such as the lack of thoroughness, the need for additional baselines, and concerns about the experimental setup. The language used is polite and constructive, with suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'One limitation of the paper is that' and 'The experiments feel a bit rushed' which are polite ways to offer criticism. The overall tone is that of a constructive critique aimed at improving the work.",30,80
"While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense. I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. It may be worthwhile very briefly mentioning the relationship of *diagonal* connections to other emerging terms for similar ideas (e.g. skip connections, etc). *Skip* seems to me to be accurate regardless of how you draw the network, whereas *diagonal* only makes sense for certain visual layouts. In response to comment in the discussion below: *leading to less over-segmentation of action bouts* (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that *per-bout* refers to *per-action event*, but now I am not certain that I have understood this correctly (i.e. can a *bout* last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly. In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being *long-term*). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it*s probably not worth it at this point, although it*s something you might want to look at in future. In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what *should* work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this. I would consider adjusting my rating to a 7 depending on future revisions.","The reviewer's sentiment appears to be cautiously optimistic and constructive. They acknowledge the interesting approach and key techniques used in the paper, which indicates a positive sentiment. However, they also express difficulty in understanding some results and suggest improvements, which tempers the positivity. The language used is polite and respectful, with suggestions framed as opportunities for clarification and improvement rather than as harsh criticism. The reviewer uses phrases like 'I found some of the results hard to understand/interpret' and 'I would be curious to know' which are polite ways to express a need for more information or clarification. The use of 'I agree' and 'it would be worthwhile' also contributes to the polite tone.",50,80
"The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are: 1. Bilinear dynamics model for predicting next frame (features) based on action and current frame 2. Formulation of servoing with a Q-function that learns weights for different feature channels 3. An elegant method for optimizing the Bellman error to learn the Q-function Pros: + The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. Cons: - While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics. Pros: + The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently. + Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + gamma min_u Q_{t+1}) fixed. Cons: - However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed. Experimental results: - Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized. - Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version. Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.","The reviewer's sentiment appears balanced, with both positive and negative aspects of the paper being discussed. The positive points are highlighted with phrases like 'does a good job', 'strikes the balance', 'seems more effective', and 'solid optimization scheme'. However, the reviewer also expresses a desire for more experimental results and points out shortcomings in the experimental validation. The language used is academic and constructive, with no signs of rudeness or personal attacks, indicating a polite tone.",0,80
"The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the *row selector*, the *scalar answer* and the *lookup answer* etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the interesting solution proposed by the paper and the extension of previous models. However, the reviewer also points out weaknesses in the experimental section and the clarity of the model's details. The language used is polite and constructive, with suggestions for improvement rather than harsh criticism. The reviewer expresses a desire to see more studies and comparisons, and wishes for better explanations, indicating a positive engagement with the content of the paper.",40,80
"This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. The paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature. That said, I have a few questions and suggestions: 1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment? 2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be? 3) How important is the normality assumption in z_n? How is the variance gamma established? 4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ? 5) Equation (3), denominator mathbf{y} should be mathbf{Y} ? 6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used. 7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen. 8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7. 9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t. 10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what’s made use of here. At least, this strand of work should be mentioned in Section 5. References: Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.","The sentiment of the review is generally positive, as the reviewer describes the paper as 'tightly written, and easy to follow' and states that the approach 'appears novel' and 'appears to have significant practical impact.' The reviewer believes the paper 'brings a valuable contribution to the literature.' These statements indicate a positive reception of the paper's content and contributions. The language used in the review is polite and constructive. The reviewer uses phrases like 'That said,' to transition to their questions and suggestions, which is a courteous way to frame criticism. The questions are posed directly and respectfully, aiming to improve the paper rather than diminish the authors' work. The reviewer also uses the conditional 'would be helpful' to suggest improvements, which is a polite way to offer feedback.",80,90
"The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the *teacher* via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions. -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings. Other comments/questions: -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper*s contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 *The performance of TestModelAQ is worse than TestAQ but better than TestQA.* is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none? -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response *(-)* instead of positive as currently shown. Preliminary Evaluation: A good first step in the research direction of learning dialogue agents from unstructured user interaction.","The reviewer's sentiment appears to be positive overall. They describe the paper as a 'good first step' and acknowledge the novelty of the idea, which suggests that they see value in the work. Additionally, they mention that the paper is 'well written' and provide a clear description of the tasks, models, and experimental settings. However, the sentiment is not overwhelmingly positive as the reviewer also points out several specific issues and questions that need to be addressed. The language used is polite and constructive, focusing on the content of the paper rather than making any personal comments about the authors. The reviewer uses professional language and provides feedback in the form of questions and observations rather than direct criticism, which is typical of academic discourse.",70,90
"I have not much to add to my pre-review comments. It*s a very well written paper with an interesting idea. Lots of people currently want to combine RL with NLP. It is very en vogue. Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task. Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow. Hence, I believe this direction hasn*t shown much promise yet and it*s not yet clear it ever will due to the slowness of RL. But many directions need to be explored and maybe eventually they will reach a point where they become relevant. It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are. Regardless, it*s an interesting exploration, worthy of being discussed at the conference.","The reviewer's sentiment appears to be cautiously optimistic. They acknowledge the paper is well-written and explores an interesting idea, which is positive. However, they also express skepticism about the current effectiveness of combining RL with NLP and the slowness of RL, which tempers the positivity. The language used is polite and constructive, with no harsh or rude words, but it does convey a critical perspective on the research's current impact.",30,80
"This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood. I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn*t depend on Parzen likelihoods. Detailed comments follow: Sec. 2: *theta(0) the* -> *theta(0) be the* *theta(t) the* -> *theta(t) be the* *what we will be using* -> *which we will be doing* I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. *q*. Having learned* -> *q*. [paragraph break] Having learned* Sec 3.3: *learn to inverse* -> *learn to reverse* Sec. 4: *For each experiments* -> *For each experiment* How sensitive are your results to infusion rate? Sec. 5: *appears to provide more accurate models* I don*t think you showed this -- there*s no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat!","The reviewer's sentiment appears to be cautiously positive, as they express liking the idea and being impressed by the visual sample quality and inpainting results. However, they also point out a lack of direct comparison in log likelihood, which suggests some reservations about the paper's claims. The language used is polite and constructive, with suggestions for corrections and improvements rather than harsh criticism. The reviewer uses phrases like 'I liked the idea' and 'particularly nice', which are positive, and the corrections are presented in a straightforward, non-confrontational manner.",70,90
"This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.","The reviewer's sentiment appears to be moderately positive, acknowledging the well-written paper, convincing experiments, and the clear value of the auxiliary tasks. However, they also note that the contribution is relatively incremental, which tempers the positivity. The language used is formal and constructive, with no negative or rude expressions, indicating a high level of politeness.",50,90
"This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks. As pointed below, this is a useful addition. However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain. In summary, the idea is a good one, but the experiments are weak.","The reviewer's sentiment appears to be cautiously optimistic but with clear reservations about the work's current state. They acknowledge the paper's contribution to the literature but also point out weaknesses in the experimental design and a lack of convincing arguments. The sentiment is not overtly negative but suggests that significant improvements are needed. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite tone.",25,75
"This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution. The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios. While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws. I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea. The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 *we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.*: why does one *must* interact with objects in order to learn about the properties? Can*t we also learn through observation? (2) Figure 1right is missing a Y-axis label. (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL. (4) Page 5 *which makes distinguishing between the two heaviest blocks very difficult*: I am a bit confused why having a small mass gap makes the task harder (unless it*s really close to 0). Shouldn*t a machine be possible to distinguish even a pixel difference of speed? If not, isn*t this just because of the network architecture? (5) Page 5 *Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.*: How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion. (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn*t this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what*s hard/easy matter much unless this is more thoroughly tested over various types of distributions? (7) Any baseline approach?","The reviewer's sentiment appears to be mixed. They acknowledge the interesting attempt and idea of the paper and find the experimental setups quite interesting. However, they also express a clear concern about the unclear contribution of the paper and the lack of new insights or deeper analysis. The language used is constructive and professional, with no signs of rudeness or personal attacks, indicating a high level of politeness. The reviewer also provides specific recommendations and questions, which is a sign of engagement and a constructive review process.",25,90
"In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.","The reviewer's sentiment appears to be quite positive, as they describe the idea as novel and the improvement over DQN as very significant. They also mention that the paper proposes a promising direction for future research. However, there is a slight hint of skepticism as they question whether improvements may be due to multi-step returns, suggesting a need for comparison with other methods. The language used is polite and constructive, with suggestions for improvement and an apology for the late review, indicating respect for the authors' work.",80,90
"This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model: 1. how does the model extend to the case with multiple variables in a single sentence? 2. If the answer is out of vocabulary, how would the model handle it? 3. I hope the authors can provide more analysis about the curriculum learning part, since it is very important for the RL model training. 4. In the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?","The sentiment in the review appears to be cautiously positive. The reviewer describes the model as 'very interesting and relatively novel' which indicates a positive sentiment towards the work. However, the use of 'simple' to describe the experimental data suggests a slight reservation. The specific questions listed are constructive and aimed at improving the paper, which supports a positive sentiment overall. The language used is formal and constructive, without any negative or rude words, indicating a high level of politeness. The reviewer's tone is inquisitive and suggests improvements rather than directly criticizing, which is a polite approach in scientific peer reviews.",60,90
"This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance. Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures. Authors have addressed all my pre-review questions and I am ok with their response. Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments? References: [R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)","The sentiment in the review appears to be generally positive, as the reviewer acknowledges the paper's proposal of a new memory module, its generality, and improvements in performance. The reviewer also notes that the authors have addressed all pre-review questions satisfactorily. However, the mention of the k-nearest neighbors approach not being completely new introduces a slight critique, but it does not overshadow the overall positive tone. The language used is formal and appropriate for a scientific peer review, with no signs of rudeness or impoliteness. The reviewer's inquiry about the release of the source code is a standard request and is phrased politely.",70,90
"This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces. A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought. Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using. -------------------- I*m increasing my score to 8 based on the rebuttal and the revised paper.","The sentiment of the review appears to be quite positive. The reviewer acknowledges the thorough evaluation and the effectiveness of the idea in both discrete and continuous action spaces. The use of 'nice' and 'simple' to describe the addition to deep RL algorithms, as well as the final comment about people probably starting to use it, indicates a positive view. However, the reviewer also provides constructive feedback and questions for improvement, which maintains a balance and does not overly praise the work. The language used is polite and constructive. The reviewer uses phrases like 'it would be nice to see' and 'just a thought', which are suggestions rather than demands, and they acknowledge the authors' efforts by increasing their score after the rebuttal and revision of the paper.",70,90
"This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines. The significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work. The proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough. It is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance.","The sentiment of the review appears to be more on the negative side, as the reviewer questions the significance of the work for the ICLR audience and suggests that the improvements over baselines are small and not compelling enough. The reviewer also criticizes the method of learning as 'quite naive' and suggests that the paper is not a favorable candidate for acceptance. However, the language used is formal and maintains a level of professionalism expected in scientific peer reviews, without any personal attacks or overly harsh language. Therefore, while the sentiment is negative due to the critical content, the politeness is maintained at a neutral to slightly polite level because of the professional tone.",-60,20
"The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It*s a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks)","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the novelty and interesting idea of the paper but also pointing out significant concerns regarding its practicality and comparison to existing methods. The language used is constructive and maintains a professional tone throughout, with clear pros and cons listed. There are no harsh or overly critical words used, and the feedback is presented in a way that is likely meant to be helpful to the authors.",30,80
"This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > _x0008_eta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated.","The reviewer's sentiment appears to be mixed. They acknowledge the interesting approach of the paper by combining pre-conditioning with binarization in a proximal framework and providing a new interpretation of existing approaches, which is positive. However, they also express significant concerns about the theoretical analysis, practicality, and assumptions made in the paper, which is negative. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone.",0,80
"This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field. Pros: + Very clear paper. It should be possible to replicate these results should one be inclined to do so. + The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it*s not better than the state of the art in image compression. It*s definitely better than other neural network approaches to compression, though. Cons: - The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. - The motivation behind Figure 1 is a bit strange, as it*s not clear what it*s trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)","The reviewer's sentiment appears to be generally positive, as they describe the results as 'impressive' and 'very promising', and note the clarity of the paper. However, they also point out some negatives, such as the 'clunky' training procedure and confusion around Figure 1. The balance of positive and negative comments suggests a moderately positive sentiment. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a high level of politeness.",60,90
"The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text. Strength: - The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) - The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work. Weaknesses: 1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2. Experimental evaluation 2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2. It would be interested if this approach generalizes to other datasets. Other (minor/discussion points) - The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. - I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process. - Page 6, last paragraph: missing “.”: “… searching This…” Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.","The sentiment of the review appears to be mildly positive, as the reviewer acknowledges the strengths of the paper, such as the sensible approach and good performance on the SQuAD dataset. However, the reviewer also points out several weaknesses and areas for improvement, which indicates a balanced view rather than outright enthusiasm. The language used is formal and constructive, with no signs of rudeness or personal attacks, suggesting a high level of politeness.",30,80
"This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization. The paper is clearly written and is easy to follow. Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers. Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{phi};f], where f is the unormalized likelihood f propto exp sum_k( - lambda_k T - c_k ||T_k||^2 ). This type of derivation is typical in all previous works using NFs for variational inference. A few experiments on more complex data would strengthen the paper*s results. The two experiments provided show good results but both of them are toy problems. Minor point: Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.","The reviewer's sentiment appears to be mildly positive towards the clarity of the writing and the ease of following the paper, but they express a clear concern about the novelty of the work, which they find lacking. The reviewer's language is constructive and respectful, providing specific feedback without any harsh or disrespectful wording. The use of phrases like 'clearly written' and 'easy to follow' contribute to a positive sentiment, while the critique of novelty and suggestion for improvement indicate a balanced view rather than outright enthusiasm or negativity. The language used is formal and typical of academic discourse, suggesting a high level of politeness.",25,80
"A well written paper and an interesting construction - I thoroughly enjoyed reading it. I found the formalism a bit hard to follow without specific examples- that is, it wasn*t clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well. In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework. Experimental results were presented clearly and well illustrated the usefulness of the metacontroller. I*m curious to see the results of using more metaexperts.","The reviewer begins with positive remarks, complimenting the paper as 'well written' and the construction as 'interesting', which indicates a positive sentiment. However, the reviewer also expresses difficulty in understanding the formalism without specific examples, which suggests a slight negative aspect, but this is framed constructively. The suggestions for improvement, such as including algorithm boxes and describing existing models, are presented in a helpful manner. The reviewer also appreciates the clarity of the experimental results and expresses curiosity about future work. Overall, the sentiment is positive with constructive criticism. The language used is polite and respectful, with phrases like 'I thoroughly enjoyed reading it' and 'might be interesting to see'. The reviewer's tone is inquisitive and encouraging rather than demanding or harsh.",70,90
"The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution. The presented approaches are entirely based on some intuitive arguments. As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point. The paper will benefit from more rigorous theoretical justification of the presented approaches.","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the interesting and potentially useful nature of the proposed approaches, but also pointing out the need for more rigorous theoretical justification. The language used is constructive and encourages further exploration, which suggests a positive sentiment, but it is tempered by the critique. The politeness is high, as the language is formal, respectful, and provides constructive feedback without any harsh or rude wording.",40,90
"The paper shows the relation between stochastically perturbing the parameter of a model at training time, and considering a mollified objective function for optimization. Aside from Eqs. 4-7 where I found hard to understand what the weak gradient g exactly represents, Eq. 8 is intuitive and the subsequent Section 2.3 clearly establishes for a given class of mollifiers the equivalence between minimizing the mollified loss and training under Gaussian parameter noise. The authors then introduce generalized mollifiers to achieve a more sophisticated annealing effect applicable to state-of-the-art neural network architectures (e.g. deep ReLU nets and LSTM recurrent networks). The resulting annealing effect can be counterintuitive: In Section 4, the Binomial (Bernoulli?) parameter grows from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), meaning that the network goes initially through a phase of adding noise. This might effectively have the reverse effect of annealing. Annealing schemes used in practice seem very engineered (e.g. Algorithm 1 that determines how units are activated at a given layer consists of 9 successive steps). Due to the more conceptual nature of the authors contribution (various annealing schemes have been proposed, but the application of the mollifying framework is original), it could have been useful to reserve a portion of the paper to analyze simpler models with more basic (non-generalized) mollifiers. For example, I would have liked to see simple cases, where the perturbation schemes derived from the mollifier framework would be demonstrably more suitable for optimization than a standard heuristically defined perturbation scheme.","The reviewer's sentiment appears to be cautiously positive, acknowledging the intuitive aspects of the paper and the originality of the authors' contribution, while also pointing out areas of confusion and suggesting improvements. The language used is constructive and respectful, focusing on the content of the paper rather than personal remarks, which indicates a high level of politeness.",40,90
"Pros: Interesting training criterion. Cons: Missing proper ASR technique based baselines. Comments: The dataset is quite small. ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP. More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection performance of out-of-vocabulary words. It would be interesting to show scatter plots for embedding vs. orthographic distances.","The sentiment of the review appears neutral to slightly negative, as the reviewer acknowledges an 'Interesting training criterion' but also points out several shortcomings such as 'Missing proper ASR technique based baselines' and the small dataset size. The suggestions for improvement, such as adding ROC curves, EER measurements, and a more detailed analysis, indicate that the reviewer sees potential for enhancement but is not fully satisfied with the current state of the work. The language used is direct and to the point, without any personal attacks or overly harsh criticism, which suggests a polite tone.",-10,70
"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a *movie.* My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the *frame rate* is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.","The reviewer's sentiment appears to be generally positive, as they acknowledge the paper's contribution to understanding nonlinear response properties of visual neurons and describe the work as an 'important step.' They also call the result 'a good first start' and 'important for people to know about,' which further indicates a positive sentiment. However, there is a slight reservation expressed about the lack of analysis on the critical parts of the RNN, which prevents the sentiment from being entirely positive. The language used is polite and constructive, with suggestions for improvement and clarification rather than harsh criticism. The use of phrases like 'it would have been nice to see' and 'I am a bit confused about' are polite ways to point out areas that could be enhanced or need more explanation.",70,90
"The paper presents two main contributions: (1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software. (2) A hybridization of GANs and VAEs called Introspective Adversarial Network. The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together. On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models. On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a *list of things to make it work* fashion. I would like to see more empirical results in that direction to help clear up things. Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication. UPDATE: The rating has been updated to a 6 following the authors* reply.","The reviewer's sentiment appears to be mildly positive towards the ideas presented in the paper but critical of its structure and lack of focus, which suggests a sentiment score that is slightly positive but close to neutral. The reviewer acknowledges the novelty and excitement of the ideas but also points out significant issues that prevent the paper from being ready for publication. The language used is constructive and respectful, with suggestions for improvement, which indicates a high level of politeness.",30,80
"First I would like to apologize for the late review. This paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al.. This allows them to train their model with less supervision than Reed & de Freitas. Overall the model is a nice extension of NPI. While it requires less supervision than NPI, it still requires *sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy*. This may limit the scope of this work. The paper claims that their *method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic). Such sequences are relatively easy to gather in many natural settings*. It would be great if the authors clarify what they mean by *relatively easy to gather in many natural settings*. They also claim that *the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems*. However, this paper only addresses two toy problems which are neither *natural settings* nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition). In the introduction, the author states that *Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.* What are the *existing techniques*, they are referring to? This work only addresses the problem of long addition and puzzle solving in a block world. Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and Sukhbaatar et al. (*Mazebase: A sandbox for learning from games*) shows that a memory network can solve puzzles in a blockworld with little supervision. In the conclusion, the author states that *remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.* However for all the experiments, they *include a small number of FULL samples* (FULL == *samples with full program traces*). Unfortunately even if this means that they need less FULL examples, they still need *full program traces*, contradicting their final claim. Moreover, as shown figure 7, their model does not use a *small number of FULL samples* but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128. *All experiments were run with 10 different random seeds*: does the environment change as well between the runs, i.e. are the FULL examples different between the runs? If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft. Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples. Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( *123+45* would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.? Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version. Overall, it is a nice extension of Reed & de Freitas, but I*m a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work). This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before. Missing references: *Learning simple arithmetic procedures*, Cottrell et al. *Neural gpus learn algorithms*, Kaiser & Sutskever *Mazebase: A sandbox for learning from games*, Sukhbaatar et al. *Learning simple algorithms from examples*, Zaremba et al.","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the paper as a 'nice extension of NPI' which indicates a positive aspect, but also points out limitations, lack of discussion about the literature, and weak experimental sections that do not support the claims. The language used is formal and academic, with no rude or inflammatory language, suggesting a high level of politeness. However, the critical nature of the feedback implies a sentiment that is not entirely positive, hence the sentiment score is not high.",20,80
"The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program*s parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.","The sentiment in the review starts off positive, acknowledging the importance of the problem and the interesting approach of the paper. However, it quickly turns negative when discussing the experimental results and the readiness of the paper for publication, ultimately recommending a weak reject. The language used is formal and constructive, providing specific questions and suggestions for improvement without any personal attacks or overly harsh criticism.",-30,70
"This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?","The sentiment of the review appears to be cautiously positive. The reviewer acknowledges that the paper addresses an important problem and provides interesting results, which suggests a positive sentiment towards the work. However, the reviewer also poses questions and asks for further clarification, indicating that there are areas of concern or aspects that need improvement. This balance of positive acknowledgment and constructive criticism leads to a sentiment score that is mildly positive. The language used in the review is formal and professional, with no use of harsh or impolite terms. The reviewer's tone is inquisitive and suggests improvements without being confrontational or dismissive, which indicates a high level of politeness.",40,90
"The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines. Pros: * It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution. * The paper is clearly written. * From an architectural standpoint, the actual comparisons seem well motivated. (For instance, I*m glad they tried DN* and BN* -- if they hadn*t tried those, I would have wanted them too.) Cons: * I*m not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization. (Oh, actually -- looking at the other reviews, everyone else seems to have noticed this too. I*ll not beat a dead horse about this any further.) * I*m nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models. I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks. In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn*t contribute that much to performance but was somewhat expensive computationally. Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached.","The reviewer's sentiment appears to be cautiously positive, acknowledging the usefulness of the paper's contribution and the clarity of writing, but also expressing concerns about the novelty and scalability of the proposed method. The language used is constructive and respectful, with the reviewer providing both praise and criticism without any harsh language or personal attacks. The use of phrases like 'very useful contribution' and 'clearly written' indicate a positive sentiment, while the concerns raised are framed as points for consideration rather than outright dismissal of the work. The overall tone is professional and aimed at improving the work, which is typical of academic discourse.",40,80
"This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted. The inverted Softmax idea is very nice. A few minor issues that ought to be addressed in a published version of this paper: 1) There is no mention of Haghighi et al (2008) *Learning Bilingual Lexicons from Monolingual Corpora.*, which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed. 2) Likewise, Hermann & Blunsom (2013) *Multilingual distributed representations without word alignment.* is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data. 3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages 4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion 5) I don*t have a better suggestion, but is there an alternative to using the term *translation (performance/etc.)* when discussing word alignment across languages? Translation implies something more complex than this in my mind. 6) The Mikolov citation in the abstract is messed up","The reviewer's sentiment appears to be cautiously positive, acknowledging that the paper addresses an interesting problem and has executed well in parts, specifically praising the 'inverted Softmax idea'. However, the reviewer also points out areas that lack in evaluation and suggests improvements and additional citations. The language used is constructive and professional, offering specific recommendations without harsh criticism or negative language, which indicates a high level of politeness.",40,90
"This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance. My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant. Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.","The reviewer's sentiment appears to be cautiously positive, as they recognize the novelty and potential of the paper, suggesting acceptance. However, they also express a concern regarding the completeness of the work, particularly the lack of effort in investigating the defense against dynamic adversaries. The language used is constructive and respectful, with the reviewer providing clear feedback and encouragement for further development. There is no use of harsh or negative language, indicating a polite tone throughout the review.",70,90
"# Review This paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)). The authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model. # Pros Their evaluation framework is public and is definitely a nice contribution to the community. This paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution. # Cons/Questions It is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set? It is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both? 16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2? I would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others? # Minor comments Table 1 is not referenced in the text and lacks description of what the different columns represent. Figure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2). Figure 2(c), I*m guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder? Are the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ? Typo in caption of Figure 3: *(c) GMMN-10* but actually showing GMMN-50 according to the graph title and subcaption.","The sentiment of the review appears to be constructive and balanced, with both positive and negative feedback provided. The reviewer acknowledges the contribution of the paper to the community and provides specific insights into the paper's evaluation of GANs. However, the reviewer also lists several concerns and questions regarding the experimental setup and requests further clarification, indicating a neutral to slightly positive sentiment. The language used is formal and respectful, with the reviewer framing their concerns as questions and using polite academic language, which suggests a high level of politeness.",30,90
"This paper proposes an online inference algorithm by using online Bayesian moment matching for HMM-GMM. The method uses transfer learning by utilizing individual sequence estimators to predict a target sequence based on a weighted combination of individual HMM-GMM. Online Bayesian moment matching has a benefit of updating HMM-GMM parameters frame-by-frame, and fits to this problem. The authors compare the proposed method with the other sequential modeling methods including RNN and EM, and show the effectiveness of the proposed method. The paper is well written overall. Comments: 1) Could you provide the average performance in table? It is difficult to compare the performance only with individual performance. Also, it seems that the EM performance is sometimes good 2) I’m curious how initialization and hyper-parameter settings affect the final performance. If you provide some information about it, that is great. 3) It would be better to provide a figure of describing the transfer-learning-based proposed methods, since this is a unique and a little bit complicated setup.","The sentiment in the review appears to be positive as the reviewer acknowledges the benefits of the proposed method, the comparison with other methods, and the overall quality of the writing. There is no negative language or criticism that isn't constructive. The language used is polite and constructive, with suggestions for improvement rather than direct criticism. Phrases like 'well written overall' and 'that is great' contribute to a positive tone. The reviewer's recommendations are framed as questions or suggestions, which is a polite way to communicate.",70,90
"The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing.","The review starts by acknowledging the novelty and main contribution of the paper, which indicates a positive sentiment. However, it also points out a weakness in the experimental section, suggesting that the paper's findings are not compared thoroughly with existing literature. This critique tempers the positivity, but does not negate it, as the reviewer still finds the paper's analysis and new learning algorithm discussion-worthy. The language used is formal and constructive, without any derogatory or harsh terms, indicating a high level of politeness.",50,90
"This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. The paper is very clear. The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature. Furthermore, their code is open-source and the live demo is looking good. The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful. It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.","The review expresses a positive sentiment towards the paper, highlighting the clarity of the paper, the inclusion of many variables in the model, the consistency of the results with the literature, the open-source code, and the promising future updates. No negative comments are made, and the suggestion to include newer network architectures is constructive and indicates interest in the work's potential. The language used is formal, respectful, and constructive, with no negative or rude expressions.",80,90
"The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.","The reviewer expresses a positive view on the idea of transferring attention maps, which suggests a positive sentiment towards the novelty of the work. However, they also express a concern about the lack of significant improvement and the need for more experiments, which indicates a moderate sentiment as they are not entirely convinced of the paper's results. The language used is constructive and respectful, with no harsh or rude words, indicating a high level of politeness.",30,80
"It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. On a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren*t we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a *fully-generative* neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","The sentiment in the review starts positively, acknowledging the efforts and contributions of OpenAI, which suggests a positive sentiment. However, it quickly transitions to a critical tone, questioning the significance and originality of the work, and urging the team to focus on more fundamental issues rather than incremental improvements. This mix of appreciation and critical feedback suggests a sentiment that is neither fully positive nor fully negative, but rather neutral with constructive criticism. The language used is formal and respectful, despite the critical nature of some comments. The reviewer uses phrases like 'it is refreshing', 'we are all very grateful', and 'welcome addition', which are polite and appreciative. However, the use of phrases like 'beating a dead horse' and 'the vain of the Boltzmann machine' could be perceived as slightly less polite, but they are used metaphorically to emphasize the point rather than to offend.",0,50
"UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance. -------------- This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder. I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments. I have the following suggestions/concerns about the paper: 1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper. 2) A similar idea is also presented in a concurrent ICLR submission *Variational Lossy Autoencoder*. It would be interesting to have a discussion included in the paper and compare these works. 3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce. 4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure. I will gladly raise the score if the authors address my concerns.","The sentiment of the review appears to be generally positive, as the reviewer acknowledges that the authors have addressed all their concerns and now recommends acceptance of the paper. The reviewer also expresses appreciation for the idea behind the paper and the hierarchical image generation experiments. However, there are still suggestions and concerns, indicating that while the sentiment is positive, it is not without reservations. The language used is polite and constructive, with the reviewer providing clear suggestions and expressing their thoughts in a respectful manner. There is no use of harsh or disrespectful language, and the tone is collaborative, aiming to improve the paper.",70,90
"This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets. The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application. While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1: “Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.” As far as I can tell, your model also uses the recent hidden state to form a query vector, which is matched by the pointer network to previous words. Can you please clarify what you mean here? In addition, quoting from section 3 which describes the model of Gulcehre et al.: “Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use” This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text. Finally, in the following quote, also from section 3: “The pointer network is not used as a source of information for the switching network as in our model.” It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is. With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well? I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.","The sentiment of the review appears to be critical and constructive, with the reviewer pointing out perceived issues and direct applications of previous work. The reviewer does not dismiss the work outright but expresses skepticism about the novelty and significance of the contributions. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite discourse. The reviewer also expresses willingness to discuss the points raised and is open to changing their opinion, which adds to the overall constructive tone.",-20,80
"This is a well written, organized, and presented paper that I enjoyed reading. I commend the authors on their attention to the narrative and the explanations. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning. That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture. A few points of criticism: -The numerical results are in my view too brief. Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1. I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances. To be clear I don*t think this is disqualifying or deeply concerning; I simply found it a bit underwhelming. - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance. I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure. - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader. I saw that another reviewer suggested perhaps ICLR is not the right venue for this work. While I certainly see the reviewer*s point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). Overall, a nice paper.","The reviewer's sentiment is generally positive, as they enjoyed reading the paper, commended the authors, and appreciated the application and dataset used. However, they also expressed that the results were not as deep as expected and provided constructive criticism, which indicates a balanced view. The language used is polite and constructive, with phrases like 'I commend the authors' and 'I enjoyed reading,' and suggestions are given in a helpful manner without harsh language.",60,80
"This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)","The reviewer's sentiment appears to be critical and constructive, pointing out both the strengths and weaknesses of the paper. The reviewer acknowledges the compactness and speed of the proposed DSL models but also highlights the lack of clarity, depth, and completeness in the explanation and experiments. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a professional tone aimed at improving the work. The reviewer's main concerns are the insufficient explanation for a broader audience, the lack of a clear description of the probabilistic model and training, and the incomplete nature of the experiments.",-40,70
"Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network*s activation. It is possible that a modified criterion - eqn (7) + lambda feature activation, (where lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don*t think this missing comparison is justified. (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary). (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts. (3) Iterative finetuning. However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept.","The reviewer's sentiment appears to be cautiously optimistic and constructive. They acknowledge the soundness of the paper technically and empirically, and propose that with additional comparisons, they would accept the paper. This indicates a positive sentiment towards the work, albeit with suggestions for improvement. The language used is formal, constructive, and focused on the content of the paper without any personal remarks or aggressive tone, which reflects a high level of politeness.",70,90
"The idea of *pruning where it matters* is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too. Extra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (mathbb symbol - two thumbs up!). The experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case? My question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the *min sum of weights* criterion compares to other approaches. How does it compare to other pruning criteria? Is it better than *pruning at random*? Overall, I liked the paper.","The reviewer expresses a positive sentiment towards the paper, praising the idea, clarity, and the experimental results. The use of phrases like 'great', 'very good job', 'extra points for clarity', and 'encouraging' indicate a strong positive sentiment. However, the reviewer also suggests that more experiments would be beneficial, which slightly moderates the positivity. The language used is polite and constructive, with no negative or rude words. The reviewer uses positive reinforcement ('two thumbs up!') and expresses curiosity rather than criticism regarding the pruning criterion, which maintains a polite tone.",80,90
"This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication. The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. While I feel that the contribution is somewhat incremental, I recommend acceptance.","The reviewer's sentiment appears to be cautiously positive, acknowledging the novelty and empirical results of the paper but also noting that the contribution is somewhat incremental. The language used is professional and constructive, without any negative or harsh wording, indicating a polite tone.",60,80
"The paper proposed a simple and effective model for QA. The paper is easy to read and result is impressive on the synthetic and real dataset. The one question is the paper is called query-reduction, but there is no place to show this reduction explicitly.","The reviewer's sentiment appears positive as they describe the model as 'simple and effective' and note that the paper is 'easy to read' with 'impressive' results. However, they also express a concern about the lack of explicit demonstration of 'query-reduction,' which suggests a minor criticism. The language used is polite and constructive, with no negative or rude words used, indicating a respectful tone towards the authors.",70,90
"Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments: 1. Actually the model in the paper is not single model, it proposed two models. One consists of *reading*, *writing*, *adaptive computation* and * Answer module 2*, the other one is *reading*, *composing*, *writing*, *gate querying* and *Answer module 1*. Based on the method section and the experiment, it seems the *adaptive computation* model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine. 2. What is the MLP setting in the composing module? 3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers? 4. It needs more ablation study about using different T such as T=1,2.. 5. According to my understanding, for the adaptive computation, it would stop when the P_T <0. So what is the distribution of T in the testing data?","The sentiment of the review appears to be mildly positive as the reviewer begins by acknowledging that the approach is interesting and the result is good. However, the reviewer then lists several comments and questions that suggest areas for improvement or clarification, which indicates a constructive but critical stance. The language used is polite and professional, with no negative or rude words, and the reviewer's tone is that of a colleague offering constructive feedback rather than harsh criticism.",30,80
"The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out.","The review starts with a positive note, acknowledging that the paper is well written and the idea is well presented. However, it quickly transitions to criticism regarding the limitations of the datasets, the constancy of hyperparameters, and the insufficiency of the experiments to support the claims. The reviewer's tone is constructive, providing specific areas of improvement without using harsh language. Therefore, the sentiment is slightly negative due to the criticism but not extremely so because of the initial praise. The language is polite and professional, focusing on the content rather than personal remarks.",-20,80
"[UPDATE] After going through the response from the author and the revision, I increased my review score for two reasons. 1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it. This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions. It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text). 2. The revised paper contains more comprehensive results than before. The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive. - Summary This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment. - Novelty The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. - Experiment The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration. - Clarity The paper is well-written and easy to follow. - Overall Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community. [Reference] Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.","The reviewer's sentiment appears to be moderately positive. They acknowledge the improvements made by the authors after the revision and the usefulness of the results to the research community. The reviewer also appreciates the comprehensive results and the empirical evidence provided. However, they note that the novelty of the proposed architecture is not strong, which suggests a balanced view. The language used is formal and constructive, with no negative or rude expressions, indicating a high level of politeness.",50,90
"This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. That being said, I think two aspects may need further improvement: (1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification). (2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.","The reviewer's sentiment appears to be cautiously optimistic about the paper's contribution, acknowledging the novelty and cleverness of the approach. However, the reviewer also points out areas that need improvement, which indicates a balanced and constructive critique rather than unreserved praise or harsh criticism. The language used is formal and constructive, without any harsh or disrespectful terms, indicating a high level of politeness.",50,90
"The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite Kümmerer et. al 2015 as a model which *learns ... indirectly rather than from explicit information of where humans look*, however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.","The reviewer's sentiment appears to be mixed, with both positive and critical elements. The reviewer acknowledges the interesting contribution of the paper but also points out several issues that need to be addressed. The positive aspect is shown by the willingness to improve the rating if the issues are resolved, indicating a generally constructive tone. However, the critical points about the evaluation of fixation prediction performance, the confusion in Table 3, and the incorrect citation suggest that the reviewer is not fully satisfied with the paper as it stands. The sentiment is therefore slightly negative but not dismissive. The language used is polite and professional, with no rude or aggressive terms. The reviewer uses phrases like 'I am missing,' 'it is not clear,' and 'I think' which are indicative of a respectful tone aimed at constructive criticism.",-20,80
"The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results.","The reviewer's sentiment appears to be positive, as they describe the technique as 'simple but yet effective' and acknowledge that the results obtained are 'quite good'. Additionally, they appreciate the technique's effectiveness on state-of-the-art topologies. The language used is polite and professional, with no negative or rude words used. The reviewer's tone is constructive and appreciative, which indicates a high level of politeness.",80,90
"The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors* own implementation of A3C as well as to published reference scores. The paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: https://github.com/muupan/async-rl One or two plots like that would complete this paper very nicely. ----------------------------------- I appreciate the additional experiments included in the revised version of the paper. The learning speed comparison makes the paper more complete and I’m slightly revising my score to reflect that. Having said that, there is still no clear demonstration that the higher throughput of GA3C leads to consistently faster learning. With the exception of Pong, the training curves in Figure 6 seem to significantly underperform the original A3C results or even the open source implementation of A3C on Breakout and Space Invaders (https://github.com/muupan/async-rl).","The reviewer's sentiment appears to be cautiously positive, acknowledging the natural architecture and automatic performance tuning strategy of the GA3C implementation. However, they also express concerns about the lack of demonstration of learning speed/data efficiency and underperformance in some cases compared to the original A3C results. The language used is constructive and professional, with no signs of rudeness or personal attacks, indicating a high level of politeness.",30,90
"I would like first to apologize for the delay. Summary: A framework for two-samples statistical test using binary classification is proposed. It allows multi-dimensional sample testing and an interpretability that other tests lack. A theoritical analysis is provided and various empirical tests reported. A very interesting approach. I have however two main concerns. The clarity of the presentation is obscured by too much content. It would be more interesting if the presentation could be somewhat self-contained. You could consider making 2 papers out of this paper. Seriously, you cram a lot of experiments in this paper. But the setting of the experiments is not really explained. We are supposed to have read Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All this is okay but reduces your public to a very few. For example, if I am not mistaken, you never explained what SCF is, despite the fact that its performances are reported. As a second point, given also that the number of submissions to this conference are exploding, I would like to challenge you with the following question: Why is this work significant to the representation learning community?","The sentiment of the review appears to be constructive but with a critical tone. The reviewer starts by acknowledging the interesting approach of the paper, which indicates a positive sentiment. However, the reviewer quickly moves on to express concerns about the clarity and the amount of content, suggesting that the paper could be split into two. Additionally, the reviewer challenges the authors to justify the significance of their work to the community, which can be perceived as a call for improvement. Overall, the sentiment seems to be slightly positive due to the initial acknowledgment of the paper's interest but is tempered by the critical feedback. The language used is polite and professional. The reviewer uses phrases like 'I would like first to apologize for the delay' and 'A very interesting approach,' which are courteous. The constructive criticism is delivered without harsh language, and the reviewer's suggestions are framed as opportunities for improvement rather than direct negative comments.",30,80
"This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks Working on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another. Revision: - the authors substantially improved the contents of the paper, including experiments on another set than Cifar - the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate I have therefore improved my rating","The reviewer's sentiment appears to be mixed. They acknowledge the simplicity and effectiveness of the heuristic but also express concerns about its limited demonstration and lack of theoretical support. Additionally, they share their own contrasting experience with different architectures, which suggests skepticism about the heuristic's generalizability. However, the reviewer also notes improvements in the paper's content and upgrades their rating, indicating a positive shift in their view. The language used is professional and constructive, with no signs of rudeness or impoliteness. The reviewer provides specific feedback and acknowledges the authors' efforts to improve the paper.",25,75
"The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version. The authors attempted to compare with wavenet model, but they didn*t manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better.","The reviewer acknowledges the novel contribution of the paper positively but also points out significant shortcomings in the lack of detail and unconvincing comparisons. The sentiment is mixed: positive for the novel contribution and negative for the lack of detail and unconvincing comparisons. The language used is direct and could be perceived as critical but not rude. The reviewer uses phrases like 'hopefully, this will be addressed' and 'instead of wasting time,' which are straightforward but not impolite.",0,50
"The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.","The reviewer's sentiment appears to be generally positive, as they describe the algorithm as 'simple and reasonable' and acknowledge the 'intuitive' formulations and 'thorough' experiments. The reviewer also appreciates the ability of the algorithm to leverage GPU speedups and its performance on benchmark datasets. However, there is a slight reservation as the reviewer admits a lack of awareness of the literature, which could suggest similar methods might exist. This slight reservation prevents the sentiment from being fully positive. The language used is polite and constructive, with no negative or rude words used. The reviewer's tone is professional and focuses on the merits of the work rather than any personal criticism.",70,90
"This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it*s neat to see a practical algorithm driven by what is essentially a spiking network. The benefits of this approach are still more theoretical than practical -- it seems unlikely to be worthwhile to do this on current hardware. I strongly suspect that if deep networks were trained with an appropriate sparse slowness penalty, the reduction in computation would be much larger.","The reviewer's sentiment appears to be moderately positive, as they describe the paper as 'clearly written' and the approach as 'clever', also appreciating the practical algorithm. However, they also express skepticism about the practical benefits of the approach with current hardware, which tempers the positivity. The language used is polite and constructive, with no rude or aggressive terms used. The reviewer provides constructive criticism without disparaging the work, maintaining a professional tone throughout.",50,80
"This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima*s well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost). This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4. The publicly available code to ensure reproducibility is also greatly appreciated. I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles. Preliminary rating: This is an interesting work with convincing experiments and clear writing. Minor note: Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.","The reviewer's sentiment appears to be quite positive, as they commend the paper for being well written, having clear figures/tables, and providing convincing results. They also express appreciation for the analysis in a specific section and the availability of code, which are all positive indicators. The minor note at the end does not detract significantly from the overall positive tone. The language used is polite and constructive, with phrases like 'I would like to see' and 'greatly appreciated', which are courteous and show respect for the authors' work.",80,90
"This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). This clustering effect can exploited for parameter quantisation and compression of the network parameters. The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process. A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. In particular a compression rate of 64.2 is obtained on the LeNet300-100 model. In section 6.1 the text refers to figure C, I suppose this should be figure 1. Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation. Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016). Comparable results are obtained in terms of compression rate and accuracy. The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model. The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training. This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques. The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets. Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.","The reviewer's sentiment appears to be cautiously positive, acknowledging the principled approach of the paper and its comparable results to state-of-the-art methods, while also suggesting improvements and pointing out limitations. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides clear and specific recommendations for improvement, which indicates a polite and professional tone.",40,80
"Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart*s paper cited for back-propagation than the Deep Learning book.","The sentiment in the review appears to be mildly positive as the reviewer acknowledges that the experimental results look reasonable and have been validated on 3 tasks. However, there is a suggestion for improvement regarding references, which indicates a slight criticism but not a strong negative sentiment. The language used is direct and to the point, without any personal attacks or overly harsh criticism, which suggests a neutral to slightly polite tone.",30,40
"This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters. The basic idea of steerability makes huge sense and seems like a very important idea to develop. It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s. This paper approaches it through a formal treatment of group theory. But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image. Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill. I*m not sure what this buys us in the end. it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful. Also the description of the experiments is fairly opaque. I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.","The reviewer acknowledges the importance of the paper's topic and the basic idea of steerability, which indicates a positive sentiment towards the concept. However, the reviewer also expresses skepticism about the necessity of the group theoretic analysis for discrete groups and points out the lack of guidance for continuous transformations, which adds a critical tone. Additionally, the reviewer finds the description of the experiments opaque, indicating difficulty in replicating the study. Overall, the sentiment is mildly positive due to the recognition of the idea's importance but tempered by critical feedback on the approach and clarity. The language used is formal and constructive without any personal attacks or harsh wording, suggesting a polite tone in delivering the critique.",30,70
"This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper. There*s a lot of interest in VAEs these days; many lines of work seek to achieve automatic *black-box* inference in these models. For example, the authors themselves mention parallel work by Blei*s lab (also others) towards this direction. However, there*s a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn*t been popular in ML. Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts. I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they*re investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention. The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer. Overall, I found this to be an interesting paper, it would be a good fit for ICLR.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the merit in the authors' bespoke approach to the SB-VAE model and finds the paper interesting, suggesting it would be a good fit for ICLR. However, the reviewer also expresses confusion and concerns regarding the methodology and motivation, which indicates some reservations. The language used is formal and constructive, with no signs of rudeness or impoliteness. The reviewer provides criticism in a professional manner, which is typical for scientific peer reviews.",40,80
"Interesting work on hierarchical control, similar to the work of Heess et al. Experiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side. (1) Like other reviewers, I find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in RL). Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. (2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result? (3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it? (4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.","The reviewer's sentiment appears to be cautiously positive, acknowledging the strength of the experiments and their ability to complete benchmarks that previous work could not. However, the reviewer also points out weaknesses in the analysis of the experiments and provides constructive criticism. The language used is professional and constructive, with no signs of rudeness or impoliteness. The reviewer's use of phrases like 'Interesting work', 'Experiments are strong', and 'elegant' contribute to a positive tone, while the constructive feedback and questions indicate engagement and a desire for clarification rather than harsh criticism.",40,80
"This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network. Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures. The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc. I think it’s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model. In sum, I think this is a solid contribution and the approach will benefit the research in other problems.","The reviewer's language is very appreciative and supportive, using phrases like 'very nice paper', 'clear', 'solid', and 'carefully done', which indicate a positive sentiment. There is no negative language or criticism in the first paragraph. The language is also very polite, with no use of harsh or demanding words, and the reviewer's tone is respectful and constructive.",90,90
"The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.","The reviewer's sentiment appears to be cautiously positive. They acknowledge that the general idea of the work is not unique but still find it interesting, especially the novel aspect of the regularization term. The reviewer also appreciates the detailed analysis and the performance of the new algorithm, although they suggest that the motivation behind the Deep-SRSC scheme could be better presented. The overall tone is constructive and recognizes the strengths of the paper while pointing out areas for improvement. The language used is formal and respectful, with no negative or rude expressions, indicating a high level of politeness.",60,90
"This paper presents a model for semi-supervised learning by encouraging feature invariance to stochastic perturbations of the network and/or inputs. Two models are described: One where an invariance term is applied between different instantiations of the model/input a single training step, and a second where invariance is applied to features for the same input point across training steps via a cumulative exponential averaging of the features. These models evaluated using CIFAR-10 and SVHN, finding decent gains of similar amounts in each case. An additional application is also explored at the end, showing some tolerance to corrupted labels as well. The authors also discuss recent work by Sajjadi &al that is very similar in spirit, which I think helps corroborate the findings here. My largest critique is it would have been nice to see applications on larger datasets as well. CIFAR and SVHN are fairly small test cases, though adequate for demonstration of the idea. For cases of unlabelled data especially, it would be good to see tests with on the order of 1M+ data samples, with 1K-10K labeled, as this is a common case when labels are missing. On a similar note, data augmentations are restricted to only translations and (for CIFAR) horizontal flips. While *standard,* as the paper notes, more augmentations would have been interesting to see --- particularly since the model is designed explicitly to take advantage of random sampling. Some more details might also pop up, such as the one the paper mentions about handling horizontal flips in different ways between the two model variants. Rather than restrict the system to a particular set of augmentations, I think it would be interesting to push it further, and see how its performance behaves over a larger array of augmentations and (even fewer) numbers of labels. Overall, this seems like a simple approach that is getting decent results, though I would have liked to see more and larger experiments to get a better sense for its performance characteristics. Smaller comment: the paper mentions *dark knowledge* a couple times in explaining results, e.g. bottom of p.6. This is OK for a motivation, but in analyzing the results I think it may be possible to have something more concrete. For instance, the consistency term encourages feature invariance to the stochastic sampling more strongly than would a classification loss alone.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's contributions and corroborating work while also suggesting areas for improvement and further research. The language used is constructive and respectful, offering specific critiques without any harsh or negative wording. The reviewer expresses a desire to see the model tested on larger datasets and with more varied data augmentations, indicating a belief in the potential of the research if expanded upon. The use of terms like 'decent gains' and 'simple approach' suggests a moderate endorsement of the work. The reviewer's language is professional and focused on the content of the paper, with no personal attacks or dismissive comments, which indicates a high level of politeness.",40,90
"The authors of the paper present a novel distribution for discrete variables called the *concrete distribution*. The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs. I think the paper is well written and sound, definitely of interest for the conference program. As to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets? Similarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not. I believe the paper should be accepted to the conference, however please consider that I*m not an expert in this field. Some minor observations/comments/issues: -Section 2.1: there is a repetition *be be* in the first paragraph. -Section 2.4: I would add a reference for the *multi-sample variational objective* -Section 3.1, just before Section 3.2: *the Gumbel is a crucial 1*. Why 1 and not *one*? -Section 3.3, last paragraph: *Thus, in addition to relaxing the sampling pass of a SCG the log...* I would add a comma after *SCG*. More in general, the second part of the paragraph is very dense and not easy to *absorb*. I don*t think it*s an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader. -Section 5.1, second paragraph: *All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k*. It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}. -Section 5.2: After *this distribution.* and *We will* there is an extra space. -If a compare the last formula in Section 5.3 with Eq. 8, I don*t see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?","The sentiment of the review is generally positive. The reviewer describes the paper as 'well written and sound' and of interest for the conference program, indicating a positive reception of the work. However, the reviewer also provides constructive criticism and questions, which are aimed at improving the paper, suggesting a balanced view rather than unbridled enthusiasm. The language used is polite and constructive, with suggestions for improvement and clarification rather than harsh criticism or negative language. The reviewer uses phrases like 'I would encourage' and 'please consider,' which are polite ways of making suggestions.",70,90
"This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation. PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results. CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It*s worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this. Other Comments: - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. - It initially seems strange to suggest a noisy-channel model as a way of addressing the *explaining away* problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.","The sentiment of the review appears to be generally positive, as the reviewer describes the paper as 'well written' and the SSNT model as 'quite interesting' and 'well motivated'. The empirical evaluation is praised as 'very well done', and the authors are credited with obtaining 'good results'. However, there are concerns raised about the complexity of the training and decoding, suggesting that the benefits might be achieved with simpler methods. The reviewer also provides constructive feedback and suggestions for improvement. Overall, the review is balanced with a lean towards the positive aspects of the paper. The language used in the review is polite and constructive. The reviewer uses phrases like 'One might be concerned' and 'it might be nice to see', which are tactful ways of expressing criticism. The use of 'PROs' and 'CONs' indicates a structured and respectful approach to feedback.",70,90
"This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point. While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions. Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.","The reviewer expresses a positive sentiment towards the novelty and elegance of the idea presented in the paper, which is indicated by phrases like 'quite elegant and novel' and 'I enjoy reading it.' However, the sentiment is tempered by the reviewer's call for more experiments and specific concerns about the methodology and results presented, which suggests a cautious optimism rather than unqualified praise. The language used is constructive and respectful, with suggestions for improvement rather than harsh criticism, which indicates a high level of politeness. Phrases like 'more experiments are needed,' 'probably the most important issue,' and 'I am not fully convinced' show a polite way of questioning and providing feedback without being confrontational.",50,80
"The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. Consequently, the optimized upper bound (log-loss) gets looser. As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. I would like to draw the author*s attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve). [1] Training Highly Multiclass Classifiers, Gupta et al. 2014. [2] Direct Loss Minimization for Structured Prediction, McAllester et al. [3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet. Final comment: I believe the material presented in this paper is of interest to a wide audience at ICLR. The problem studied is interesting and the proposed approach is sound. I recommend to accept the paper and increase my score (from 7 to 8).","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'well written' and the main observation as 'presented in a clear and refreshing way.' Additionally, the reviewer's final comment includes a recommendation to accept the paper and an increase in their score, which indicates a positive view. The language used is polite and constructive, with suggestions for improvement and additional references that could enrich the paper. The reviewer uses phrases like 'I would like to draw the author's attention to...' and 'A discussion on this could enrich the quality of the paper,' which are courteous and aimed at constructive feedback.",70,90
This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.,"The sentiment of the review appears to be generally positive, as the reviewer acknowledges that the paper is 'clearly written' and the model 'seems to work well.' The reviewer also appreciates that the code will be provided for re-implementation. However, there are concerns raised that suggest improvements, but these do not negate the overall positive tone. The language used is formal and constructive, with no signs of rudeness or personal attacks. The reviewer's tone is that of a critical but fair academic, aiming to provide constructive feedback rather than to disparage the work.",70,90
"This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.","The review expresses clear dissatisfaction with the paper, stating that the results are weaker than previous publications, the main theorem is unconvincing, and the paper is poorly written with a weak experimental section and formatting issues. There is no positive feedback or constructive tone detected in the review.",-80,-50
"After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper. Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity. I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper*s current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that. If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission. [Addendum: based on discussions here & revisions, I have revised my score]","The reviewer expresses a mixed sentiment. They acknowledge the value of the work and its worthiness for publication, but also express difficulty in understanding the paper and suggest that the current form may not be suitable for the intended conference venue. The reviewer's tone is constructive and focuses on the potential of the work, while also highlighting significant issues with clarity and presentation. The language used is polite and professional, offering criticism without being harsh or disrespectful.",25,75
"Sorry for the late review -- I*ve been having technical problems with OpenReview which prevented me from posting. This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to *learn to learn* by acquiring the ability to learn distributions from small numbers of examples. Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read. The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that *someone who thinks up statistics*. The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method. The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples? (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) Will the authors release the code?","The reviewer's sentiment appears to be generally positive, as they describe the paper as a 'nice addition to the literature' and praise its conceptual simplicity, elegance, and clear writing. They also commend the method's performance and the well-chosen experiments. However, there is a slight criticism regarding the paper's title and a question about the density of inputs in one of the figures, which prevents the sentiment from being entirely positive. The language used is polite and constructive, with the reviewer providing specific feedback and asking questions in a respectful manner. The apology for the late review and the overall tone suggest a polite and considerate approach to the review process.",70,90
"This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters. The authors assume that the network is already pruned and aim for compressing the non-pruned parameters. The problem of network compression is a well-motivated problem and of interest to the ICLR community. The main drawback of the paper is its novelty. The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks. It should be noted that the proposed method in this paper has not been proposed before. The paper is well-structured and easy to follow. Although it heavily builds on Han 2015, it is still much longer than Han 2015. I believe that there is still some redundancy in the paper. The experiments section starts on Page 12 whereas for Han 2015 the experiments start on page 5. Therefore, I believe much of the introductory text is redundant and can be efficiently cut. Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy. Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1? Some comments: 1) It is not clear whether the procedure depicted in figure 1 is the authors’ contribution or has been in the literature. 2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix. Can the authors please explain how this approximation affects the final compression? Also how much does one lose by making such an approximation? minor typos (These are for the revised version of the paper): 1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned 2) Page 2, one para to the end, last line: assigned for -> assigned to 3) Page 5, line 2, same as above 4) Page 8, Section 5, Line 3: explore -> explored","The reviewer's sentiment appears to be mildly positive, acknowledging the relevance of the work to the ICLR community and the good compression performance of the proposed method. However, the reviewer also points out the lack of novelty and redundancy in the text, suggesting that the paper is an incremental improvement over previous work. The language used is constructive and professional, with specific recommendations and questions for improvement, which indicates a polite tone.",30,80
The paper proposed a multi-memory mechanism that memorizes different information into different components/entities. It could be considered as a mixture model in RNN. This is a very interesting model and result is convincing. A limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.,"The reviewer expresses a positive sentiment towards the paper by describing the model as 'very interesting' and the results as 'convincing'. However, they also mention a limitation, which slightly moderates the overall positive sentiment. The language used is formal and constructive, with no negative or rude words, indicating a high level of politeness.",70,90
"The paper shows a different approach to a ternary quantization of weights. Strengths: 1. The paper shows performance improvements over existing solutions 2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning. Weaknesses: 1. The paper is very incremental. 2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is *what is new in the topic* update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience. 3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation *it is related to mobile, therefore it is cool* sufficient. This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience. Also - the code was not released is my understanding.","The sentiment in the review appears to be mostly critical, highlighting the paper's incremental nature, narrow audience, and lack of convincing motivation. The reviewer does acknowledge the paper's strengths, such as performance improvements and the novel idea of learning quantization, but these are overshadowed by the criticisms. Therefore, the sentiment is negative but not extremely so, as there is some recognition of the paper's contributions. The language used is polite and professional, with no use of rude or inflammatory language, despite the critical content of the review.",-60,80
"The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge. The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn*t learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules. If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though. I*m not sure we can realistically draw any conclusions about Figure 6 in the paper*s current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images. --- Added after rebuttal: I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.","The reviewer's sentiment appears to be mixed. They acknowledge the clear win and significant contribution of the paper but also express a lack of excitement and find the results relatively unsurprising. The reviewer's language is polite and constructive, offering specific recommendations without any harsh or rude comments.",20,80
"The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights. This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer. Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity. As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient. The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights. The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance. The paper is presented very clearly, presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data. The result tables are a bit confusing unfortunately. minor issues: p1 english mistake: “while networks *that* consist of convolutional layers”. p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse: In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.","The reviewer's sentiment appears to be quite positive, as they describe the paper as 'very clearly presented', containing 'very interesting ideas' and being 'state of the art for compression'. They also mention that the approach 'opens many new avenues of research' which is a strong endorsement. However, they do point out some confusion with the result tables and minor issues, which indicates that there is room for improvement. Therefore, the sentiment is not entirely positive. The language used is formal and constructive, with no signs of rudeness or impoliteness. The reviewer provides clear and specific feedback, which is a sign of a respectful and professional tone.",80,90
"This paper looks at how to train if there are significant label noise present. This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label. The second directly integrates out the true label and simply optimizes the p(z|x). Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated. Cons: the results on mnist is all synthetic and it*s hard to tell if this would translate to a win on real datasets. - comments: Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes? It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.","The reviewer's sentiment appears to be mildly positive, acknowledging the paper as 'good' and recognizing the real-world relevance of the problem being addressed. However, the sentiment is tempered by concerns about the novelty of the ideas and the applicability of the results to real datasets. The language used is formal and constructive, with suggestions for improvement, which indicates a polite tone. There is no use of harsh or disrespectful language.",30,80
"The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks. The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared *layers* being dependent of the task of interest. The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks. The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale. Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results. Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.","The sentiment of the review appears to be slightly critical but constructive. The reviewer acknowledges the work done and the readability of the paper but expresses concerns about the novelty and the experimental section. The language used is academic and neutral, with no strong positive or negative words, which suggests a sentiment score leaning towards the neutral side but with a slight negative bias due to the criticism. The politeness score is high because the language is formal, respectful, and follows the conventions of scientific critique without any harsh or rude language.",-20,80
"This paper proposes a model to learn across different views of objects. The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object. The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Furthermore, a comparison against human perception on the *Tenenbaum objects” is shown. Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below). The paper is reasonably written. Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach. More details: The “image purification” paper is very related to this work: [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015. There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval. If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]). It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/). Somewhat related to the proposed method is recent work on multi-view 3D object retrieval: [B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016. There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object). Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view. Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape): [C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016. [D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016. For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean). It has been shown that different layers and feature comparisons perform differently for a given task, e.g., [E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015.","The sentiment of the review appears to be slightly negative, as the reviewer points out both positives and negatives, but emphasizes the lack of novelty and missing references which are critical aspects of a scientific paper. The reviewer also suggests comparisons and improvements which indicate that the paper could be significantly enhanced. The language used is formal and constructive, without any personal attacks or harsh criticism, which is typical for academic discourse. The reviewer provides specific feedback and references to related work, which is a polite way of suggesting improvements.",-20,80
"This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what*s presented in the paper) that need to generate tree structures given an unstructured data. One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture. A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice. I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.","The reviewer's sentiment appears to be generally positive, acknowledging the well-motivated architecture and its potential applications. The reviewer also appreciates the authors' experimentation with different design decisions. However, they do point out a weakness regarding the limited number of experiments, suggesting a desire for more diverse applications. The language used is constructive and respectful, with suggestions for improvement rather than harsh criticism. The reviewer ends on a positive note, recommending acceptance of the paper for conference contribution.",70,90
"This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities. The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. Pros: - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow. Cons: - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. - The test accuracy performance on CIFAR-10 reported in the paper doesn*t look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, Other comments: - If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.","The reviewer's sentiment appears to be mixed. They acknowledge the novelty and theoretical analysis of the paper, which is positive, but they also express concerns about the convincing nature of the experiments and the performance reported, which is negative. The language used is formal and constructive, without any harsh or rude words, indicating a polite tone.",0,80
"This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let*s rename it to L*) should be the same as that in Eq. 2.1. For example, L* could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L* is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn*t the RHS in Eq 3.5 be sum 	ilde{y_{t,i}}(frac{hat{y}_t}{	ilde{y_{t,i}}} - e_i) ?","The reviewer's sentiment appears to be cautiously positive, acknowledging the 'nice setup' of the theoretical justification and the 'good' experimental results, while also pointing out areas of concern or confusion regarding the construction of the estimate and the use of the projection matrix L*. The language used is formal and constructive, with suggestions for improvement and clarification rather than harsh criticism. The reviewer also provides specific, minor comments that are helpful rather than dismissive or aggressive.",40,80
"This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below. It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. Minor comments: - E(z_k) in (3) and (4) are not defined. - E_x in (19) is not defined. - Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the theoretical analysis and the proposed reparametrization approach, which indicates an appreciation of the work. However, the reviewer also points out areas that are unclear and could be improved, which tempers the positivity of the sentiment. The language used is formal and constructive, without any negative or harsh words, suggesting a polite tone.",25,75
"This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs. The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we*re able to find such a z, then the generator can generate this particular training example. It*s demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor*s proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it*s harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z. The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric. There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: https://openreview.net/pdf?id=HJKkY35le Whether this paper is accepted or not, I encourage the authors to investigate this approach further, since the method is promising and interesting. # Post-rebuttal review The authors have incorporated changed in the paper by adding more experiments. These experiments now demonstrate the claims of the paper better. The paper was already well-written and introduced a novel idea and addressed an important problem. The only thing holding this paper back was unconvincing experiments, which now has been corrected. Thus, I would increase my score by 2 points, and recommend accepting the paper.","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'very clearly written' and acknowledge the significance and novelty of the problem being addressed. However, they also express concerns about the experiments and suggest improvements, indicating a balanced view. The language used is polite and constructive, with suggestions for further research and improvement rather than harsh criticism. The reviewer's tone remains respectful throughout, and they acknowledge the authors' efforts in revising the paper, which leads to an increased score recommendation.",50,90
"This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. -It will be more interesting to show results in other domains such as texts and images. -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.","The sentiment in the review appears to be generally positive, with the reviewer acknowledging the paper's major contribution, the careful ablation studies, and the impressive visual and quantitative results. The use of '+' signs indicates positive aspects, while '-' signs indicate areas for improvement, which are not overly critical but rather suggestions for enhancing the paper. The language used is formal and constructive, with no signs of rudeness or negativity, suggesting a high level of politeness.",70,90
"TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., *variable computation*) of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline. === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism. Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and _x0008_ar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are. This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated. A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well. === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell. === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments. The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions. One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar. === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion! References Chung et al., *Hierarchical Multiscale Recurrent Neural Networks,* in 2016. Graves et al., *Adaptive Computation Time for Recurrent Neural Networks,* in 2016. Wu et al., *On Multiplicative Integration with Recurrent Neural Networks,* in 2016.","The sentiment of the review appears to be critical and points out several shortcomings of the paper, such as unconvincing experimental results, limited comparison to other work, and a lack of explanation for certain choices. However, the reviewer does acknowledge a novel aspect of the work, which is the gating mechanism, and finds an experiment in the paper 'cool'. The language used is professional and constructive, with no personal attacks or overly harsh wording, but it is direct in its criticism. The reviewer also provides specific recommendations for improvement, which indicates a constructive approach despite the critical sentiment.",-40,70
"The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s. It is only an incremental but important step. Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its *creative* regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over. We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights. Some disagreements with the theoretical suppositions in the paper: i) The VAE-s posterior converges to the prior faster than we would like because the gradients of the *generative* error (the KL divergence of prior and posterior) w.r.t. mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp. gradients of the reconstruction error. Especially when more *hairy* decoders like pixelCNN are used. We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain. Dumbing-down the decoder via variations of dropout or *complexifying* the sampler as in here, or slapping the generative error with a *DeepMind* constant (beta_VAE), are the natural band-aids, but seem to fail in the *creative* regime, for real-life sets like CIFAR10 or more complex ones. Other conceptual solutions are needed, some are discussed in [2]. ii) The claim near the end of section 2.2 that *the extra coding cost a.k.a. variational error will exist and will not be negligible* is a speculation, which, in our empirical experience at least, is incorrect. The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work. In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc. Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations. On the lighter side, guys, please do not recycle ridiculous terms like *optimizationally challenged*, as in section 2.2! The English language has recently acquired *mentally-challenged*, *emotionally-challenged*, etc, and now political correctness has sadly found its way to machines? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","The sentiment of the review appears to be mixed with a slight positive inclination. The reviewer acknowledges the elegance of the AR prior and its importance in improving VAEs, which suggests a positive sentiment. However, they also describe it as 'incremental' and express disappointment with certain aspects, indicating some reservations. The use of phrases like 'unfortunately poor generative qualities' and 'lacking on theoretical insights' in reference to other papers suggests a critical tone, but the overall sentiment is not entirely negative as the reviewer hopes to see the paper accepted. The language used is mostly formal and technical, with some informal phrases like 'CS mumbo-jumbo' and 'ridiculous terms'. The reviewer's use of humor and sarcasm, as seen in the comment about 'political correctness', could be perceived as slightly impolite, but it is not overtly rude. Overall, the language is critical but not disrespectful.",20,40
"The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings. The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4. Detailed comments: 1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA. What do you think are the two dominant latent factors encoded in figure 1 (c)? 2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this? 3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left.","The sentiment of the review appears to be slightly critical but constructive. The reviewer acknowledges the method out-performs competing algorithms, which is positive, but also points out that the contribution is relatively small and lacks clarity on the source of its advantage. The language used is formal and constructive, with no signs of rudeness or personal attacks, focusing on the content and methodology of the work. Specific recommendations and questions are posed in a direct but polite manner, aiming to improve the clarity and depth of the research.",20,80
"The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008). The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.","The sentiment in the review is generally positive, as the reviewer acknowledges the theoretical motivation and appreciates the authors' approach to visualizing input feature maps. The reviewer also commends the authors for presenting results across multiple networks and classes, indicating that the method identifies class-discriminative features. However, there is a constructive criticism regarding the comparison of visualization methods, which suggests a slight deduction in the overall positive sentiment. The language used is polite and professional, with the reviewer providing constructive feedback and encouragement to the authors to include additional material in the appendix. There is no use of harsh or disrespectful language, and the tone is that of a collegial discussion aimed at improving the work.",70,90
"This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods. Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?","The reviewer's sentiment appears to be cautiously positive, acknowledging the interest and insightfulness of the work but also pointing out areas that require further investigation. The language used is constructive and respectful, suggesting improvements without dismissing the work. The reviewer does not express strong negative emotions but rather provides a balanced critique.",50,80
"SUMMARY This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. PROS The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. CONS The main result appears to address only strongly convex univariate functions. SPECIFIC COMMENTS - Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. I still have not read the revision and maybe you have already addressed some of these points there. - The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. - In Lemma 3, there is an i that should be x - In Theorem 4, ``	ilde f** is missing the (x). - Theorem 11, the lower bound always increases with L ? - In Theorem 11, _x0008_f xin [0,1]^d?","The sentiment of the review appears to be generally positive, as the reviewer acknowledges the interesting combination of tools and the nice result on the exponential superiority of depth. However, there is a slight critique regarding the scope of the main result, which is limited to strongly convex univariate functions. The language used is polite and constructive, with suggestions for improvement and clarification rather than harsh criticism. The reviewer uses phrases like 'Thanks for the comments' and 'I would suggest,' which are courteous and offer guidance without being confrontational.",70,90
"This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.","The reviewer's sentiment appears to be positive, as they acknowledge the paper sets a new state-of-the-art, improves over previous models, and provides a focused contribution that could be useful to the NLP community. They also use affirming language such as 'very nice!' and express hope for the paper's acceptance. The language used is polite and constructive, with no negative or rude remarks, and includes praise for the paper's contributions.",80,90
"The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability. Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs. The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case. Overall, the paper bears great potential. However, I do see some points. 1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available. I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks. 2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis. 3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away. An extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2) Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.","The sentiment of the review appears to be mixed with a lean towards the negative side. The reviewer acknowledges the paper as well-written and the method as clearly represented, which would suggest a positive sentiment. However, the reviewer also expresses significant concerns about the lack of a complete hyperparameter search, the insufficient mathematical investigation of the method, and the limited types of data sets used in the experiments. These criticisms indicate that the reviewer is not fully convinced of the paper's contributions, leading to a negative sentiment. The language used in the review is formal and constructive, without any personal attacks or overly harsh wording, indicating a high level of politeness.",-30,80
"The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question. The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. This of course does not handle complex factors like *type of legs* but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!) By complete luck, the authors may have hit upon something else of fundamental importance: the letter *beta* for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative *energy* and *entropy* and is partially addressed in section 2.9 of reference [1]. The correct definition of *generative temperature* is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the *correct* physical model instead. A question to the authors: creating a new metric (*disentanglement*) as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","The sentiment of the review appears to be mixed with a lean towards the negative. The reviewer acknowledges the importance and difficulty of the problem addressed in the paper, which could be seen as positive. However, they quickly pivot to a critical tone, expressing skepticism about the proposed solution and its usefulness, and they compare it unfavorably to existing work. The reviewer also uses a somewhat sarcastic tone, referring to the 'curious but hardly useful toy' and the 'schizophrenia' metaphor, which suggests a negative sentiment towards the paper's contributions. The politeness of the language is formal and maintains the decorum expected in scientific discourse, but the use of phrases like 'slapping a large multiplicative factor' and the metaphorical reference to schizophrenia could be perceived as slightly disrespectful or dismissive. Overall, the language is critical but not overtly rude.",-30,40
"1. the QA model is not novel, very similar to the existing model. 2. The IQA model is very confusing. If it needs human interactive in the training process, how could it be practical to ask human to join the training in each iteration? It sounds impractical. If the human interactive questions are predefined, then it is not interactive at all, since it is not based on the current state of model output.","The reviewer's comments suggest a negative sentiment as they express that the QA model is not novel and that the IQA model is confusing and impractical. There is no positive feedback or constructive suggestions provided. The language used is direct and to the point, without any personal attacks or harsh language, which indicates a neutral level of politeness.",-60,20
"The system described works comparably to bi-directional LSTM baseline for NMT, and CNN*s are naturally parallelizable. Key ideas include the use of two stacked CNN*s (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN*s for translation has been attempted previously (as described by the authors), but presumably it is the authors* combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN*s, whereas earlier attempts were not. They describe system*s sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN*s). The experimental results are well reported in detail. One or two figures would definitely be required to help clarify the architecture. This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","The reviewer's sentiment appears to be cautiously positive, acknowledging the system's comparability to existing models and detailing the combination of architectural choices that contribute to its performance. However, there is a hint of skepticism regarding the paper's fit for the conference, which suggests a slight reservation. The language used is formal and constructive, with no signs of rudeness or strong negative criticism, indicating a high level of politeness.",50,90
"This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames. Pros: - Paper is well written and easy to follow. - Model is clear to understand. Cons: - The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope. This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","The reviewer's sentiment appears to be mildly positive, acknowledging the paper's clarity and the successful demonstration of the model's capabilities. However, the reviewer also points out the limited scope and incremental nature of the work, suggesting that it is a precursor to a potentially more substantial paper. The language used is constructive and respectful, with clear pros and cons, which indicates a polite approach to critique.",30,80
"The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks). Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP *complexity*) of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call *successive regularization*. Experiments: - The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable; - The information *transferred* from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing. - The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.","The review provides a balanced critique of the authors' work, acknowledging the value of the experimental analysis while also pointing out areas that lack clarity or require further investigation. The reviewer's tone is constructive, focusing on the substance of the work rather than the authors themselves. There is no use of harsh language or personal attacks, which indicates a polite approach to the critique. The reviewer's sentiment seems neutral to slightly positive, as they recognize the value in the authors' approach but also suggest areas for improvement without dismissing the work outright.",20,80
"This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words. At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this. However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. - I cannot find where f_{symbkey} / f_{voca} / f_{copy} are defined - w^v, w^s are confusing. - e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough. - (h_t, c_t) = f_LSTM(x_{t?1}, h_{t?1}) c_t is not used? - The notion of “fact embeddings” is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end). For the anchor / “topic-itself” facts, do you learn the embedding for the special relations and use the entity embeddings from TransE? On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first. Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention “Obama” for the entity “Barack Obama”?","The reviewer expresses a positive sentiment towards the motivation of the paper, indicating an appreciation for the idea of incorporating knowledge base facts into language modeling. However, the sentiment becomes neutral to slightly negative when the reviewer points out difficulties in following the paper's details and suggests that the writing needs significant improvement. The language used is polite and constructive, with the reviewer providing specific questions and feedback without any harsh or rude language.",20,80
"The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.’s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful. I have a few comments and reservations with the paper: 1) Although not mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons. 2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. 3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature. 4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4) 5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. 6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed. All in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's contributions and potential impact while also pointing out areas for improvement. The language used is constructive and respectful, with suggestions for further development and comparisons. Politeness is maintained throughout the review, with criticisms framed as recommendations rather than direct negative comments.",60,90
"The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. Although the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in *LSTM: A Search Space Odyssey*). Besides, those extensions are not really novel.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges that the paper is well written, which is positive, but also points out significant shortcomings in the experimental section, which is negative. The balance between the positive acknowledgment and the critical feedback suggests a slightly negative sentiment, but not overly so, as the criticism is aimed at improving the work. The language used is formal and appropriate for a scientific peer review, with no use of rude or disrespectful language, indicating a high level of politeness.",-30,80
"The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines). On these networks, the question of optimization has so far not been studied as extensively as for more standard networks. A study specific to this class of models is therefore welcome. Results consistently show better optimization properties from adding noise in the training procedure. One issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise. A comparison to results obtained in the literature would be desirable. For example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy). I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.","The sentiment of the review appears to be mildly positive, as the reviewer acknowledges the welcome nature of the study and notes that the results consistently show better optimization properties. However, the reviewer also expresses a concern regarding the effectiveness of the proposed optimization strategy in learning good models, suggesting that the paper lacks a comparison with results obtained in the literature. The language used is formal and constructive, with no signs of rudeness or personal attacks, indicating a high level of politeness.",30,80
"Comments: *This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied, which often have telltale noise* Is this really true? If it were the case, wouldn*t it imply that training *against* adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)? Pros: -The question of whether adversarial examples exist in generative models, and indeed how the definition of *adversarial example* carries over is an interesting one. -Finding that a certain type of generative model *doesn*t have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result. -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014). Is this because it*s actually harder to find adversarial examples in these types of generative models? Issues: -Paper is significantly over length at 13 pages. -The beginning of the paper should more clearly motivate its purpose. -Paper has *generative models* in the title but as far as I can tell the whole paper is concerned with autoencoder-type models. This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they*re distinct from a paper called *adversarial examples for generative models*. -I think that the introduction contains too much background information - it could be tightened.","The sentiment of the review appears to be critical but constructive. The reviewer questions the validity of a statement made in the paper and challenges the authors to consider the implications of their claim. They also highlight both positive aspects (interesting question, convincing examples) and issues (over length, lack of clarity, and narrow focus). The language used is direct and professional, without any personal attacks or overly harsh criticism. The reviewer's tone suggests they are engaged with the content and aim to improve the paper, rather than dismiss it outright.",-10,70
"The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks. The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos). Pros: 1) Put an old algorithm to good use in a new setting 2) The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention). This contributes to the efficient trainability of the model 3) Seems to dovetail nicely with other results to encourage sparsity--that is, it can be used simultaneously--and is quite generalizable. Cons: 1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity--some sort of comparison involving training time would be great, and is currently lacking. EDIT: Authors addressed this by addition of results in Appendix B. 2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment. Is this a general feature? Is this a statistical fluke? etc. Even if the answer is *it is not obvious, and determining why goes outside the scope of this work*, I would like to know it! EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A. 3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field. I don*t think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper. EDIT: Authors addressed this by followup to question and additional text in the paper. Additional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7. The core of this paper is quite solid, it just needs a little bit more polishing. EDIT: Score has been updated. Note: the authors probably meant *In order to verify* in the first sentence of Appendix A.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the solid core of the paper and the successful application of an old algorithm in a new setting. The reviewer also appreciates the improvements made by the authors in response to previous comments, which is indicative of a positive sentiment towards the authors' efforts. However, the reviewer still has some reservations and suggestions for further improvement, which prevents the sentiment from being entirely positive. The language used in the review is polite and constructive. The reviewer uses phrases like 'it would be nice to see' and 'I would very much like to see,' which are courteous ways of making suggestions. The use of 'please' or similar language is not present, but the overall tone is respectful and focused on the content rather than personal criticism, which indicates a high level of politeness.",60,80
"The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important. However, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances. Main concerns: 1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance! 2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks. 3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well. Minor concern: 1) Why did the authors choose to use batch normalization in NiN and AlexNet?","The reviewer acknowledges the effort put into the paper and the importance of the topic, which contributes to a slightly positive sentiment. However, they also express a clear disappointment regarding the practical value of the findings and the lack of discussion on realistic circumstances, which indicates a negative sentiment. The language used is formal and constructive, without any personal attacks or harsh words, which suggests a high level of politeness.",-20,80
"First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here. Major points: -- *As we saw in the previous section, the minima of deep network loss functions are for the most part decent.* All you said in the previous section was that theory shows that there are no bad minima under *strong assumptions*. There is no practical proof that minima do not vary in quality. -- *This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a *decent minima quickly* is reduced to the task of finding any minima quickly.* First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being *reduced to finding any minima quickly*. -- Figure 1 I don*t like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don*t think you did. Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don*t join up horizontally. Please be more careful. -- Misuse of the transient phase / minimization phase concept In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training? -- Only 1 dataset You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset. -- Many figures are unclear For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some. -- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc. -- Lack of confidence intervals The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented. -- Lack of information regarding learning rate There is big question mark left open regarding how all your results would change if different learning rates were used. You don*t even tell us how you chose the learning rates from the intervals you gave in section 3.4. -- Lack of information regarding the absolute distance of interpolated points In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big *hump* between them, it means that those points are more *brittle* than if they are far apart and there is a big *hump* between them. Minor points: -- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used. -- The font size for the legends in the upper triangle of Table 1 is too small. You can*t just write *best viewed in zoom* in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the effort put into the paper and expresses a desire to see an improved version published, which is positive. However, they also point out that the paper is below the standard for a conference paper and list several significant issues that need to be addressed. This mix of appreciation for the effort and critical feedback on the content suggests a sentiment that is neither fully positive nor fully negative, hence a score leaning towards the neutral-positive range. The language used is direct and critical but maintains a professional tone without personal attacks or harsh language, indicating a high level of politeness.",30,80
"This paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross-view similarity between neighborhoods of data samples. The motivation comes from information retrieval tasks. Authors position their work as an alternative to CCA-based multiview learning; note, however, that CCA based techniques have very different purpose and are rather broadly applicable than the setting considered here. Main points: - I am not sure what authors mean by time complexity. It would appear that they simply report the computational cost of evaluating the objective in equation (7). Is there a sense of how many iterations of the L-BFGS method? Since that is going to be difficult given the nature of the optimization problem, one would appreciate some sense of how hard or easy it is in practice to optimize the objective in (7) and how that varies with various problem dimensions. Authors argue that scalability is not their first concern, which is understandable, but if they are going to make some remarks about the computational cost, it better be clarified that the reported cost is for some small part of their overall approach rather than “time complexity”. - Since authors position their approach as an alternative to CCA, they should remark about how CCA, even though a nonconvex optimization problem, can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation. The method proposed in the paper does not seem to be tractable, at least not immediately. - The empirical results with synthetic data are a it confusing. First of all the data generation procedure is quite convoluted, I am not sure why we need to process each coordinate separately in different groups, and then permute and combine etc. A simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable. I am also baffled why CCA does not recover the true subspace - arguably it is the level of additive noise that would impact the recoverability - however the proposed method is nearly exact so the noise level is perhaps not so severe. It is also not clear if authors are using regularization with CCA - without regularization CCA can be have in a funny manner. This needs to be clarified.","The reviewer's sentiment appears to be critical but constructive. They question the authors' claims and methods, suggesting that the paper's approach is not clearly superior or well-justified compared to existing techniques. The reviewer also points out potential issues with the empirical results and asks for clarifications. Despite the critical nature, the review does not express negativity towards the authors' efforts but rather focuses on areas for improvement. The language used is formal and academic, with no personal attacks or harsh wording, indicating a polite discourse.",-20,80
"The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It*s interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.","The sentiment of the review appears to be constructive but with a critical tone, as the reviewer acknowledges the interesting approach but points out several concerns with the work. The language used is formal and provides clear feedback without any personal attacks or overly harsh criticism, which indicates a neutral to slightly negative sentiment due to the concerns raised. The politeness is high, as the reviewer uses respectful language and encourages further experimentation, which is typical of academic discourse.",-20,80
"This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input. The paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character? Although it is zooming in, shouldn’t it attend to relevant parts of the character? Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from. While the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.","The sentiment of the review appears to be mildly positive with constructive criticism. The reviewer acknowledges the significant improvements and state-of-the-art results, which contributes to a positive sentiment. However, they also point out areas that are still lacking, suggesting room for improvement. This balance of praise and constructive feedback suggests a sentiment that is slightly positive but not overly so. The language used is polite and professional, with no use of rude or aggressive language. The reviewer uses phrases like 'significantly improved', 'reflects changes', and 'could benefit from', which are indicative of a respectful tone aimed at helping the authors improve their work.",30,80
"This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","The sentiment in the review is generally positive towards the contribution of the paper, acknowledging the value of applying the HDP-HMM technique to bioacoustics segmentation. However, it also expresses a concern regarding the lack of comparison with simpler methods, which suggests a constructive criticism. The language used is professional and provides clear feedback without any negative or harsh wording, indicating a polite tone.",50,80
"This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs). While this idea in itself isn*t new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data. Broadly this is fine, but unfortunately the paper then falls short in a number of ways. For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion. Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion. This paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods. In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don*t really add anything to the story.","The sentiment of the review starts neutral by acknowledging the paper's topic and the idea of using aligned text across languages. However, it quickly turns negative as the reviewer points out the shortcomings of the paper, such as the convoluted model section and the lack of comparison with other work. The reviewer also criticizes the paper for not evaluating the learned embeddings in downstream tasks and for providing insufficient results. The language used is formal and professional, with no personal attacks or overly harsh wording, but it does convey clear criticism of the work. Therefore, the sentiment is negative but not extremely so, and the politeness is maintained throughout the critique.",-60,70
"This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard “fine-tuning” approach. Pros: -The method is simple and clearly explained. -Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest. -Experiments are performed in multiple domains -- vision and NLP. Cons: -The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the “stiched” network). These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important. -Given these large additional costs, the core of the idea is not sufficiently validated, to me. In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed: (1) Allowing the original network weights to be learned for the target task, as well as the additional module. Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network. (2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task. Outperforming this baseline would verify that having a set of weights which never “sees” the source dataset is useful. -The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks. I’ve never seen networks pre-trained on CIFAR deployed anywhere, and it’s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet. (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.) Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application. =============== Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction. However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the *stitched network* idea which is the real novelty of the paper. Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (*ResNet+VGG > ResNet* would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don*t sufficiently validate the stitched network idea, in my opinion.","The reviewer's sentiment appears to be cautiously critical. They acknowledge the simplicity and clarity of the proposed method and its relevance due to the wide use of standard fine-tuning. However, they also express significant concerns about the practicality of the method due to increased computational costs, lack of sufficient validation, and absence of evaluation on a key dataset (ImageNet). The reviewer's tone is constructive, providing specific recommendations for improvement, but the overall sentiment is not fully positive due to the highlighted shortcomings. The language used is polite and professional, with the reviewer using academic tone and providing criticism in a respectful manner without any harsh or rude language.",-20,80
"The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov*14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding. For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents. Pros: - the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton *09 - the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton *09, but since in the meantime document representations improved (Le & Mikolov*14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM Cons: - the insertion of the sigmoid to produce binary codes (from Lin & al. *15) in the training process is incremental - the explanation is too abstract and difficult to follow for a non-expert (see details below) - a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR*16 Detailed comments: Section 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why *embedding lookup* and *linear projection* cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this). p2: *This way, the length of binary codes is not tied to the dimensionality of word embeddings.* -> why not? section 3: This is the experimental setup of Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups. *similarity of the inferred codes*: say here that codes are compared using Hamming distances. *binary codes perform very well, despite their far lower capacity* -> do you mean smaller size than real vectors? fig 5: these plots could be dropped if space is needed. section 3.1: one could argue that *transferring* from Wikipedia to anything else cannot be called transferring, since Wikipedia*s purpose is to include all topics and lexical domains section 3.2: specify how the 300D real vectors are compared. L2 distance? inner product? fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.","The sentiment of the review appears to be constructive with a balance of positive and negative feedback. The reviewer acknowledges the strengths of the paper, such as the binary representation outperforming another method and the sound experimental approach. However, they also point out areas for improvement, such as the incremental nature of the method, the abstract explanation, and missing comparisons. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite tone. The reviewer provides detailed comments and suggestions for improvement, which is a sign of engagement and a positive attitude towards the work, despite the criticisms.",20,80
"This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc. The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC. The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS. With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed. Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal. [R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf PROS: Novel and intriguing idea Strong theoretical guarantees CONS: Resulting boosted model is un-normalized Discriminator based boosting is expensive, due to sampling via MCMC Weak experimental section","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the novelty and potential impact of the work while also pointing out significant issues with the experimental section and the practicality of the approach. The language used is constructive and respectful, with the reviewer expressing willingness to overlook some issues in anticipation of future improvements. The reviewer's tone is critical but not dismissive, indicating a balanced perspective.",25,75
"This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN*s(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training. The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. The empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation. Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary. In addition, it is not clear how sensitive the boosting to the selection of injection point. This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant. Pros -provides some preliminary results for boosting of Res Nets Cons -not sufficiently novel: an incremental approach -empirical analysis is not satisfactory","The reviewer's sentiment appears to be critical and points out several shortcomings of the paper, such as lack of novelty, unsatisfactory empirical analysis, and incomplete comparison with state-of-the-art methods. However, the language used is formal and maintains a professional tone without any personal attacks or overly harsh language. The reviewer also acknowledges the preliminary results provided by the authors, which indicates a balanced approach to the critique.",-60,70
"CONTRIBUTIONS This paper extends end-to-end CNN+RNN image captioning model with auxiliary attribute predictions. It proposes five variants of network architectures, which take the attribute/image features in alternating orders or at every timestamp. The attributes (i.e., 1,000 most common words on COCO) are obtained by attribute classifiers trained by a multiple instance learning approach. The experiment results indicated that having these attributes as input improves captioning performance on standard metrics, including BLEU, METEOR, ROUGE-L, CIDEr-D. NOVELTY + SIGNIFICANCE All five variants of the network architectures, shown in Fig. 1, have followed a standard seq-to-seq LSTM model. The differences between these variants come from two aspects: 1. the order of image/attribute inputs; 2. whether to input attribute/image features at each time step. No architectural changes have been added to the proposed model over a standard seq-to-seq model. The proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas. In particular, Fang et al. 2015 and You et al. 2016 have both used attributes for image captioning. This work has used the same multiple instance learning procedure as Fang et al. 2015 to train visual detectors for common words and used detector outputs as conditional inputs to a language model. In addition, the idea of using image feature as input to every RNN timestamp has been widely explored, for instance, in Donahue et al. 2015. The authors did not offer a clear explanation about the technical contribution of this work over these existing approaches. CLARITY First, it is not clear to me which image features have been used by the baseline methods. As the baselines may rely on different image representations, the experiments would not offer a completely fair comparison. For example, the results of the attention-based models in Table 1 are directly copied from Xu et al., 2015, which were reported with Oxford VGG features, instead of GoogLeNet used by LSTM-A. Even if the baselines do not use the same types of features, it should at least be explicitly mentioned. Besides, the fact that the results of Table 1 and Table 2 are reported with different features (GoogLeNet v.s. ResNet) are not described clearly in the paper. “We select 1,000 most common words on COCO…” How would this approach guarantee that the selected attributes have clear semantic meanings, as many words among the top 1000 would be stop words, abstract nouns, non-visual verbs, etc.? It would be interesting to perform some quantitative analysis to see whether these 1000-dimensional attribute vectors actually carry semantic meanings, or merely capture biases of word distributions from the ground-truth captions. One possible way to justify the importance of semantic attributes is to experiment with ground-truth attributes or attribute classifiers trained on annotated sources such as COCO-Attribute (Patterson et al. 2016) or Visual Genome (Krishna et al. 2016). EXPERIMENTS It would be interesting to analyze how the behavior of the seq-to-seq model changes with respect to the additional attribute inputs. My hypothesis is that the model’s word choices will shift towards words with high scores. This experiments will help us understand how the model can take advantage of the auxiliary attribute inputs. Since the attributes are selected as the most frequent words from COCO, it is likely for the model to overfit the metrics, such as BLEU, that rely on word matching. On the other hand, it has been shown that these automatic caption evaluation metrics do not necessarily correlate with human judgment. Therefore, I think it is necessary to conduct a human study to convince the readers the quality improvement of the proposed model is not caused by overfitting to metrics. SUMMARY This paper demonstrates that image captioning can be improved by having attributes as auxiliary inputs. However, the model has minor novelty given existing work that has explored similar ideas. Besides, more analysis is necessary to demonstrate the semantic meanings of attributes. A human study is recommended to justify the actual performance improvement. Given these points to be improved, I would recommend rejecting this paper in its current form.","The sentiment of the review is critical and constructive, suggesting improvements and pointing out the lack of novelty and necessary analyses. The reviewer does not express satisfaction with the paper and recommends rejection in its current form, indicating a negative sentiment. However, the language used is formal and respectful, adhering to academic standards of critique without any personal attacks or harsh language, which reflects a high level of politeness.",-60,80
"The proposed regularizer seems to be a particular combination of existing methods. Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion.","The reviewer's sentiment appears to be mildly critical, acknowledging an intriguing aspect of the work but ultimately finding it not significantly better than similar methods. The language used is formal and constructive without any personal attacks or overly harsh criticism, which indicates a neutral to slightly negative sentiment and a polite tone.",-30,70
"Summary: The paper presents an approach – Neural Answer Construction Model for the task of answering non-factoid questions, in particular, love-advice questions. The two main features of the proposed model are the following – 1) it incorporates the biases of semantics behind questions into word embeddings, 2) in addition to optimizing for closeness between questions and answers, it also optimizes for optimum combination of sentences in the predicted answer. The proposed model is evaluated using the dataset from a Japanese online QA service and is shown to outperform the baseline model (Tan et al. 2015) by 20% relatively (6% absolutely). The paper also experiments with few other baseline models (ablations of the proposed model). Strengths: 1. The two motivations behind the proposed approach – need to understand the ambiguous use of words depending on context, and need to generate new answers rather than just selecting from answers held by QA sites – are reasonable. 2. The novelty in the paper involves the following – 1) incorporating biases of semantics behind questions into word embeddings using paragraph2vec like model, modified to take as inputs - words from questions, question title token and question category token, 2) modelling optimum combination of sentences (conclusion and supplement sentences) in the predicted answer, 3) designing abstract scenario for answers, inspired by automated web-service composition framework (Rao & Su (2005)), and 4) extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism (attention mechanism is similar to Tan et al. 2016). 3. The proposed method is shown to outperform the current best method (Tan et al. 2015) by 20% relatively (6% absolutely) which seems to be significant improvement. 4. The paper presents few ablations studies that provide insights on how much different components of the model (such as incorporating biases into word embeddings, incorporating attention from conclusion to supplement) are helping towards performance improvement. Weaknesses/Suggestions/Questions: 1. How are the abstract patterns determined, i,e., how did the authors determine that the answers to love-advice questions generally constitute of sympathy, conclusion, supplement for conclusion and encouragement? How much is the improvement in performance when using abstract patterns compared to the case when not using these patters, i.e. when candidate answers are picked from union of all corpuses rather than picking from respective corpuses (corpuses for sympathy, conclusion etc.). 2. It seems that the abstract patterns are specific to the type of questions. So, the abstract patterns for love-advice will be different from those for business advice. Thus, it seems like the abstract patterns need to be hand-coded for different types and hence one model cannot generalize across different types. 3. The paper should present explicit analysis of how much combinational optimization between sentences help – comparison between model performance with and without combinational optimization keeping rest of the model architecture same. The authors could also plot the accuracy of the model as a function of the combinational optimization scores. This will provide insights into how significant are the combinational optimization scores towards overall model accuracy. 4. Paper says that current systems designed for non-factoid QA cannot generalize to questions outside those stored in QA sites and claims that this is one of the contributions of this paper. In order to ground that claim, the paper should show experimentally how well the proposed method generalized to such out-of-domain questions. Although the questions asked by human experts in the human evaluation were not from the evaluation datasets, the paper should analyze how different those questions were compared to the questions present in the evaluation datasets. 5. For human evaluation, were the outputs of the proposed model and that of the QA-LSTM model judged each judged by both the human experts OR one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system? If both the sets of outputs were each judged by both the human experts, how were the ratings of the two experts combined for every questions? 6. I wonder why the authors did not do a human evaluation where they just ask human workers (not experts) to compare the output of the proposed model with that of the QA-LSTM model – which of the two outputs they would like to hear when asking for advice. Such an evaluation would not get biased by whether each sentence is good or not, whether the combination is good or not. Looking at the qualitative examples in Table 4, I personally like the output of the QA-LSTM more than that of the proposed model because they seem to provide a direct answer to the question (e.g., for the first example the output of the QA-LSTM says “You should wait until you feel excited”, whereas the output of the proposed model says “It is better to concentrate on how to confess your love to her” which seems a bit indirect to the question asked.) 7. Given a question, is the ground-truth answer different in the two tasks -- answer selection and answer construction? 8. The paper mentions that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (section 5.2). So, why is its accuracy lower than that of QA-LSTM in table 1. The authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy. But, did these issues not exist in the dataset used by Tan et al. 2016? 9. The paper says the proposed method achieves 20% gain over current best (in Conclusion section) where they refer to QA-LSTM as the current best method. However, in the description of Attentive LSTM (section 5.2), the paper mentions that Attention LSTM is the current best method. So, could authors please clarify the discrepancy? 10. Minor correction: remove space between 20 and % in abstract. Review Summary: The problem of non-factoid QA being dealt with in the paper is an interesting and useful problem to solve. The motivations presented in the paper behind the proposed approach are reasonable. The experiments show that the proposed model outperforms the baseline model. However, the use of abstract patterns to determine the answer seems like hand-designing and hence it seems like these abstract patterns need to be designed for every other type of non-factoid question and hence the proposed approach is not generalizable to other types. Also, the paper needs more analysis of the results to provide insights into the contribution of different model components.","The sentiment of the review appears to be constructive and balanced, with the reviewer acknowledging the strengths of the paper while also providing critical feedback on areas that need improvement. The reviewer does not express any extreme negative sentiment but rather focuses on the potential of the paper and how it could be enhanced. The language used is formal and academic, with no signs of rudeness or disrespect. The reviewer uses phrases like 'The motivations presented in the paper behind the proposed approach are reasonable' and 'The experiments show that the proposed model outperforms the baseline model,' which indicate a positive sentiment towards the work. However, the reviewer also lists several weaknesses and suggestions for improvement, which tempers the overall sentiment. The language is polite and respectful, with the reviewer using phrases such as 'Weaknesses/Suggestions/Questions' to frame their critique, which is typical of scholarly discourse.",50,90
"This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA. Technical comments: - In equation (1), there is a position-relevant component call *positional importance*. I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order. - A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of with respect to the whole document. - For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. - I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures. - It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.","The sentiment of the review appears to be constructive and neutral, with the reviewer acknowledging the strengths of the paper while also providing specific areas for improvement. The reviewer states that the results are 'either better or close to the SOTA' which is a positive comment, but also points out areas of potential weakness or necessary clarification, such as the importance of certain components and the simplicity of the decoding algorithm. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite tone. The reviewer's suggestions are framed as questions or thoughts rather than direct negative comments, which contributes to the overall politeness of the review.",20,80
"The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one. The presentation of the paper is not very clear, the writing can be improved. Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful. Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. Overall, I think this work should be clarified and improved to be a good fit for this venue.","The reviewer's sentiment appears to be more on the critical side, pointing out lack of clarity, the need for improvement in writing, and questioning the practicality and originality of the work. However, the critique is constructive rather than dismissive, suggesting that the work could be a good fit for the venue with clarifications and improvements. The language used is formal and maintains a professional tone without any personal attacks or overly harsh language, indicating a polite approach to criticism.",-30,70
"Description: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised *Bloom embeddings*. The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives. Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs. Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class. The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach. Evaluation: It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test. The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step. In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?) Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable. Minor points: As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned. The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.","The sentiment of the review appears to be critical and points out a lack of novelty in the paper, straightforward application of existing techniques, and insufficient comparison in the experiments. The reviewer does not express enthusiasm or positive feedback, which suggests a negative sentiment. However, the language used is formal and constructive, without any personal attacks or harsh words, indicating a polite tone. The reviewer provides specific points of critique and suggestions for improvement, which is typical for academic discourse.",-60,80
"Apologies for the late review. This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample. Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised). The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present). Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place. Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure. Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.","The reviewer's sentiment appears to be constructive but critical. They acknowledge the novelty of the method but also point out that it is incremental and critique the evaluation metrics as flawed. The language used is professional and provides clear feedback without any personal attacks or harsh language, indicating a neutral to slightly negative sentiment due to the criticism. The politeness of the language is high, as the reviewer uses academic tone and provides criticism in a respectful manner, without any rude or aggressive language.",-20,80
"The authors proposes an interesting idea of connecting the energy-based model (descriptor) and the generator network to help each other. The samples from the generator are used as the initialization of the descriptor inference. And the revised samples from the descriptor is in turn used to update the generator as the target image. The proposed idea is interesting. However, I think the main flaw is that the advantages of having that architecture are not convincingly demonstrated in the experiments. For example, readers will expect quantative analysis on how initializing with the samples from the generator helps? Also, the only quantative experiment on the reconstruction is also compared to quite old models. Considering that the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison to that model. ** Minor - I*m wondering if the analysis on the convergence is sound when considering the fact that samples from SGLD are biased samples (with fixed step size). - Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?","The sentiment in the review appears to be mildly positive towards the idea but critical of the execution and evidence provided. The reviewer calls the idea 'interesting' twice, which suggests a positive view of the concept. However, the use of 'However' indicates a pivot to a more critical stance, and the reviewer points out the lack of convincing demonstration and comparison to relevant models. The language used is formal and constructive, with no personal attacks or overly harsh criticism, indicating politeness. The reviewer uses phrases like 'I think' and 'wondering if' which softens the critique and maintains a respectful tone.",20,80
"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. Pros: - The use of OBs is novel and interesting. - Clearly written and explained. Cons: - No comparison to previous state of the art, only with author-generated results. - More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1* etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (*) that are actually beneficial. - While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results. - Why not use a graph-based decoder like in Fig 2 b? Overall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.","The sentiment of the review appears to be constructive with a balance of positive and negative feedback. The reviewer acknowledges the novelty and clarity of the paper but also points out significant areas for improvement, such as the lack of comparison to the state of the art and the need for more ablation studies. The language used is formal and provides specific suggestions for improvement, which indicates a neutral to slightly positive sentiment. As for politeness, the language is respectful and maintains a professional tone throughout, with no use of rude or harsh language, indicating a high level of politeness.",30,90
"The paper tackles the task of music generation. They use an orderless NADE model for the task of *fill in the notes*. Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures. This is a well written paper - great job. My main problem with the paper is that having read Uria and Yao, I don*t know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don*t see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao*s method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.","The sentiment in the review starts off positive, acknowledging the well-written nature of the paper and the successful application of the blocked Gibbs sampling method to music generation. However, it shifts to a more critical tone when discussing the novelty of the work in the context of the ICLR submission requirements, suggesting that the contribution may not be significant enough for this venue. The language used throughout the review is polite and constructive, offering specific feedback and suggestions for improvement without any harsh or disrespectful wording.",30,90
"The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. The paper contains two major parts: DRAGNN and demonstrations of its usages. Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there*s a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering. In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.","The reviewer's sentiment appears to be mixed with a slight lean towards the positive due to the acknowledgment of the architecture's power and utility in multitask learning, but with reservations about its novelty. The reviewer appreciates the neatness of the tool and its applications but questions the originality, suggesting that the contribution is more in software engineering than in a scientific breakthrough. The language used is formal and constructive, without any harsh or disrespectful wording, indicating a high level of politeness.",30,90
"The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me. I*m not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an *introduction to training CNNs* course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants). The paper states that *it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer*, which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (*the nature of design patterns is that they only apply some of the time*) does not excuse making such sweeping claims. This should probably be removed. *We feel that normalization puts all the layer*s input samples on more equal footing, which allows backprop to train more effectively* (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with *we feel*, as this doesn*t seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper. The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don*t think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x. Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.","The reviewer's sentiment appears to be mostly critical and negative, as they express doubts about the merit of the paper, the novelty of the proposed rules, and the effectiveness of the experimental results. Phrases like 'relatively poor performance', 'unclear to me', 'does not warrant publication', and 'unconvincing' indicate a negative view of the paper's contributions. However, the reviewer does acknowledge that summarizing observations could be useful for newcomers, which adds a slightly positive note. The language used is formal and academic, with no personal attacks or unprofessional wording. The use of 'we feel' is critiqued, but the overall tone remains polite and focused on the content rather than the authors personally.",-60,70
"The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout. Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author’s other ICLR submission: « Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications ». The two works differ in the supervised initialization strategy employed. While layer-wise initialization strategies are worthy of further exploration, the paper doesn’t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art. Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. One important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (« In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers ») or kernel SVMs as mentioned later « For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function». In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?). Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight.","The reviewer's sentiment appears critical and skeptical, pointing out the lack of novelty and insufficiently convincing results. The reviewer also highlights issues with clarity and potential scaling problems. However, the language used is formal and constructive, focusing on specific areas for improvement without any personal attacks or overly harsh wording.",-40,70
"The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. In general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. It would be interesting to add or discuss the following issues: - what is the complexity of the proposed method, esp. the representation learning part? - would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel. - the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. - the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.","The reviewer's sentiment appears to be generally positive towards the authors' method, acknowledging that it is well described and effective for the studied datasets. However, the reviewer also points out areas for improvement and missing elements in the related work section, which indicates a constructive but critical perspective. The language used is formal and constructive, with suggestions for improvement rather than direct criticism, which reflects a polite tone.",70,90
The paper compare three representation learning algorithms over symbolized sequences. Experiments are executed on several prediction tasks. The approach is potentially very important but the proposed algorithm is rather trivial. Besides detailed analysis on hyper parameters are not described.,"The sentiment in the review appears to be slightly negative due to the use of the word 'trivial' to describe the proposed algorithm and the criticism regarding the lack of detailed analysis on hyperparameters. However, the reviewer also acknowledges the potential importance of the approach, which adds a slightly positive aspect. The language used is direct and to the point, without any personal attacks or overly harsh wording, which suggests a neutral to slightly polite tone.",-30,70
"This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it*s easy to follow the paper and understand how everything fits together. Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I*d be interested in seeing a discussion on the variance of the samples in (6). A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely eta (eq. (3)) and alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I*d be interested in a comment concerning how this is done (what*s the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time. I really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets. Overall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.","The reviewer's sentiment appears to be cautiously optimistic. They acknowledge the simplicity and clarity of the model construction and presentation, as well as the well-motivated addition of the regularization term. However, they also express concern about the practicality of the approach due to the need for parameter tuning. The reviewer's favor towards accepting the paper, despite the concerns, indicates a generally positive sentiment. The language used is formal, constructive, and respectful, with no signs of rudeness or impoliteness. The reviewer uses phrases like 'I really liked' and 'well motivated', which are positive and polite, while also providing constructive criticism without any harsh language.",60,90
"This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference. In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it*s not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There*s no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations. [Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005. There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there*s been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5]. [Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007. [Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006. [Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012. [Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013. I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn*t been done before and is actually a potentially very useful model. However, the question is, what is the proper way of extending VAE to multiple views? The paper didn*t convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn*t seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn*t seem to promote the SOA either. Of course one wouldn*t expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently. Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network. The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn*t convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the utility of the model and its novel combination of multi-view modeling and VAEs, but they also point out several shortcomings and areas where the paper does not convince them. The reviewer does not dismiss the work outright but suggests that it requires more work. This indicates a sentiment that is not entirely negative but rather suggests improvements, placing it in the mildly negative range. The language used is formal and academic, with no personal attacks or overly harsh criticism, which suggests a high level of politeness.",-30,80
"This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. Strengths: - The training objective is reasonable. In particular, high-level features show translation invariance. - The proposed methods are effective for initializing neural networks for supervised training on several datasets. Weaknesses: - The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). - The paper is experimentally misleading. The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.","The reviewer's sentiment appears balanced, acknowledging the strengths of the paper while also pointing out significant weaknesses. The language used is professional and constructive, without any personal attacks or overly harsh criticism. The reviewer provides specific examples and comparisons to other work, which indicates a neutral to slightly negative sentiment due to the concerns raised about the fairness of comparisons and potential limitations of the proposed method. The politeness is high, as the critique is delivered in a respectful and academic tone, focusing on the work rather than the authors personally.",-10,90
I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.,"The reviewer's sentiment appears critical but constructive, pointing out areas for improvement without using harsh language. The use of 'I think' softens the statements, indicating they are opinions rather than direct accusations. The sentiment is not overtly negative as it suggests improvements rather than dismissing the work entirely. The language is formal and lacks any personal attacks or highly charged words, which suggests a level of politeness in the critique.",-30,70
The paper addresses an important problem - namely on how to improve diversity in responses. It is applaudable that the authors show results on several tasks showing the applicability across different problems. In my view there are two weaknesses at this point 1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper 2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted. Having said this the gist of the proposed solution seems interesting but somewhat premature.,"The reviewer's sentiment appears to be mixed. They acknowledge the importance of the problem and the applicability of the results, which is positive. However, they also point out what they perceive as weaknesses, such as the minor improvements and the ad-hoc nature of the approach. The language used is constructive and does not contain any strong negative or positive words, suggesting a neutral sentiment. The language is polite and professional, with no use of rude or confrontational language, hence the high politeness score.",0,80
"I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it*s still missing. After going over this paper couple of times I*m still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning. As an example of the difficulty, take section 4.3. It talks about training *split block* which is a function that can assign each element to either partition 0 or partition 1. At this point I*m looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as *we must create artificial targets at every node of the generated tree from the available final target partition*. What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.","The sentiment of the review appears to be constructive but points out significant issues with the paper, indicating a neutral to slightly negative sentiment as the reviewer is unable to reproduce the experiments and finds the paper lacking in necessary details. However, the reviewer does not dismiss the work outright and suggests improvements, which implies a willingness to see the paper succeed with revisions. The language used is polite and professional, focusing on the content and clarity of the paper rather than personal criticism of the authors. The reviewer uses phrases like 'the paper needs to be improved' and 'would help with understanding,' which are polite ways to suggest changes.",-20,80
"Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed. The techniques are novel as far as I know. The paper is clear, the results are plausible. The evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods. The authors delineate *static* acceleration techniques (e.g. reduced bit-width, weight pruning) from *dynamic* acceleration techniques which are changes to the inference algorithm itself. The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case. For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work). For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation. In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines--the case for the superiority of SPET might be difficult to make. The authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include *low power* and *deeply embedded* applications. In these sorts of applications, a mobile GPU would be used, not a Xeon. A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper.","The reviewer's sentiment appears to be cautiously positive towards the novelty of the techniques presented but critical of the evaluation and practical implications, suggesting a mixed sentiment. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite tone.",20,80
"The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM. Pros: + Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers + Encouraging results on the curriculum learning on hybrid discrete and continuous attentions Cons: - Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance? - Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.","The reviewer's sentiment appears to be cautiously positive, acknowledging the contributions of the paper while also pointing out areas for improvement. The positive sentiment is shown by the recognition of 'Comprehensive comparisons' and 'Encouraging results'. However, the sentiment is tempered by the critical comments regarding the weak NTM baseline and the lack of clarity in Section 3. The language used is polite and constructive, with suggestions for improvement rather than harsh criticism. The use of phrases like 'Maybe it is worthwhile to reproduce' and 'The overall clarity of the paper needs improvement' are indicative of a respectful tone.",30,80
"The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can*t find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer*s questions and re-read the paper again and I still do not fully understand the setting and thus can*t really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.","The reviewer expresses difficulty in understanding the paper and criticizes the lack of clarity, absence of a related work section, and missing references. These points indicate a negative sentiment towards the paper's current state. However, the language used is not rude or insulting; it is direct and professional, pointing out specific areas for improvement without personal attacks or overly harsh language.",-70,70
"Paper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory. Review summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience. Pros: * 10x faster * 10x smaller * Design rationale described in detail Cons: * The quality of the reference baseline is low. For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU. Thus the results are of limited interest. * The results that support the design rationale are not provided. It is important to provide the experimental evidence to support each claim. Quality: the work is interesting but feels incomplete. If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ? The paper focuses only on nimbleness at the cost of quality (using a weak baseline). This limits the interest for the ICLR audience. Clarity: the overall text is somewhat clear, but the model description (section 3) could be more clear. Originality: the work is a compendium of “practitioners wisdom” applied to a specific task. It has thus limited originality. Significance: I find the work that establishes a new “best practices all in one” quite interesting, but however these must shine in all aspects. Being fast at the cost of quality, will limit the impact of this work. Minor comments: * Overall the text is proper english but the sentences constructions is often unsound, specific examples below. * To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same “collected wisdom” can be used both to reach high speed and high quality (with the proper trade-off curve being shown). Aiming for only one end of the quality versus speed curve limits too much the paper. * Section 1: “mobile or battery powered … require rates > 10 fps“. 10 fps with which energy budget ? Should not this be > 10 fps && < X Watt. * “Rules and ideas” -> rules seem too strong of a word, “guidelines” ? * “Is of utmost importance” -> “is of importance” (important is already important) * “Presents a trainable network … therefore we compare to … the large majority of inference the same way”; the sentence makes no sense to me, I do not see the logical link between before and after “therefore” * Scen-parsing -> scene-parsing * It is arguable if encoder and decoder can be called “separate” * “Unlike in Noh” why is that relevant ? Make explicit or remove * “Real-time” is vague, you mean X fps @ Y W ? * Other existing architectures -> Other architectures * Section 3, does not the BN layer include a bias term ? Can you get good results without any bias term ? * Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ? * Section 4, non linear operations. What do you mean by “settle to recurring pattern” ? * Section 4, dimensionality changes. “Computationally expensive”, relative to what ? * Section 4, dimensionality changes. “This technique ... speeds-up ten times”, but does not provide the same results. Without an experimental validation changing an apple for an orange does not make the orange better than the apple. * Section 4, dimensionality changes. “Found one problem”, problem would imply something conceptually wrong. This is more an “issue” or an “miss-match” when using ResNet for semantic labelling. * Section 4, factorizing filters. I am unsure of why you call nx1 filter asymmetric. A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2). Why not simply call them rectangular filters ? * Section 4, factorizing filters. Why would this change increase the variety ? I would have expected the opposite. * Section 4, regularization. Define “much better”. * Section 5.1; “640x360 is adequate for practical applications”; for _some_ applications. * Section 5.2, “very quickly” is vague and depends on the reader expectations, please be quantitative. * Section 5.2, Haver -> have * Section 5.2, in this work -> In this work * Section 5.2, unclear what you use the class weighting for. Is this for class balancing ? * Section 5.2, Cityscapes was -> Cityscapes is * Section 5.2, weighted by the average -> is each instance weighted relative the average object size. * Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes","The sentiment of the review is mostly critical, pointing out several shortcomings of the paper such as limited interest for the ICLR audience, weak baseline comparison, lack of experimental evidence, and limited originality. However, the reviewer also acknowledges the interesting aspects of the work, such as its speed and memory efficiency. The language used is formal and constructive, offering specific guidance for improvement without any personal attacks or overly harsh wording. The reviewer's tone is critical but maintains a level of professionalism expected in scientific peer reviews.",-30,70
"This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. Comments: - The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation. - The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. - The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p times. More specifically, Eq (2) shows T_broadcase_BE/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE/ T_broadcase_LP < 2, the authors should state T_broadcase_BE/ T_broadcase_LP > 1 when n approaches infinity. - It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper.","The sentiment of the review appears to be constructive but with a critical tone, as the reviewer acknowledges the interesting results but points out several areas for improvement, such as clarity of writing, the need for more intuition, and issues with the analysis presented. The language used is polite and professional, with no rude or aggressive terms used. The reviewer provides clear and specific feedback without personal attacks, maintaining a respectful tone throughout.",25,75
"Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations. Contributions: The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document. Writing: The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. Pros: - The proposed model is a simple extension to the model to the model proposed in [2] for summarization. - The results are better than the baselines. Cons: - The improvements are not that large. - Justifications are not strong enough. - The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal. - The paper is very application oriented. Question: - How does the training speed when compared to the regular LSTM? Some Criticisms: A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn’t consider the application of that on the summarization task a significant contribution. The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage. As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from “read-again” mechanism or the use of “pointing”. The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak. It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are The writing of this paper needs more work. In general, it is not very well-written. Minor comments: Some of the corrections that I would recommend fixing, On page 4: “… better than a single value … ” —> “… scalar gating …” On page 4: “… single value lacks the ability to model the variances among these dimensions.” —> “… scalar gating couldn’t capture the ….” On page 6: “ … where h_0^2 and h_0^*2 are initial zero vectors … “ —> “… h_0^2 and h_0^*2 are initialized to a zero vector in the beginning of each sequence …* There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2. Better naming of the models in Table 1 is needed. The location of Table 1 is a bit off. [1] Zaremba, Wojciech, and Ilya Sutskever. *Reinforcement learning neural Turing machines.* arXiv preprint arXiv:1505.00521 362 (2015). [2] Gulcehre, Caglar, et al. *Pointing the Unknown Words.* arXiv preprint arXiv:1603.08148 (2016).","The sentiment of the review is critical but constructive. The reviewer acknowledges the contributions and improvements of the paper but also points out several weaknesses, such as the need for a better writeup, unclear justifications, and the lack of significant contributions. The language used is direct and points out specific areas for improvement without being harsh or offensive, indicating a neutral to slightly negative sentiment. The politeness of the language is formal and provides specific feedback without personal attacks, maintaining a professional tone throughout the review.",-30,70
"This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family. I*m not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them. The description of the experiments in Section VI is insufficient for reproducibility. Is *The cart moved right* supposed to be *a positive force is applied to the cart*? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes? The footnote at the bottom of page 8 threw me off. If you*re using Newton*s method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf I might be missing something basic here. The control variates thing seems cool. I only read up on it now and I don*t think I*ve seen it before in the RL literature. Seems like a powerful tool. Section 6.2 has too much business jargon, I could barely read it.","The reviewer's sentiment appears to be mixed, with some positive remarks about the potential of the algorithm to accelerate expectation-based algorithms, and an interest in the control variates concept. However, there are also several critical comments regarding the lack of detail in the experimental section, confusion about the methodology, and difficulty understanding due to jargon. The language used is generally polite, with no rude or offensive terms, but it does convey a level of frustration or confusion at points, which might be perceived as slightly less polite.",10,70
"This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks. The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings. Pros: -The idea is interesting and interpretable models/representations is an important topic. -Generating embeddings to interpret SPNs is a novel idea. -The experiments are interesting but could be extended. Cons: -The author*s contribution isn*t fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? -A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade. I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.","The sentiment of the review appears to be slightly negative due to the reviewer's critical analysis of the paper's claims and contributions. The reviewer acknowledges the interesting idea and the importance of the topic, but also points out that the author's contributions are not clear and that there are unsupported claims, suggesting a need for further evidence and comparisons. The language used is polite and constructive, with the reviewer providing specific feedback and suggestions for improvement without any harsh or disrespectful wording.",-20,80
"The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block. Positives: The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives. Negatives: The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear.","The reviewer's sentiment appears balanced, acknowledging both the strengths ('technically sound', 'introduces a new way') and weaknesses ('performance gain is not clear', 'improvement is quite small') of the paper. The language used is constructive and respectful, with suggestions for improvement, which indicates a neutral to slightly positive sentiment and a polite tone.",25,75
"This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet. Comments: - The first section of the paper is clear and well-motivated. - The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I*m not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD). - If I understand correctly, Eq (7) is a noncontinuous function w.r.t W_lambda and the simple SGD algorithm will not be able to find its minimum. - For dependency parsing, there are standard splits of PTB. I would suggest the authors follow the same splits of train, dev, and test in order to compare with existing results. Minor comments: several sentences are misleading and should be rewritten carefully. - Beginning of Section 3: *A major advantage of neural network is that once trained, inference is extremely efficient.* This sentence is not generally right, and I guess the authors mean if using greedy search as inference method, the inference is efficient. - The description in the end of section 2 is awkward. To me, feed-forward and RNN are general families that cover many specific types of neural networks, and the training procedures are not necessarily to aim to optimize Eq. (2). Therefore, the description here might not be true. In fact, I don*t think there is a need to bring up feed-forward networks here; instead, the authors should provide more details the connection between RNN and Eq (2) here. - The second paragraph of section 3 is related to [1], where it shows the search space of the inference can be represented as an imperative program. [1] Credit assignment compiler for joint prediction, NIPS 2016","The reviewer's sentiment appears to be cautiously critical. They acknowledge the interesting idea and the importance of the problem being addressed, but they also point out several significant issues that suggest the paper is not ready for publication. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite approach to the review process.",-30,70
"This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes. In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear. In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained). The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper. A few suggestions for improvement: - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper. - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network. - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?) - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great. - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results. 1/19/17 UPDATE AFTER REBUTTAL: Given that experiments were added to the latest version of the paper, I*m increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.","The reviewer's sentiment appears to be cautiously positive with constructive criticism. They acknowledge the paper's interesting exploration and potential as a workshop paper but express that the results may not be compelling enough for a full ICLR paper. The language used is formal and constructive, providing specific suggestions for improvement without any harsh or negative wording. The reviewer's update after the rebuttal shows a willingness to reconsider their position, which is a positive indicator of open-mindedness.",40,80
"Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don*t add up to a quantum of progress in the study of what can be done with the Neural GPU model. Minor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories. Basic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 “filters” in the model ? If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3. The preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should. The remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU. The point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something). The notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.","The reviewer's sentiment appears to be mixed with a lean towards the negative side. While they acknowledge the clarity of the paper and the status of the researchers, they also express that the paper's contributions do not amount to significant progress and question the practical utility of the Neural GPU model. The language used is formal and constructive, with no personal attacks or overly harsh criticism, indicating a polite tone. The reviewer provides specific, detailed feedback and questions, which is a sign of a respectful and professional review.",-30,70
"This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It’s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. The proposed idea seems reasonable, but I’m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can’t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.) I’m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it’s not clear what this represents. In terms of experimental validation, there aren’t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit.","The sentiment of the review appears to be critical and constructive, but not overtly negative. The reviewer acknowledges the reasonableness of the proposed idea but expresses difficulty in understanding the paper and identifies several areas that require clarification or comparison. There is no use of inflammatory language or personal attacks, indicating a neutral to slightly negative sentiment. The language used is formal and appropriate for a scientific peer review, suggesting a high level of politeness despite the critical content.",-20,80
"The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy. The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear. The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models? Given the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR. More comments: - The title does not make it clear that the paper focuses on wide and shallow text classification models. Please revise the title. - The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper. Please make the paper self-contained and include any supplementary material in the appendix. - In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments. - The paper argues the wide and shallow models are the state of the art in small datasets. Is this really correct? What about transfer learning?","The sentiment of the review appears to be critical and constructive, but not overtly negative. The reviewer acknowledges the importance of the problem and the well-written methods section but criticizes the lack of novelty and the missing baselines, suggesting that the paper is not ready for publication. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone despite the critical content.",-30,70
"This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored. On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future. Detailed/minor points below: 1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure. 2) The tables need better and more descriptive labels. 3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this? 4) Why was *Enriched CBOW* not included in the analogy task? 5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn*t been enough work on this. That feels a little misleading.","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the plausibility of the main idea but points out significant weaknesses in the experimental evaluation and model exploration. The suggestion to revise and improve indicates a negative sentiment towards the current state of the paper but not entirely dismissive, suggesting a score leaning towards the negative side but not extremely so. The language used is polite and professional, with suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'The paper would be stronger if' and 'encouraging the authors to submit a revised and improved version,' which are constructive in nature. However, the reviewer's directness in pointing out the need for a native English speaker to revise the paper could be perceived as slightly less polite, thus not achieving the highest score in politeness.",-40,70
"This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient’s medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a “basic” embedding, and composes a “final” embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code’s ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive. I have the following comments: - Why is the patient’s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn’t this bias for/against the number of codes in the visit? - I don’t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that? - Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that. Finally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.","The sentiment in the review appears to be cautiously positive, as the reviewer acknowledges that the paper is well written and the approach is intuitive. However, the reviewer also poses several questions and requests for clarification, which indicates a constructive but critical engagement with the paper. There is no overt praise or criticism, suggesting a balanced and neutral-to-positive sentiment. The language used is formal and respectful, with the reviewer providing clear and specific feedback without any harsh or disrespectful wording. The use of phrases like 'I have the following comments' and 'I think this is an interesting application paper' are indicative of a polite and professional tone.",50,80
"Summary: The authors propose a multi-hop *gated attention* model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. The proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results. Pros: 1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation. 2. The presentation is clear with thorough experimental comparison with the latest results. Comments: 1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently. It is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear: (1) how much multiple-hops of gated-attention contribute to the performance. (2) how important is it to have a specialized query encoder for each layer. Understanding the above better, will help simplify the architecture. 2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method. There is a significant performance drop when C(w) is absent (e.g. in *GA Reader--*; although there are other changes in *GA Reader--* which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.","The sentiment of the review appears to be cautiously positive. The reviewer acknowledges the 'nice idea' and 'clear presentation' with 'thorough experimental comparison,' which suggests a positive view of the work. However, the reviewer also points out areas that require further clarification, such as the contribution of multiple-hops and the importance of a specialized query encoder, indicating a constructive critique rather than outright praise or rejection. The language used is formal and academic, with no use of negative or disrespectful terms, indicating a high level of politeness. The reviewer's tone is constructive, aiming to provide feedback that could help improve the paper, rather than to disparage it.",60,90
"The contribution of this paper can be summarized as: 1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution. The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015). 2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering. 3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries. Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me. The paper writing also needs to be improved. More comments below: [Major comments] - My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing. Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. - Conjunctive queries: the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets. - The model is named as “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature. [Minor comments] - I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer? - Besides “entity recognition”, usually we still need an “entity linker” component which links the text mention to the KB entity.","The sentiment of the review appears to be mixed with a lean towards the negative side. The reviewer acknowledges the nice properties of the Gaussian parameterization but expresses concerns about the convincing nature of the experimental results and the quality of the paper's writing. The use of 'not convincing enough' and 'needs to be improved' indicates a critical stance, but the reviewer does not dismiss the work outright, suggesting that improvements could enhance the paper's value. The language used in the review is formal and adheres to academic standards, with no use of rude or inflammatory language. The reviewer provides constructive criticism without personal attacks, maintaining a professional tone throughout.",-30,70
"The paper proposes modification to seq2seq model to help it handle the problems when long responses are needed. Though the technical contributions may be of value, the work in my personal opinion is not in the right direction towards helping dialog systems. Essentially we try to generate long responses that sound ``nice* yet are not grounded to any reality, they just need to be related to the question and not suffers from obvious mistakes. Yet, the architectural innovations proposed may be of merit.","The reviewer's sentiment appears mixed. They acknowledge the technical contributions of the paper but express a personal opinion that the work is not moving in the right direction for dialog systems. The use of quotation marks around 'nice' suggests a degree of skepticism about the value of the work. However, there is no strong negative language used, and the reviewer does recognize the merit in the architectural innovations. The language used is formal and appropriate for a scientific peer review, without any personal attacks or overly harsh criticism.",10,70
"Our understanding of GAN to date is still vague. Although there have been some efforts relating GAN to energy models, I personally consider that the perspective of this paper, namelying understanding GAN (a variant of GAN, to be more precise) as variational training of an energy model is the most natural and elegant. The derivation up to equation (5) and the reduction to (7) are very nice. I think this is the most important contribution of the paper. The techniques introduced in sections 5 and 6 are somewhat ad hoc, and lack clarity. Referring to the version I looked at (not sure though if it is the latest), section 6 contain some errors/typos (stuff around p_z(x|	ilde{x}). The presentation of section 6 needs to improve in clarity. But I think this does not shadow the main contribution of the paper, namely, that perspectives given in sections 2-4. Overall I very much enjoy the presented insight of this paper into GAN. I do have some comment/question regarding equation (7). This equation formulates a variant of GAN, or a model resembling GAN. I am happy to see that the entropy term pops up there, which should save GAN from degenerating its generative distribution or from missing modes. The swapping of the min-max order in this formulation however makes me wonder if this variant of GAN indeed reflects the *principle* of GAN, or it is in fact a different principle, which happens to gives rise to a model that *resembles* GAN. Of course, my question may be merely philosophical rather than mathematical, and I won*t expect a precise anwer. Nonetheless, if the author can provide additional insignts on this, it would be appreciated.","The reviewer's sentiment appears to be generally positive, as they describe the perspective of the paper as 'the most natural and elegant' and praise the derivation and reduction of equations as 'very nice.' They also express enjoyment of the insights provided into GAN. However, there are some criticisms regarding the ad hoc nature of techniques and lack of clarity in sections 5 and 6, as well as errors/typos mentioned. The reviewer's language is polite and constructive, with phrases like 'I think' and 'I am happy to see' indicating a respectful tone, and they express their criticisms without harsh language. They also show an interest in further insights from the author, which is a positive and engaging approach.",70,90
"This paper proposes an interesting application of the GAN framework in steganography domain. In addition to the normal GAN discriminator, there is a steganalyser discriminator that receives the negative examples from the generator and positive examples from the generator images that contain a hidden payload. As a result, the generator, not only learn to generate realistic images by fooling the discriminator of the GAN, but also learn to be a secure container by fooling steganalyser discriminator. The method is tested by training an independent steganalyser S* on real images and generated images. Given that in the ICLR community, not many people are familiar with the literature of steganography, I think this paper should have provided more context about how exactly this method can be used in practice, what are the related works on setganalysis-secure message embedding and probably a more thorough sets of experiments on more than one dataset. The proposed SGAN framework (Figure 2) does make sense to me, and I think it is very general and can have more applications other than the steganography domain. But it is not clear to me why fooling the steganalyser discriminator S, necessarily mean that we can fool an independent discriminator S*? Also I find it surprising that a different seed value, can make such a huge difference in the accuracy. In short, the ideas of this paper are interesting and potentially useful, but I think the presentation of this paper should be improved so that it becomes more suitable for the ICLR and machine learning community.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the interesting application of the GAN framework and the general sense of the proposed SGAN framework. However, the reviewer also points out areas that need improvement, such as providing more context, related works, and thorough experiments. The language used is polite and constructive, with suggestions for improvement rather than direct criticism. The reviewer uses phrases like 'I think' and 'it is not clear to me,' which soften the critique and maintain a respectful tone.",40,80
"This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel. The generator/discriminator pairing is shuffled according to a periodic schedule. Pros: + The proposed approach is simple and easy to replicate. Cons: - The paper is confusing to read. - The results are suggestive but do not conclusively show a performance win for GAP. The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes. The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage. And for convergence it is difficult to assess the effect of GAP on the basis of learning curves. The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with. Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score. Perhaps a more systematic way to determine GAP*s effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting. Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models. Overall the approach seems promising but there are too many open questions regarding the paper in its current form. * Section 2: *Remark that when...* => seems like a to-do. * Section A.1: The proposed metric is not described in adequate detail.","The sentiment of the review is somewhat mixed but leans towards the negative due to the reviewer's criticism of the paper's clarity and inconclusive results. However, the reviewer acknowledges the promise of the approach, which adds a slightly positive aspect. The language used is formal and constructive, without any harsh or rude wording, indicating a high level of politeness.",-30,80
"While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following: 1) motivation based on the incorrect assumption that the Paragraph Vector wouldn*t work on unseen data 2) Numerous basic formatting and Bibtex citation issues. Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.","The reviewer's sentiment appears to be critical and negative, as they point out incorrect assumptions, formatting issues, and lack of novelty in the paper. However, the reviewer acknowledges 'decent accuracy numbers,' which prevents the sentiment from being entirely negative. The language used is direct and could be perceived as blunt, but it is not rude or disrespectful. It is typical of academic critique, aiming to be constructive rather than personal.",-60,20
"This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The interior gradient is the gradient measured on a scaled version of the input. The integrated gradient is the integral of interior gradients over all scaling factors. Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance. While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work. Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison. Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance. Lack of comparison to any of these methods is a major weakness of the paper. I do not believe it is fit for publication without such comparisons. My pre-review question articulated this same concern and has not been answered.","The reviewer's sentiment appears to be critical of the paper, pointing out a major weakness regarding the lack of comparison to other methods. The use of phrases like 'lacks both qualitative and quantitative comparison' and 'I do not believe it is fit for publication without such comparisons' indicate a negative sentiment towards the paper's current state. However, the language used is professional and not personal, focusing on the content and methodology rather than the authors themselves. The reviewer also acknowledges the appealing aspects of the paper, which suggests that the sentiment is not entirely negative. The language is polite and constructive, aiming to guide the authors on how to improve the paper.",-60,70
"1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer. 2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt? 3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.","The review contains critical observations and questions regarding the hierarchical memory, the experimental section, and the robustness of the proposed method. The reviewer's tone is direct and points out perceived flaws without any positive feedback, suggesting a negative sentiment. However, the language used is not rude or offensive; it is straightforward and to the point, which is typical in academic discourse.",-60,40
"The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems. Summary: ——— The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is reasonable but limited to binary data at this point in time. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge. Details: ——— 1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment? 2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.","The reviewer's sentiment appears to be cautiously positive towards the novelty of the approach but expresses concerns about the limited scope and scale of the experimental results. The language used is constructive and professional, with suggestions for improvement and requests for clarification, indicating a polite tone.",30,80
"The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks. Some points. 1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight. 2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture. 3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine. 4) It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this. I like the basic idea of the paper, but the points above make me think it is not ready for publication.","The reviewer's sentiment appears neutral to slightly negative, as they acknowledge the basic idea of the paper but also point out several shortcomings and areas that need improvement. They do not express enthusiasm or strong support for the paper's current state, suggesting it is not ready for publication. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite approach to delivering the feedback.",-20,80
"The author works to compare DNNs to human visual perception, both quantitatively and qualitatively. Their first result involves performing a psychophysical experiment both on humans and on a model and then comparing the results (actually I think the psychophysical data was collected in a different work, and is just used here). The specific psychophysical experiment determined, separately for each of a set of approx. 1110 images, what the noise level of additive noise would have to be to make a just-noticeable-difference for humans in discriminating the noiseless image from the noisy one. The authors then define a metric on neural networks that allows them to measure what they posit might be a similar property for the networks. They then correlate the pattern of noise levels between neural networks that the humans. Deep neural networks end up being much better predictors of the human pattern of noise levels than simpler measure of image perturbation (e.g. RMS contrast). A second result involves comparing DNNs to humans in terms of their pattern errors in a series of highly controlled experiments using stimuli that illustrate classic properties of human visual processing -- including segmentation, crowding and shape understanding. They then used an information-theoretic single-neuron metric of discriminability to assess similar patterns of errors for the DNNs. Again, top layers of DNNs were able to reproduce the human patterns of difficulty across stimuli, at least to some extent. A third result involves comparing DNNs to humans in terms of their pattern of contrast sensitivity across a series of sine-grating images at different frequencies. (There is a classic result from vision research as to what this pattern should be, so it makes a natural target for comparison to models.) The authors define a DNN correlate for the propertie in terms of the cross-neuron average of the L1-distance between responses to a blank image and responses to a sinuisoid of each contrast and frequency. They then qualitatively compare the results of this metric for DNNs models to known results from the literature on humans, finding that, like humans, there is an apparent bandpass response for low-contrast gratings and a mostly constant response at high contrast. Pros: * The general concept of comparing deep nets to psychophysical results in a detailed, quantitative way, is really nice. * They nicely defined a set of *linking functions*, e.g. metrics that express how a specific behavioral result is to be generated from the neural network. (Ie. the L1 metrics in results 1 and 3 and the information-theoretic measure in result 2.) The framework for setting up such linking functions seems like a great direction to me. * The actual psychophysical data seems to have been handled in a very careful and thoughtful way. These folks clearly know what they*re doing on the psychophysical end. Cons: * To my mind, the biggest problem wit this paper is that that it doesn*t say something that we didn*t really know already. Existing results have shown that DNNs are pretty good models of the human visual system in a whole bunch of ways, and this paper adds some more ways. What would have been great would be: (a) showing that they metric of comparison to humans that was sufficiently sensitive that it could pull apart various DNN models, making one clearly better than the others. (b) identifying a wide gap between the DNNs and the humans that is still unfilled. They sort of do this, since while the DNNs are good at reproducing the human judgements in Result 1, they are not perfect -- gap is between 60% explained variance and 84% inter-human consistency. This 24% gap is potentially important, so I*d really like to see them have explored that gap more -- e.g. (i) widening the gap by identifying which images caused the gap most and focusing a test on those, or (ii) closing the gap by training a neural network to get the pattern 100% correct and seeing if that made better CNNs as measured on other metrics/tasks. In other words, I would definitely have traded off not having results 2 and 3 for a deeper exploration of result 1. I think their overall approach could be very fruitful, but it hasn*t really been carried far enough here. * I found a few things confusing about the layout of the paper. I especially found that the quantitative results for results 2 and 3 were not clearly displayed. Why was figure 8 relegated to the appendix? Where are the quantifications of model-human similarities for the data shown in Figure 8? Isn*t this the whole meat of their second result? This should really be presented in a more clear way. * Where is the quantification of model-human similarity for the data show in Figure 3? Isn*t there a way to get the human contrast-sensitivity curve and then compare it to that of models in a more quantitively precise way, rather than just note a qualitative agreement? It seems odd to me that this wasn*t done.","The reviewer's sentiment appears to be mixed, with recognition of the paper's strengths but also significant criticism of its lack of novelty and depth in certain areas. The reviewer appreciates the general concept, the defined linking functions, and the careful handling of psychophysical data, which suggests a positive sentiment. However, the sentiment turns negative with the criticism that the paper does not add much new knowledge and the suggestion that the authors should have explored certain aspects more deeply. The language used is academic and constructive, with no personal attacks or overly harsh wording, indicating a polite tone. The reviewer provides constructive feedback and suggestions for improvement, which is typical of a professional and courteous peer review.",25,75
"In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. Pros: - The proposed approach demonstrates strong performance on FB15k dataset. - The idea of using shared memory for knowledge base completion is new and interesting. - The proposed approach is general and can be applied in various tasks. Cons: - There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. - The introduction section can be improved. Specifically, the authors should motivate *shared memory* more in the introduction and how it different from existing methods that using *unshared memory* for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion. The concept of shared memory and search controller only make sense to me after reading through section 2.","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the strong performance and novel aspects of the proposed model, but also pointing out areas that need improvement. The language used is constructive and respectful, with clear delineation between the strengths and weaknesses of the paper.",30,80
"This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper in its current form is not acceptable due to the following reasons: 1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2. 2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors *must be doing*. What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it*s P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)? Comments: 1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done? 2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.","The sentiment of the review is negative as the reviewer clearly states that the paper in its current form is not acceptable, pointing out significant issues such as lack of quantitative evaluation and unclear descriptions. The language used is direct and critical but not rude or offensive. The reviewer uses phrases like 'I had to indulge in a lot of charity to interpret' which indicates frustration but is not impolite. The use of 'encourage' in the final comment adds a constructive tone.",-60,40
"The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training. The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems. I have the following concerns about the paper - The proposed method is VARIANT to arbitrary shifts and scaling to the cost function. - A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of d_t. I suspect 1/d_t just shrinks as an exponential decay from Figure 2. - Three additional hyper-parameters: k, K, eta_3. Overall, I think the method has its fundamental flew and the paper offers very limited novelty. There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method. Furthermore, it is hard for me to follow Section 3.2. The writing quality and clarity of the method section can be further improved.","The sentiment of the review is negative, as the reviewer expresses concerns about the paper, stating that the method has a 'fundamental flew' and offers 'very limited novelty.' The reviewer also criticizes the lack of theoretical justification and potential failure modes, as well as the writing quality and clarity. The language used is direct and critical but not rude or offensive. The reviewer uses academic language to convey their concerns and does not include personal attacks or unprofessional remarks.",-60,40
"First up, I want to point out that this paper is really long. Like 17 pages long -- without any supplementary material. While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer*s shoes and did not take undue advantage of this rule. Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair. Now for the review: The paper proposes a new artificial dataset for sequence learning. I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits. In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call *incremental learning*. The experiments show that their proposed schedule is better than not having any schedule on this data set. Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules. The authors verify this by doing some ablation studies over the model on the proposed dataset. I have following issues with this paper: -- I did not find anything novel in this paper. The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences. Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015. The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used. -- Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated. Why not use it on a real sequential dataset, such as, language modeling. Does the technique not work in that scenario? In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper. -- I*m not convinced about the value of having this artificial dataset. Already there are so many real world sequential dataset available, including in text, speech, finance and other areas. What exactly does this dataset bring to the table is not super clear to me. While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work. It would have been so much better had the authors shown experiments on other datasets. -- As I said, the paper is way too long. A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper. For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper. Same is true with the transfer learning experiments of Section 6.4.","The reviewer's sentiment appears to be quite critical of the paper, pointing out its excessive length, lack of novelty, and the artificial nature of the dataset used. The reviewer also questions the value of the dataset and the relevance of some experiments to the paper's main message. However, the language used is not rude or insulting; it is direct and professional, focusing on the content and methodology of the paper rather than personal attacks. Therefore, the sentiment is negative due to the critical content, but the language is polite as it maintains a professional tone.",-70,70
"Paper summary This paper develops a generalization of dropout using information theoretic principles. The basic idea is that when learning a representation z of input x with the aim of predicting y, we must choose a z such that it carries the least amount of information about x, as long as it can predict y. This idea can be formalized using the Information Bottleneck Lagrangian. This leads to an optimization problem which is similar to the one derived for variational dropout, the difference being that Information dropout allows for a scaling factor associated with the KL divergence term that encourages noise. The amount of noise being added is made a parameterized function of the data and this function is optimized along with the rest of the model. Experimental results on CIFAR-10 and MNIST show (small) improvements over binary dropout. Strengths - The paper highlights an important conceptual link between probabilistic variational methods and information theoretic methods, showing that dropout can be generalized using both formalisms to arrive at very similar models. - The presentation of the model is excellent. - The experimental results on cluttered MNIST are impressive. Weaknesses - The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless the axis label is a typo). It is not clear why the test set was not used. This makes it hard to compare to results reported in Springenberg et al, as well as other results in literature. Quality The theoretical exposition is high quality. Figure 2 gives a nice qualitative assessment of what the model is doing. However, the experimental results section can be made better, for example, by matching the results on CIFAR-10 as reported in Springenberg et al. and trying to improve on those using information dropout. Clarity The paper is well written and easy to follow. Originality The derivation of the information dropout optimization problem using IB Lagrangian is novel. However, the final model is quite close to variational dropout. Significance This paper will be of general interest to researchers in representation learning because it highlights an alternative way to think about latent variables (as information bottlenecks). However, unless the model can be shown to achieve significant improvements over simple dropout, its wider impact is likely to be limited. Overall The paper presents an insightful theoretical derivation and good preliminary results. The experimental section can be improved. Minor comments and suggestions - - expecially -> especially - trough -> through - There is probably a minus sign missing in the expression for H(y|z) above Eq (2). - Figure 3(a) has error bars, but 3(b) doesn*t. It might be a good idea to have those for Figure 3(b) as well. - Please consider comparing Figure 2 with the activity map of a standard CNN trained with binary dropout, so we can see if similar filtering out is happening there already.","The reviewer's sentiment appears to be generally positive, as they acknowledge the paper's strengths, such as the important conceptual link, excellent presentation, and impressive results on cluttered MNIST. However, they also point out areas for improvement, particularly in the experimental results section. The language used is constructive and respectful, with suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'The paper highlights,' 'The presentation of the model is excellent,' and 'The theoretical exposition is high quality,' which contribute to a positive sentiment. The politeness is evident in the way the reviewer frames their criticism and suggestions for improvement, using phrases like 'can be made better' and 'Please consider comparing,' which are polite and constructive.",70,90
"Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. Feedback The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model? What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's contribution to the research dialogue and recommending acceptance despite underwhelming results. The language used is constructive and respectful, providing specific recommendations for improvement without harsh criticism.",60,80
This paper analyzes dependency trees vs standard window contexts for word vector learning. While that*s a good goal I believe the paper falls short of a thorough analysis of the subject matter. It does not analyze Glove like objective functions which often work better than the algorithms used here. It doesn*t compare in absolute terms to other published vectors or models. It fails to gain any particularly interesting insights that will modify other people*s work. It fails to push the state of the art or make available new resources for people.,"The reviewer's sentiment appears to be critical of the paper, pointing out several shortcomings such as the lack of thorough analysis, not comparing to other models, and not providing new insights or resources. However, the language used is not insulting or aggressive; it is straightforward and professional, focusing on the content rather than the authors personally.",-60,40
"This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand. The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.","The reviewer's sentiment appears to be cautiously positive, acknowledging the worth of exploring the combination of DRQN with eligibility traces and the use of the Adam optimizer, as well as the clarity and understandability of the paper. However, the reviewer also points out the lack of novelty in the methods and the limited scope of the experimental results. The language used is constructive and respectful, providing clear recommendations for improvement without any harsh or negative wording.",50,80
"This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following: 1) No other dataset reported. The authors haven*t mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 3) Performance gain is also limited.","The reviewer's sentiment appears to be slightly negative, as they lean toward rejecting the paper due to significant concerns about dataset availability, technical novelty, and limited performance gains. However, the language used is not harsh or overly critical, indicating a professional tone. The reviewer acknowledges the clarity of writing and the practical experiences shared, which adds a slightly positive aspect to the sentiment. The language is polite and constructive, focusing on the content of the paper rather than personal criticism of the authors.",-30,70
"This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model. It is motivated by the method in which (it is assumed) human translators operate. The paper is interesting and imaginative. However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method? And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors? That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs. This paper does not convince me on these points. Indeed, unsurprisingly, the authors note that *the probability of correctly labelling a word as a mistake remains low (62%)* - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes. The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. Although I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model. Minor comments: I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i? In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?","The reviewer expresses a clear skepticism about the approach presented in the paper, questioning the effectiveness and novelty of the iterative method. Despite finding the paper interesting and imaginative, the reviewer remains unconvinced about the main objective of iterative improvement. The language used is formal and academic, with no personal attacks or unprofessional remarks. The reviewer's tone is critical but constructive, aiming to provide feedback for improvement rather than simply dismissing the work.",-20,80
"The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing. The proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the straightforwardness of the model and its effectiveness on certain datasets but express skepticism about the contribution of modality-specific encoders and the principles behind some inference methods. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a neutral to slightly negative sentiment due to the expressed concerns. The politeness is high, as the critique is delivered in a professional manner without any rude or disrespectful language.",-30,80
"This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments: 1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing. 2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature. Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.","The reviewer's sentiment appears to be neutral to slightly positive towards the paper's content, acknowledging the well-written nature and sensible architectural choices of the paper. However, the reviewer also points out significant shortcomings in the experiments, suggesting improvements and questioning the paper's fit for the ICLR community. The language used is formal and constructive, with no personal attacks or overly harsh criticism, indicating a high level of politeness.",20,90
"This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset. The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful. I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix). I think this paper would make an interesting ICLR paper.","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'nicely written' and acknowledge the importance of the problem studied. They also consider the experimental results to justify the validity of the approach, despite noting that improvements could be made. The mention of the paper potentially making an 'interesting ICLR paper' also adds to the positive sentiment. The language used is polite and constructive, with suggestions for improvement and appreciation for the authors' responses to previous concerns.",70,90
"This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model. +This is is well written and more of ideas are clearly presented. +It demonstrates an interesting application of LSTM sequential modeling to HIDS problem -The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. -The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.","The reviewer's sentiment appears to be mixed, with positive comments on the clarity and presentation of ideas, and the interesting application of LSTM to the problem at hand. However, there are also critical remarks about the novelty of the work and the need for further evaluation of the proposed method. The language used is polite and constructive, with the use of phrases like 'well written' and 'clearly presented', and the criticism is presented in a professional manner without any harsh language.",25,75
"Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons. Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image. Promising results on classification and semantic labeling are reported. Review summary: The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim. Overall it is not a earth shattering paper, but a good piece of incremental science. Pros: * Clear description * Well built experiments * Simple yet effective idea * No overclaiming * Detailed comparison with related work architectures Cons: * Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016) * Results are good, but do not improve over state of the art Quality: the ideas are sound, experiments well built and analysed. Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below) Originality: minor, this is a different combination of ideas well known. Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification). Specific comments: * Section 2.2 “we introduction more nonlinearities (through the convolutional layers and ...”. Convolutional layers are linear operators. * Section 2.2, why exactly RNN cannot have pooling operators ? I do not see what would impede it. * Section 3 “into the computational block”, which block ? Seems like a typo, please rephrase. * Figure 2b and 2c not present ? Please fix figure or references to it. * Maybe add a short description of GRU in the appendix, for completeness ? * Section 5.1, last sentence. Not sure what is meant. The convolutions + relu and pooling in ResNet do provide non-linearities “between layers” too. Please clarify * Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table ? If there is a human in the loop, what is the variance in results between “two human schedulers” ? * Section 5.2.1, last sentence; “we certainly have a strong baseline”; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results. So no, 64.4 is not “certainly strong”. Please tune down the statement. * Section 5.2.3 Modules -> modules * The results ignore any mention of increased memory usage or computation cost. This is not a small detail. Please add a discussion on the topic. * Section 6 “adding multi-scale spatial” -> “adding spatial” (there is nothing inherently “multi” in the RNN) * Section 6 Furthermoe -> Furthermore * Appendix C, redundant with Figure 5 ?","The reviewer's sentiment appears to be generally positive, acknowledging the clarity of the text, the well-constructed experiments, and the detailed comparison with related work. However, the reviewer also notes that the paper's idea is somewhat incremental and not earth-shattering, indicating a tempered enthusiasm. The language used is polite and constructive, with suggestions for improvement and clarification rather than harsh criticism. The reviewer uses phrases like 'good piece of incremental science' and 'seems like a good step forward,' which are polite and encouraging.",70,90
"This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences. The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length. Improving on these would greatly strengthen the paper, as the core idea is interesting.","The sentiment in the review is generally positive towards the novelty and presentation of the paper's idea, as well as the potential practical advantages of the proposed memory structure. However, the reviewer also points out significant weaknesses regarding the lack of experimental comparison and analysis, which suggests that the review is constructively critical. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite tone.",50,80
"This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks. It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net. More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved. The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.","The reviewer's sentiment appears critical and skeptical of the claims made in the paper, pointing out that the proposed approach is not sufficiently novel and that the empirical results do not convincingly demonstrate the claimed improvements. The reviewer also compares the work unfavorably to existing research, suggesting that the paper's results are not competitive with the latest models. The language used is formal and academic, without any personal attacks or overly harsh wording, indicating a professional tone.",-60,70
"The paper is straightforward, easy to read, and has clear results. Since all these parameterisations end up outputting torques, it seems like there shouldn*t be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions? Would we get the same result if there was no reference-pose cost, only a locomotion cost? Would we get the same result if the task was to spin a top? My guess is no. This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited. The video is nice.","The reviewer's sentiment appears to be mildly positive towards the clarity and readability of the paper but expresses skepticism about the generalizability of the results, which indicates a mixed sentiment. The language used is polite and constructive, with no harsh or rude words, but it does convey a critical tone in questioning the potential limitations of the research.",25,75
"This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by: 1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. 2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp. Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper. While the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the improvements and contributions of the paper but also points out missing critical information. The reviewer's language is constructive and focuses on the potential interest of the ICLR community in accepting the paper. The politeness is high, with the reviewer using respectful language and providing constructive feedback without any harsh or rude comments.",50,90
"This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted.","The sentiment in the review appears to be cautiously optimistic. The reviewer acknowledges that the results may not be state-of-the-art but recognizes the potential of the proposed method to improve many architectures. The paper is described as 'very well-written' and the analysis as 'mostly interesting,' which indicates a positive sentiment towards the work. The recommendation for acceptance also contributes to a positive sentiment. The language used in the review is formal and constructive, with no negative or rude words used, indicating a high level of politeness.",70,90
"The submission proposes to modify the typical GAN architecture slightly to include *encrypt* (Alice) and *decrypt* (Bob) modules as well as a module trying to decrypt the signal without a key (Eve). Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code. Examples are given on toy data: *As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed.* The idea considered here is cute. If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact: 1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize. The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time). I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee. 2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere. The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point. Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.","The sentiment of the review appears to be mildly positive towards the concept but skeptical about its practical application. The reviewer calls the idea 'cute' and acknowledges the thought experiment, but also points out significant barriers to practical impact, expressing doubt about the reliability and necessity of the proposed method in the face of existing solutions. The language used is polite and constructive, with no harsh or rude words used. The reviewer provides a balanced critique without dismissing the work outright, maintaining a professional tone throughout.",20,80
"I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement. The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it. I*m not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that (Waldspurger, 2016) suggests that higher order nonlinearities might be beneficial for sparsity. But unless I*m missing something, that work seems to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this? On the other hand, adding a second order term to the descriptor seems an interesting direction, as long as stability to small variations is preserved (which should be shown experimentally) The experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates. Questions: - Can you show empirically that the proposed higher order nonlinearity produces sparser representations than the complex modulus? Other minor issues: - The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition - *Hadamart* -> Hadamard - *Valid set* -> Validation set - *nonzeros coefficients* -> nonzero coefficients - Figure 3 is difficult to understand. Please provide more details. - Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn*t seem to be such comparison in that figure. Please explain. - Please verify the references. The first reference states *MALLAT*.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the promise of the work but point out significant issues with the mathematical derivation, experimental limitations, and presentation quality. The language used is direct and points out flaws without being personal or offensive, maintaining a professional tone throughout. The use of 'please' when asking for clarifications and the absence of harsh language contribute to the politeness of the review.",-30,70
"The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well. Overall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it*s unclear how much broader insight the results yield.","The reviewer's sentiment appears to be quite positive, as they describe the work as 'high quality', 'clearly written', and 'well thought out'. They also commend the detailed explanations and the final results. However, they do express a concern regarding the novelty of the methods and the limitations of the dataset, which prevents the sentiment from being entirely positive. The language used is formal and constructive, with no signs of rudeness or impoliteness. Phrases like 'One gripe is that' and 'it's unclear how much broader insight the results yield' are critical but expressed in a professional manner.",70,90
"The main merit of this paper is to draw again attention to how crucial initialization of deep network *can* be; and to counter the popular impression that modern architectures and improved gradient descent techniques make optimization local minima and saddle points no longer a problem. While the paper provides interesting counter-examples that showcase how bad initialization mixed with particular data can lead the optimization to get stuck at a poor solution, these feel like contrived artificial constructs. More importantly the paper does not consider popular heuristics that likely help to avoid getting stuck, such as: non-saturating activation functions (e.g. leaky RELU), batch-norm, skip connections (resnet), that can all be thought of as contributing to keep the gradients flowing. The paper puts up a big warning sign about potential initialization problems (with standard RELU nets), but without proposing new solutions or workarounds, nor carrying out a systematic analysis of how this picture is affected by most commonly used current heuristic techniques (in architecture, initialization and training). Such a broader scope analysis, especially if it did lead to insights of practical relevance, could much increase the value of the paper for the reader.","The reviewer acknowledges the merit of the paper in highlighting the importance of initialization in deep networks, which is a positive aspect. However, the reviewer also criticizes the paper for presenting what they consider 'contrived artificial constructs' and for not considering popular heuristics that could mitigate the issues discussed. The reviewer's tone is constructive but critical, suggesting that while the paper has value, it is limited by its scope and lack of practical solutions. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite approach to critique.",10,80
"The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks. An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). However, there are few problems with the evaluation: - In the highway network experiment, the author does not compare with a baseline. We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer (first layer of size , second layer of size , third layer of size ) and not in the gate functions. Also, how did you select the hyperparameter values?. - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison. Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline. -Author claims state-of-art in the memory task. However, their approach uses more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size. - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task. - it is not clear when to use low-rank or low-rank + diagonal from the experiments. Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.","The sentiment of the review appears to be critical and points out several shortcomings in the author's work. The reviewer questions the novelty of the proposed framework, the lack of comparison with baselines, the high parameter count in comparison to other models, and the unclear experimental settings. However, the language used is professional and maintains a constructive tone, focusing on the content of the work rather than personal criticism of the author.",-40,70
"Paper Summary: This paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset. Paper Strengths: -- The questions in the dataset are real queries from users instead of humans writing questions given some context. -- Context passages are extracted from real web documents which are used by search engines to find answers to the given query. -- Answers are generated by humans instead of being spans in context. -- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries. Paper Weaknesses: -- The authors say, *We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.*, but the statement is not backed up with any study. -- The paper doesn*t clearly present what additional information can today*s QA models learn from MS MARCO which they can*t from existing datasets. -- The paper should talk about what challenges are involved in obtaining a good performance on this dataset. -- What are the human performances as compared to the models presented in the paper? -- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset? -- What is DSSM mentioned in row 2, Table 5? -- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset. -- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there? -- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text. Preliminary Evaluation: The proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset*s worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.","The reviewer's sentiment appears to be cautiously optimistic about the potential of the dataset but critical of the analysis and evaluation provided in the paper. The reviewer acknowledges the uniqueness and potential usefulness of the dataset but also points out several weaknesses and areas that require further development or clarification. The language used is constructive and focuses on how the paper could be improved, rather than being dismissive or overly critical. Therefore, the sentiment is slightly positive because the reviewer sees value in the work but expects more rigorous analysis. The language is polite and professional, offering criticism in a way that is aimed at improving the work rather than disparaging it.",30,80
"The authors propose a *hierarchical* attention model for video captioning. They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption. Related to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism. I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper. I appreciate the ablation study presented in Table 1. Not enough researchers bother with this kind of analysis. But it does show that the value of the contributions is not actually clear. In particular the case for the TEM is quite weak. Regarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of *fair* comparators from the literature. Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it. The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted. Overall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the effort in conducting an ablation study but question the significance of the paper's contributions, suggesting that the case for the TEM is weak and the selection of comparators is too narrow. The reviewer does not see the work as sufficient for publication in its current form, which indicates a negative sentiment towards the paper's current state. However, the language used is professional and does not contain any personal attacks or harsh criticisms, which suggests a polite tone.",-60,70
"The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (*it cannot possibly capture more complex aspects of the data distribution*, *critical restriction*, etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to *fill up* parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don*t explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of *multi-modality.* There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don*t think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.","The reviewer's sentiment appears to be mixed with a lean towards the critical side. They acknowledge the improvements and interesting aspects of the work, but also point out several weaknesses, such as the lack of clarity and motivation, the need for more analysis, and the surprising nature of some results. The language used is academic and constructive, with no personal attacks or overly harsh criticism, indicating a polite discourse.",-20,80
"This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a *residual recurrent neural network*, that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring. This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below. The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?) In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines. Finally, the paper*s *previous work* section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance *Action-Conditional Video Prediction using Deep Networks in Atari Games* should have been an obvious *must cite*. Minor comments: - Notations are unusual, with *a* denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations - Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product - The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward - c_i is defined as *The control that was performed at time i*, but instead it seems to be the control performed at time i-1 - There is a recurrent confusion between mean and median in 3.2.2 - x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization - The inequality in Observation 1 should be about |x_i|, not x_i - Observation 1 (with its proof) takes too much space for such a simple result - In 3.2.3 the first r_j should be r_i - The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model - *Our approach is not able to learn from good strategies* => did you mean **only* from good strategies*? - Please say that in Fig. 4 *fc* means *fully connected* - It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015) - Please clarify r_j2 as per your answer in OpenReview comments - Table 3 says *After one iteration* but has *PRL Iteration 2* in it, which is confusing - *Figure 5 shows that not only there is no degradation in Pong and Demon Attack*=> to me it seems to be a bit worse, actually - *A model that has learned only from random play is able to play at least 7 times better.* => not clear where this 7 comes from - *Demon Attack*s plot in Figure 5c shows a potential problem we mentioned earlier* => where was it mentioned?","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the originality and soundness of the idea but points out several significant issues that need to be addressed. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone. The reviewer provides detailed feedback and suggestions for improvement, which suggests a positive intention to help the authors improve their work, despite the critical nature of the comments.",-20,80
"The paper proposed conditional biGAN and its extension to multi-view biGAN. The main idea of conditional biGAN is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (	ilde{x}) and the output (y), respectively, in addition to standard biGAN formulation. The description on MV-GAN require more revision. Specifically, the definition on aggregating model, Phi, mapping v and a variable s should be clarified. Looking at Equation (8), I can*t find a term that constrains the output domain of function v to be the same as a data domain. Experimental results are not convincing. In most generation results, the observation is not very well preserved. For example, in Figure 6 the second row, background changes significantly from the observation. We also observe such behavior in digit generation example. Preserving attributes like gender is interesting but doesn*t seem to be a strong indication that the model learn to correlate observation and the output through latent variable.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the main idea of the paper but point out that the description requires more revision and that the experimental results are not convincing. The language used is direct and focuses on the content without any personal attacks or overly harsh criticism, which suggests a neutral to slightly negative sentiment. The language is formal and appropriate for a scientific peer review, indicating a high level of politeness despite the critical feedback.",-30,80
"The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. The online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments. The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. Still: this is a nice work!","The reviewer's sentiment appears to be cautiously positive, acknowledging the interest and potential of the work while also pointing out areas that lack detail or depth. Phrases like 'very interesting', 'quite reasonable', 'generally well written', and 'nice work' indicate a positive sentiment. However, the reviewer also notes that the idea is not completely new and that the analysis is minimal, which tempers the positivity. The language used is polite and constructive, with no harsh or rude words; it provides criticism in a respectful manner.",60,90
"It would seem that the shelf life of a dataset has decreased rapidly in recent literature. SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%. This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP’16, and that the reported machine performance (at the time of paper submission) was only at 51%. One reasonable speculation is that the dataset may have not been hard enough. NewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies. Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions. Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD. In sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don’t yet have satisfying datasets. While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today. That said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better. As a result, I do wonder about the quality of the dataset. For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%). I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain. After all, not all humans have the same level of carefulness or even the same level of reading comprehension. I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance. If anything, I don’t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators. I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one. I can imagine that people might end up asking similar generic questions when not enough context has been presented. Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them. Yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly — turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.","The reviewer's sentiment appears to be cautiously optimistic about the potential value of the NewsQA dataset, acknowledging its attempt to present a harder challenge in the field. However, the reviewer also expresses concerns about the quality of the dataset and the haste in which the paper seems to have been prepared. The language used is constructive and offers specific suggestions for improvement, indicating a neutral to slightly positive sentiment. The reviewer's language is formal and respectful, providing criticism without harshness, which suggests a high level of politeness.",25,80
"For more than a decade, near data processing has been a key requirement for large scale linear learning platforms, as the time to load the data exceeds the learning time, and this has justified the introduction of approaches such as Spark Deep learning usually deals with the data that can be contained in a single machine and the bottleneck is often the CPU-GPU bus or the GPU-GPU-bus, so a method that overcomes this bottleneck could be relevant. Unfortunately, this work is still very preliminary and limited to linear training algorithms, so of little interest yet to ICLR readership. I would recommend publication to a conference where it can reach the large-scale linear ML audience first, such as ICML. This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task. Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR, but can the flash memory FPGA handle it? For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it)","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the clear writing and potential relevance of the work but also points out that the work is preliminary and of limited interest to the intended conference's readership. The recommendation to publish in a different venue suggests a neutral to slightly negative sentiment, as it implies the work is not yet suitable for the targeted conference. The language used is formal and provides constructive criticism without any harsh or offensive wording, indicating a high level of politeness.",-20,80
"This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods. Comments: - Last two sentences of related work claim that other methods can only *examine a limited subset of source code*. Aside from being a vague statement, it isn*t accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn*t increase the complexity of the computation. - In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn*t predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?","The reviewer's sentiment appears to be mildly positive towards the relevance and importance of the problem addressed in the paper, as well as the improvement over previous approaches. However, the sentiment turns neutral to slightly critical when discussing the sufficiency of the contribution and suggesting areas for improvement. The language used is formal and constructive, without any personal attacks or overly harsh criticism, indicating a polite tone.",30,80
"This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective. The paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference. I have a few comments and some important reservations with the paper: 1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure. 2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/). Likewise, combinatorial optimization problems of various difficulties can be found here: http://www.mat.univie.ac.at/~neum/glopt/test.html. It would greatly add to the paper depth to compare the NCO solution quality on some of these problems. (The CPLEX solver famously evaluates its own progress on such a public library of problem instances.) 3) The paper should explain more clearly the structure of the critic network, and how it performs the mapping from a sequence of cities into a baseline prediction. 4) Suggestions for Algorithm 1: Line 5, 6: the notation is not clear; would be what’s intended? Line 7: I’m assuming that this assignment must be done , as in lines 5,6? Line 8: is used in two different ways, on the LHS and RHS — slight abuse of notation (but we understand the intent). 5) Suggestions for Algorithm 2: Line 13: same as for Algorithm 1 Line 15: use instead of to indicate multiplication","The reviewer's sentiment appears to be cautiously positive, acknowledging the paper's clarity and the novelty of the approach, while also expressing interest in its potential for the conference. However, the reviewer also lists several important reservations and suggestions for improvement, which indicates a balanced view rather than outright enthusiasm. The language used is formal and constructive, with no signs of rudeness or disrespect, indicating a high level of politeness.",50,90
"The authors introduce a semi-supervised method for neural networks, inspired from label propagation. The method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008). As possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset. Experiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric. In summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.","The reviewer's sentiment is negative, as they point out that the method proposed by the authors is not novel and is essentially the same as one previously proposed in 2008. They also criticize the lack of convincing results and scientific novelty. The language used is direct and critical but not rude or offensive. The reviewer maintains a professional tone throughout, which is expected in scientific peer reviews.",-70,70
"This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. As pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author*s also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the sensible approach of the paper but also point out significant issues with the methodology and the marginal gains reported. The reviewer also expresses doubt about the effectiveness of the proposed model. Therefore, the sentiment is not overtly negative but rather neutral with a critical tone, leading to a slightly negative score. The language used is formal and respectful, with no signs of rudeness or personal attacks, which suggests a high level of politeness.",-20,80
"The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. The motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1).","The reviewer's sentiment appears to be constructive but critical, pointing out both the strengths and weaknesses of the paper. The reviewer acknowledges the novel approach and the performance of the proposed method but also highlights several areas for improvement, such as clarity, motivation, computational intensity, and the need for more extensive empirical evaluations. The language used is professional and provides specific feedback without any personal attacks or overly harsh criticism, indicating a neutral to slightly negative sentiment and a polite tone.",-20,80
"This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use. The experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear. The experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability—but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1. Fig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task? “Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.” This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn’t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance—i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text. Overall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.","The reviewer's sentiment appears to be quite positive, as they describe the paper's optimization formulations as 'interesting' and note that the experiments are done to a 'high standard' and yield 'useful insights'. The reviewer also states that the writing is 'clear'. These positive remarks suggest a sentiment score on the higher end of the scale. The language used in the review is formal, constructive, and respectful, indicating a high level of politeness. The reviewer provides specific recommendations and questions without any negative or harsh language, which contributes to a high politeness score.",80,90
"This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length. There doesn*t seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity. For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks. The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context. Some findings seem trivial. detailed comments p2 *Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide* I don’t think so. In *Deep Belief Networks are Compact Universal Approximators* by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n?1 + 1 layers of n units (with n the number of input neutron). “Comparing architectures in such a fashion limits the generality of the conclusions” To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again). It is much harder to generalise the approach you propose, based on random networks which are not used in practice. “[we study] a family of networks arising in practice: the behaviour of networks after random initialisation” These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not. “results on random networks provide natural baselines to compare trained networks with” random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii). p5 “As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.” As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional. p6 the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial p7 in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.","The reviewer's sentiment appears to be quite critical of the paper, pointing out a lack of solid justification for the measures of expressivity, calling the paper obscure and too long, and suggesting that some findings are trivial. The reviewer also challenges the relevance and generalizability of the study's approach. Despite the critical nature, the language used is not rude but rather direct and professional, focusing on the scientific content and methodology rather than personal attacks or unprofessional remarks.",-70,70
"In this paper, the authors study the problem of discovering options for reinforcement learning. They introduce the Bi-POMDP model, which is a POMDP where the observations are structured as a pair of elements, with the first element only available to the option-choosing component (the *option level*) and the second element only to the action-choosing component (the *action level*) and the termination component (the *acquisition model*). They also detail the BONN learning model, which consists of three artificial neural network that implement these three components. Finally, they suggest optimizing a tradeoff between the value achieved by the model and the cost of switching between options (the *cognitive effort*), and demonstrate this approach in three simple domains: Cart-Pole, Lunar Lander and two variants of a grid-world maze. The paper is interesting, and adds considerably to the increasing body of research in hierarchical reinforcement learning (HRL). I found no critical flaws in the paper, but also no high-impact insights or impressive improvements. This paper offers some good ideas that are moderately novel and may advance the field, but has some issues. The first issue is that it is unclear how much easier it is to compose Bi-POMDPs than hand-crafted options or subgoals. If Bi-POMDPs are to alleviate the design costs of using HRL with human-defined structure, one needs to show that the splitting of observations into two elements (x, y) is easier to do well enough. For example, in Section 5 the authors are correct in pointing out that sequences of actions (*macro-actions*) are open-loop, and therefore not as expressive as closed-loop options. However, by setting x empty in all but one experiment, the authors also restrict themselves to open-loop sub-policies, albeit stochastic ones. Such sub-policies may be sufficiently expressive for the simple domains in this paper, but this is unlikely so in more realistic domains. In the MAZE_2 domain, x is the agent*s position relative to the current room. Designing this domain-specific observation model requires domain knowledge, arguably no less than designing relevant subgoals. It is hard to judge the effectiveness of this approach without design principles for these domain-specific observation models, and more realistic experiments to evaluate their quality. Finally, it is revealing that the drive for hierarchy is only achieved by limiting x. In Section 3.3 the authors mention that acquisition of y is *crucial for discovering a good policy: an agent only using the observations x_t would be unable to solve the task*, which suggests that the design choice of x directly impacts one side of the trade-off between the value and the cognitive effort. Such an important factor should be addressed explicitly. The authors may be interested in the paper *Learning and Transfer of Modulated Locomotor Controllers* (Heess et al., 2016), which has a similar split observation model, and suffers from the same issue. The second issue is that the option space seems to be expressive enough to represent y with high fidelity. If this is the case, and if the learned option model indeed maintains a good image of y, then the algorithm is really solving a different problem: reinforcement learning with costly observability of y. This means that it learns how to act given a stale value of y and when to refresh it. It should then be framed accordingly and compared with the relevant literature. It may or may not be as interesting or novel. In contrast, the standard options framework calls for compression of y into the choice of option o (usually in a small finite space). The agent should learn to extract subtask-relevant information from y, in a way that generalizes to unseen states or subtasks. That said, the embedding of y in the option space can be interesting in itself, even if it is lossless (1-to-1). Unfortunately, no such analysis was offered by the authors. Minor issues: - In 3.3: error in citation, (?) appears instead. - In 4.1: it is confusing to say that *the environments are more stochastic* when epsilon is increased. It is the agent*s policy, not the environment, that becomes more stochastic, which is useful for exploration. If this hurts performance, some discussion is needed of why too much exploration is detrimental to learning. In particular, the paper does not make explicit the number of iterations in the experiments, and it is not clear whether learning with larger epsilon is worse after some fixed number of iterations or asymptotically.","The reviewer's sentiment appears to be moderately positive towards the paper, acknowledging that it is interesting and adds to the body of research in hierarchical reinforcement learning. However, the reviewer also points out that there are no high-impact insights or impressive improvements, and they identify specific issues with the paper's approach and experiments. The language used is constructive and professional, with the reviewer providing detailed feedback and suggestions for improvement without any harsh or disrespectful wording. The reviewer's tone is critical but fair, aiming to provide constructive criticism rather than simply dismissing the work.",40,80
"This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model*s ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model*s ability to generate new and interesting object types. I am not very familiar with the literature on computational creativity research, so I can*t judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking. As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation. Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.","The reviewer expresses a positive sentiment towards the paper, finding the ideas 'new, interesting and thought-provoking' and appreciating the effort to formalize computational creativity within machine learning. They also consider the paper well-written and deserving of its place at the conference. The language used is polite and constructive, with no negative or rude remarks, and the reviewer acknowledges their own limitations in familiarity with the literature.",70,90
"The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments. My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems. Questions: 1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful. 2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components? 3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed. 4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible? The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix. I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.","The reviewer's sentiment appears to be slightly negative, as they express concerns about the generalizability and applicability of the presented framework, leading to a 'weak reject' recommendation. However, the reviewer acknowledges the reported speedup factors and the theoretical model's match with experiments, which adds a slightly positive aspect. The language used is formal and constructive, with no rude or aggressive terms, indicating a high level of politeness. The reviewer provides clear questions and recommendations for improvement, which is typical of a professional and courteous peer review.",-20,90
"This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce. I feel that there might be some fundamental misunderstanding on SGD. **The combiner matrixM generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f) space and time, where f is the number of features. In contrast,M is a f f matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have thousands if not millions of features.* I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i*). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i* v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author*s assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way. I suggest authors to make the following changes to make this paper more clear and theoretically solid - provide computational complexity per step of the proposed algorithm - convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.","The reviewer's sentiment appears critical and skeptical of the paper's assumptions, methodology, and results. The use of phrases like 'fundamental misunderstanding,' 'seriously doubt,' and 'unclear for me' indicate a negative sentiment towards the paper's content. However, the reviewer does not use insulting or overly harsh language, maintaining a professional tone throughout the critique. The language is direct and to the point, which is typical in academic peer reviews, but it does not cross into rudeness. The reviewer also provides constructive feedback and suggestions for improvement, which is a polite way to communicate criticism.",-60,70
"The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature. There is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of *maybe* *could mean*, *perhaps* etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is. The theoretical section could be made a little clearer. Finally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.","The reviewer expresses interest in the topic but also shows concern about the paper's maturity and clarity, indicating a mix of positive and negative sentiments. The use of phrases like 'I find this research very interesting' suggests a positive sentiment, while 'I am concerned' and 'the paper is a bit premature' indicate negative sentiments. The language used is formal and constructive, without any personal attacks or harsh words, which is typical for academic discourse. Phrases like 'in my opinion' and 'I am not convinced' are polite ways of expressing doubt or disagreement.",10,80
"The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new *rivalry metric*. These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research. That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I*m not very excited about this paper. I was hoping for a more significant scientific contribution to accompany in this new environment. It*s not clear if this is necessary for publication, but also it*s not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better *venue*, JMLR has an associated journal track for accompanying papers: http://www.jmlr.org/mloss/) --- Post response: Thank you for the clarifications. Ultimately I have not changed my opinion on the paper. Though I do think RLE could have a nice impact long-term, there is little new science in this paper, ad it*s either too straight-forward (reward shaping, policy-shaping) or not quite developed enough (rivalry training).","The reviewer's sentiment appears to be mixed with a lean towards negative. While they acknowledge the potential contribution of the RLE to the field, they express disappointment in the lack of significant scientific contribution and polish in the paper. The reviewer's use of phrases like 'I'm not very excited about this paper' and 'it's disappointing' indicate a negative sentiment. However, they do not dismiss the work entirely and recognize the long-term impact, which prevents the sentiment from being entirely negative. The language used is formal and constructive, without any personal attacks or overly harsh criticism, which suggests a relatively polite tone. The reviewer provides clear and specific feedback, which is a sign of a professional and respectful review.",-40,70
"This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results. Pros: - The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] - Good visualisations. Cons: - No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps. - Not clear how the query is fed into the localisation networks of baselines. - The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets. - No real novelty.","The sentiment of the review is negative, as the reviewer points out a lack of novelty and significant results in the paper. They mention that the idea of progressive attention has been done before, and they list several specific areas where the paper falls short, such as not evaluating progressive baselines and not providing clear performance gains on standard datasets. The language used is direct and critical but not rude or disrespectful. The reviewer provides a balanced view by mentioning a positive aspect (good visualizations), but the overall tone is critical of the work's originality and impact.",-60,70
"The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I think it is a simple, straightforward, novel extension, but I am not fully convinced its advantages.","The reviewer's sentiment appears to be cautiously critical. They acknowledge the paper as a 'simple, straightforward, novel extension' which is positive, but also express doubt about its advantages over the original work, indicating a lack of full conviction in the paper's contributions. This suggests a sentiment that is not overtly negative but includes skepticism. The language used is polite and academic, with no use of rude or confrontational language. The reviewer's critique is framed constructively, focusing on the comparison of the method with previous work and questioning its efficacy in a professional manner.",20,80
"The authors try to address the issue of data efficiency in deep reinforcement learning by meta-learning a reinforcement learning algorithm using a hand-designed reinforcement learning algorithm (TRPO in this case). The experiments suggest comparable performance to models with prior knowledge of the distribution over environments for bandit tasks, and experiments on random maze navigation from vision is shown as well, though the random maze experiments would benefit from a clearer explanation. It was not obvious from the text how their experiments supported the thesis of the paper that the learned RL algorithm was effectively performing one-shot learning. The subject of the paper is also strikingly similar to the recently-posted paper Learning to Reinforcement Learn (https://arxiv.org/pdf/1611.05763.pdf), and while this paper was posted after the ICLR deadline, the authors should probably update the text to reflect the state of this rapidly-advancing field.","The reviewer's sentiment appears to be cautiously constructive, acknowledging the authors' attempt to address a significant issue in deep reinforcement learning and noting that the experiments suggest comparable performance. However, the reviewer also points out areas that require improvement, such as the need for clearer explanations and the necessity to update the paper to reflect recent advancements in the field. The language used is professional and offers constructive criticism without any harsh or overly negative wording. Therefore, the sentiment is slightly positive as the reviewer recognizes the work's potential but also sees room for improvement. The politeness is high, as the critique is delivered in a respectful and academic tone.",25,75
The paper proposes an architecture called ReasoNet that reason over the relation. The paper addresses important tasks but there are many other related works. The comparison to other methods are not comprehensive. The Graph Reachability dataset is not a good example to use.,"The sentiment of the review appears to be critical but not overtly negative, as it acknowledges the importance of the tasks addressed by the paper but points out the lack of comprehensive comparison and the unsuitability of the dataset used. The language used is straightforward and does not contain any personal attacks or overly harsh criticism, suggesting a neutral to slightly negative sentiment. The politeness is maintained throughout the review; the reviewer provides feedback without using inflammatory or disrespectful language, indicating a polite tone.",-40,70
"The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method seems potentially quite interesting but the paper has serious problems in the presentation. Quality: The method relies mainly on techniques presented in a NIPS 2015 paper by (mostly) the same authors. The experimental procedure should be clarified further. The results (especially Table 2) seem to depend critically upon the sparsity of the reported clusters, but the authors do not explain in sufficient detail how the sparsity hyperparameter is determined. Clarity: The style of writing is terrible and completely unacceptable as a scientific publication. The text looks more like an industry white paper or advertisement, not an objective scientific paper. A complete rewrite would be needed before the paper can be considered for publication. Specifically, all references to companies using your methods must be deleted. Additionally, Table 1 is essentially unreadable. I would recommend using a figure or cleaning up the table by removing all engineering notation and reporting numbers per 1000 so that e.g. *0.475 +/- 9e-4* would become *475 +/- 0.9*. In general figures would be preferred as a primary means for presenting the results in text while tables can be included as supplementary information. Originality: The novelty of the work appears limited: the method is mostly based on a NIPS 2015 paper by the same authors. The experimental evaluation appears at least partially novel, but for example the IBD detection is very similar to Hochreiter (2013) but without any comparison. Significance: The authors* strongest claim is based on strong empirical performance in their own benchmark problems. It is however unclear how useful this would be to others as there is no code available and the details of the implementation are less than complete. Furthermore, the method depends on many specific tuning parameters whose tuning method is not fully defined, leaving it unclear how to guarantee the generalisation of the good performance.","The reviewer's sentiment appears to be quite negative, as they describe serious problems with the paper's presentation, call the writing style 'terrible and completely unacceptable,' and suggest a complete rewrite. They also criticize the originality and significance of the work. However, they do acknowledge that the method is 'potentially quite interesting,' which prevents the sentiment from being the lowest possible. The language used is direct and harsh, especially with phrases like 'terrible and completely unacceptable,' which are not typical of polite academic discourse. Despite the harshness, the reviewer does not resort to personal attacks or unprofessional language, which prevents the politeness score from being the lowest possible.",-80,-70
"The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.","The reviewer's sentiment appears to be mixed, with positive remarks about the paper being 'quite nice and looks technically sound' but also expressing a 'mixed feeling' due to the similarity with previous work. The reviewer's concerns about the evaluation criteria and the model's scalability for large datasets suggest a critical but balanced view. Therefore, the sentiment is neither strongly positive nor negative, but cautiously optimistic with reservations. The language used is formal and constructive, with no signs of rudeness or disrespect, indicating a high level of politeness.",25,75
"This paper proposes the RIMs that unrolls variational inference procedure. The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments. While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free* configuration, where model and inference are not separable and can be learnt jointly. However I do not quite agree the authors* argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2]. Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the interesting perspective of the paper but disagree with some of the author's arguments, suggesting that the novelty is somewhat limited. The language used is academic and respectful, with no signs of rudeness or personal attacks. The reviewer provides specific feedback and comparisons with other works, which indicates a neutral to slightly negative sentiment due to the criticism but remains polite and professional in their discourse.",-30,70
"This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes. Positives ------------ - Clear definition of the task (handling missing values for classification of time series) - Many interesting baselines to test the new model against. - The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters). - The extensive tests done on the datasets is probably the greatest asset of this paper. Negatives ------------- - The paper could use some double checking for typos. - The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space. - No mention of any methods from the statistics litterature. Here are the two main points of this review that informs my decision: 1. The results, while promising, are below expectations. The paper hasn’t been able to convince me that GRU-simple (without intervals) isn’t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it’s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn’t significantly outperforms it. 2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient’s condition. That would mean that an expert system based on this data would always seem to be one step behind. The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that “simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values” is unsubstantiated and actually disproven later in the paper. Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. Given that, while I find that the work that has been put into the paper is above average, I wouldn’t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the clear definition of the task, interesting baselines, and extensive testing as positives, which suggests a somewhat positive sentiment. However, the negatives, such as typos, misplaced sections, and unsubstantiated claims, along with the reviewer's overall conclusion that the paper should not be accepted without changes, indicate a critical stance. The sentiment is therefore not entirely negative but shows significant criticism, warranting a score closer to neutral but on the negative side. The language used in the review is formal and constructive, with specific recommendations and a focus on the paper's content rather than personal remarks. The reviewer's tone is critical but not rude, indicating a high level of politeness in academic discourse.",-30,80
"This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions. Despite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses. The first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right? A related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set). To mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task. The other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper. Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.","The sentiment of the review is somewhat mixed but leans towards the negative due to the identification of major weaknesses and areas of confusion in the paper. The reviewer acknowledges the interesting main idea but criticizes the execution and clarity, suggesting that the paper was rushed. The language used is polite and constructive, focusing on how the paper could be improved rather than dismissing the work outright. The reviewer uses phrases like 'does a great job' and 'definitely worth pursuing further' which are positive, but these are offset by the critical analysis that follows.",-30,80
"The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks. Comments: The paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable. The error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax? Although the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing. This might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition. More motivation is necessary for the proposed smoothing.","The sentiment of the review appears to be constructive and neutral, as the reviewer acknowledges the work done on the label smoothing technique and its application to various tasks, but also points out areas that require improvement or further investigation. There is no overt praise or criticism, suggesting a balanced view. The language used is formal and academic, with no use of slang or offensive language, indicating a high level of politeness. The reviewer provides clear and specific recommendations without any negative tone.",0,80
"This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A. The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A. Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning. General comments/questions: - As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task. All experiments show that you can improve on the new task only. - The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading. The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data. - One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together. That is, you train the model from scratch on both tasks simultaneously (which sharing parameters). It is not clear this is the same as what is referred to in Sec. 8 as *joint training*. Can you please explain more clearly what you refer to as joint training? - Why can*t we find the same baselines in both Table 2 and Table 3? For example Table 2 is missing *joint training*, and Table 3 is missing GRU trained on the target task. - While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task). I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task. Writing comments - The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely. - Ensemble methodology paragraph does not fit the flow of the paper. I would rather explain it in the experiments section, rather than including it as part of your approach. - Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the method's potential for improving performance on a new task but points out several shortcomings and areas that lack evidence or clarity. The tone is not overtly negative but rather focused on providing constructive feedback for improvement. The language used is polite and professional, adhering to academic standards. The reviewer uses phrases like 'As far as I can tell,' 'I find a bit misleading,' and 'Can you please explain more clearly,' which are courteous ways of expressing doubt or requesting clarification. There is no use of rude or confrontational language.",-20,80
"The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy. The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below. 1- The paper is not clear that it is only focusing on neural network inference. Please include the word *inference* in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different. 2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference. 3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper. 4- The whole discussion about *efficient customized precision search* seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples. 5- Nvidia*s Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited. More comments: - Parts of the paper discussing *efficient customized precision search* are not clear to me. - As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.","The sentiment of the review appears to be cautiously optimistic and constructive. The reviewer leans toward a weak accept, indicating a positive inclination, but also lists several areas for improvement, which suggests a balanced view. The language used is formal and constructive, with suggestions for improvement rather than harsh criticism, reflecting a polite tone. The reviewer uses phrases like 'nice overview' and 'important aspect' which are positive, but also phrases like 'could be improved' and 'not clear to me' indicating areas of concern. The overall tone is professional and aimed at helping the authors improve their work.",30,80
"This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. Pros: The method is very simple and easy to understand and apply. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network. Cons: There is little novelty -- the method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation. The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what’s done in the Inception BN results in Table 1-2?) The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN. Section 3.3: it’s not clear to me what point is being made here. Overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”). If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.","The reviewer's sentiment appears balanced, acknowledging the simplicity and effectiveness of the method while also critiquing the lack of novelty and suggesting removal or revision of certain sections. The language is constructive and focuses on the content rather than personal remarks, indicating a neutral to slightly positive sentiment. The reviewer's language is polite and professional, offering criticism without harsh language and providing clear recommendations for improvement.",50,80
"This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows: 1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user*s job, it wouldn*t be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn*t it only because you are somehow serialize the communication? And it would be maybe much faster if a *MPI_Reduce* would be used (even if we wait for the slowest guy)? 2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain. 3.they propose to take gradient from the first *N* workers out of *N+b* workers available. My concern here is that they focused only on the workers, but what if the *parameter server* will became to slow? What if the parameter server would be the bottleneck? How would you address this situation? But still if the number of nodes (N) is not large, and the deep DNN is used, I can imagine that the communciation will not take more than 30% of the run-time. My largest concern is with the experiments. Different batch size implies that different learning rate should be chosen, right? How did you tune the learning rates and other parameters for e.g. Figure 5 you provide some formulas in (A2) but clearly this can bias your Figures, right? meaning, that if you tune *gamma, eta* for each N, it could be somehow more representative? also it would be nicer if you run the experiment many times and then report average, best and worst case behaviour. because now it can be just coinsidence, right?","The reviewer's sentiment appears to be constructive and somewhat positive, as they acknowledge the clarity of the paper and its main idea. However, they also express concerns and suggest improvements, indicating a balanced view rather than outright enthusiasm or negativity. The language used is polite and professional, with the reviewer providing feedback in a respectful manner without any harsh or rude language. They use phrases like 'from my own experience' and 'I am not sure,' which soften the critique and show openness to discussion.",30,80
"The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach?","The sentiment of the review appears to be critical and questioning the advantages of the proposed method over existing methods. The reviewer points out the lack of theoretical guarantees and only marginal empirical improvements. Additionally, the reviewer notes that the key idea has been previously applied and questions the computational cost without providing positive feedback or commendation. The language used is formal and academic, with no personal attacks or overly harsh wording, but it lacks positive reinforcement or encouragement, which is typical in constructive feedback.",-50,70
"This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn*t propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I*m not sure the paper is appropriate for an ICLR conference track as it doesn*t provide any greater theoretical insights into sampling generative models and there are no comparisons / quantitative evaluations of the techniques proposed. Overall, I*m very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.","The reviewer's sentiment appears to be cautiously positive, acknowledging the relevance and usefulness of the techniques presented in the paper, while also expressing uncertainty about the paper's fit for the ICLR conference track due to a lack of theoretical insights and quantitative evaluations. The language used is polite and constructive, with no negative or rude remarks, and the reviewer expresses a willingness to reconsider their position if the author can justify the appropriateness of the venue for the work.",50,80
"The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively. The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified. Is sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer. Experiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty.","The reviewer's sentiment appears to be critical and points out several perceived shortcomings in the paper. The use of terms like 'hacky' and 'not really well justified' indicate a negative sentiment towards the work. However, the reviewer does acknowledge the consistent improvement over the baseline model, which prevents the sentiment from being extremely negative. The language used is direct and could be perceived as blunt, but it is not rude or disrespectful. It is typical of academic discourse to be straightforward in critiques. The reviewer uses technical language appropriate for a peer review and does not include personal attacks or unprofessional remarks.",-60,40
"This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term. Pros: The application of object detection techniques to extreme weather event detection problem is unique, to my knowledge. The paper is well-written and describes the method well, including a survey of the related work. The best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature. Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach. Cons: The benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result. It’s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result; I’d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases. The paper does acknowledge this and provide potential explanations in Sec. 4.3, however. As other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion. On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events. Still, it would be useful to also report results at higher overlap thresholds. Minor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself. Minor: table 4 -- shouldn’t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers? Overall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain. The results aren*t striking, but the model is ablated appropriately and shown to be beneficial. For a final version, it would be nice to see results at higher overlap thresholds.","The sentiment of the review is generally positive, as the reviewer acknowledges the uniqueness of the application of object detection techniques to the problem, the well-written nature of the paper, and the methodological approach including a survey of related work. The reviewer also appreciates the use of 3D convolutions and unsupervised learning, noting that they are validated and show performance improvements. However, the reviewer also expresses some disappointment with the underwhelming benefits of the 3D convolutional architecture and unsupervised learning, and points out some areas for improvement, which indicates a balanced view rather than unreserved praise. The language used in the review is polite and constructive. The reviewer uses phrases like 'to my knowledge,' 'well-written,' and 'it would be nice to see,' which are courteous and show respect for the authors' work. The reviewer also provides constructive criticism without harsh language, maintaining a professional tone throughout.",50,90
"This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks. They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets. Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance.","The review starts with a summary of the paper's content and acknowledges the novelty and effectiveness of the proposed algorithm. The reviewer mentions that the algorithm is 'simple yet an effective way' to improve classification performance and that it shows 'comparable or better object recognition performance' to the state-of-the-art. This indicates a positive sentiment towards the paper's contributions. The language used is formal, technical, and constructive, with no negative or harsh words, indicating a high level of politeness.",80,90
"This paper extends the *order matters* idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. I think the problem is interesting and the results are promising. However, there are some problems about technical details: - Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)? - Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function? - What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.","The sentiment of the review appears to be cautiously positive. The reviewer describes the problem as 'interesting' and the results as 'promising,' which indicates a positive view of the paper's contributions. However, the reviewer also points out several 'problems about technical details,' suggesting that there are areas that need clarification or improvement. This mix of positive feedback with constructive criticism leads to a sentiment score that is moderately positive. The language used in the review is formal and typical of academic discourse. The reviewer uses polite language to express their concerns and questions, without any harsh or rude words, which is why the politeness score is high.",50,90
"The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. The model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf). Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850. The difficult part in training the proposed model is backpropagation through the discrete decisions. Typically, reinforcement learning techniques are used. In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model. The results indicate that such specially supervised model is better than the generic soft-attention model that doesn*t require any problem-dependent external supervision. However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison. That being said the authors reash state-of-the-art results against other domain specific methods. I believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.","The reviewer's sentiment appears to be cautiously positive, acknowledging the novelty of the paper but also pointing out that similar architectures have been proposed and suggesting that the comparison with other models could be improved. The language used is professional and constructive, without any harsh or overly critical words, indicating a polite tone.",30,70
"Paper summary: This work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight. Review summary: The topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear. This looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR. Pros: * Interesting topic * Black-box setup is most relevant * Multiple experiments * Shows that with flipping only 1~5% of pixels, adversarial images can be created Cons: * Too long, yet key details are not well addressed * Some of the experiments are of little interest * Main experiments lack key measures or additional baselines * Limited technical novelty Quality: the method description and experimental setup leave to be desired. Clarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise. Originality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising. Significance: the work is incremental, the issues in the experiments limit potential impact of this paper. Specific comments: * I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments. * Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB]. * [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB] * The modification is done after normalization, is this realistic ? * Alg 2, why not clamping to [LB, UB] ? * Section 6, “implementing algorithm LocSearchAdv”, the text is unclear on how p is adjusted; new variables are added. This is confusion. * Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ? * Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. “in many cases far less average perturbation” claim). * Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the “effectiveness” of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. * How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ? * Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims. * Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting. * If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the “top N” largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest. Minor comments: * The is an abuse of footnotes, most of them should be inserted in the main text. * I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB) * Table 1,2,3 should be figures * Last line of first paragraph of section 6 is uninformative. * Very tiny -> small","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the interesting topic and relevance of the black-box setup but points out several issues with the paper, such as verbosity, disorganization, and limited interest of the experiments. The reviewer's tone is not harsh or dismissive but rather suggests improvements and clarifications, indicating a neutral to slightly negative sentiment. The language used is polite and professional, with suggestions framed as advice rather than commands, and criticism is specific and actionable, not personal or offensive.",-30,80
"The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). Pros: + Comprehensive analysis on the co-label similarity. Cons: - Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. - The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.","The reviewer's sentiment appears to be more on the critical side, pointing out the lack of novelty and weak baselines. However, they do acknowledge the comprehensive analysis on co-label similarity, which adds a slightly positive aspect. Overall, the sentiment leans towards negative due to the criticism of the paper's novelty and experimental results. The language used is direct and to the point, without any personal attacks or overly harsh wording, suggesting a neutral to slightly polite tone. The reviewer's use of 'I am not sure' and 'This is probably not enough' indicates a reserved criticism rather than an aggressive one.",-40,40
"The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST. Both two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al. Regarding to the experiment on PTB (section 5.2), I*m skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model. Pros: - good experimental results Cons: - ideas are quite trivial - the experiment on PTB was carried out improperly","The reviewer's sentiment appears to be slightly negative, as they describe the ideas in the paper as 'trivial' and criticize the experimental setup. However, they acknowledge good experimental results, which adds a slightly positive aspect. The language used is direct and critical but not rude or offensive, maintaining a professional tone throughout the review.",-30,70
"Summary: This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks. Contributions: The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps. Questions: A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input. Criticisms: The paper is really badly written, authors should rethink the organization of the paper. Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. The justification is not convincing enough. Experimental results are lacking, only results on a single dataset are provided. Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong. The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks). High-level Review: Pros: - A simple modification of the model that seems to improve the results and it is an interesting modification. Cons: - The authors need to use test-set labels. - Writing of the paper is bad. - The authors assume that they have access to the ground-truth labels during the test-set. - Experimental results are lacking","The reviewer's sentiment appears to be more on the negative side due to the criticisms mentioned such as the paper being 'really badly written', the organization needing rethinking, and the lack of convincing justification. Additionally, the reviewer points out the lack of experimental results and incorrect claims about state-of-the-art results. However, there is a recognition of the potential improvement the proposed method could bring, which adds a slightly positive aspect. The language used is direct and could be perceived as harsh, especially with phrases like 'really badly written', but it does not contain personal attacks or highly disrespectful language, thus it is not extremely rude.",-60,-20
"The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency. Comments: 1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation. 2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao*s compute units). 3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.","The reviewer's sentiment appears to be mixed. They acknowledge the interesting aspect of the bit-serial computation introduced by TARTAN but also express concerns about the novelty and the practicality of the approach, citing a severe area overhead and questioning the energy efficiency improvements. The language used is professional and constructive, with no personal attacks or overly harsh criticism, suggesting a polite tone.",0,75
"This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.","The reviewer's sentiment appears to be critical and points out several flaws in the paper. They mention theoretical issues, lack of justification for claims, insufficient experiments, and missing real-case scenarios. However, the language used is formal and does not contain any personal attacks or overly harsh wording. The critique is directed at the work and not the authors personally.",-60,70
"The authors have put forward a sincere effort to investigate the *fundamental nature of learning representations in neural networks*, a topic of great interest and importance to our field. They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning. This is an interesting idea and one that could potentially be instructive, though in total I don*t think that has been achieved here. First, I find the introduction of pruning lengthy and not particularly novel or surprising. For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0. The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question. However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion. Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface. The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value. The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally. Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the authors' effort and the interest of the topic, which is positive, but they also express disappointment with the depth, novelty, and insights provided by the paper, which is negative. The language used is formal and academic, without any personal attacks or overly harsh wording, indicating a level of politeness.",-30,70
"The paper is about channel sparsity in Convolution layer. The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity. The paper contains detailed literature review. In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices. The main point is that the paper needs more detailed investigation on different dropout schedule. As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'well written' and acknowledge that it 'elaborately discussed and investigated different approaches for applying sparsity' and 'contains detailed literature review.' Additionally, they recognize the utility of the paper's results in embedded applications. However, they also suggest that the paper 'needs more detailed investigation on different dropout schedules' and that the implementation could be optimized to reduce computation time, which indicates some areas for improvement. The language used is constructive and respectful, with no negative or harsh wording, indicating a high level of politeness.",70,90
"The paper proposes an approach to learning models that are good for planning problems, using deep netowork architectures. The key idea is to ensure that models are self-consistent and accurately predict the future. The problem of learning good planning models (as opposed to simply good predictive models is really crucial and attempts so far have failed. This paper is conceptually interesting and provides a valuable perspective on how to achieve this goal. Its incorporation of key RL concepts (like discounting and eligibility traces) and the flexibility to learn these is very appealing. Hence, I think it should be accepted. This being said, I think the paper does not quite live up to its claims. Here are some aspects that need to be addressed (in order of importance): 1. Relationship to past work: the proposed representation seems essentially a non-linear implementation of the Horde architecture. It is also very similar in spirit to predictive state representations. Yet these connections are almost not discussed at all. The related work paragraph is very brief and needs expansion to situate the work in the context of other predictive modelling attempts that both were designed to be used for planning and (in the case of PSRs) were in fact successsfully used in planning tasks. Some newer work on learning action-conditional models in Atari games are also not discussed. Situating the paper better in the context of existing model learning would also help understand easier both the motivations and the novel contributions of the work (otherwise, the reader is left to try and elucidate this for themselves, and may come to the wrong conclusion). 2. The paper needs to provide some insight about the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and *bulky* (very high number of parameters). Is this really necessary in all tasks? Can one implement the basic ideas outside of the particular architecture proposed? Can we use feedforward approximations or is the recurrent part somehow necessary? At the very least the paper should expand the discussion on this topic, if not provide some empirical evidence. 3. The experiments are very restricted in their setup: iid data drawn from fixed distributions, correct targets. So, the proposed approach seems like an overkill for these particular tasks. There is an indirect attempt to provide evidence the learned models would be useful for planning, but no direct measurement to support this*d claim (no use of the models in planning). Compared to the original Horde paper, fewer predictions are learned, and these are more similar to each other. While I sympathize with the desire to go in steps, I think the paper stops short of where it should. At the very least, doing prediction in the context of an actual RL prediction task, with non-iid inputs, should be included in the paper. This should only require minor modifications to the experiments (same task, just different data). Ideally, in the case of the mazes, the learned models should be used in some form of simplified planning to learn paths. This would align the experiments much better with the claims in the presentation of the architecture.","The reviewer's sentiment appears to be generally positive towards the conceptual aspects of the paper, acknowledging its interesting approach and valuable perspective. However, the sentiment becomes more critical when discussing the paper's shortcomings, such as its relationship to past work, necessity of the architecture, and the restricted nature of experiments. Despite the criticisms, the overall tone suggests that the reviewer believes the paper has merit and potential, leading to a mildly positive sentiment. The language used is formal and constructive, with no signs of rudeness or personal attacks, indicating a high level of politeness.",40,90
"Summary: This paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces. Pros: Formalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks Cons: Analysis seems quite preliminary, and no novel theoretical results or clear practical conclusions. The main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn*t seem ready yet.","The reviewer's sentiment appears to be cautiously optimistic but leans towards the critical side, acknowledging the potential of the research direction while pointing out that the analysis is preliminary and lacks novel theoretical results or practical conclusions. The language used is formal and constructive, without any harsh or overly critical words, suggesting a polite approach to critique.",20,80
"This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets. The paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper. This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).","The review starts with a positive note, acknowledging that the paper is well-written and provides insightful explanations. The reviewer appreciates the technical contributions and the novel results presented. However, the reviewer also provides constructive criticism, suggesting areas where the paper could be improved, such as eliminating unrealistic assumptions and providing additional empirical support for the claims made. The language used is formal and constructive, with no signs of rudeness or negativity, indicating a polite tone throughout the review.",70,90
"Pros: * The general idea behind the paper seems pretty novel and potentially quite cool. * The specific technical implementation seems pretty reasonable and well-thought through. * The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities. * The writing is pretty clear. I basically felt like I could replicate much of what they did from their paper descriptions. Cons: * The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. * The specific tasks that they try are quite simple. I really don*t know whether their approach is better than a bunch of simpler things on these tasks. Taking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper. (I know how it feels to be under a deadline without a complete set of results.) If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine. Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great. But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it*s really hard to tell how much of an advance is made here. But it does seem like a potentially fruitful research direction.","The reviewer's sentiment appears to be cautiously optimistic about the paper's novelty and clarity, but critical of the evaluation depth and task simplicity. The positive remarks about the novelty and clarity of the writing suggest a positive sentiment, while the criticism indicates concerns, leading to a balanced, slightly positive sentiment overall. The language used is formal and constructive, without any rude or overly harsh wording, indicating a high level of politeness.",30,80
"The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP. In my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements. A deep neural network is fundamentally different from a single layer---it is the *deep* part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn*t surprising, that a cascade of layers *is*. For any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it*s entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the interesting direction of research but express that the paper is not ready for publication and lacks sufficient explanation for the phenomenon in deep neural networks. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite approach to critique.",-50,70
"This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional “background suppression” term. Pros: Proposes a “group-wise model diversity” loss term which is novel, to my knowledge. The use of foreground segmentation masks to improve image classification is also novel. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012. Cons: The evaluation is lacking. There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term. The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor. It would be nice to see the results with “Incomplete Privileged Information” on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it’s available. This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data. The presentation overall is a bit confusing and difficult to follow, for me. For example, Section 4.2 is titled “A Unified Architecture: GoCNN”, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence). Minor: calling eq 3 a “regression loss” and writing “||0 - x||” rather than just “||x||” is not necessary and makes understanding more difficult -- I’ve never seen a norm regularization term written this way or described as a “regression to 0”. Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG). An additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2? Are these not the same setting? The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important. Besides additional experiments, the paper could also use some reorganization and revision for clarity. =============== Edit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation *privileged information* is beneficial even with the full labeled ImageNet dataset -- I*ve upgraded my rating from 4 to 6. (I*ll reiterate a very minor point about Figure 1 though: I still think the *0* and *1* labels in the top part of the figures should be swapped to match the other labels. e.g., the topmost path in figure 1a, with the text *suppress foreground*, currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)","The reviewer's sentiment appears to be cautiously optimistic but with clear reservations about the paper's current state. They acknowledge the novelty of the proposed method and its potential promise, but they also point out several areas where the evaluation is lacking and the presentation is confusing. The reviewer's language is constructive and focused on how the paper could be improved, rather than being dismissive or overly critical. The politeness of the language is high, as the reviewer maintains a professional tone throughout, provides specific feedback, and acknowledges the novel aspects of the work. There is no use of rude or inflammatory language, and the reviewer's critiques are framed as suggestions for improvement.",30,90
"The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction. Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality. Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying *if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results*, that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.","The sentiment of the review appears to be negative, as the reviewer points out that the proposed model does not handle multimodality and that the authors dismissed a suggestion without properly understanding it. The reviewer also questions the novelty of the contribution. The language used is direct and critical but not rude or offensive. The reviewer's tone is professional, focusing on the scientific content and methodology rather than personal remarks.",-60,40
"This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix). I like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today*s standards. Specific comments: Based on common practices in ML literature, I have a strong bias to think of as inputs and as network weights. Latent variables are often or . Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model. nit: number all equations for easier reference sec 2.2 -- It*s weird that the transformation is fixed, but is still written as a function of x. sec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I*m not sure which direction I*m confused in, but probably good to clarify the text either way. BTW -- there*s another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales. sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables? I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images. ==== post rebuttal update Thank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.","The reviewer's sentiment appears to be mixed with a positive appreciation for the motivation behind the algorithm but expresses disappointment in the scope and depth of the experiments. The language used is constructive and professional, with suggestions for improvement and clarification, indicating a polite tone. The reviewer's disappointment with the experiments and the suggestion that they are outdated could be interpreted as slightly negative, but the overall tone remains constructive and focused on the content rather than personal criticism.",10,80
"The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness. I see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard. One issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines. In my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.","The reviewer's sentiment appears to be cautiously optimistic about the method's potential but critical of the current lack of convincing results and limited application scope. The language used is constructive and respectful, suggesting improvements without dismissive or harsh tones. The reviewer acknowledges the attractiveness and intuitiveness of the approach but emphasizes the need for stronger experimental evidence.",30,80
"This paper proposes a method that attempts to *understand* what is happening within a neural network by using linear classifier probes which are inserted at various levels of the network. I think the idea is nice overall because it allows network designers to better understand the representational power of each layer in the network, but at the same time, this works feels a bit rushed. In particular, the fact that the authors did not provide any results in *real* networks, which are used to win competitions makes the results less strong, since researchers who want to created competitive network architectures don*t have enough evidence from this work to decides whether they should use it or not. Ideally, I would encourage the authors to consider continuing this line of research and show how to use the information given by these linear classifiers to construct better network architectures. Unfortunately, as is, I don*t think we have enough novelty to justify accepting this work in the conference.","The reviewer expresses a positive sentiment towards the idea of the paper, acknowledging it as 'nice overall' which suggests a positive view on the concept. However, they also express concerns about the lack of results in real networks and the perceived rush in the work, which indicates a mixed sentiment. The language used is constructive and maintains a professional tone without any harsh or rude words, indicating a high level of politeness despite the criticism.",30,80
"This paper makes three main methodological contributions: - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron - ranking of neurons based on color selectivity - ranking of neurons based on class selectivity The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods. However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are: - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported. - Color selective neurons are found even in higher layers. (25% color selectivity in conv5) - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation! Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence. Other small comments: - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite) - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described. - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)","The reviewer's sentiment appears to be cautiously optimistic. They acknowledge the paper's weaknesses, stating that the methodological contributions are not very significant and that there are no singularly significant results. However, they also highlight the strengths and interesting conclusions of the paper, even calling one observation 'Great'. The recommendation for acceptance is based on the potential future importance of the smaller conclusions. The language used is constructive and respectful, with the reviewer providing specific examples and questions for clarification, which indicates a polite tone.",40,80
"The authors explore whether the halting time distributions for various algorithms in various settings exhibit *universality*, i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble. The idea of the described universality is very interesting. However I see several shortcomings in the paper: In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view. Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution. Additionally, I found the paper quite hard to read. Here are some clarity issues: - abstract: *even when the input is changed drastically*: From the abstract I*m not sure what *input* refers to, here - I. Introduction: *where the stopping condition is, essentially, the time to find the minimum*: this doesn*t seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached? - I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful) - I.3 *We use x^ell for ell in Z={1, dots, S} where Z is a random sample from of training samples* This formulation doesn*t make sense. Either Z is a random sample, or Z={1, ..., S}. - II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the interesting idea of universality but points out several shortcomings and areas that could be improved or clarified. The language used is polite and professional, with the reviewer providing specific feedback and questions rather than general criticism. The reviewer's tone is that of someone who is engaged with the material and interested in seeing it improved, rather than dismissive or harsh.",-20,80
"In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization. While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016, Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.","The reviewer's sentiment appears to be quite positive, as they acknowledge the novelty and effectiveness of the paper's approach, mentioning 'the gains are large' and suggesting that the paper should be accepted. There is no negative language or criticism in the first paragraph, only praise for the technique and its results. The language used is formal and academic, with no impoliteness or rudeness detected. The reviewer uses phrases like 'I believe', 'the technique is simple', and 'the authors perform extensive ablation studies', which are all indicative of a respectful and constructive tone.",80,90
"This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation. A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs. The experiments are solid, comprehensive and very useful in practical terms. It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3). However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here. A major criticism I have with this paper is that there is little novelty here. The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016). So although the work is solid, the lack of originality lets it down. Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?","The reviewer acknowledges the comprehensiveness and practical usefulness of the experiments, which indicates a positive sentiment towards the paper's empirical contributions. However, the reviewer also expresses disappointment regarding the limited scope of experiments in one section and the lack of novelty, which introduces a negative sentiment. The overall sentiment seems balanced between appreciation for the solid work and criticism for the lack of originality. The language used is formal and constructive without any personal attacks or harsh words, indicating a high level of politeness.",20,80
"The paper shows how group convolutions (for two dimensional commutative groups) can be performed by standard CNNs if the input is warped using a fixed warp. The idea is practical and seems to work well. The paper is well written. I agree with reviewer 1 that the ‘theorems’ do not deserve to be labelled as such. Theorem 1 is equivalent to the second equation from this section of the wikipedia page on convolution: https://en.wikipedia.org/wiki/Convolution#Convolutions_on_groups. The existence of Haar measure (from which commutation of Lg and convolution follows immediately) is a well known and elementary theorem in harmonic analysis, which would be treated in the first few pages of any textbook on the subject. It also immediately clear that any commutative group has an additive parameterization (theorem 2). The fact that the paper does not present new deep mathematical results is not a significant weakness in my opinion, but the derivations should not be camouflaged as such. The claim that previous methods that use group convolutions are slow and that the presented approach has better computational complexity is not supported by empirical evidence, and the theoretical analysis is still a bit misleading. For example, the authors write “Unfortunately these approaches do not possess the same memory and speed benefits that CNNs enjoy. The reason is that, ultimately, they have to enumerate all possible transformations”. The presented method also has to enumerate all transformations (in a limited range, on a discretized grid), and this is feasible only because the group is only 2 dimensional (and indeed this is also true for standard CNNs which enumerate translations). As noted in my pre-review question, I believe the computational complexity analysis is not entirely correct, and assume the authors will correct this. Equation 4 is presented as a new invention, but this has been used in previous works and is well known in mathematics, so a citation should be added. The main advantage of the presented method over several earlier methods is that it is very simple to implement, and can re-use highly optimized convolution routines. As I understand it, Dieleman et al. and Cohen & Welling also use standard convolutions (after a fixed filter / feature map warp), but these papers only consider discrete groups. So it seems like this paper occupies a unique place in the space of equivariant convolutional networks: non-commutative (-), low-dimensional (-), continuous (+) groups, simple (+) and efficient (+) algorithm. Given the proximity though, a more thorough and balanced appraisal of the merits and demerits, as well as the novelty, relative to each of the previous works would be useful. Provided that these issues are cleared up, I would recommend the paper for publication.","The reviewer's sentiment appears to be cautiously positive, acknowledging the practicality and effectiveness of the paper's idea, as well as the quality of the writing. However, the reviewer also points out several shortcomings, such as the overstatement of theorems and the lack of empirical evidence for computational complexity claims. The reviewer's language is constructive and professional, offering specific critiques without any harsh or disrespectful wording. The reviewer ends on a positive note, suggesting that with certain clarifications, they would recommend the paper for publication.",30,80
"There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2. The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. *Learning acoustic frame labeling for speech recognition with recurrent neural networks*, 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey*s LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors* implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu*s CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.","The sentiment in the review appears to be cautiously positive, acknowledging the novelty and interest in the paper's approach but also pointing out areas that need improvement or clarification. The reviewer does not express overt enthusiasm but recognizes the potential contribution of the paper. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides specific recommendations and questions that suggest a desire to improve the paper rather than dismiss it.",30,80
"The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries. While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. *End-to-end Learning of Deep Visual Representations for Image Retrieval*). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.","The reviewer's sentiment appears to be mildly critical but constructive. They acknowledge the technical clarity of the paper but express a need for further clarification on certain aspects, such as the handling of multiple scales. Additionally, they point out that the paper does not address end-to-end learning frameworks, which are considered superior in recent studies. The language used is formal and academic, with no signs of rudeness or disrespect. The reviewer uses phrases like 'I am still not entirely clear' and 'I can partially accept this argument,' which indicate a respectful tone while also expressing a reserved stance on the paper's content.",-20,80
"Summary: In this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. They do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights. This technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution. As this is a constant, it increases correlation between neurons. Experiments test this out on MNIST and SVHN Comments: This is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. Equation (5) seems to be very straightforwards? It seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy? The paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.","The reviewer's sentiment appears to be cautiously optimistic about the potential of the NoiseOut technique, acknowledging the clarity and interest of the paper while suggesting the need for more experiments. The sentiment is not overly enthusiastic but recognizes the contribution, which suggests a positive but reserved sentiment. The language used is formal and constructive, with no negative or rude expressions, indicating a high level of politeness.",50,90
"In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications. I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer*s stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well. Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard. If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7. Summary: + An interesting approach is presented that might be useful for real-world limited data scenarios. + Limited data results look promising. - Adversarial examples are not investigated in the experimental section. - No realistic small-data problem is addressed. Minor: - The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days. - Some typos: tacke, developping, learni. [1] https://arxiv.org/abs/1610.00768v3 [2] https://arxiv.org/abs/1511.04599","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'well written' and the contribution as 'novel' with 'promising' results. They also express a positive view on the potential practical applications of the work. However, they do point out areas that were not addressed, such as the effect of stable bottom layers on adversarial examples and the evaluation on real limited data problems, which indicates a constructive criticism rather than outright negativity. The language used is polite and constructive, with suggestions for improvement and a willingness to increase the score if certain conditions are met. The use of phrases like 'I feel', 'I wish', and 'I would be happy' contribute to a polite tone. The minor points and typos are also pointed out in a non-confrontational manner.",70,90
"This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results. I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear. The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations? Also only mnist does not have to generalize to other benchmarks. Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for *original with BN update* therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update. There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly. The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization. In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures. None of the empirical results have data augmentation. It is not clear if the initialization or batch normalization update will help or make it worse for that case. Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures. This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.","The sentiment of the review is critical and points out several shortcomings in the paper, such as unconvincing results, lack of clear dataset and empirical setup, unfair comparisons, and insufficient parameter tuning. The reviewer also notes the absence of significant improvements and the lack of comparison to batch normalization. Despite the critical nature, the language used is formal and constructive, without any personal attacks or overly harsh wording. The reviewer provides specific feedback and questions that indicate a desire for improvement in the work rather than outright dismissal.",-60,70
"The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments: -What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations. -Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter. -Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate? -In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.","The sentiment of the review appears to be cautiously critical. The reviewer acknowledges the interest in the results but also points out difficulties in assessing them fairly, implying a mixed sentiment that is neither fully positive nor negative. The language used is formal and constructive, with no use of harsh or disrespectful terms, indicating a polite approach to critique. Specific concerns and questions are raised, which is typical for a peer review, and the tone remains professional throughout.",0,80
"Summary === This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. This work shows that the previous regularizer is equivalent to a direct penalty on the magnitude of feature values weighted differently per example. The analysis goes to to provide two examples where a feature penalty favors a better representation. The first example addresses the XOR problem, constructing a network where a feature penalty encourages a representation where XOR is linearly separable. The second example analyzes a 2 layer linear network, showing improved stability of a 2nd order optimizer when the feature penalty is added. One last bit of analysis shows how this regularizer can be interpreted as a Gaussian prior on both features and weights. Since the prior can be interpreted as having a soft whitening effect, the feature regularizer is like a soft version of Batch Normalization. Experiments show small improvements on a synthetic XOR test set. On the Omniglot dataset feature regularization is better than most baselines, but is worse than Moment Matching Networks. An experiment on ImageNet similar to Hariharan and Girshick 2016 also shows effective low-shot learning. Strengths === * The core proposal is a simple modification of Hariharan and Girshick 2016. * The idea of feature regularization is analyzed from multiple angles both theoretically and empirically. * The connection with Batch Normalization could have broader impact. Weaknesses === * In section 2 the gradient regularizer of Hariharan and Girshick is introduced. While introducing the concept, some concern is expressed about the motivation: *And it is not very clear why small gradients on every sample produces good generalization experimentally.* This seems to be the central issue to me. The paper details some related analysis, it does not offer a clear answer to this problem. * The purpose and generality of section 2.1 is not clear. The analysis provides a specific case (XOR with a non-standard architecture) where feature regularization intuitively helps learn a better representation. However, the intended take-away is not clear. The take-away may be that since a feature penalty helps in this case it should help in other cases. I am hesitant to buy that argument because of the specific architecture used in this section. The result seems to rely on the choice of an x^2 non-linearity, which is not often encountered in recent neural net literature. The point might also be to highlight the difference between a weight penalty and a feature penalty because the two seem to encourage different values of b in this case. However, there is no comparison to a weight penalty on b in section 2.1. * As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy loss. A more general class of losses for which eq. 3 holds is not provided. This should be made clear before eq. 3 is presented. * The Omniglot and ImageNet experiments are performed with Batch Normalization, yet the paper points out that feature regularization may be similar in effect to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are clear improvements over that baseline, the proposed regularizer has a clear additional positive effect. However, results should be provided without Batch Norm so a 1-1 comparison between the two methods can be performed. * The ImageNet experiment should be more like Hariharan and Girshick. In particular, the same split of classes should be used (provided in the appendix) and performance should be measured using n > 1 novel examples per class (using k nearest neighbors). Minor: * A brief comparison to Matching Networks is provided in section 3.2, but the performance of Matching Networks should also be reported in Table 1. * From the approach section: *Intuitively when close to convergence, about half of the data-cases recommend to update a parameter to go left, while the other half recommend to go right.* Could the intuition be clarified? There are many directions in high dimensional space and many ways to divide them into two groups. * Is the SGM penalty of Hariharan and Girshick implemented for this paper or using their code? Either is acceptable, but clarification would be appreciated. * Should the first equal sign in eq. 13 be proportional to, not equal to? * The work is dense in nature, but I think the presentation could be improved. In particular, more detailed derivations could be provided in an appendix and some details could be removed from the main version in order to increase focus on the results (e.g., the derviation in section 2.2.1). Overall Evaluation === This paper provides an interesting set of analyses, but their value is not clear. There is no clear reason why a gradient or feature regularizer should improve low-shot learning performance. Despite that, experiments support that conclusion, the analysis is interesting by itself, and the analysis may help lead to a clearer explanation. The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016. Some points are not completely clear, as mentioned above.","The sentiment of the review appears to be cautiously positive, with the reviewer acknowledging the simplicity and multiple angles of analysis of the core proposal, as well as the potential broader impact of the connection with Batch Normalization. However, the reviewer also points out several weaknesses and areas where the paper lacks clarity or completeness. The overall evaluation suggests that the paper is interesting and somewhat novel, but leaves some questions unanswered. The language used in the review is formal and constructive, with the reviewer providing specific feedback and questions rather than general criticism. The reviewer's tone is critical but polite, aiming to provide constructive feedback rather than to disparage the work.",30,80
"The problem of utilizing all available information (across modalities) about a product to learn a meaningful *joint* embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the *cold start* scenario. I*m unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me: 1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn*t seem to be the case for the Amazon dataset (although I haven*t worked with this dataset myself so perhaps I*m missing something... either way it*s not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach. 2. The discussion of *pairwise residual units* is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don*t find the provided explanation convincing (in what way does the residual approach reduce parameter count?). 3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me. More care could be given to motivating the choices made in the paper. Finally, I*m not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios. Minor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian).","The reviewer's sentiment appears to be cautiously optimistic about the potential of the approach but critical of the execution, suggesting a mixed sentiment. The initial acknowledgment of the problem's interest and the novelty of combining many modalities is positive. However, the subsequent criticism of the architecture's sub-optimality, lack of joint fine-tuning, confusing discussion, and insufficient explanation of choices indicates a negative sentiment towards the paper's content. The language used is formal and constructive, with no personal attacks or overly harsh language, indicating a polite tone.",10,80
"A method for click prediction is presented. Inputs are a categorical variables and output is the click-through-rate. The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not. The embedding vector is passed through a series of SUM/MULT gates and K-most important interactions are identified (K-max pooling). This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate. Authors claim: (1) Use of gates and K-max pooling allow modeling of interactions that lead to state of art results. (2) It is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning. Theoretically convolutions can act as “sum” gates between pairs of input dimensions. Authors make these interactions explicit (i.e. imposed structure) by using gates. Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates. This baseline is critically missing – i.e. Embedding Vector followed by a series of convolution/pooling layers. Another related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not. For instance – what is the total number of parameters in the CCPM model v/s the proposed model? Overall, there is no new idea in the paper. This by itself is not grounds for rejection if the paper outperforms established baselines. However, such comparison is weak and I encourage authors to perform these comparisons.","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the method presented but points out the lack of a critical baseline comparison and questions the novelty of the idea. The reviewer does not dismiss the work outright but encourages further comparison to establish the merit of the proposed method. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a neutral to slightly negative sentiment due to the critical nature of the feedback. The politeness is maintained throughout the review, with the reviewer providing clear and structured feedback without any rude or offensive language, suggesting a high level of politeness.",-20,80
"This paper proposes a new model for sentence classification. Pros: - Some interesting architecture choices in the network. Cons: - No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not. - No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.","The reviewer's sentiment appears to be more on the critical side, pointing out the lack of evaluation on the architecture choices and standard datasets, which suggests that the paper does not meet certain scientific standards. However, the reviewer does acknowledge that there are 'some interesting architecture choices,' which adds a slightly positive aspect to the review. The language used is direct and to the point, without any personal attacks or overly harsh wording, indicating a professional tone.",-30,70
"Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. Authors claim that this: 1. is a very interesting finding. 2. CNN has figured out game rules. 3. Cross modal supervision is applicable to higher-level semantics. I don*t think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only *one* stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. For (3), author*s definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It*s unclear what points authors are trying to make. For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of *what do do* and *what will happen*. They claim by supervising for *what will happen*, the CNN can automatically learn about *what to do*. This is extensively studied in the model predictive control literature. Where model is *what will happen next*, and the model is used to infer a control law - *what to do*. However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. For further analysis of what the CNN has learnt I would recommend: (a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning). (b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of *generalizable* features the CNN pays attention to. In summary, understanding why CNN*s make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the interesting area of research but points out that many claims made by the authors are not supported by experiments. The reviewer's tone is not dismissive but rather suggests improvements and further analysis, which indicates a neutral to slightly negative sentiment as they are not fully endorsing the findings. The language used is polite and academic, with suggestions for improvement rather than direct criticism, which indicates a high level of politeness.",-20,80
"This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being *one shot* and *near optimal* that cannot be supported: it is *N-shot* in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is *near optimal.* Pros: - Nice taxonomy of pruning levels - Comparison to the recent weight-sum pruning method Cons: - Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet) - Paper is somewhat hard to follow - Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements Another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It*s possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction. In summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the paper's contributions such as the taxonomy of pruning levels and the comparison to other methods. However, they also point out several shortcomings, including the lack of evidence for the claims made, incomplete experimental evaluation, and the paper's readability. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone despite the critical feedback.",-20,80
"The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair.","The sentiment in the review appears to be critical of the work, noting that the proposed method has only minor differences from previous work and that the comparisons in the experiments are unfair due to the use of more item features. This critique suggests a negative sentiment as the reviewer does not seem impressed with the novelty or the experimental design of the work. However, the language used is not harsh or insulting, which suggests that the review is not extremely negative. The language used in the review is formal and to the point, without any personal attacks or overly harsh criticism, which indicates a level of politeness.",-50,70
"Description: This paper presents a reinforcement learning architecture where, based on *natural-language* input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks. The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an *analogy-making* regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs). The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not. Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen. The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types. It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization. Evaluation: The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the *right* way to do it. I do not feel the grid world here really represents a *large-scale task*: in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work. Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the reasonableness of the proposed architecture but points out significant limitations, such as the small scale of the grid world and the lack of comparison to state-of-the-art alternatives. The reviewer's tone is not dismissive but rather suggests areas for improvement. Therefore, the sentiment is not entirely negative but rather neutral to slightly negative, as it does not dismiss the work but provides critical feedback. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a high level of politeness.",-20,80
"Update: I thank the authors for their comments! After reading them, I decided to increase the rating. This paper proposes a variant of the convolution operation suitable for a broad class of graph structures. For each node in the graph, a set of neighbours is devised by means of random walk (the neighbours are ordered by the expected number of visits). As a result, the graph is transformed into a feature matrix resembling MATLAB’s/Caffe’s im2col output. The convolution itself becomes a matrix multiplication. Although the proposed convolution variant seems reasonable, I’m not convinced by the empirical evaluation. The MNIST experiment looks especially suspicious. I don’t think that this dataset is appropriate for the demonstration purposes in this case. In order to make their method applicable to the data, the authors remove important structural information (relative locations of pixels) thus artificially increasing the difficulty of the task. At the same time, they are comparing their approach with regular CNNs and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset). I guess, to justify the presence of MNIST (or similar datasets) in the experimental section, the authors should modify their method to incorporate additional graph structure (e.g. relative locations of nodes) in cases when the relation between nodes cannot be fully described by a similarity matrix. I believe, in its current form, the paper is not yet ready for publication but may be later resubmitted to a workshop or another conference after the concern above is addressed.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the authors' efforts by thanking them and noting an increase in rating, which suggests a positive aspect. However, the reviewer also expresses significant concerns about the empirical evaluation and the appropriateness of the dataset used, which indicates skepticism about the paper's readiness for publication. The language used is formal and respectful, with suggestions for improvement rather than outright dismissal, which reflects a polite tone.",-20,80
"* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature. * Review: - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers? - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. * Some Requests: -Can you add the size of the models to the Table 1? - Can you add some of the failure cases of your model, where the model failed to translate correctly? * An Overview of the Review: Pros: - The paper is well written - Extensive analysis of the model on various language pairs - Convincing experimental results. Cons: - The model is complicated. - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs. [1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.","The sentiment of the review appears to be generally positive, as the reviewer describes the paper as 'very well written' and acknowledges the 'very compelling results' and 'extensive analysis'. However, the reviewer also points out the limited novelty and potential drawbacks of the model, which suggests a balanced view rather than unreserved praise. The language used is polite and constructive, with suggestions for improvement and references to prior work, rather than direct criticism or harsh language.",50,90
"The paper presents a simple method for constructing a visual hierarchy of ImageNet classes based on a CNN trained on discriminate between the classes. It investigates two metrics for measuring inter-class similarity: (1) softmax probability outputs, i.e., the class confusion matrix, and (2) L2 distance between fc7 features, along with three methods for constructing the hierarchy given the distance matrix: (1) approximation central point, (2) minimal spanning tree, and (3) multidimensional scaling of Borg&Groenen 2005. There are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks. Regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity. The constructed trees thus can’t be expected to reflect the evolutionary hierarchy, and in fact there are no quantitative experiments that demonstrate that they do. Regarding (2), the technical depth of the exploration is not sufficient for ICLR. I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2). In summary, this paper is unfortunately not ready for publication at this time.","The reviewer's sentiment appears to be negative, as they conclude that the paper is 'not ready for publication at this time.' They acknowledge the paper's attempts but also point out significant shortcomings, such as the lack of quantitative experiments and insufficient technical depth. The language used is polite and professional, with no use of rude or inflammatory language. The reviewer provides constructive criticism without personal attacks, maintaining a respectful tone throughout.",-60,80
"One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346). While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.","The reviewer's sentiment is negative, as they express that the paper is 'misleading' and adds 'noise to the field.' They also criticize the lack of novelty in the approach. However, the language used is not rude or insulting; it is direct and professional, focusing on the content and contribution of the paper rather than personal attributes of the authors.",-70,40
"It is not clear to me at all what this paper is contributing. Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights. Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t. the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA.","The reviewer's sentiment appears negative as they express confusion about the paper's contribution and imply that the work may not be novel ('Deep CCA (Andrew et al, 2013) already gives the gradient derivation...'). The language used is direct and challenges the authors' claims, which can be interpreted as critical but not rude. There is no use of offensive language or personal attacks, which suggests that the reviewer is maintaining a professional tone.",-60,20
"This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. The main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author*s previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments? Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. I suggest a reject.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the interesting idea of reweighing but express clear concerns about the writing, clarity, and lack of convincing evidence for some of the paper's claims. The sentiment is not entirely negative as they do see potential in the work, but the recommendation to reject tilts the sentiment towards the negative side. The language used is direct and points out flaws without being personal or offensive, maintaining a professional tone throughout the review.",-40,70
"This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions: (1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument. (2) The computational cost is not discussed.","The reviewer acknowledges the technical foundation and plausible rationale of the paper, which indicates a recognition of the authors' efforts and a positive aspect of the review. However, the reviewer also expresses skepticism about the practical value of the work and raises specific concerns regarding the assumptions made and the lack of discussion on computational costs. The sentiment is not entirely negative as there is an acknowledgment of the paper's strengths, but it leans towards the critical side due to the questions raised about the paper's practical implications and experimental validation. The language used is formal and constructive, without any personal attacks or harsh wording, which suggests a high level of politeness.",-30,80
"The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments. Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.","The reviewer's sentiment appears to be moderately positive towards the algorithm itself, describing it as 'definitely interesting, quite simple and nice, with many possible applications.' However, the sentiment turns neutral to slightly negative when discussing the superficiality of experiments and the fit with the conference scope, suggesting that the work should be re-submitted elsewhere. The language used is polite and constructive, with no harsh or rude words used, and the critique is delivered in a professional manner.",40,80
"This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply. The overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. “Almost All Learning Machines are Singular”, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further—a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. “Identifying and attacking the saddle point problem in high dimensional non-convex optimization” NIPS 2014 or the work of Amari and others. Experimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out. This paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.","The reviewer's sentiment appears to be cautiously optimistic and constructive. While they acknowledge the importance of the paper's topic and find value in the characterization of behavior for simple systems, they also point out that the results are preliminary and that the paper does not sufficiently situate itself within the existing literature. Additionally, they express concern about the generalizability of the results to larger networks. The language used is polite and professional, offering criticism without harshness and providing clear recommendations for improvement.",20,80
"The paper aims to consolidate some recent literature in simple types of *reading comprehension* tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into *aggregation readers* and *explicit reference readers.* The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance. I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the *explicit reference readers* need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for *dramatic improvements in performance* mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus. I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.","The reviewer's sentiment appears to be mixed with a lean towards the positive due to the appreciation of the paper's efforts in organizing and categorizing reading comprehension models. However, there is also a clear expression of concerns regarding the simplicity of the predicate structure and the relevance of the paper's insights for future model development. The sentiment is not entirely positive because of these concerns, but it is not negative either as the reviewer acknowledges the paper's contributions. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides clear and specific feedback without any harsh language, maintaining a professional tone throughout.",30,80
"SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of *learning end-to-end * an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an *end-to-end trained* system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don*t understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results. Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don*t feel the paper is ready in its current form to be accepted to the conference. Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).","The reviewer's sentiment appears to be critical but constructive. They acknowledge the paper's attempt at novelty and its success over baseline methods but also point out significant shortcomings in novelty and performance compared to the state of the art. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite tone.",-30,70
"Summary: This paper presents a differentiable histogram filter for state estimation/tracking. The proposed histogram filter is a particular Bayesian filter that represents the discretized states using beliefs. The prediction step is parameterized by a locally linear and translation-invariant motion model while the measurement model is represented by a multi-layered neural network. The whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks (1D and 2D). The major claim of this paper is that the problem-specific model structure (Bayesian filter for state estimation) should improve pure deep learning approach in data-efficiency and generalization ability. +This paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks. +An end-to-end histogram filter is derived for state estimation and unsupervised learning is possible in this model. -This paper seems to have a hidden assumption that deep learning (RNN) is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including LSTM baselines. However, this assumption itself may not be true, because Bayesian filter is a first-established approach for this classic problem, so it it more important to justify if deep learning is even necessary for solving the tasks presented. This requests pure Bayesian filter baselines in the experiments. -The derived histogram filter seems to be particularly designed for discretized state space. It is not clear how well it can be generalized to continuous state space using the notation *x*. More interestingly, the observation is discrete (binary) as well, which eventually makes it possible to derive a closed-form measurement update model. This setup might be too constrained. Generalizing to continuous observations is not a trivial task, not even to mention using images as observations like Haarnoja et al 2016. These design choices overall narrow down the scope of applicability.","The review starts with a summary of the paper's content, highlighting both strengths (+) and weaknesses (-) of the work. The positive points include the paper's arguments about the importance of prior knowledge and the development of an end-to-end histogram filter. However, the reviewer also points out significant concerns, such as the assumption that deep learning is a natural choice for recursive state estimation and the potential limitations of the model due to its design for discretized state space. The sentiment is mixed, with appreciation for the paper's contributions but also clear criticism of its assumptions and scope. The language used is formal and constructive, with no personal attacks or overly harsh wording, indicating a polite approach to critique.",0,80
"This paper is refreshing and elegant in its handling of *over-sampling* in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the *creative* regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between! On a secondary note, regarding terminology: Pls avoid using *the KL term* as in section 2.1, there are so many *KL terms* related to VAE-s, it ultimately gets out of control. *Generative error* is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a *KL term* (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2]. [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1509.00519.pdf","The reviewer's sentiment appears to be largely positive, as indicated by the use of words such as 'refreshing', 'elegant', and 'sensible solution'. The reviewer also expresses hope that the paper will be accepted to the conference, which suggests a positive view of the paper's contribution. However, there is a constructive criticism regarding the lack of real-life dataset trials and a suggestion to improve terminology, which slightly moderates the overall positive sentiment. The language used is polite and constructive, with suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'We hope' and 'Pls avoid using', which are courteous and show respect for the authors' work.",70,90
"This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the *additional* attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details. Pros: 1. This paper provides a good comparison of the performances for the selected methods. 2. Section 3.3 (the *additional* attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. 3. Overall, this paper provides interesting and inspiring experimental results about the selected methods. Cons: 1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. 2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. 3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. 4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. Minor comments: Page 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3). [1] https://arxiv.org/abs/1507.00677 [2] https://arxiv.org/abs/1511.03034","The sentiment in the review appears to be mildly positive, as the reviewer acknowledges the interesting observations and inspiring results provided by the paper. However, the reviewer also points out several shortcomings, such as missing methods from the literature, lack of details, and limited contribution due to insufficient explanations and analyses. The language used is polite and constructive, with the reviewer providing a structured critique and specific recommendations without any harsh or disrespectful wording.",30,80
"The topic is very interesting, but the paper is not convincing. Specifically, the experiment part is weak. The study should include datasets that are familiar to the community as well as the ones *that are not often addressed by deep learning*. The comparison to other approaches is not comprehensive.","The sentiment of the review is somewhat negative as the reviewer states that the paper is 'not convincing' and specifically points out weaknesses in the experiment part. However, the language used is not harsh or insulting, indicating that the reviewer is providing constructive criticism rather than simply disparaging the work. Therefore, the sentiment is negative but not extremely so. The language used is formal and to the point, without any personal attacks or overly harsh wording, which suggests a polite approach to delivering the critique.",-50,70
"This paper aims to characterize the perceptual ability of a neural network under different input conditions. This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image. MSE and qualitative results are shown and compared for the different input conditions. Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions. For example, at the end of sec 4.4, *This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....*. But this hypothesis is left dangling: What detailed regions are needed, and from where? For what sort of tasks? Secondly, it isn*t clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss? A prime example is texture, which the autoencoder fails to recover. But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region. So then the network*s inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures. A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn*t be able to generate it in a pixelwise l2 sense. Similarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene: If there is an ambiguity, the network won*t know which to *choose* (white flower or yellow flower?) and output an average, which is why there are so many sepia tones. However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs. There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction. But overall it*s hard to know what to take away from this paper. What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about? Can these be more thoroughly explored with more focus?","The sentiment of the review appears to be critical and constructive, pointing out the lack of focus and concrete conclusions in the paper. The reviewer provides specific examples of where the paper falls short and asks pertinent questions that suggest a need for further exploration and clarification. The language used is polite and academic, with no personal attacks or harsh words, but rather a focus on the content and methodology of the paper. The reviewer's tone is that of a critical academic peer, aiming to improve the work rather than dismiss it.",-20,80
"First I would like to apologize for the delay in reviewing. summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. Here is what I understand are their several experiments to transfer learning, but I am not 100% sure. 1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1) 2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2). 3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3) I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ? Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed. Minor: unexplained acronyms: GRU, BT, CBT. benfits p. 2 subsubset p. 6","The sentiment in the review appears to be constructive and mildly positive, as the reviewer acknowledges the interesting aspects of the work and suggests improvements without harsh criticism. The use of 'Interesting and unpretentious work' indicates a positive sentiment. The reviewer also apologizes for the delay, which shows consideration. The language used is polite and professional, with suggestions framed as questions or recommendations rather than demands. The reviewer uses phrases like 'Could you better explain' and 'The clarity of the presentation could be improved', which are polite ways to suggest changes.",40,80
"The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). Clarity of the paper is a bit low. - Do you use only p-th group*s activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? - In Equation (7), RHS misses the summation over p, and wondering it is a simple typo. - Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs). Other comments are as follows: - Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn*t. I think the fair comparison should be to autoencoders that combines classification for their objective function. - Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn*t look like 1 (e.g., very last column looks like 3). - Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments? - The comparison to the baseline model is missing. I believe the baseline model shouldn*t be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of alpha and eta. Missing reference: Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.","The reviewer's sentiment appears to be constructive but critical, pointing out several areas where the paper could be improved, such as clarity, methodology, and comparison with baseline models. The reviewer does not express satisfaction or dissatisfaction with the paper's content but focuses on providing specific feedback for improvement. The language used is formal and academic, with no personal attacks or overly harsh criticism, indicating a polite tone aimed at professional communication. The reviewer's use of questions and suggestions rather than direct negative statements also contributes to a more polite tone.",-10,80
"The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks. The authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair. Since hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck. Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge","The reviewer's sentiment appears to be critical but constructive. They acknowledge the complexity and potential of the proposed method but express skepticism regarding its practicality in terms of memory and computational cost. The language used is direct and to the point, without any personal attacks or overly harsh wording, indicating a neutral to slightly critical sentiment. The reviewer's tone is professional, focusing on the scientific aspects of the work rather than the authors themselves, which suggests a polite approach to criticism.",-20,70
"The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The paper’s presentation is inefficient and muddled, and the results seem incremental. Presentation: The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL. While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper*s architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper). It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction. Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another. The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof. Results: The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication.","The sentiment of the review is negative, as the reviewer criticizes the paper's presentation, clarity, and the incremental nature of the results. Phrases like 'inefficient and muddled', 'unsatisfying', and 'extravagant' indicate a strong negative sentiment. The reviewer also expresses disappointment that the paper does not fulfill the expectations set by the abstract and introduction. However, the language used is formal and does not contain personal attacks or highly charged words, which suggests that the reviewer is maintaining a level of professionalism. Therefore, the politeness score is not extremely low, as the critique is directed at the work and not the authors personally.",-70,20
"This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant. The idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations. Regarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don’t think this technique will be very useful. Minor comments: -R^{nxn} should be R^{n 	imes n} -In eq. 2: ‘equivaraince’ -In 3.3, argmax is not properly formatted -I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them. - About the claim “This is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects”. The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). - Equivariance in eq. 2 should be called “non-equivariance”. If the value is low, the representation is equivariant, while if it is high it is non-equivariant. - “Eq. 2 also uses the paradigm that”, uses the word paradigm in a strange manner - In the definition of x’_ij, should one of the g_j be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.","The reviewer's sentiment appears to be critical and skeptical of the paper's contributions, emphasizing the lack of novelty and practical utility. The reviewer questions the significance of the results and the practicality of the proposed method, suggesting that the paper does not provide evidence of substantial improvements in performance. The language used is direct and to the point, with a focus on the paper's shortcomings. However, the reviewer maintains a professional tone throughout, providing specific examples and constructive feedback without resorting to personal attacks or overly harsh language.",-60,70
This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller number of learners. This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware. This paper looks good overall and makes some connection between algorithm design and hardware properties. My main concern is that Lemma 1 looks incorrect to me. The factor D_f / S should be D_f/ (S*M) for me. Please clarify this and check the subsequent theorem.,"The reviewer's sentiment appears to be generally positive, acknowledging that the paper 'looks good overall' and makes connections between algorithm design and hardware properties. However, there is a concern raised about the correctness of Lemma 1, which suggests a mix of positive and constructive criticism. The language used is polite and professional, with a request for clarification and verification rather than direct criticism or harsh language. The reviewer's tone is collaborative, aiming to improve the paper rather than dismiss its contributions.",50,80
"The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model*s generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models. To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters 	heta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation). The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text. The second contribution is more interesting. In optimising generative parameters (	heta) and variational parameters (phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters phi as *global* variational parameters, and the predicted mean mu(x) and covariance Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to. Some things aren*t entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify). The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_	heta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data. The final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document. About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question. The analysis of singular values seems like an interesting way to investigate how the model is using its capacity. However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them. As for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads *we create a document comprising a subset of words in the the context’s Wikipedia page.* which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b. In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.","The sentiment of the review appears to be constructive and balanced, with the reviewer acknowledging both positive aspects and areas for improvement in the paper. The reviewer describes the second contribution as 'more interesting' and appreciates the derivation from the familiar SVI procedure, which indicates a positive sentiment towards some of the paper's content. However, the reviewer also expresses confusion about certain parts of the paper and suggests that some aspects 'aren't entirely clear,' indicating a mix of positive and negative sentiments. Overall, the review seems to lean towards a neutral to slightly positive sentiment as the reviewer sees value in the contributions but also identifies areas that need clarification or further development. The language used is formal and respectful, with the reviewer providing constructive feedback without any harsh or rude language, indicating a high level of politeness.",30,90
"The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated. The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow. The paper is written in poor English and is sometimes a bit painful to read. Alternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).","The reviewer's sentiment appears mixed. They find the theoretical framework very interesting, which is positive, but also mention that the paper is difficult to follow and poorly written in English, which is negative. The reviewer's language is direct and includes some negative comments about the writing quality, but it does not contain personal attacks or highly charged language. Therefore, the sentiment is slightly negative, but not extremely so, and the language is somewhat polite but straightforward in its criticism.",-30,40
"The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.","The sentiment of the review is negative, as the reviewer expresses dissatisfaction with the lack of formal theoretical argument, insufficient intuition provided, and unconvincing experimental results. The reviewer also questions the practicality of implementing the proposed method, suggesting that the work is not ready for publication. However, the language used is not rude or offensive; it is direct and professional, which is typical for scientific peer reviews. Therefore, the sentiment is negative but the politeness is neutral to slightly formal.",-70,20
"A method for training neural networks to mimic abstract data structures is presented. The idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak. The paper would be significantly strengthened if the method could be shown to be useful in a realistic application, or be shown to work better than standard RNN approaches on algorithmic learning tasks. The claims about mental representations are not well supported. I would remove the references to mind and brain, as well as the more philosophical points, or write a paper that really emphasizes one of these aspects and supports the claims.","The reviewer's sentiment appears to be constructive but critical. They acknowledge the interesting and promising idea of the paper but point out the lack of empirical support and suggest improvements. The language used is formal and provides clear guidance without any personal attacks or overly harsh criticism, indicating a polite tone.",-20,80
"This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation 	ilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to *exercise* the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances. Minor: - I feel the title is a bit too general for the content of the paper. I personally don*t agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word *structure* is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.","The reviewer's sentiment appears to be mildly positive towards the paper, acknowledging that it is 'well written, interesting' and noting 'strong results', which suggests a positive reception. However, the reviewer also points out the lack of methodological novelty and issues with the title and the contrast made in the paper, which tempers the positivity. The language used is formal and constructive, with no signs of rudeness or personal attacks, indicating a high level of politeness.",30,80
"This paper is a follow-up on the NIPS 2016 paper *Unsupervised learning of spoken language with visual context*, and does exactly what that paper proposes in its future work section: *to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units* using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this. My main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks. Apart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.","The reviewer expresses a positive sentiment towards the direction and quality of the research, appreciating the analysis and stating that the paper is well written. However, there is a concern about the novelty of the work, suggesting that it may not be sufficient for a high conference ranking. The language used is constructive and respectful, with suggestions for improvement rather than harsh criticism. The reviewer's tone is polite, offering specific recommendations for enhancing the paper.",50,80
"This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set** to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error. One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program? Strengths: - Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work - The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis. - The experimental setup seems reasonable - The differentiable set seems like a useful (albeit simple) modelling tool Weaknesses: - The setup is very toy, and it*s not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer - The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it*s not clear if it is a useful general construct or not. Overall: I think it*s an interesting start, and I*m eager to see how this line of work progresses. In my opinion, it*s a bit too early to accept this work to ICLR, but I*d be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","The reviewer's sentiment appears to be cautiously optimistic and constructive. They acknowledge the novelty and potential of the work, but also point out its limitations and areas for improvement. The language used is professional and offers a balanced view, suggesting that while the work is a good start, it is not yet ready for acceptance at a conference. The reviewer expresses eagerness to see future developments, which indicates a positive outlook. Politeness is maintained throughout the review, with criticism being delivered in a constructive manner, and strengths being recognized alongside weaknesses.",30,80
"The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here: Please do not make incredibly unscientific statements like this one: *The working procedure of this model is just like how we human beings read a text and then answer a related question. * Really, *humans beings* have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts. Generally, your experiments are about simple classification and the methods you*re competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like *Learning to Understand* in the title. Lastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them. Other reviewers included further missing related work and fitting this paper into the context of current literature. Given that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.","The reviewer expresses clear dissatisfaction with the authors' lack of response to pre-review comments and criticizes the unscientific statements made in the manuscript. The use of phrases like 'Please do not make incredibly unscientific statements' and 'Can you cite an actual neuroscience paper for such a claim?' indicate a negative sentiment. The reviewer also suggests changes to the title, abstract, and introduction, implying that the current version is not up to the expected standards. The tone is direct and somewhat confrontational, especially with the use of 'The answer is no, so please delete such statements from future drafts.' This indicates a lack of politeness, as the reviewer does not use softening language or constructive phrasing.",-80,-60
"This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training. Comments 1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually. 2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited. 3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks. In summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.","The sentiment of the review appears to be mildly critical, pointing out the limited novelty of the paper and suggesting that the experiments could be improved. The reviewer acknowledges the effort but also clearly states the shortcomings. Therefore, the sentiment is not entirely negative but rather constructively critical. The language used is formal and professional, with no personal attacks or overly harsh wording, indicating a polite tone in the context of academic discourse.",-30,70
"The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations. The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.","The sentiment in the review appears to be mildly positive towards the clarity and the data generation procedure of the paper, but it also expresses a clear concern about the motivation and originality of the work. The reviewer appreciates the clarity and the interesting aspects of the data generation but criticizes the lack of motivation and the absence of certain architectural considerations. The language used is formal and constructive, without any personal attacks or overly harsh criticism, which indicates a polite tone.",30,80
"Strengths - interesting to explore the connection between ReLU DNN and simplified SFNN - small task (MNIST) is used to demonstrate the usefulness of the proposed training methods experimentally - the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor) Weaknesses -no results are reported on real tasks with large training set -not clear exploration on the scalability of the learning methods when training data becomes larger -when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”. -would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the interesting aspects of the research and the simplicity of implementation for the proposed methods. However, the reviewer also points out significant weaknesses, such as the lack of results on real tasks with large training sets and unclear exploration of scalability, which suggests a balanced view with a lean towards constructive criticism. The language used is polite and academic, with no use of derogatory or harsh terms, and the critique is presented in a professional manner.",20,80
"The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer. The approach is thus to be added to a growing list of heuristic layer-wise initialization schemes. The particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary. The paper lacks clarity in the description of the approach: MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?). The question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?). More importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. *All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. The Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn’t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages. I would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart.","The reviewer's sentiment appears to be critical and points out several shortcomings in the paper, such as the lack of motivation for the chosen initialization strategy, unclear explanations, and potential methodological problems. However, the critique is constructive and aims to guide the authors towards improvement. The language used is formal and academic, with no personal attacks or overly harsh wording, indicating a polite tone despite the critical content.",-50,70
"Because the authors did not respond to reviewer feedback, I am maintaining my original review score. ----- This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing. Strengths: - The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models. - The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable. - This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders. Weaknesses: - The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional *baselines* described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach. - The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?). - It*s unclear what is the purpose of introducing the inequality in Eq. 9. - Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren*t particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU). - The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question). This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","The reviewer's sentiment appears to be mixed, with recognition of the paper's strengths and a worthwhile research direction, but also with significant criticism regarding the presentation, experimental results, and comparison with related work. The reviewer maintains their original review score due to the authors not responding to feedback, which suggests a neutral to slightly negative sentiment. The language used is polite and constructive, with suggestions for improvement and a clear explanation of the paper's weaknesses. The reviewer uses academic language appropriate for a peer review without any personal attacks or rude remarks.",-10,80
"The paper proposes a semantic embedding based approach to multilabel classification. Conversely to previous proposals, SEM considers the underlying parameters determining the observed labels are low-rank rather than that the observed label matrix is itself low-rank. However, It is not clear to what extent the difference between the two assumptions is significant SEM models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. As such, it is a neural network. The proposed training algorithm is slightly more complicated than vanilla backprop. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty.","The reviewer's sentiment appears to be neutral to slightly negative, as they acknowledge the paper is well written and the main idea is clearly presented, but they also express concerns about the significance of the results and the lack of conceptual novelty. The language used is formal and constructive without any harsh or overly critical words, suggesting a polite tone.",-10,80
"The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. However, there are several concerns. 1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g., - “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 - “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015, - “Retrieving Similar Styles to Parse Clothing, TPAMI 2014, etc It is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art. 2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work? 3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text? While the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.","The sentiment in the review is constructive but critical. The reviewer acknowledges the interesting exploration and the nice job discussing challenges, which is positive. However, the concerns listed about comparisons, dataset details, and missing baselines indicate significant areas for improvement, which adds a critical tone. The balance between the positive comments and the critical feedback suggests a slightly negative sentiment, but not overly so, as the criticism is aimed at improving the work. The language used is formal and professional, with no use of rude or aggressive language, indicating a high level of politeness.",-20,90
"The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don*t find the proposed system different from the framework of artificial neural network. Although for today*s neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.",The sentiment in the review appears to be critical but constructive. The reviewer acknowledges the proposed system's efficiency in computing but also points out that it does not differ significantly from existing neural network frameworks. The language used is not overtly negative but expresses a clear skepticism about the novelty of the proposed system. The politeness of the language is neutral to formal; the reviewer uses academic language and provides a critique without using harsh or disrespectful words. The use of an asterisk in 'don*t' seems like a typographical error rather than an intentional deviation from formal language.,-30,70
"This paper proposes to use a causality score to weight a sparsity regularizer. In that way, selected variables trade off between being causal and discriminative. The framework is primarily evaluated on a proprietary health dataset. While the dataset does give a good motivation to the problem setting, the paper falls a bit short for ICLR due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of Chalupka et al., arXiv Preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation. At the core of this paper, the approach is effectively to weight a sparsity regularizer so that *causal* variables (as determined by a separate objective) are more likely to be selected. This is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent. A theorem on identifiability of causal+discriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.","The reviewer's sentiment appears to be critical but constructive. They acknowledge the good idea behind the paper but point out several shortcomings, such as the lack of controlled experiments, straightforward methodology, and insufficient theoretical motivation. The language used is formal and provides clear feedback without any personal attacks or overly harsh criticism, indicating a neutral to slightly negative sentiment and a polite tone.",-30,70
"The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training. The experimental section is equally confusing and unconvincing. Other comments: -- The figures still say LeCun-5 -- The legends of the plots are not in english. Hence I*m not sure what is going on there. -- The paper is riddled with typos and hard to understand phrasing.","The reviewer's sentiment is quite negative, as indicated by the use of phrases such as 'extremely hard to understand,' 'not at all clear,' 'equally unclear,' 'confusing,' and 'unconvincing.' The reviewer also points out that the paper has 'typos and hard to understand phrasing,' which adds to the negative sentiment. However, the language used is not rude or insulting; it is direct and professional, focusing on the content and clarity of the paper rather than personal attacks on the authors. Therefore, the politeness score remains neutral to slightly positive, reflecting the professional tone despite the critical content.",-80,20
"This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. The idea of the paper is interesting there are some encouraging results, but the current version doesn*t seem ready for publication: Performance: The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It*s not clear how this method compares against them. Missing explanation: Experimental setting for k-shot learning should be more detailed. Measure: Accuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. Many grammatical errors and inappropriate formatting of citations, such as: M. et al. (2011) ImageNet (Alex et al. (2012)) Judy et al. (2013): this reference appears three times in the reference section.","The sentiment of the review appears to be constructive but critical. The reviewer acknowledges the interesting idea and encouraging results, which suggests a somewhat positive sentiment. However, the reviewer also points out that the paper is not ready for publication due to several issues, including the need for comparison with other methods, lack of detailed experimental settings, and concerns about the measure used. Additionally, the reviewer notes grammatical errors and inappropriate formatting, which indicates significant room for improvement. The language used is formal and appropriate for a scientific peer review, with no personal attacks or overly harsh language, indicating a high level of politeness.",-20,80
"This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error. It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the “integrated prediction” obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted. It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly. More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).","The sentiment of the review appears to be constructive but critical, as the reviewer acknowledges the preliminary results but also points out several areas that require clarification or additional work. The reviewer does not dismiss the work but encourages further investigation and refinement, which indicates a neutral to slightly negative sentiment. The language used is polite and professional, with no use of rude or confrontational language, and the reviewer poses questions and suggestions rather than direct negative criticism.",-20,80
"This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution. This is an area of research that indeed warrants more experimental investigation. One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual. Critique: - As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization. While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification. - Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work. - No new ideas are presented, and the analysis in the paper is quite limited. As it stands, this would be more appropriate for a workshop.","The sentiment of the review appears to be mildly positive towards the research area but critical of the paper's approach and lack of innovation. The reviewer acknowledges the importance of the research area and finds one of the findings interesting, which suggests a positive sentiment towards the potential of the work. However, the critique points out significant shortcomings in the paper, such as the lack of adaptive dropout and limited analysis, which indicates a negative sentiment towards the execution of the research. Overall, the sentiment seems to balance between recognition of the field's importance and criticism of the paper's contribution, leaning slightly towards the negative due to the specific critiques listed. The language used in the review is formal and constructive, without any personal attacks or overly harsh wording, indicating a high level of politeness.",-30,80
"The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn’t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed. What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?). On the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE*s test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant.","The reviewer's sentiment appears to be critical and somewhat negative, as they describe the paper's contribution as 'incremental' and express doubts about the validity and completeness of the proposed formal motivation. They also point out weaknesses in the empirical support for the approach. However, the language used is formal and academic, without any personal attacks or overly harsh wording, indicating a level of professionalism and restraint in the critique. Therefore, the sentiment is not extremely negative, but it is certainly not positive. The politeness is maintained throughout the review.",-60,70
"My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don*t know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. My own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can*t have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can*t learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. I would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.","The sentiment of the review is critical and expresses significant skepticism regarding the underlying hypothesis of the work. The reviewer challenges the assumption that gradients are sufficient to solve long-term dependency problems and points out potential issues with the approach. The tone is not overtly negative but is questioning and demanding of more evidence, which suggests a sentiment that is more negative than neutral. However, it is not extremely negative as the reviewer is engaging with the content constructively. The language used is polite and academic, with no personal attacks or unprofessional wording. The reviewer uses technical language appropriate for a scientific peer review and maintains a respectful tone while expressing their concerns.",-40,80
"This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets. The solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. - It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them. - If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (http://rtw.ml.cmu.edu/papers/lao-emnlp11.pdf) which could learn rules from KB graph directly. - As there is only one KNN baseline, the experimental results seem pretty weak. At the least, it is necessary to show the original precision / recall of RESCAL, together with the proposed rule mining approach (with different max length), so we know how much the current information the current rule miner could recover. In addition, the four datasets are all very small. Would it be able to scale it to WordNet or Freebase? [Minor comments] “Relational embedding” and “relation embedding” are used mixedly throughout the paper. I am not sure if they are well-defined terms (it is better to cite relevant paper).","The sentiment of the review appears to be critical but constructive. The reviewer acknowledges the ability of the paper to extract rules but questions the necessity and clarity of the approach, suggesting comparisons and additional baselines for stronger validation. The language used is formal and direct, with no personal attacks or overly harsh wording, indicating a professional tone. The reviewer's use of phrases like 'sounds plausible' and 'reasonable rules' suggests a neutral to slightly negative sentiment as they are not fully convinced of the paper's approach. The politeness is maintained throughout the review, with the reviewer providing clear and specific feedback without resorting to disrespectful or aggressive language.",-30,70
"The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term phi^t_{i,j} is never defined, only phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context). The paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR) The study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions - “the overall sample importance is different under different epochs” => yes the norm of the gradient is expected to vary - “Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops” => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables The question of Figure 4 is absurd “Is Sample Importance the same as Negative log-likelihood of a sample?”. Of course not. The results are very bad on CIFAR which discredits the applicability of those results. On Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20% Despite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.","The sentiment of the review appears to be critical but constructive. The reviewer points out several issues with the paper, such as unclear definitions, poor performance in experiments, and trivial conclusions. However, the reviewer also acknowledges that the paper raises some interesting points and compliments the presentation of the experiments. Therefore, the sentiment is not entirely negative but rather mixed with a leaning towards the critical side. The language used is direct and to the point, without any personal attacks or overly harsh wording, which suggests a neutral to slightly polite tone.",-30,40
"This paper has no machine learning algorithmic contribution: it just uses the the same combination of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM. There are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art. So I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.","The reviewer's sentiment appears to be negative, as they clearly state that the paper lacks algorithmic contribution and does not offer improvement over the state-of-the-art, which suggests that the paper does not meet the conference's standards. However, the reviewer does acknowledge that the part describing handwriting tasks is well written, which adds a slightly positive note. The language used is direct and to the point, without any personal attacks or overly harsh wording, which indicates a level of professionalism and restraint in the critique.",-70,40
"I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection.","The reviewer expresses skepticism and doubt about the claims made in the paper, indicating a negative sentiment towards the presented results and methodology. However, the language used is not aggressive or disrespectful, maintaining a professional tone throughout the review. The reviewer's sentiment is not extremely negative as they acknowledge the topic's interest but leans towards rejection due to the perceived shortcomings in the experimental setup and claims. The politeness is evident in the structured critique and absence of harsh language, suggesting a constructive albeit critical approach.",-40,70
"This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets. Strength: 1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. 2, The paper is well organized. Weakness: 1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. 2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings. Detailed Questions: 1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake. 2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is.","The sentiment of the review appears to be cautiously positive, as the reviewer acknowledges the interesting idea and well-organized nature of the paper but also points out significant weaknesses and areas for improvement. The language used is constructive and focuses on how the paper could be enhanced, rather than being dismissive or overly critical. The politeness of the language is high, as the reviewer uses respectful language and provides clear, specific feedback without any harsh or rude wording.",50,80
"The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking. In order to get the *interpretability*, the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well. The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect. Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.","The reviewer's sentiment appears to be cautiously optimistic about the research direction but critical of the current work's execution. They encourage further pursuit of the topic but express significant doubts about the methods and presentation in the current form. The language used is constructive and respectful, with suggestions for improvement rather than outright dismissal. Politeness is maintained throughout the critique, with no use of harsh or disrespectful language.",10,80
"Strengths ?-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. ?-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules. ?-- x50 less memory usage than AlexNet, keeping similar accuracy ?-- strong experimental results Weaknesses ?--Would be nice to test Sqeezenet on multiple tasks ?--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?","The sentiment of the review appears to be mostly positive, as the reviewer acknowledges the strengths of the paper, such as the interesting proposal, balanced exploration, memory efficiency, and strong experimental results. However, the reviewer also points out areas for improvement, suggesting further testing and a deeper analysis, which indicates a constructive critique rather than outright negativity. The language used is polite and academic, with suggestions framed as opportunities for enhancement rather than as failures, and no harsh or rude language is present.",70,90
"The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided. Summary: ——— I think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is great. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Detailed comments: ——— 1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., alpha-expansion and belief propagation are not `scene-understanding algorithms’ but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model’ should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)’ and I’m wondering how the `approximate MAP state* can be optimal. Etc. 2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible. 3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.","The reviewer's sentiment appears to be cautiously optimistic but critical. They acknowledge the potential of the technique but express clear concerns about the experimental results and writing clarity. The sentiment is not overtly negative but suggests significant room for improvement, hence a slightly negative score. The language used is formal and constructive, without any harsh or rude wording, indicating a high level of politeness.",-20,80
"Summary: In this paper, the authors explore the advantages/disadvantages of using a sin activation function. They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions. They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well. They then try different algorithmic tasks, where the periodicity of the functions is helpful. Pros: The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational. Cons: Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.","The reviewer provides a balanced view, acknowledging the interesting aspects of the paper as well as pointing out its preliminary nature and the need for more evidence. The language used is constructive and focuses on the content rather than the authors, which suggests a neutral to slightly positive sentiment. The reviewer's language is formal and respectful, indicating a high level of politeness.",30,90
"This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.","The reviewer's sentiment appears to be mildly positive as they acknowledge that the paper 'shows good results' and 'provides good insights,' which suggests that the reviewer sees value in the work. However, the mention of the comparison not being comprehensive indicates a slight criticism, but it is not harsh or dismissive. Therefore, the sentiment is not entirely positive but leans towards the positive side. The language used is formal and constructive without any negative or disrespectful connotations, indicating a high level of politeness.",50,90
"The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective. Training involves sampling and then learning a transition-based inference to *walk back* samples to the data. Because of its focus on transitions, it can be used to learn a raw transition operator rather than purely learning an energy-based model. The objective is intuitively appealing because of its similarity to previous successful but less principled training methods for MRFs like Contrastive Divergence. The idea for the algorithm is appealing, and it looks like it could find a nice place in the literature. However, the submission in its current form is not yet ready for publication. Experiments are qualitative and the generated samples are not obviously indicative of a high model quality. As pointed out elsewhere, the mathematical analysis does not currently demonstrate tightness of the variational bound in the case of a learned transition operator. More evaluation using e.g. annealed importance sampling to estimate held-out likelihoods is necessary. Assuming that the analysis can be repaired, the ability to directly parametrize a transition operator, an interesting strength of this method, should be explored in further experiments and contrasted with the more standard energy-based modeling. This looks like a promising idea, and other reviews and questions have already raised some important technical points which should help strengthen this paper for future submission.","The reviewer's sentiment appears to be cautiously optimistic about the potential of the method presented, acknowledging its appeal and possible place in the literature. However, they also point out significant shortcomings in the current submission, such as the qualitative nature of experiments and the lack of mathematical analysis. The language used is constructive and professional, offering specific recommendations for improvement without any harsh or disrespectful wording. The overall tone is critical but supportive, aiming to guide the authors towards strengthening their paper.",30,80
The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.,"The sentiment of the review starts with a positive note by acknowledging the interest in the findings but quickly shifts to a critical tone, pointing out the incompleteness of the paper due to lack of proper experimental justification. The reviewer's language is constructive and focuses on what is missing in the paper rather than being dismissive or overly critical. Therefore, the sentiment is slightly negative as it expresses a need for improvement but recognizes potential. The language used is polite and professional, offering specific recommendations without any harsh wording or personal attacks.",-30,80
"Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model*s architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I*m going to bump up my score from a clear rejection to a borderline. ----- This paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher *feature fusion* (i.e., dense) layer to combine the LSTM*s and ConvNet*s outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN). Strengths: - A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning. - Accepting the author*s assumptions as true for the moment, the proposed architecture seems intuitive and well-designed. Weaknesses: - Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from *on high* (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends. - The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary. - Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments. - The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive. - Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak. One thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently. Regarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above *separate trends from noise* problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013. I appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML).","The reviewer's sentiment appears to be cautiously optimistic, acknowledging the authors' efforts in improving their manuscript by incorporating feedback and conducting additional experiments. However, the reviewer maintains concerns regarding the independent tuning of model architectures and the need for additional baselines. The sentiment is not entirely positive due to these reservations, but it is not negative either, as the reviewer has upgraded their score from a clear rejection to a borderline. The language used is formal and constructive, with no signs of rudeness or disrespect. The reviewer provides detailed feedback and justifies their concerns, which is indicative of a professional and polite tone.",25,75
"The paper proposes two methods for what is called wild variational inference. The goal is to obtain samples from the variational approximate distribution q without requiring to evaluate the density q(z) by which it becomes possible to consider more flexible family of distributions. The authors apply the proposed method to the problem of optimizing the hyperparamter of the SGLD sampler. The experiments are performed on a 1-d mixture of gaussian distribution and Bayesian logistic regression tasks. The key contribution seems to connect the previous findings in SVGD and KSD to the concept of inference networks, and to use them for hyperparameter optimization of SGLD. This can not only be considered as a rather simple connection/extension, but also the toyish experiments are not enough to convince readers on the significance of the proposed model. Particularly, I*m wondering how the particle based methods can deal with the multimodality (not the simple 1d gaussian mixture case) in general. Also, the method seems still to require to evaluate the true gradient of the target distribution (e.g., the posterior distribution) for each z ~ q. This seems to be a computational problem for large dataset settings. In the experiments, the authors compare the methods for the same number of update steps. But, considering the light computation of SGLD per update, I think SGLD can make much more updates per unit time than the proposed methods, particularly for large datasets. The Bayesian logistic regression on 54 dimensions seems also a quite simple experiment, considering that its posterior is close to a Gaussian distribution. Also, including Hamiltonian Monte Carlo (HMC) with automatic hyperparameter tuning mechanism (like, no u-turn sampler) would be interesting. The paper is written very unclearly. Especially, it is not clear what is the exact contributions of the paper compared to the other previous works including the authors* works. The main message is quite simple but most of the pages are spent to explain previous works. Overall, I*d like to suggest to have more significant high-dimension, large scale experiments, and to improve the writing.","The reviewer's sentiment appears to be critical and suggests significant room for improvement in the paper. They question the novelty and significance of the work, the choice of experiments, and the clarity of writing. However, the language used is formal and maintains the decorum expected in a peer review, without any personal attacks or overly harsh language. Therefore, the sentiment is negative due to the critical nature of the feedback, but the language is polite.",-60,80
"The papers investigates the task of unsupervised learning with deep features via k-means clustering. The entire pipeline can be decomposed into two steps: (1) unsupervised feature learning based on GAN framework and (2) k-means clustering using learned deep network features. Following the GAN framework and its extension InfoGAN, the first step is to train a pair of discriminator network and generator network from scratch using min-max objective. Then, it applies k-means clustering on the top layer features from discriminator network. For evaluation, the proposed unsupervised feature learning approach is compared against traditional hand-crafted features such as HOG and supervised method on three benchmark datasets. Normalized Mutual Information (NMI) and Adjusted RAND Index (ARI) have been used as the evaluation metrics for experimental comparison. Although the proposed method may be potentially useful in practice (if refined further), I find the method lacks novelty, and the experimental results are not significant enough.","The sentiment of the review appears to be slightly negative, as the reviewer acknowledges the potential usefulness of the method but criticizes the lack of novelty and the significance of the experimental results. The language used is formal and constructive, without any harsh or rude words, indicating a polite tone.",-30,70
"This paper is about learning unsupervised state representations using multi-task reinforcement learning. The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound. Positives: + Gating to enable learning a joint representation + Multi-task learning extended from a single task in prior work + Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation) Negatives: - Parameters choice is arbitrary (w parameters) - Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks - The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare. I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.","The reviewer's sentiment appears to be generally positive, as they describe the paper as 'clearly written' and 'theoretically sound,' and they acknowledge the 'promising results' of the research. They also list several positives about the paper, indicating an overall favorable view. However, they do mention some negatives, which suggests that while the sentiment is positive, it is not without reservations. The language used is polite and constructive, with the reviewer providing specific recommendations for improvement rather than harsh criticism. The use of phrases like 'I would recommend' and the absence of any aggressive or disrespectful language contribute to a high politeness score.",70,90
"In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching. The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective. In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x). My main problem with this paper is that it is unclear why any of this is useful. The connection to density estimation is interesting, but any derived conclusions between the two seem questionable. For example, in previous density estimation literature, the Pearson divergence is more stable. The authors claim that the same holds for GANS and try to show this in their experiments. Unfortunately, the experiments section is very confusing with unilluminating figures. Looking at the graph of density ratios is not particularly illuminating. They claim that for the Pearson divergence and modified KL-divergence, *the learning did not stop* by looking at the graph of density ratios. This is completely hand-wavey and no further evidence is given to back this claim. Also, why was the normal GAN objective not tried in light of this analysis? Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work. I think this paper would be much improved if it was rewritten in a clear fashion. As it stands, it is difficult to understand the motivation or intuition behind this work.","The reviewer's sentiment appears to be quite critical of the paper, expressing clear dissatisfaction with the clarity, usefulness, and experimental evidence of the work. The reviewer questions the motivation and usefulness of the paper, describes the experiments section as 'confusing' and 'unilluminating', and calls the evidence 'hand-wavey'. However, the language used is not rude or insulting; it is direct and professional, focusing on the content and methodology of the paper rather than personal attacks on the authors. The reviewer also provides constructive feedback suggesting that the paper should be rewritten for clarity.",-60,70
"This paper argues that being able to handle recursion is very important for neural programming architectures — that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data. Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces — this paper trains NPI models on traces that have recursive calls. The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems. What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see. I’m actually not sure what the right take-away is — does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?). For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest. In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic — and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols. What is a problem where we have access to execution traces but cannot infer it using the proposed method?","The reviewer's sentiment appears to be generally positive, as they describe the paper's idea as 'super simple' and acknowledge the impressive results with the NPI architecture inferring Bubblesort and the Tower of Hanoi problems. However, there is a hint of skepticism or questioning towards the end, pondering the broader implications and limitations of the research, which prevents the sentiment from being entirely positive. The language used is polite and academic, with no signs of rudeness or disrespect. The reviewer uses phrases like 'What I like' and frames their questions in a constructive manner, suggesting a respectful tone.",70,90
"Two things I*d like to see. 1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured. One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons. Without knowing this, it*s hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example. 2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will. A better comparison would be with WebP performance. Or, even better, both. Very nice results. Is a software implementation of this available to play with?","The reviewer's sentiment appears to be constructive and mildly positive, as they acknowledge 'Very nice results' and express interest in a software implementation. However, they also point out what they perceive as weaknesses and areas for improvement, which indicates a balanced view rather than outright enthusiasm. The language used is polite and professional, with no negative or rude expressions. The reviewer makes suggestions for improvement in a respectful manner, using phrases like 'I'd like to see' and 'A better comparison would be,' which are indicative of a polite tone.",50,80
"This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. Several tricks re-used from (Andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated. The experiments are convincing. This is a strong paper. My only concerns/questions are the following: 1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs: - Samy Bengio PhD thesis (1989) is all about this ;-) - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994) - I am convince Schmidhuber has done something, make sure you find it and update related work section. Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.","The reviewer expresses a generally positive sentiment towards the paper, noting that it is 'globally well written' and 'the presentation of the main material is clear.' The reviewer also describes the paper as 'strong' and the experiments as 'convincing,' which further indicates a positive sentiment. However, the reviewer does have some concerns and questions, which suggests that the sentiment is not entirely positive. Therefore, the sentiment score is positive but not at the maximum. The language used is polite and constructive, with phrases like 'My only concerns/questions are the following' and 'It would be great if,' which are courteous ways to introduce criticism. The reviewer also uses positive reinforcement, such as 'I like the paper' and 'I believe the discussed material is relevant,' which adds to the politeness of the review.",80,90
"Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of high significance. Two important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper: 1) The performance of end2end ML approaches is still insufficient for goal oriented dialogs. 2) When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions. While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops. While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than for revision. First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact. Would the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one. For the concierge data, what would happen if ‘correct’ meant being the best, not among the 5-best? While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code ‘Indian’ as ‘type of cuisine’, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.","The sentiment of the review appears to be quite positive. The reviewer acknowledges the significance of the work, the strong effort made by the authors, and the excellent clarity of the paper. They also express being impressed by certain aspects of the paper, such as the appendix and experimental protocol. However, the reviewer also points out limitations and areas for future research, which does not detract from the overall positive sentiment. The language used is polite and constructive, with the reviewer providing detailed feedback and explanations without any harsh or disrespectful wording.",70,90
"This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed. Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison?","The reviewer expresses a positive sentiment towards the paper, acknowledging the valuable contribution and expressing interest in the perturbation trick introduced. The language used is constructive and encouraging, suggesting further discussion and empirical study without any negative or harsh criticism. The reviewer's tone is respectful and shows appreciation for the authors' work, which indicates a high level of politeness.",80,90
"This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea. The paper is well written and easy to follow by any reader with deep RL expertise. Can authors comment about the computational resources needed to train the UNREAL agent? The overall architecture is quite complicated. Are the authors willing to release the source code for their model? -------------------------------------------------------- After rebuttal: No change in the review.","The reviewer's sentiment appears positive as they acknowledge the paper's contribution to the conference and describe the UNREAL agent's performance as 'significantly better' than A3C. They also compliment the paper as 'well written and easy to follow.' However, they do not express surprise at the results, implying an expectation that the results are somewhat obvious, which slightly tempers the positivity. The language used is polite and professional, with no negative or rude words. The reviewer asks questions directly and respectfully, indicating a willingness to engage further with the authors.",70,90
"Thank you for an interesting read. Pros - This paper tackles a very crucial problem of understanding communications between 2 agents. As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question. Is the problem solving approach of machines similar to that of humans. - The task is simple enough to make the post learning analysis intuitive. - It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols. Cons - The task effectively boils down to image classification, if the 2 images sent are from different categories. The symbols used are effectively the image class which the second agent learns to assign to either of the images. By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.","The review starts with a 'Thank you' which is polite and sets a positive tone. The reviewer acknowledges the importance of the paper's topic and the simplicity of the task for post-learning analysis, which is positive feedback. However, the reviewer also points out what they perceive as a limitation or a simpler alternative to the approach used in the paper, which could be seen as a constructive criticism rather than outright negative. Overall, the language is formal and constructive without any harsh or rude words.",30,80
"This paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. Extensive experiments support the main argument of the paper. Reflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking. While this work doesn’t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models, it does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood. In my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field. Detailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?","The reviewer's sentiment appears to be quite positive, as they describe the paper's empirical observation as 'very interesting' and note that the extensive experiments support the main argument. They also appreciate the thought-provoking nature of the discussions on expressivity and regularization. The reviewer acknowledges that the paper raises good questions and points to paradoxes, which they believe can lead to fundamental progress, thus valuing the contribution highly. The language used is formal, respectful, and constructive, indicating a high level of politeness. There is no use of negative or confrontational language, and the reviewer even poses a detailed question in a courteous manner.",80,90
"This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve. The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage. This is a well written paper on an interesting topic with strong results. I recommend it be accepted.","The reviewer's sentiment is positive, as indicated by the use of phrases like 'important part of our field', 'well written paper', 'interesting topic', 'strong results', and 'I recommend it be accepted'. There is no negative language or criticism that isn't constructive, suggesting a high sentiment score. The language used is formal, constructive, and encouraging, which indicates a high level of politeness. Phrases such as 'While one would always want to see' and 'This is a well written paper' demonstrate respect and courtesy towards the authors' work.",85,90
"**Edit: Based on the discussion below, my main problem (#2) was not correct. I have changed my overall rating from a 3 to a 7** This paper makes a fascinating observation: one can introduce an action-dependent baseline (control variate) into REINFORCE, which introduces bias, and then include a correction term to remove the bias. The variance of the correction term is low relative to the REINFORCE update and the action-dependent baseline, and so this results in benefits. However, the paper is poorly executed. Below I list my concerns. 1. The paper tries to distinguish between *policy gradient* methods and *actor critic* methods by defining them in a non-standard way. Specifically, when this paper says *policy gradient* it means REINFORCE. Historically, the two have meant different things: some policy gradient algorithms are actor-critics (e.g., Degris et al*s INAC algorithm) while others are not (e.g. REINFORCE). 2. The proposed Q-Prop algorithm includes many interesting design choices that make in unclear what the real source of improved performance is. Is the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)? Or, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer? Comparisons are not performed between variants of Q-Prop that show the importances of these different components. Rather the authors opt to show better performance on a benchmark task. I find this to be non-scientific, and more of a paper showing a feat of engineering (by combining many different ideas) than it is a research paper that studies the details of which parts of Q-Prop make it work well. For example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer. At the very least I would have expected comparisons to Q-Prop using a state-value baseline (which would then be a variant of REINFORCE using off-policy data and a replay buffer, and which would show whether the action-dependent baseline is important). 3. There is a fair amount of discussion about unbiased policy gradient algorithms, which is not accurate. Most policy gradient algorithms are biased, and making them unbiased tends to hurt performance. This is discussed in the paper *Bias in Natural Actor-Critic Algorithms*, which applies to non-natural algorithms as well. Also, I suspect that the use of GAE(lambda) results in the exact sort of bias discussed in that paper, even when lambda=1. As a result, Q-Prop may act more like an average reward method than expected. This should be discussed. 4. The proposed algorithm can be applied to deep architectures, just as most linear-time policy gradient algorithms can. However, it does not have to be applied to deep architectures. The emphasis on *deep* therefore seems to detract from the core ideas of the paper. 5. The paper repeatedly says that importance sampling based methods result in high variance. This ignores weighted importance sampling methods that have very low variance. A good example of this is Mahmood et al*s WIS-LSTD algorithm. WIS-LSTD has high computational complexity, so it would only be compared to on non-deep RL problems, of which there are plenty. Alternatively, algorithms like Retrace(lambda) have quite low variance since the likelihood ratios are never bigger than one. Others might argue that ETD algorithms are currently the most effective. The simple dismissal of these algorithms because the original importance sampling estimator proposed in 2000 has high variance is not sufficient. 6. The paper does not compare to natural actor-critic algorithms. Once the weights, w, have been computed, REINFORCE uses samples of states from the normalized discounted state distribution and samples of the corresponding returns to estimate the policy gradient. One of the main reasons Q-Prop should work better than REINFORCE is that it includes a control variate that reduces the variance of the policy gradient update after w has been computed. Now, compare this to natural policy gradient algorithms. Once the weights, w, have been computed (admittedly, using compatible features for the advantage estimation but any features for the state-value estimation) the resulting update is = w. That is, is has zero variance and does not require additional sampling. It is as though a perfect control variate was used. Furthermore, natural gradient algorithms can be applied to deep architectures. Degris et al*s INAC algorithm is linear time. Desjardin et al*s *natural neural networks* paper also discusses efficient implementations of natural gradients for neural networks. Dabney*s Natural Temporal Difference algorithms have linear time variants that fit this paper*s description of actor-critic algorithms. To summarize, given the weights w, REINFORCE has high variance, and Q-Prop claims to reduce the variance of REINFORCE. However, natural policy gradient methods have zero variance given the weights w. So, what is the benefit of Q-Prop over natural gradient algorithms using off-policy value function estimation methods to estimate Q (or A)? That is, why should we expect Q-Prop to perform better than NAC-LSTD using GAE(lambda) with experience replay in place of LSTD? 7. Equation (2) is false. The right side is proportional to the left side, not equal to it. There is a (1-gamma) term missing. There are also other typos throughout (e.g., Q and A sometimes are missing their action arguments). Although I have listed my concerns, I would like to re-iterate that I do find the idea of an action-dependent baseline fascinating. My problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.","The sentiment of the review is critical but constructive. The reviewer acknowledges the fascinating observation made in the paper but points out several flaws in the execution, which suggests a mixed sentiment leaning towards negative due to the criticism. However, the reviewer's acknowledgment of the paper's core idea's novelty and impact tempers the negativity. The language used is formal and academic, with no personal attacks or unprofessional wording, indicating a high level of politeness despite the critical content of the review.",-30,80
"Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition. The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state. There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work. In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal. The paper is well-communicated and the empirical results compelling and will be of significant interest. Some minor potential improvements: There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed. The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.","The sentiment of the review appears to be quite positive, as the reviewer acknowledges the success of the algorithm in solving RL in large state spaces, its performance in novel 3D environments, and its victory in the Visual Doom AI competition. The reviewer also appreciates the empirical comparison to prior work and the well-communicated paper. However, the reviewer does suggest some minor improvements, which indicates that while the sentiment is positive, it is not without constructive criticism. The language used is polite and professional, with the reviewer offering suggestions for improvement in a constructive manner without any harsh or rude language.",80,90
"Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well","The sentiment expressed in the review is positive, as the reviewer describes the paper as 'Interesting' and acknowledges that it 'definitely provides value to the community.' There is no negative language or criticism in the provided text. The language used is polite and constructive, with no harsh or rude words. The reviewer's tone is respectful and acknowledges the contribution of the paper.",70,90
"This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.","The reviewer expresses a positive sentiment towards the paper, noting the approach as 'plausible' and 'very clearly explained.' They also commend the reuse of a simple idea in a broader context and the clear improvement over predecessors, which indicates a positive view of the work. The language used is formal, constructive, and free of any negative or rude expressions, which is typical for academic discourse and suggests a high level of politeness.",80,90
"The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments: 1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art. 2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection? 3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work. 4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size. 5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector? Overall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.","The sentiment of the review appears to be generally positive, as the reviewer describes the paper as 'a great paper' and acknowledges the novelty and potential impact of the work. However, the reviewer also points out several areas that require clarification or additional work, indicating a balanced view that is not overly enthusiastic. The language used is polite and constructive, with the reviewer providing specific comments and questions rather than blunt criticism. The use of phrases like 'I still have several comments' and 'Can the author also provide' are indicative of a respectful tone aimed at collaborative improvement.",70,90
"This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing. The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising. The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question “how useful are all these graph operations” remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR*17 (see match type in *LEARNING END-TO-END GOAL-ORIENTED DIALOG* by Bordes et al) The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime? Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu. In the references, the author could mention the pioneering work of Lee Giles on representing graphs with RNNs. Revision: I have improved my rating for the following reasons: - Pointers to an highly readable and well structured Theano source is provided. - The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper. - Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives. - The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance","The reviewer's sentiment appears to be cautiously optimistic and constructive. They acknowledge the paper's originality and potential impact, despite noting the complexity and preliminary results. The reviewer also appreciates the improvements made over the review process. The language used is formal and academic, with constructive criticism and suggestions for improvement, indicating a polite tone. There are no harsh or rude comments, and the reviewer's tone remains respectful throughout.",60,80
